{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fb8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U scikit-learn==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a056840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U acceleratehttp://is2.isa.ru:8096/notebooks/notebooks/storage/test_top2vec.ipynb#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79892e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bf013-5bf9-411e-b683-fba0510ac0ca",
   "metadata": {},
   "source": [
    "## classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e307cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1c6df",
   "metadata": {},
   "source": [
    "# Filter out all the \"well-written\" stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"segm_model/my_model\", num_labels=4)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "#    return accuracy.compute(predictions=predictions, references=labels)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors = 'pt')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segm_model/my_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    #no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    #train_dataset=tokenized_ds[\"train\"],\n",
    "    #eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "def remove_trash(txt_):\n",
    "    txts = []\n",
    "    texts = sent_tokenize(txt_)\n",
    "    for txt in texts:\n",
    "        tokens = tokenizer(txt, truncation=True)\n",
    "        predictions = trainer.predict([tokens])\n",
    "        lbl = np.argmax(predictions.predictions, axis=-1)\n",
    "        if lbl != 1:\n",
    "            txts.append(txt)\n",
    "    if len(txts) == 0:\n",
    "        return txt_\n",
    "    else:\n",
    "        return \" \".join(txts) \n",
    "\n",
    "def preprocess_reviews(fname): \n",
    "    df = pd.read_excel(fname)\n",
    "    dat = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        dat.append([i,row['rate'], remove_trash(row[\"review\"])])\n",
    "\n",
    "    return pd.DataFrame(dat,columns=(\"id\",\"score\",\"review\"))\n",
    "\n",
    "df = preprocess_reviews(\"tp_2017conference.xlsx\")\n",
    "\n",
    "del model\n",
    "\n",
    "df.to_csv(\"preprocessed_rewies.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45577599",
   "metadata": {},
   "source": [
    "# Do classifiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"preprocessed_rewies.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a655a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([str(e) for e in examples[\"text\"]], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "is_new_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"is_new_modelxlm-roberta-large/checkpoint-50\", num_labels=2).to(device)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest',\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"is_new_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    #eval_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    #no_cuda = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=is_new_model,\n",
    "   args=training_args,\n",
    "   #train_dataset=tokenized_ds[\"train\"],\n",
    "   #eval_dataset=tokenized_ds[\"test\"],    \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "for dataset_type in [\"pure\",\"question\",\"is_new\"]:\n",
    "\n",
    "    high_score_lens = []\n",
    "    med_score_lens = []\n",
    "    low_score_lens = []\n",
    "\n",
    "\n",
    "    #df = pd.read_excel(\"tp_2017conference.xlsx\")\n",
    "    dat = []\n",
    "    for i, row in df.iterrows():\n",
    "        score = int(row['score'][row['score'].rfind(\"#\")+1:row[\"score\"].rfind(\":\")])\n",
    "        qtags = row[\"review\"].count(\"?\")\n",
    "        text = row[\"review\"]\n",
    "        if dataset_type == \"is_new\":\n",
    "            tokens = tokenizer(text, truncation=True)\n",
    "            predictions = trainer.predict([tokens])\n",
    "            if np.argmax(predictions.predictions, axis=-1).sum() == 0:     \n",
    "                text = \"IS_NEW\" + \" \" + text                                  \n",
    "        if score > 6:\n",
    "            score = \"high\"\n",
    "            high_score_lens.append(row[\"review\"].count(\"?\"))\n",
    "        else:\n",
    "            if score > 4:\n",
    "                score = \"medium\"\n",
    "            else:\n",
    "                score = \"low\"\n",
    "                med_score_lens.append(row[\"review\"].count(\"?\"))\n",
    "        if dataset_type == \"is_new\" or  dataset_type == \"question\":\n",
    "            if qtags > 1:\n",
    "                text = \"MANY_QUESTIONS\" + \" \" + text    \n",
    "        dat.append([text,score])\n",
    "\n",
    "    dataset = pd.DataFrame(dat,columns=[\"text\",\"label\"])\n",
    "    dataset.to_csv(\"cls_dataset.csv\",sep=\";\")\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    if dataset_type == \"is_new\":\n",
    "        del is_new_model\n",
    "        del trainer\n",
    "\n",
    "    for model_name in ['xlm-roberta-large','xlm-roberta-base','roberta-base','roberta-large','distilbert-base-uncased','distilroberta-base','bert-base-uncased','bert-large-uncased']:\n",
    "        dataset = pd.read_csv(\"cls_dataset.csv\",sep=\";\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer([str(e) for e in examples[\"text\"]], truncation=True, max_length=512)\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(dataset['label'])\n",
    "\n",
    "        cls_model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=len(encoder.classes_))\n",
    "\n",
    "        dtrain, dtest = train_test_split(dataset, test_size=0.2, train_size=0.8, shuffle = True)  \n",
    "\n",
    "        dtrain = Dataset.from_pandas(dtrain)\n",
    "        dtest = Dataset.from_pandas(dtest)\n",
    "\n",
    "        dataset = DatasetDict({\"train\": dtrain,\"test\":dtest})\n",
    "\n",
    "        def map_labels(example):\n",
    "            return {\n",
    "                'label': encoder.transform([example['label']])[0]\n",
    "            }\n",
    "\n",
    "        dataset = dataset.map(map_labels, num_proc=16)\n",
    "\n",
    "        tokenized_ds = dataset.map(preprocess_function, batched=True).remove_columns(\"text\").with_format(\"torch\")\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "                tokenizer=tokenizer,\n",
    "                padding='longest',\n",
    "                max_length=512,\n",
    "                pad_to_multiple_of=8,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, -1)\n",
    "            return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"review_clf_model2\" + dataset_type + model_name,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=20,\n",
    "            weight_decay=0.01, \n",
    "            warmup_ratio=0.15,\n",
    "            max_grad_norm=1.0,    \n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False,\n",
    "            #no_cuda=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "           model=cls_model,\n",
    "           args=training_args,\n",
    "           train_dataset=tokenized_ds[\"train\"],\n",
    "           eval_dataset=tokenized_ds[\"test\"],    \n",
    "           tokenizer=tokenizer,\n",
    "           data_collator=data_collator,\n",
    "           compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "        preds = np.argmax(predictions.predictions, axis=-1)\n",
    "        prec = evaluate.load(\"precision\")\n",
    "        rec = evaluate.load(\"recall\")\n",
    "        f1 = evaluate.load(\"f1\")\n",
    "        p = prec.compute(predictions=preds, references=predictions.label_ids,average='macro')['precision']\n",
    "        r = rec.compute(predictions=preds, references=predictions.label_ids,average='macro')['recall']\n",
    "        f1 = f1.compute(predictions=preds, references=predictions.label_ids,average='macro')['f1']\n",
    "\n",
    "        print(model_name,p,r,f1)\n",
    "        train_data.append([model_name,p,r,f1])\n",
    "\n",
    "    pd.DataFrame(train_data, columns=['Model','P','R','F1']).to_csv(\"review_classifier2\" + dataset_type + \".csv\",sep=\";\")     \n",
    "    # del trainer\n",
    "    # del is_new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff2619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
