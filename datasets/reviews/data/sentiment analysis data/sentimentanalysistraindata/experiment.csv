text	label
one could argue that experiments could be conducted on different environments or that the novelty is limited, but i feel that 'correct' (nononsense, experimentally sound on doom, appendix providing details for reproducibility) and 'milestone' (vizdoom winner) papers should get published.	0
the two experiments provided show good results but both of them are toy problems.	0
in practice, this result does not guarantee that the resulting network will not over or underfit the training data, but some initial experiments show that this does not seem to be the case.	0
in general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.	0
the authors have done a fair set of experiments but i would really like to see results of b1, b2 and the answer to question in b3.	0
i have currently set my rating to a weak reject, but i am happy to raise my ratings to – “good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.	0
the experiments in this paper are interesting enough for publication, but the paper could have been more thorough.	0
i'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well.	0
because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g. linear relationship between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.	0
the main concern of the paper is that experiments are not sufficient.	0
the main cons of the paper is that the experiments are not sufficient.	2
the lack of experiments on this point is the main concern from myself.	0
this is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.	0
i will list these concerns in the following (in arbitrary order)  the paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach.	0
i understand that not all of these can be evaluated thoroughly in a conference paper but i feel like some more experiments or at least some discussion would have helped here.	0
the current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example longrange syntax constraints like brackets)	0
the approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches.	2
3) the answer to the prereview questions made the architecture details of the paper much more clear, but i still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code.	0
on the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results.	0
these experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.	0
4) conclusion overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors.	0
the appendix provides some more context, i think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.	0
the experiments could be made stronger by mixing continuous and discrete problems but are convincing.	0
this is an exploratory paper, in that the ultimate goal is to use this method in a bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions.	0
a simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution.	0
those weaknesses aside (and i recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly.	0
this work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared.	0
the paper also proposes two novel rnn architectures (ugrnn and rnn); experiments suggest that the ugrnn has similar perparameter capacity as the ungated rnn but is much easier to train, and that deep (8layer) rnn models are easier to train than existing architectures.	0
i found the experiments interesting, but i don’t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that i think about rnns, or the way that i will use them in my own future work.	0
2. the proposed method produces visually appealing results on several datasets 3. the authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the svhn>mnist task 4. the paper is wellwritten and easy to read cons: 1. the novelty of the method is relatively minor (i consider fconstancy term as the main contribution) 2. it feels like the proposed approach would break for more dissimilar domains.	0
simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.	0
 it might have been good to also compare with methods explicitly trying to explore better with valuefunctions (e.g. prioritized experience replay, schaul et al 2015)  at the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.	0
i know that evaluating gans is in itself not an easy task but i wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance.	0
overall, the deepdsl framework seems to have real value in its use of scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks.	0
the only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.	0
the experiments also compare to some bayesian optimization methods, but not to the most relevant very closely related multitask bayesian optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: 'multitask bayesian optimization' by swersky, snoek, and adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several followup papers showing even larger speedups.	0
for the experiments in our paper, we excluded cvst due to the aforementioned theoretical differences and because cvst is not an anytime algorithm, but as we perform more experiments, we will update the draft to reflect this comparison.'	0
effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources.	0
an overview of the review: pros:  the paper is well written  extensive analysis of the model on various language pairs  convincing experimental results.	2
the experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program.	0
the experiments are interesting but i'm still not completely convinced by the regression results in figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.	0
the correct definition of 'generative temperature' is not published yet, but used extensively in experiments and can be privately communicated upon request.	0
much broader experiments, including a variety of models (rnns, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model.	0
 the links to dueling networks (wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as i can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the 'variant of asynchronous deep qlearning' being used is essentially such a dueling network, but it is not clearly stated).	0
the main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities.	0
first, lstms are very simple and quite standard nowadays  there is a lack of comparison to any basic stacked lstm architecture in all the experiments.	0
this work uses mechanical turk to do real experiments, which again is important to assess if these methods, particularly fp, work on real language.” point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper.	0
the authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.	0
 it should be noted that residual/highway architectures do have a type of anytime property, as shown by lesioning experiments in srivastava et al and viet et al.  the architecture specific droppath regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.	0
this statement is too strong, and is actually disproved in the experiments — repeating an action is helpful in many tasks, but not in all tasks.	0
i was skeptical reading through this, especially given the fact that you only use k=5 in your experiments, but the results seem quite good.	0
as the authors comment in the discussion section, “in our experiments, entropysgd results in a comparable generalization error as sgd, but always has a lower crossentropy loss.” it's not clear to me how to reconcile those two claims.	0
intuitively, i agree with the authors that the proposed regularization is an interesting direction, but i don’t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where srsc is used as a feature extractor are very modest.	0
i might be mistaken but i don't see you reporting anything on the actual number of loops necessary in the reported experiments.	0
for part [2], the experiments cover most variables in adversary training, yet lack technical depth.	0
again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations).	0
in addition, the mdc blocks are used in the generator of the model but their efficacy is measured via discriminative experiments.	0
2) i understand the desire to combine the extracted reward function with a simple rl method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory.	0
overall, the paper is very interesting, but it seems to me like the experiments do not support the claims, nor the usefulness, enough.	0
motivation for why one should want to do this is that it enables composing program sketches with other differentiable models like standard neural networks, but the experiments focus on sorting and addition tasks with relatively small degrees of freedom for how to fill in the holes.	0
on the other side, i have some concerns are about the experimental part, which i consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets.	0
each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments.	0
the experiments are also not very convincing: layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better.	0
my overall impression is that there isn't very much work here (e.g., much of the text is similar to jaini, and most of the other experiments are repeated verbatim from jaini), but again i may be missing something (this manuscript does mostly jaini).	0
my main concern are experiments  it would be nice to see a comparison to some of the neural networks mentioned in related work.	0
the exposition is ok, and i think the approach is sensible, but the main issue with this paper is that it is lacking experiments on nonsynthetic datasets.	0
in summary the paper lacks experiments and results are inconclusive and i do not believe the proposed method would be quite useful and hence not a conference level publication.	0
you say reinforce has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).	0
further experiments and analysis of the main contribution would strengthen the paper, but i would recommend resubmission to a more suitable venue.	0
the downside is that experiments are limited to a fairly simple and notwidelyknown domain of honeybee marker classification.	0
pro: ' well written ' exhaustive set of experiments ' learning algorithms with decimal representation ' available source code cons: ' no coherent hypothesis/premise advanced ' two or three bold statements without explanation or references ' some unclarity in experimental details ' limited novelty and originality factor typos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).	0
significance:  the findings and shown experiments are interesting, but i not sure if the scale and amount of contribution is significant enough for the main conference track.	0
the paper is well written and the use of traces in deep rl is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.	0
my major concern about this paper is the experiments.	0
i agree that the general approach here generalizes to nonlinear projections easily, but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of cca and other multiview learning algorithms limits the significance of the current paper.	0
the experiments in 3.1 are interesting, but you need to be clearer about the relationship of your resception method to the stateoftheart.	0
in general, this paper shows some interesting results on the fc7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments.	0
the experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer’s knowledge).	0
as with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the subsubfield of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible.	0
this paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating arabic digits and english letters.	0
this paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task a in learning a new task b, which not only improves performance on the new task b, but also tries to avoid degradation in performance on a. the general idea is based on encouraging a model trained on a, while training on the new task b, to match fake targets produced by the model itself but when it is trained only on the original task a. experiments show that this method can help in improving the result on task b, and is better than other baselines, including standard finetuning.	0
pros  the paper is the first of my knowledge to explicitly measure the bits per parameter that rnns can store  the paper experimentally confirms several intuitive ideas about rnns:  rnns of any architecture can store about one number per hidden unit from the input  different rnn architectures should be compared by their parameter count, not their hidden unit count  with very careful hyperparameter tuning, all rnn architectures perform about the same on text8 language modeling  gated architectures are easier to train than nongated rnns cons  experiments do not reveal anything particularly surprising or unexpected  the ugrnn and rnn architectures do not feel wellmotivated  the utility of the ugrnn and rnn architectures is not wellestablished	2
regardless, the experiments are fairly sparse and ablation studies and more discussion lacking.	0
i find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2d and 3d model variants.	0
preliminary rating: i think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends.	0
the paper, model and experiments are decent but i have some concerns: 1. the proposed model is not exceptionally novel from a technical perspective.	0
one issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications.	0
overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern. '	0
while the dataset does give a good motivation to the problem setting, the paper falls a bit short for iclr due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of chalupka et al., arxiv preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.	0
this is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent.	0
a theorem on identifiability of causaldiscriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.	0
third, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but i don't think we really have any meaningful quantitative evidence that causality has been learned.	0
it is an interesting idea to go after saddle points in the optimization with an sr1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as adam, other hessian free methods (martens 2012), pearlmutter fast exact multiplication by the hessian.	0
overall:  the proposed architecture seems novel and potentially interesting, but experiments are only proofofconcept and clarity can be improved.	0
the results would be more convincing if they would be accompanied by confidence intervals but i understand that some of the experiments must have taken very long to run.	0
the main shortcomings of the paper are in the experiments: 1) the full model (ratingtext) is only applied to one and relatively small dataset.	0
this paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal largescale behavior relevant to practice.	0
while there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.	0
regarding the evaluation, experiments on cifar are interesting, but only as proof of concept.	0
there are comparisons provided for face completion experiments  but even there comparisons with descriptor or generator network trained separately or with other deep autoencoders are missing.	0
this can not only be considered as a rather simple connection/extension, but also the toyish experiments are not enough to convince readers on the significance of the proposed model.	0
the overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments.	0
 update: given that the authors made the code available (i do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments.	0
pros: ' interesting topic ' blackbox setup is most relevant ' multiple experiments ' shows that with flipping only 1~5% of pixels, adversarial images can be created cons: ' too long, yet key details are not well addressed ' some of the experiments are of little interest ' main experiments lack key measures or additional baselines ' limited technical novelty quality: the method description and experimental setup leave to be desired.	2
in conclusion, i think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at iclr.	0
overall, i am in the borderline mode, and i will gladly raise the score if the authors address my concerns regarding the experiments.	0
because they are in this setting, there is a lot lacking from the experiments.	0
the experiments are interesting but could be extended.	0
i should probably also list (as the other other reviewers) flaws and issues with the experiments  but given the detailed comments by the other reviewers there seems to be little additional value in doing so so in essence the paper simply does not deliver at this point on its promise (as far as i am concerned) and in that sense i suggest a very clear reject for this conference.	0
i found this paper to be wellwritten and to have very thorough experiments/analysis, but i have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors 'suggest' that their approach is sufficient for shorter sequences, but don't compare against the approach of chorowski et al. 2015 or jailty el at 2016).	0
the experiments lack of strong comparisons with other graph models (e.g iterative classification, 'learning from labeled and unlabeled data on a directed graph', ...).	0
in summary, here are the pros and cons of this paper: cons  the approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task  only one experimental setup that evaluates learned policy with multitask state representation  no experiments on more realistic scenarios, such 3d environments or highdimensional control problems pros:  this approach enables using the same network for multiple tasks, which is often not true for transfer and multitask learning approaches  novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful  experimentally validated on two toy tasks.	2
here is what i understand are their several experiments to transfer learning, but i am not 100% sure.	0
pros: introduces an energy function having the leakyrelu as an activation function introduces a novel sampling procedure based on annealing the leakiness parameter similar sampling scheme shown to outperform ais cons: results are somewhat out of date missing experiments on binary datasets (more comparable to prior rbm work) missing pcd baseline cost of projection method	2
 figure 4  the reference for batch normalization should be ioffe and szegedy instead of morimoto et al. overall i think the paper has some really promising ideas and encouraging results but is missing a few exploratory/ablation experiments and some polish.	0
experiments test this out on mnist and svhn comments: this is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well.	0
the combined ideas (1)  (2) do produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on.	0
experiments are made on bandit problems, but also on maze problems and show the interesting properties of such an approach, particularly on the maze problem where the agent seems to learn to first explore the maze, and then to exploit its knowledge to quickly find the goal.	0
demonstrating the stitched network idea on imagenet, comparing with the corresponding vggonly or resnetonly finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.	0
overall i think it is an interesting idea and i would love to see it better developed, thus i am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.	0
the idea of having a piecewise constant prior for latent variables is interesting, but the paper is not wellwritten (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.	0
significance: the work is incremental, the issues in the experiments limit potential impact of this paper.	0
my main concern is that the first set of experiments allows images that are not in image space.	0
given that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.	0
but the current version lacks the theoretical motivations and convincing experiments.	0
the authors take it as a given that “learning sparse features and transformations jointly” is an important goal in itself, but this is never really argued or demonstrated with experiments.	0
weaknesses:  the lstms (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound 'very' under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences).	0
with that said, i am concerned about the experiments and their results.	0
cons  no discussion of prior work on lowprecision recurrent networks  experimental results are not sufficient to validate the method  many experimental details are missing  results of key experiments (image captioning and machine translation) are missing	2
my main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components.	0
in summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient  i wouldn't be able to recreate the algorithm used in experiments  and the paper contains a number of questionable claims.	0
overall evaluation === the experiments presented here are novel, but i am not sure they are very significant or offer clear conclusions.	0
from eq 1, universality means that the centered/scaled halting time fluctuations (which depend on a, epsilon, n, e) can be approximated by a distribution that only depends on a (not on epsilon, n, e) but in the experiments only e varies (figures 2,3,4,5).	0
finally the experiments do not apply to all or nearly all computations but only to very few specific algorithms.	0
the results of the experiments could be improved but still justify the validity of applying distillation for transfer learning.	0
the experiments show that the proposed method works, but they are not entirely convincing.	0
the main merits of the paper is to present extensive experiments on how well the vanilla bloom filter approach can perform, but overall the novelty is fairly limited.	0
there's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.	0
the proposed method outperforms standard optimizers like sgd, adam and rmsprop in experiments conducted on mnist and cifar 10. i have two main concerns.	0
my second concern is with the experiments.	0
overall, i think this is a good paper presenting a sensible idea, but i am not convinced by the experiments that the specific approach is achieving its goal.	0
this seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.	0
the oracle experiments are rather meaningless  they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not.	0
i think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper.	0
my largest concern is with the experiments.	0
the only major concern i have  and i apologize to the authors for not raising this earlier  are the experiments in section 3. in particular, i don't quite get the scenario.	0
it is empirically competitive, but this insensitivity places a strong upper bound on its performance.	0
i think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive.	0
although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.	0
 even though the cca objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine).	0
hence authors claim that steady increase in the l_2 norm of the weights will maintain the this feature but setting for figure 1 is restrictive to empirically support the claim.	0
 please mention the fact that auxiliary tasks are not trained with 'true' qlearning since they are trained offpolicy with more than one step of empirical rewards (as discussed in the openreview comments) minor stuff:  'policy gradient algorithms adjust the policy to maximise the expected reward, l_pi = ...' => that's actually a loss to be minimized  in eq. 1 lambda_c should be within the sum  just below eq. 1 r_t^(c) should be r_tk^(c)  figure 2 does not seem to be referenced in the text, also figure 1(d) should be referenced in 3.3  'the features discovered in this manner is shared' => are shared  the text around eq. 2 refers to the loss l_pc but that term is not defined and is not (explicitly) in eq. 2  please explain what 'clip' means for dueling networks in the legend of figure 3  i would have liked to see more ablated versions on atari, to see in particular if the same patterns of individual contribution as on labyrinth were observed  in the legend of figure 3 the % mentioned are for labyrinth, which is not clear from the text.	0
3) empirically illustrate the performance of the method, and show:  asynchronous fisher block inversions do not adversely affect the performance of the method (cifar10)  kfac is faster than synchronous sgd (with and without bn, and with momentum) (imagenet)  doublyfactored kfac method does not deteriorate the performance of the method (imagenet and resnet)  favorable scaling properties of kfac with minibatch size pros:  paper presents interesting ideas on how to make computationally demanding aspects of kfac tractable.	2
further, i appreciated the detailed analysis of model behaviour in section 3. the main downside to this submission is in its relative weakness on the empirical front.	0
 figure 3 gave a clear visualization for the iterative unrolling view cons  even though, the perspective is interesting few empirical results were shown to support the argument.	2
as a simple modification of existing nlp models, but with good empirical success, simplicity and practicality, it is probably more suitable for an nlpspecific conference.	0
the lack of empirical comparison on large scale dataset, such as imagenet or coco makes this largely a theoretical contribution.	0
it is empirically competitive, but this insensitivity places a strong upper bound on its performance.	0
preliminary rating: i think this is an interesting paper but lacks sufficient empirical evaluation of its many components.	0
the concern i have is regarding the current empirical evaluation.	0
overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.	0
overall the paper is an enjoyable read and the proposed approach is interesting, pros:  address an important problem  nice empirical evaluation showing the benefit of their approach  demonstrate up to 16x speedup relatively to a lstm cons:  somewhat incremental novelty compared to (balduzizi et al., 2016) few specific questions:  is densely layer necessary to obtain good result on the imdb task.	2
i think compared to lots of variance reduction techniques such as nvil and vimco, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases.	0
this paper proposes to use an ssnt model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. the authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.	0
also, authors give a list of some improvements over inception (szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.	0
[update: some of these concerns were addressed in openreview comments] 2. the empirical evaluation does not compensate, in my opinion, for the lack of theory.	0
i think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive.	0
it seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.	0
this is not necessarily bad but makes the empirical evaluation all the more important.	0
while the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.	0
the key idea, which is very sensible, is to use a classbased hierarchical softmax, but where the clusters/hierarchy are distributed such that the resulting matrix multiplications are optimallysized for gpu computation, based on their empirical tests.	0
some sentences like the one given below suggest that the study is too superficial: 'one of the interesting empirical observation is that we often observe is that the incremental improvement of optimization methods decreases rapidly even in nonconvex problems.'	0
this validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toyish settings with perfect information and a fixed number of questions may be too simple.	0
the idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak.	0
as a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant.	0
i think it is therefore not proper to call the proposed model a novel model with a new name neural graph machine, but rather making it clear in the paper that this is an empirical study of the model proposed by weston et al. 2012 to different problems would be more acceptable.	0
overall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.	0
pros provides some preliminary results for boosting of res nets cons not sufficiently novel: an incremental approach empirical analysis is not satisfactory	2
3. the empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of wellestablished baseline for some new applications (e.g., the recipe generation task).	0
weaknesses: the objective function shown in the middle of pg 3 is highly empirical, not directly linked to how nonparallel data helps to improve the final prediction results.	0
an algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension n, the probability distribution/random ensemble.	0
it is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.	0
pros :  provides an investigation of regularization on colabel similarity during training cons: the empirical results do not support the intuitive claims regarding proposed procedure iterative version can be unstable in practice	0
one could argue that experiments could be conducted on different environments or that the novelty is limited, but i feel that 'correct' (nononsense, experimentally sound on doom, appendix providing details for reproducibility) and 'milestone' (vizdoom winner) papers should get published.	0
pros:  simple and effective method to improve convergence  good evaluation on well known database cons:  connection of introduction and topic of the paper is a bit unclear  fig 2, 4 and 5 are hard to read.	2
the experimental suite is ok but i was disappointed that it is 100% synthetic.	0
the experimental data is simple, but the model is very interesting and relatively novel.	0
the experimental section is convincing, but i would appreciate a precision (and small discussion) of this sentence 'to extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled mdps' with numbers for the different scenarios (or the replacement of superset by 'union' if this is the case).	0
the authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.	0
i found the experiments interesting, but i don’t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that i think about rnns, or the way that i will use them in my own future work.	0
the experimental validation is interesting and well carried out, but remains of limited scope.	0
so, the paper only has a marginal interest for the rl community @pros: ' original problem with well design experiments ' simple adaptation of the actorcritic method to the problem of learning sub policies @cons: ' very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical rl or learning from instructions ' no strong underlying applications that could help to 'reinforce' the interest of the approach	2
i suppose this is up to the organisers and the program committee, but i thought it important to mention this, because i don't think this paper merits acceptance based on its experimental results alone.	0
 define u(x) clearly before defining u'(x) there are several concerns with the experimental evaluations.	0
in general, i like this work, but the vague details around the lstm training raise serious red flags about their experimental results, at least the comparison vs. lstms, and i have concerns about how well it meets iclr's cfp.	0
this is why this reviewer considers this paper borderline  it's a first step, but a very basic one and without sufficiently large experimental underpinning.	0
one small downside of the experimental method (or maybe just the results shown) is that by picking top5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings.	0
i sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.	0
significance: the experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent.	0
theorem 2: 'a nash equilibrium ... exists' ' sec 3: should be 'several papers were presented' overall, i have some concerns with the related work and experimental evaluation sections, but i feel the model is novel enough and is welljustified by the optimality proofs and the quality of the generated samples.	0
in summary, i think in it current form the paper lacks the evaluation and experimental results for an iclr publication.	0
originality:  this is mainly an experimental paper, but the question it asks is interesting and worth investigation.	0
my biggest concern is with the experimental evaluation.	0
weaknesses:  the authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice not to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point.	0
concerning the experimental section, authors focus on text classification methods.	0
on the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by zhu et al. [1] is not discussed in enough detail.	0
it does lead to reduction in number of parameters but it is not clear how much that helps experimentally.	0
4. the paper seems to be general at the beginning, but the claim of the benefit of the hadamard product is only shown experimentally on the vqa dataset.	0
it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.	0
this would have been fine if it was supported by a strong and convincing experimental section, but unfortunately, the experimental section is a bit weak: the tasks studied are relatively simple and the baselines are not very strong.	0
theoretical results are satisfactory but i particularly like the experimental setup where their methods are tested on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.	0
on the other side, i have some concerns are about the experimental part, which i consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets.	0
i recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs nonamortized approaches.	0
admittedly, it would not succeed except on the simplest tasks, but i think some of their experimental tasks are simple enough for 'nongenerating code ' nns to succeed on.	0
the experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.	0
concerning the experimental results, reported performance are very good; given the approach is very close to previous work, it is hard to know if good performance come from better neural net architectures or something else.	0
experimental results are very good, but it is hard to know where comes the difference in performance with previous (very related) work.	0
although the proposed method may be potentially useful in practice (if refined further), i find the method lacks novelty, and the experimental results are not significant enough.	0
in summary, the ideas in this paper are potentially interesting but this paper should not be accepted in its current form due to lack of experimental results and comparisons.	0
the general topic of unsupervised learning is important, and the proposed approach makes some sense, but experimental evaluation is very weak and does not allow to judge if the proposed method is competitive with existing alternatives.	0
the experimental results are based on using a fixed learning rate for the different regularization strengths.	2
also, the indices (i_t) are undefined in algorithm 1. overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.	0
third, it is next to impossible to interpret the experimental results, in particular figures 2, 3, 4. the authors claim that these figures show that 'learning does not stop', but such behavior can also be attributed to the typical chaotic dynamics of gans.	0
experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the bgm.	0
[r1] https://papers.nips.cc/paper/2275selfsupervisedboosting.pdf pros: novel and intriguing idea strong theoretical guarantees cons: resulting boosted model is unnormalized discriminator based boosting is expensive, due to sampling via mcmc weak experimental section	2
although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness: first, a very simple multitask learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.	0
the use of reconstruction loss to improve results in the datasparse setting is interesting, but the experimental results are inconclusive.	0
i usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models.	2
given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to kingma et al. 2014. by the way, i was wondering if the experimental setup is the same as in kingma et al. 2014 for the results of fig. 5 (bottom)  the authors mention that they use cnns for feature extraction but from the paper it's not clear if kingma et al. do the same.	0
9th), there are two strong weaknesses remaining: analysis of related work, and experimental evidence.	0
since it is an experimental paper, my main concern is about its clarity.	0
summary: ——— i think the paper presents a compelling technique for hierarchical reasoning in mrfs but the experimental results are not yet convincing.	0
there are confusing points, some of the claims are lack of evidence and the experimental results are incomplete.	0
i felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections.	0
the above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results.	0
 edit after rebuttal: thanks to the authors for addressing the experimental validation concerns.	0
i also have some concerns regarding the experimental results.	0
it rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.	0
the authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feedforward networks and iht (a surprising result), and progress naturally to a detailed experimental section.	0
i had two main comments that concern the experimental design, and the relationship to previous work: 1. in the context of deployment on mobile devices, computational costs in terms of both system memory as well as processing are important consideration.	0
third, there is no essential algorithmic, architectural, or mathematical insight, which i expect out of all but the most heavily experimental papers.	0
it appears applicable to the problem of online advertising, but is not mentioned in the corresponding experimental section.	0
the experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in gnvdm.	0
there is a long experimental section, but i am not sure what the conclusion is.	0
the general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications.	0
pros:  nice taxonomy of pruning levels  comparison to the recent weightsum pruning method cons:  experimental evaluation does not touch upon recent models (resnets) and large scale datasets (imagenet)  paper is somewhat hard to follow  feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers finegrained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning.	2
in summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.	0
review summary: albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the iclr audience.	0
pros:  the binary representation outperforms the semantic hashing method from salakhutdinov & hinton '09  the experimental approach sound: they compare on the same experimental setup as salakhutdinov & hinton '09, but since in the meantime document representations improved (le & mikolov'14), they also combine this new representation with an rbm to show the benefit of their binary pvdbow/pvdm cons:  the insertion of the sigmoid to produce binary codes (from lin & al. '15) in the training process is incremental  the explanation is too abstract and difficult to follow for a nonexpert (see details below)  a comparison with efficient indexing methods used in image retrieval is missing.	2
i still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pretrained imagenet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch.	0
energy efficiency and training speed one of the main claims of the paper is that sparse gradients can be exploited in hardware to reduce the training speed and improve the energy efficiency of recurrent network training, but these benefits are neither quantified nor demonstrated experimentally.	0
this is an interesting experimental result, but it is not discussed in the paper.	0
cons  no discussion of prior work on lowprecision recurrent networks  experimental results are not sufficient to validate the method  many experimental details are missing  results of key experiments (image captioning and machine translation) are missing	2
overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature.	0
these are experimentally evaluated on cifar10 and cifar100, but seem to achieve relatively poor performance on these datasets (table 1), so their merit is unclear to me.	0
of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.	0
details: ——— 1. my main concern is related to the experimental evaluation.	0
 the experimental setup seems reasonable  the differentiable set seems like a useful (albeit simple) modelling tool weaknesses:  the setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer  the models are mostly very simple.	0
pros  the paper is clearly written and easy to follow cons  the paper's two contributions are too minor to merit publication  experimental results should include at least the caltech pedestrian dataset but likely also the kitti pedestrian dataset  recent work from eccv 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed my rating is due primarily to the lack luster contributions.	2
pros:  the paper is clear and easy to follow  the experimental results seem to show some benefit from the proposed approach cons: (1) the paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation (2) no comparison with an ensemble (3) full experiments on imagenet under the 'partial privileged information' setting would be more impactful this paper is promising and i would be willing to accept an improved version.	2
the main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration.	0
the experimental evaluation appears at least partially novel, but for example the ibd detection is very similar to hochreiter (2013) but without any comparison.	0
> experiments: experimental validation is lacking somewhat in my opinion.	0
my main concern with this paper is that in the experimental section the iterative approach tries to improve upon only one type of machine translation.	0
while this is an interesting area of research, i am not convinced by the proposed approach, and experimental evidence is lacking.	0
while i am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, i think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.	0
i do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, i think, should be accepted.	0
overall, since evaluation of image generation is such an hard problem, i think the paper still has sufficient strengths to warrant publication in iclr.	2
pros:  good motivation for the effectiveness of resnets and highway networks  convincing analysis and evaluation cons:  the effect of this finding of the interpretation of batchnormalization is only captured briefly but seems to be significant  explanation of findings in (zeiler & fergus (2014)) using uie viewpoint missing remarks:  missing word in line 223: 'that it 'is' valid'	2
preliminary rating: i think this is an interesting paper but lacks sufficient empirical evaluation of its many components.	0
# pros their evaluation framework is public and is definitely a nice contribution to the community.	0
using reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model.	0
the concern i have is regarding the current empirical evaluation.	0
the concerns with the paper is mainly with evaluation, which in places appears to be weak (see below).	0
an important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair.	0
the 'crippling' method used feels handcrafted and very taskdependent, and the qualitative evaluation of the 'lossyness' of the learned representation is carried out on three datasets (mnist, omniglot and caltech101 silhouettes) which feature blackandwhite images with littletono texture.	0
overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.	0
let's say i want to tune hyperparameters on imagenet and each hyperparameter evaluation takes 1 week, but i have 100 gpus, then random search will give a decent solution (the best of 100 random configurations) after 1 week.	0
overall the paper is an enjoyable read and the proposed approach is interesting, pros:  address an important problem  nice empirical evaluation showing the benefit of their approach  demonstrate up to 16x speedup relatively to a lstm cons:  somewhat incremental novelty compared to (balduzizi et al., 2016) few specific questions:  is densely layer necessary to obtain good result on the imdb task.	2
the paper is mostly well executed but somewhat lacks in evaluation.	0
this paper proposes to use an ssnt model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. the authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.	0
pros:  the idea is novel  the approach is described clearly cons:  the experimental evaluation is not convincing, e.g. no improvement on svhn  number of parameters should be mentioned for all models for fair comparison  the effect of droppath seems to vanish with data augmentation	2
 the atari evaluation does show convincing improvements over a3c on games requiring extended exploration (e.g. freeway and seaquest), but it would be nice to see a full evaluation on 57 games.	0
[update: some of these concerns were addressed in openreview comments] 2. the empirical evaluation does not compensate, in my opinion, for the lack of theory.	0
theorem 2: 'a nash equilibrium ... exists' ' sec 3: should be 'several papers were presented' overall, i have some concerns with the related work and experimental evaluation sections, but i feel the model is novel enough and is welljustified by the optimality proofs and the quality of the generated samples.	0
this is expected because it is an extensive evaluation study, but still.	0
in summary, i think in it current form the paper lacks the evaluation and experimental results for an iclr publication.	0
my biggest concern is with the experimental evaluation.	0
on the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by zhu et al. [1] is not discussed in enough detail.	0
qualitative results are nice, but with the current surge of interest in generative models it gets very difficult to rely on qualitative evaluations: many methods produce visually similar results, and unless there is an obvious large jump in the quality of the produced images, it is unclear how to compare these.	0
pros  simple modification to feedforward neural style transfer with several improvements over prior work  strong qualitative results  wellwritten  opensource code has already been released cons  slightly incremental  somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic	2
it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.	0
my main concern is the evaluation 'score'.	0
this is not necessarily bad but makes the empirical evaluation all the more important.	0
 evaluation of generative models is difficult, but the authors could have done better.	0
in the paper, the authors claim that “dynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted”, and i am afraid to tell that this assumption is very misleading.	0
speed of evaluation seems to be one of the main focal points of the paper, but it’s not a major selling point to the iclr audience because it seems about ¼ as fast as e.g. cudnn on standard (e.g. aws nodes) nvidia hardware.	0
cons:  the cost of running the evaluation could be large in the multiclass setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.	0
more importantly, the evaluation methods in this paper are extremely lacking.	0
the general topic of unsupervised learning is important, and the proposed approach makes some sense, but experimental evaluation is very weak and does not allow to judge if the proposed method is competitive with existing alternatives.	0
these are reasonable choices for an initial evaluation, but are both toy problems and don't shed much light on the practical aspects of the proposed approaches.	0
also, the indices (i_t) are undefined in algorithm 1. overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.	0
evaluation is a big concern.	0
although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness: first, a very simple multitask learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.	0
looking at the intermediate values may shed some light on why the usuallyworking models fail on e.g. the pathological cases identified in table 3. the preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.	0
i am concerned that the evaluation may be insufficient to assess the effectiveness of this method.	0
i usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models.	2
the reported log likelihood of cvae using 10k samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10k kmeans mixture model to the data (eg as done in 'a note on the evaluation of generative models').	0
paper weaknesses:  human evaluation is weak.	0
weaknesses: 1. the human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions).	0
the tests are indicative, but the results are very similar to the tested approaches, and it is not clear what the best evaluation metric ought to be.	0
overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely: 1) the evaluation is unusual.	0
concerning the evaluation, the authors introduce two criteria.	0
please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.	0
regarding the evaluation, experiments on cifar are interesting, but only as proof of concept.	0
more comments below: [major comments]  my main concern is that that evaluation results are not strong.	0
 the paper provides good benchmarks for intrinsic evaluation but the message could be conveyed more strongly if we see improvement in a downstream task.	0
i also have a concern regarding the evaluation.	0
given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth.	0
main concerns: 1) the evaluation does not tell me much for realistic scenarios, that mostly involve finetuning networks, as ilsvrc is just a starting point in most cases.	0
(in other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network finetuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an imagenet evaluation.)	0
overall i think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of imagenet evaluation calls into question its realworld application.	0
strengths: using variable precision for each layer of the network is useful  but was previously reported in judd (2015) good evaluation including synthesis  but not place and route  of the units.	2
also this evaluation is identical to that in judd (2016b) weaknesses: the idea of combining bitserial arithmetic with the dadn architecture is a small one.	0
in the quantitative evaluation of nvdm, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>.	0
the general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications.	0
pros:  nice taxonomy of pruning levels  comparison to the recent weightsum pruning method cons:  experimental evaluation does not touch upon recent models (resnets) and large scale datasets (imagenet)  paper is somewhat hard to follow  feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers finegrained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning.	2
in summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.	0
i still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pretrained imagenet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch.	0
the convnetbased model was faster at evaluation, but it is not very clear what is the main speedup factor.	0
details: ——— 1. my main concern is related to the experimental evaluation.	0
pros  the paper is clearly written and easy to follow cons  the paper's two contributions are too minor to merit publication  experimental results should include at least the caltech pedestrian dataset but likely also the kitti pedestrian dataset  recent work from eccv 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed my rating is due primarily to the lack luster contributions.	2
the method is evaluated on two standard and relatively largescale vision datasets: imagenet and pascal voc 2012. cons: the evaluation is lacking.	0
the main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration.	0
in addition, the evaluation is rather lacking.	0
the experimental evaluation appears at least partially novel, but for example the ibd detection is very similar to hochreiter (2013) but without any comparison.	0
in practice, this result does not guarantee that the resulting network will not over or underfit the training data, but some initial experiments show that this does not seem to be the case.	0
it is argued that this is especially useful for data modalities where the assumption of locality breaks down, as in e.g. spectrograms, where correlations between harmonics are often relevant to the task at hand, but they are not local in frequency space.	0
improved performance is demonstrated on two tasks that are fairly nonstandard, but i think that is fine given that the proposed approach probably isn't useful for the vast majority of popular benchmark datasets (e.g. mnist, cifar10), where the locality assumption holds and a square filter shape is probably close to optimal anyway.	0
 a few things mentioned in the paper that were unclear to me ('syllables', 'exclude data that represent obvious noise', choice of 'max nonzero elements' parameter) have already been adequately addressed by the authors in their response to my questions, but it would be good to include these answers in the manuscript as well.	0
i'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well.	0
i don't think 'remarkably' is justified in the statement 'remarkably, we found that opnets similarity judgement matches a set of data on human similarity judgement, significantly better than alexnet' i'm not an expert on human vision, but from browsing online and from what i've learned before it seems that 'object persistence' frequently relates to the concept of occlusion.	0
the argument could be improved by using morerealistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.	0
this does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.	0
2) standard 1st person imitation learning using agent a data, then apply the policy on agent b. here, i expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.	0
3) reinforcement learning using agent a data, and apply the policy on agent a. i expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents).	0
i understand this is how the expert data is collected for the demonstrator, but i don’t see the performance results from just using this procedure on the learner (to compare to fig.3 results).	0
the lack of empirical comparison on large scale dataset, such as imagenet or coco makes this largely a theoretical contribution.	0
the experimental data is simple, but the model is very interesting and relatively novel.	0
may main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why ngram smoothing based data noising would be effective for recurrent neural network based language modeling.	0
seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in zaremba's or gal's!	0
it is not clear to me that an architecture’s ability to model random data should be beneficial in modeling realworld data; indeed, the experiments in section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in section 3 show that these same architectures do not significantly vary in their capacity to model realworld data.	0
this paper addresses one of the major shortcomings of generative adversarial networks  their lack of mechanism for evaluating heldout data.	0
it is shown that the subsequence length used for truncated bptt affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales.	0
the 'crippling' method used feels handcrafted and very taskdependent, and the qualitative evaluation of the 'lossyness' of the learned representation is carried out on three datasets (mnist, omniglot and caltech101 silhouettes) which feature blackandwhite images with littletono texture.	0
figures 1a and 2a do show that reconstructions discard lowlevel information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets.	0
the degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a highlevel representation certainly warrants some thought.	0
the only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.	0
it has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough.	0
some might also question whether iclr is the appropriate venue to introduce a new dataset, but personally i think it's a great idea to submit it here, seeing as it will reach the right people.	0
overall, i think the ideas presented in this paper show good potential, but i would like to see an extended analysis in the line of figures 3 and 4 for more datasets before i think it is ready for publication.	0
one possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some welldefined factor to measure the quantitative performance.	0
the proposed approach appears to beat the lstm across all tasks (including sleep stage classification), possibly due to the lack of training data.	0
interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for hmm based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g., i. mcgraw, i. badr, and j. glass, 'learning lexicons form speech using a pronunciation mixture model,' in ieee transactions on audio, speech, and language processing, 2013 l. lu, a. ghoshal, s. renals, 'acoustic datadriven pronunciation lexicon for large vocabulary speech recognition', in proc.	2
the authors replied  but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data  but these concerns are marginal.	0
this work uses mechanical turk to do real experiments, which again is important to assess if these methods, particularly fp, work on real language.” point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper.	0
my primary concern remains about novelty: the extra data introduced here is welcome enough, but probably belongs in a 'acl short paper or a technical report.	0
pros: provides an interesting architecture compared to resnet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis cons:  number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters the claims are intuitive but not supported well with empirical evidence path regularization does not yield improvement when the data augmentation is used  the empirical results do not show whether the method is promising for “ultradeep” networks	2
in terms of discriminative power (the ability to tell two distributions apart), mmd and cmd should be equivalent, but in practice i can understand that cmd may be better as mmd tries to match the raw moments which may over penalize data that are not zero centered.	0
for example, it would be interesting to see whether the proposed method is better than training with only less but high quality data.	0
my main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations).	0
wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context.	0
weaknesses:  the authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice not to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point.	0
in particular, i have three main concerns with respect to the results presented in this paper: (1) in the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes.	0
the application of adversarial training to text data is a simple but not trivial extension.	0
strengths  this paper explores an interesting way to evaluate representations in terms of their generalizability to outofdomain data, as opposed to more standard methods which use train and test data drawn from the same distribution.	2
also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. dai & le 2015. i would also suggest to report test results on the dataset from lerrer et al 2016 (i understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.	0
as a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.	0
each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments.	0
the priors as such are ok, but some of them seem rather heuristic and should, in my opinion, be learned from data and it should be discussed how the performance changes if you remove them.	0
i was also unfamiliar with the datasets, so i had no intuitive understanding of the algorithms performance, even when viewed as a blackbox.	0
some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “moving mnist” data.	0
you claim several times that a limitation of reinforce is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one d_t, to the whole multiepoch training procedure.	0
the second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so i am unsure of which is what.	0
overall, i recommend acceptance, but encourage the authors to perform experiments on more datasets.	0
the submission seems overall ok, but somewhat light on actual datadriven or theoretical insights.	0
benchmarking on highend compute server hardware is an interesting point of reference, but the questions that come to mind for me when reading this paper are (1) how would this fit into a livevideo processing application on a mobile device (2) what kind of a “cluster” would this present to someone trying to do distributed deep learning in the wild by drawing on idle graphics cards: how much memory do they have, how might we handle data for training on such computers, what is the compute speed vs. communication latency and bandwidth.	0
in norb symmetries, ckms show better performance than convnets with small data, but the convnets seem not converged yet.	0
to get a sense of the properties the strengths and limitations  of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes.	2
this paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. pros  the approach is rather simple and hence can be applied to existing methods.	0
3. bendavid et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data.	0
1) the main concern is that there are no comparisons or even mentions of the work done by tamara berg’s group on fashion recognition and fashion attributes, e.g.,  “automatic attribute discovery and characterization from noisy web data” eccv 2010  “where to buy it: matching street clothing photos in online shops” iccv 2015,  “retrieving similar styles to parse clothing, tpami 2014, etc it is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.	0
i understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper.	0
the authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful.	0
trend targets are not provided from 'on high' (by data oracle) but extracted from raw data using a deterministic algorithm.	0
the part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but i am no expert in the field.	0
if more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper.	2
the github links for data and code will be really helpful for reproducing results (i haven't looked carefully, but this is great).	0
the use of reconstruction loss to improve results in the datasparse setting is interesting, but the experimental results are inconclusive.	0
whereas the additional unlabeled data seems to hurt in the 2d case but improve results for the 3d model.	0
the reported log likelihood of cvae using 10k samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10k kmeans mixture model to the data (eg as done in 'a note on the evaluation of generative models').	0
this paper introduces three tricks for training deep latent variable models on sparse discrete data: 1) tfidf weighting 2) iteratively optimizing variational parameters after initializing them with an inference network 3) a technique for improving the interpretability of the deep model the first idea is sensible but rather trivial as a contribution.	0
 not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than squad, the authors should either calculate the human performance the same way as squad or calculate human performances on both newsqa and squad in some other consistent manner on large enough subsets which are good representatives of the complete datasets.	0
 suggestion: answer validation step is nice, but maybe the dataset can be released in 2 versions  one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step.	0
overall the paper is ok, but it has a flavor of 'we ran lstms on an existing dataset'.	0
overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern. '	0
at the same time, they are comparing their approach with regular cnns and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset).	0
at the very least other performance measures (rmse or auc) should be reported for completeness, even if the results are not strong 2) given that the contribution is fairly simple (i.e., the 'standard' recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken.	0
while the dataset does give a good motivation to the problem setting, the paper falls a bit short for iclr due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of chalupka et al., arxiv preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.	0
a theorem on identifiability of causaldiscriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.	0
i appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then i think this work is premature, since there is no way to really validate.	0
that being said i take the point about why the authors had to subsample the yelp data, but keeping only users with 'hundreds' of events means that you're left with a very biased sample of the user base.	0
the main shortcomings of the paper are in the experiments: 1) the full model (ratingtext) is only applied to one and relatively small dataset.	0
(b) the restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation.	0
finally, while i think the bach chorales are interesting musical pieces that deserve to be subject of the analysis but i find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.	0
while there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.	0
“the underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.” i believe this perspective is already contained in several of the works cited as not belonging to this perspective.	0
the response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude—and indeed the training does converge without issue for scaling up to four orders of magnitude.	0
 in fig 1, 2, 3, and 4, results of the application of lots are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer.	0
thus mono has not been exposed the same training data and we can't be sure that the proposed model is better because mono does not observe the data or lacks the computational power.	0
none of this addresses the problems one might suspect on highdimensional and real data, such as the lack of scalability for the kernel, the comparison to salimans et al. (2015) for the langevin variational approximation, and any note of runtime or difficulty of training.	0
cons: 1) it would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsitysome sort of comparison involving training time would be great, and is currently lacking.	0
for the lowdata regime, the hybrid network sometimes gives better accuracy than nin, but this is quite an old architecture and its capacity has not been tuned to the dataset size.	0
the experiments lack of strong comparisons with other graph models (e.g iterative classification, 'learning from labeled and unlabeled data on a directed graph', ...).	0
weaknesses/suggestions: 1. the paper does not report human performance on the dataset.	0
agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned.	0
i'm just concerned about the validity of the proposed dataset and what these sequences really represent (although i agree that it can still be relevant as a sequence learning dataset, even if it does not reflect the way people write).	0
in figure 5, for the blue curve, i was expecting to see an increase of the error when new data are added to the set, but there doesn't seem to be much correlation between these two phenomenons.	0
it'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is nontrivial (but then for most imagerelated applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.	0
the authors don’t compare to more conventional approaches to variable precision  using bitparallel arithmetic units but data gating the lsbs so that only the relevant portion of the arithmetic units toggle.	0
3. the empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of wellestablished baseline for some new applications (e.g., the recipe generation task).	0
weaknesses: the objective function shown in the middle of pg 3 is highly empirical, not directly linked to how nonparallel data helps to improve the final prediction results.	0
finding good neural data is challenging, but whatever the result, the comparison would be interesting.	0
weaknesses:  the lstms (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound 'very' under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences).	0
hypothetically, if quantizing the inputs is really useful, the raw data lstms should be able to learn this transformation, but if they are under capacity, they might not be able to dos.	0
in paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later.	0
on the omniglot dataset feature regularization is better than most baselines, but is worse than moment matching networks.	0
first, there is a claim that a multinomial prior with equal class probabilities assigns the same number of data points to each class on average; this is true a priori but certainly not true given data.	0
originality: the suggested idea is reasonable but limited to binary data at this point in time.	0
the method is evaluated on two standard and relatively largescale vision datasets: imagenet and pascal voc 2012. cons: the evaluation is lacking.	0
they do get a substantial boost, but it is not clear if this will transfer to more data/layers.	0
there have also been a number of papers on “automatic post editing”, including the shared task at wmt2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with humangenerated data.	0
it is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations “training set” could be interpreted in variety of ways.	0
i'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well.	0
because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g. linear relationship between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.	0
the lack of empirical comparison on large scale dataset, such as imagenet or coco makes this largely a theoretical contribution.	0
3) suggestions for improvement static dataset bias: in response to the prereview concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame.	0
the experiments also compare to some bayesian optimization methods, but not to the most relevant very closely related multitask bayesian optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: 'multitask bayesian optimization' by swersky, snoek, and adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several followup papers showing even larger speedups.	0
it has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough.	0
some might also question whether iclr is the appropriate venue to introduce a new dataset, but personally i think it's a great idea to submit it here, seeing as it will reach the right people.	0
the same models run over the cbt dataset show a comparable but less convincing demonstration of the variations between the models.	0
i still believe that, if a model was found that could better handle longer term dependencies, it would do better on this wikipedia dataset, but at least within the realm of what .	0
this of course does not handle complex factors like 'type of legs' but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for reallife datasets that is!)	0
cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.	0
my main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations).	0
wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context.	0
i was concerned that results are only given on one dataset, ptb, which is now kind of old in that literature.	0
the paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.	0
4. the paper seems to be general at the beginning, but the claim of the benefit of the hadamard product is only shown experimentally on the vqa dataset.	0
i think gmu did a nice work on movie dataset, but i would also expect other techniques, including finetuning, dropout, distillation may help too.	0
i am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets).	0
al. 2016 and physnet for the newly proposed dataset, but it would help the paper to have baselines which are directly comparable to the proposed results.	0
also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. dai & le 2015. i would also suggest to report test results on the dataset from lerrer et al 2016 (i understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.	0
on the other side, i have some concerns are about the experimental part, which i consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets.	0
each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments.	0
i was also unfamiliar with the datasets, so i had no intuitive understanding of the algorithms performance, even when viewed as a blackbox.	0
to get a sense of the properties the strengths and limitations  of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes.	2
this paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. pros  the approach is rather simple and hence can be applied to existing methods.	0
using only one dataset (cifar10), the proposed method performs slightly better than the crelu baseline, but the improvement is quite small (0.5% in the test set).	0
another dataset would be a big plus (both datasets concern gray digits and usps and are arguably somewhat similar).	0
perhaps my biggest concern is that this paper is missing baselines (e.g. non recurrent models, attribute classification instead of detection) and comparisons to prior work by berg et al. 'our policy restricts to reveal much more details about the internal dataset' this is a significant issue.	0
i'm not fully up to date on the latest of penn tree bank language modeling results but i do know that it is a hotly contested and wellknown dataset.	0
the offered solution is to introduce sparsity for the latent representation: for every input only a few latent distributions will be activated but across the dataset many latents can still be learned.	0
newsqa, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different qa collection strategies.	0
while not without its own weaknesses, i think this dataset presents potential values compared to what are available out there today.	0
if anything, i don’t think it looks favorable for newsqa if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the qa collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators.	0
 not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than squad, the authors should either calculate the human performance the same way as squad or calculate human performances on both newsqa and squad in some other consistent manner on large enough subsets which are good representatives of the complete datasets.	0
 suggestion: answer validation step is nice, but maybe the dataset can be released in 2 versions  one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step.	0
at the same time, they are comparing their approach with regular cnns and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset).	0
while the dataset does give a good motivation to the problem setting, the paper falls a bit short for iclr due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of chalupka et al., arxiv preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.	0
i appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then i think this work is premature, since there is no way to really validate.	0
the main shortcomings of the paper are in the experiments: 1) the full model (ratingtext) is only applied to one and relatively small dataset.	0
(b) the restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation.	0
for the lowdata regime, the hybrid network sometimes gives better accuracy than nin, but this is quite an old architecture and its capacity has not been tuned to the dataset size.	0
pros:  probably a more sophisticated scheduling technique than a simple decay term  reasonable results on the cifar dataset (although with comparably small neural network) cons:  effect of momentum term would be of interest  the adam reference doesn't point to the conference publications but only to arxiv  comparison to adam not entirely conclusive	2
given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth.	0
weaknesses/suggestions: 1. the paper does not report human performance on the dataset.	0
in addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call 'incremental learning'.	0
i'm just concerned about the validity of the proposed dataset and what these sequences really represent (although i agree that it can still be relevant as a sequence learning dataset, even if it does not reflect the way people write).	0
lack of wordlevel comparisons also makes it difficult to determine the importance of using sentencelevel information vs. choices in model architecture/decoding, and finally, the grid dataset itself appears limited with the grammar and use of a ngram dictionary.	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
there have been concerns about cnn/dailymail dataset (chen et al. acl’16) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds.	0
in that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.	0
(in other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network finetuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an imagenet evaluation.)	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
the authors do show results comparing the model without a termination gate on the graph reachability dataset, and the full model does seem to perform quite a bit better, but i would like this to also be done on the cnn/ daily mail datasets, and for there to be more insights into why the performance is improved vs. the reasonetlast model.	0
one of the contributions i like most from this paper is not the actual model, but the graph reachability dataset.	0
on the omniglot dataset feature regularization is better than most baselines, but is worse than moment matching networks.	0
pros  the paper is clearly written and easy to follow cons  the paper's two contributions are too minor to merit publication  experimental results should include at least the caltech pedestrian dataset but likely also the kitti pedestrian dataset  recent work from eccv 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed my rating is due primarily to the lack luster contributions.	2
the method is evaluated on two standard and relatively largescale vision datasets: imagenet and pascal voc 2012. cons: the evaluation is lacking.	0
here is the evidence we have (i'll use stanford dogs as an example, but any of the datasets have the same conclusion): (1) softmax w 3 stages of pt: 26.6% (from rippel paper) and 32.7% (from authors' reproduction) (2) magnet w 3 stages of pt: 24.9% (3) softmax w full pt: 18.3% (4) magnet w full pt: not shown from this all i see is that pt is critical for getting absolute good results.	0
experimental results are lacking, only results on a single dataset are provided.	0
the experiment is only conducted on one dataset, reporting stateoftheart result, but unfortunately this is not true.	0
in summary: pros:  interesting idea  seems to improve performances cons:  paper writing  weak evaluation (only one dataset)  compare only with approaches that does not use the lasttimestep error signal	2
while one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it.	0
it would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.	0
improved performance is demonstrated on two tasks that are fairly nonstandard, but i think that is fine given that the proposed approach probably isn't useful for the vast majority of popular benchmark datasets (e.g. mnist, cifar10), where the locality assumption holds and a square filter shape is probably close to optimal anyway.	0
cons:  the paper could benefit substantially from additional experiments on different datasets.	0
they illustrate the neural cache model on not just the penn treebank but also wikitext2 and wikitext103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size.	0
the 'crippling' method used feels handcrafted and very taskdependent, and the qualitative evaluation of the 'lossyness' of the learned representation is carried out on three datasets (mnist, omniglot and caltech101 silhouettes) which feature blackandwhite images with littletono texture.	0
figures 1a and 2a do show that reconstructions discard lowlevel information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets.	0
paper weaknesses:  there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of moe and other alternatives in terms of computational efficiency and other factors.	0
the experiments also compare to some bayesian optimization methods, but not to the most relevant very closely related multitask bayesian optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: 'multitask bayesian optimization' by swersky, snoek, and adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several followup papers showing even larger speedups.	0
it has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough.	0
overall, i think the ideas presented in this paper show good potential, but i would like to see an extended analysis in the line of figures 3 and 4 for more datasets before i think it is ready for publication.	0
this of course does not handle complex factors like 'type of legs' but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for reallife datasets that is!)	0
the paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.	0
i am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets).	0
the proposed method outperforms yanardag et al. 2015 and niepert et al., 2016 on social networks graphs but are quite inferior to niepert et al., 2016 on bioinformatics datasets.	0
theoretical results are satisfactory but i particularly like the experimental setup where their methods are tested on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.	0
on the other side, i have some concerns are about the experimental part, which i consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets.	0
i was also unfamiliar with the datasets, so i had no intuitive understanding of the algorithms performance, even when viewed as a blackbox.	0
the exposition is ok, and i think the approach is sensible, but the main issue with this paper is that it is lacking experiments on nonsynthetic datasets.	0
overall, i recommend acceptance, but encourage the authors to perform experiments on more datasets.	0
to get a sense of the properties the strengths and limitations  of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes.	2
another dataset would be a big plus (both datasets concern gray digits and usps and are arguably somewhat similar).	0
although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.	0
if more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper.	2
 not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than squad, the authors should either calculate the human performance the same way as squad or calculate human performances on both newsqa and squad in some other consistent manner on large enough subsets which are good representatives of the complete datasets.	0
(b) the restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation.	0
it'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is nontrivial (but then for most imagerelated applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.	0
experiments test this out on mnist and svhn comments: this is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well.	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
in that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.	0
(in other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network finetuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an imagenet evaluation.)	0
pros:  nice taxonomy of pruning levels  comparison to the recent weightsum pruning method cons:  experimental evaluation does not touch upon recent models (resnets) and large scale datasets (imagenet)  paper is somewhat hard to follow  feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers finegrained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning.	2
in summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
the authors do show results comparing the model without a termination gate on the graph reachability dataset, and the full model does seem to perform quite a bit better, but i would like this to also be done on the cnn/ daily mail datasets, and for there to be more insights into why the performance is improved vs. the reasonetlast model.	0
these are experimentally evaluated on cifar10 and cifar100, but seem to achieve relatively poor performance on these datasets (table 1), so their merit is unclear to me.	0
paper summary: the authors proposed to use edgeboxes  fastrcnn with batch normalization for pedestrian detection review summary: results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and overall the work lacks novelty.	0
the method is evaluated on two standard and relatively largescale vision datasets: imagenet and pascal voc 2012. cons: the evaluation is lacking.	0
there have also been a number of papers on “automatic post editing”, including the shared task at wmt2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with humangenerated data.	0
here is the evidence we have (i'll use stanford dogs as an example, but any of the datasets have the same conclusion): (1) softmax w 3 stages of pt: 26.6% (from rippel paper) and 32.7% (from authors' reproduction) (2) magnet w 3 stages of pt: 24.9% (3) softmax w full pt: 18.3% (4) magnet w full pt: not shown from this all i see is that pt is critical for getting absolute good results.	0
the only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.	0
weaknesses:  the lstms (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound 'very' under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences).	0
 the experiments are thorough with a nice selection of baselines, but i wonder if perhaps futoma, et al. [1] would be a stronger baseline than choi, che, or lipton.	0
since mimic is publicly available, then readers ought (hypothetically) to be able to reproduce the experiments, but that is not currently possible.	0
strengths:  the derivation of the dual formulation is novel  the dual formulation simplifies adversarial training  the experiments show the better behavior of the method compared to adversarial training for domain adaptation weaknesses:  it is unclear that this idea would generalize beyond a logistic regression classifier, which might limit its applicability in practice  it would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which gans are often used  it would be interesting to see if the da results with a kernel classifier are better (comparable to the state of the art)  the mathematical derivations have some errors detailed comments:  the upper bound used to derive the formulation applies to a logistic regression classifier.	2
 the paper is reasonably clear, but could be improved with some more details on the mathematical derivations (e.g., explaining where the constraints on .alpha come from), and on the experiments (it is not entirely clear how the distributions of accuracies were obtained).	0
experiments in the introduction it is claimed that the method of liu et al. cannot capture correlations on different length scales because it lacks disentanglers.	0
 the experiments in celebroom are quite nice, and a good result, but we are still missing a detailed analysis for most of the assumptions and improvements claimed in the paper.	0
the experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition.	0
in summary, the proposed method may be promising, but far more experiments are needed.	0
some of the conclusions could be further clarified with additional experiments (e.g., sec 3.6 ‘while the reason that rms also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the imagenet dataset’).	0
positives:  the output kernel update is well justified  experimental results are encouraging negatives:  the methodological contribution of the paper is minimal  the proposed approach to maintain the budget is simplistic  no theoretical analysis of the proposed algorithm is provided  there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multiclass problem, so we would want to see comparisons with some good multiclass algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)	0
however, this paper, claiming to be learn more rubust representations, lacks solid supporting experiments.	0
the paper provide nice illustrative experiments arguing why transport operators may be a useful modeling tool, but does not go beyond illustrative experiments.	0
 reallife experiments are lacking.	0
the experiments are weak 1) in the sense that they are not compared against simple baselines like p(x) (from, say, just thresholding a vae, or using a more powerful p(x) model  there are lots out there), 2) other than knns, only compared with classprediction based novelty detection (entropy, thresholds), and 3) in my view perform consistently, but not significantly better, than simply using the entropy of the class predictions.	0
pros:  good application of gan models  good writing and clarity  solid experiments and explanations cons:  results weak relative to naive baseline (entropy)  weak comparisons  lack of comparison to density models [1] louizos, christos, and max welling.	2
third, the experiments section lacks crucial information needed to understand the experiments.	0
it is unfortunate but understandable that the gpn model experiments are confined to another paper.	0
 the experiments are set up and executed with care, but section 4 could be improved by providings details (as much as in section 5).	0
most of these concerns are potentially quirks in the exposition rather than any issues with the experiments conducted themselves.	0
is it really because of an intrinsic limitation or lack of scalability of previous approaches or just because the authors of the corresponding papers did not care to present larger scale experiments?	0
pros/cons pros adresses an important problem in representation learning the paper proposes interesting assumptions and results for measuring the complexity of semantic mappings a new cross domain mapping is proposed large set of experiments cons some parts deserve more formalization/justification too many materials for a conference paper the cost of the algorithm seems high summary: this paper studies the problem of unsupervised learning of semantic mappings.	2
many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks, identity mapping initialization, recent interpretations of resnets/densetnets, etc. the title suggests that the analysis is performed on densenet architectures, but experiments focus on comparing both resnets and densenets to sequential convolutional networks and assessing the importance of skip connections.	0
the experiments are somewhat interesting but they appear rather preliminary.	0
pros:  the proposed modification to vin is simple, wellmotivated, and addresses the nondifferentiability of vin  experiments on synthetic data demonstrate a significant improvement over the standard vin method cons:  some important references are missing (e.g., maxent ioc with deeplearned features)  although intuitive, more detailed justification could be provided for replacing the maxoveractions with an exponentiallyweighted average  no baselines are provided for the experiments with real data  all the experimental scenarios are fairly simple (2d gridworlds with discrete actions, 1channel input features) the proposed method is simple, wellmotivated, and addresses a real concern in vins, which is their nondifferentiability.	2
the experiments are also a bit lacking in a few ways.	0
 my main concerns with the experiments is that they are not answering two main questions: 1. what is svin/vin bringing to the table as a function approximator as opposed to using a more traditional but similar capacity cnn?	0
however, the experiments are poorly organized since some necessary descriptions and discussions are missing.	0
more importantly, i suspect the the neighborhood functions are important: when translating between indoeuropean languages as in these experiments, local swaps are reasonable; but in translating between two different language families (as would often be the case in the motivating lowresource scenario that the paper does not actually test), it seems likely that other neighborhood functions would be important, since structural differences would be much larger.	0
based on its incremental nature and weak experiments, i'm on the margin with regards to its acceptance.	0
the numbers in the tables are good but i have several comments on the motivation, originality and experiments.	0
paper weaknesses: this paper has the following issues so i vote for rejection: (1) the experiments have been performed on a toy environment, which is similar to the environments used in the 80's.	0
3) the experiments are somewhat substandard:  on mnist the authors use a tiny poorlyperformance network, and it is no surprise that one can beat it with a bigger dynamic filter network.	0
cons: experiments: only small datasets were used in the experiments, it would be more convincing if the author could use larger datasets.	0
 the experiments do not directly test whether the theoretical insight holds in practice, but instead a derivate method is tested on various benchmarks.	0
the major problem with the paper, in my eyes, is the lack of experiments specific to test the hypothesis.	0
while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.	0
for instance, there are no experiments against what is called adaptive blackbox adversaries; one could also imagine finding adversarial examples that are trained to fool all models in a predefined collection of models.	0
i also have concerns regarding the experiments.	0
c) the paper should run some experiments on language applications where rnn is widely used d) i might be wrong on this point, but it seems that the gpu utilization of the method would be very poor so that it's kind of impossible to scale to large datasets?	0
it clearly suggests that the entropy regularized wasserstein_2 distance should be used in numerical experiments but this point is not supported by experimental results.	0
finally, the experiments are reasonable, but the choice of rnn setting isn't clear to me.	0
they have added extra experiments, and clarified the speedups concern raised by others.	0
such a paper can also provide value, but to do that right, the tricks need to be obvious 'in retrospect only' and/or the experiments need to show a lot of precise practical lessons.	0
however, as an an applications paper, the bread of experiments are a little bit lacking  with only that one potentially interesting dataset, which happens to proprietary.	0
yes, in principle the neural model used to model y could detect them, but no experiments are shown to really tease this case apart 2) validating gwas results is really hard, because no causal information is usually available.	0
overal the paper is interesting and relatively wellwritten but some important details are missing and way more experiments need to be done to show the effectiveness of the approach. '	0
pros and cons ============  good results  interesting idea of using the algorithm for rlfd  weak experiments for an application paper  not clear what's new	2
from the experiments i would guess that mbtrpo is about two to three orders of magnitude slower, but having this information would be useful.	0
it is an incremental modification of prior work (resnext) that performs better on several experiments selected by the author; comparisons are only included relative to resnext.	0
through a series of experiments and a newly defined dataset, it exposes the shortcomings of current seq2seq rnn architectures.	0
figure 4 is thus relevant, but the semisupervised results of mnist or the sample quality experiments give hardly any evidence to support the method.	0
"the proposition 1.4 is trivial to prove, however i do not understand the following: ""with such vanishing gradients, it is possible to find series of inputsequences that diverge in l2(z) while their outputs through the rnn are a cauchy sequence"" how would you prove it or do you have some numerical experiments to do so?"	0
my guess is that it is because this basis sparsify the input signal, but it would require some additional experiments, in particular to understand how the nn uses it.	0
below i'll discuss my concerns with the experiments and description of the results.	0
there is an impressive number of experiments presented in the paper, but the results are a bit mixed, and it is not always clear that adding more tasks help.	0
i have one small lingering concern, which is not big enough to warrant acceptance: r2's point 10 is valid—the use of multiple rnns trained on different objectives in the ablation experiments unexpected and unusual, and deserves mention in the body of the paper, rather than only in an appendix.	0
in general, lack of substantial experiments makes it difficult to appreciate the novelty of the work.	0
the application is very nice and complex, but i find that the experiments are a little bit too limited.	0
however, i strongly encourage the authors to perform additional experiments with careful experiment design to address some common concerns in the reviews/comments for the acceptance of this paper.	0
the idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on mnist.	0
"secondarily, the paper asserts that ""our architecture can handle datasets more diverse than clevr"", but runs no experiments to validate this."	0
however i think the authors needs to 1) better describe how their method differs from prior work and 2) compare their method to more baselines for the experiments to fully convincing	0
i further have concerns about the experimental procedure, with the only the outcomes of single runs presented for each experiment and different experiments run for different numbers of iterations without justification.	0
the paper presents an interesting idea that potentially has useful applications, however the experiments are not convincing enough.	0
(this is answered at page 7 experiments:  the accuracy figures obtained are impressive, but i’m not convinced the enas learning is the important ingredient in obtaining them (rather than a very good baseline)  specifically, in the cifar 10 example it does not seem that the networks chooses the number of maps in a way which is diverse or different from layer to layer.	0
although drelu’s expectation is smaller than expectation of relu, but it doesn’t explain why drelu is better than very leaky relu, elu etc. 2. cifar10/100 is a saturated dataset and it is not convincing drelu will perform will on complex task, such as imagenet, object detection, etc. 3. in all experiments, elu/lrelu are worse than relu, which is suspicious.	0
2) i believe the control experiments are encouraging, but i do not agree that other techniques like dropouts are not useful.	0
the arguments for skipping this experiments are respectful, but not convincing enough.	0
perhaps the authors have taken out some components of the deeplab scheme for these experiments, such as multiscale processing, but the question then is “why?”.	0
importance: somewhat lack of originality and poor experiments lead to low importance.	0
pros:  the paper is clearly written  the method is new and somehow theoretically guaranteed by the proof of the proposition 1  the experiments are clearly explained with detailed configurations  the performance of the method in the model compression task is promising cons:  the “simple deduction” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis  it is confusing whether the output of the gate is sampled based on or computed directly by the function g  the experiments lack many recent baselines on the same dataset (penn treebank: melis et al. (2017) – on the state of the art of evaluation in neural language models; wmt: ashish et.al. (2017) – attention is all you need)  the experiment’s result is only slightly better than the baseline’s  to be more persuasive, the author should include in the baselines other method that can “binerize” the gate values such as the one sharpening the sigmoid function.	2
you state that you ran experiments on adam, adadelta and adagrad, but you do not show the adam results.	0
i am not saying that this particular secondorder information could not be useful, but you need to make a distinction in your paper between network secondorder info and error function secondorder info and make explicit that you only use the former in your experiments.	0
this is discussed in section 4.1, but i think some of these differences could have been alleviated if there were more experiments done.	0
additionally, i feel this paper’s experiments are poorly designed.	0
on the negative side, i felt that the experiments were thin, and that the work was not well framed in terms of the literature of statespace identification and planning (there are a zillion ways to plan using a model; couldn't we have compared to at least one of them?	0
al. they obviously get better results on the addition and nanocraft domains, but i would have liked a more clear explanation and/or some experiments providing insights into what enables these improvements (or at least an admission by the authors that they don't really understand what enabled the performance improvements).	0
paper weaknesses: i have the following concerns about this paper: (1) the paper performs the experiments on modelnet40, which is a toy dataset for this task.	0
the policy of another agent is interesting cons: not very well setup experiments performance is lower than you would expect just using supervised training not clear what parts are working and what parts are not	0
the paper is fairly wellwritten but has several major weaknesses:  privacy parameter eps = 8 used in the experiments implies that the likelihood of any event can change by e^8 which is roughly 3000, which is an unacceptably high privacy loss.	0
review: the paper is well written and explains the experiments well, however the results are somehow obvious.	0
 is the scale needed i assume the authors did these experiments when they developed the method but it is unclear how important these choices are.	0
the biggest concern i have with this paper is the unconvincing experiments.	0
quality and significance  due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments.	0
i will detail concerns for the specific experiments below.	0
pros and cons: pros : see above cons: my problem with the paper is lack of experiments on public datasets.	0
pros: 1. the approach is simple, solves a task of practical importance, and performs well in the experiments.	2
the proposed regularizes are rather simple, but perform well in the experiments.	0
cons: as mentioned above, i didn’t find the paper conceptually novel, but this isn’t a significant detraction as its value (at least for vae researchers) is primarily in the experiments (figure 3).	0
while the broadness of the experiments are encouraging, the main task which is to propose an effective taskincremental learning procedure is not conclusively tested, mainly due to the lack of thorough ablation studies (for instance when convolutional layers are fixed) and the architecture seems to change from one baseline (method) to another.	0
 the same concern as above applies to the transferability and dataset decider experiments	0
a more systematic set of experiments could compare learning the proposed weightings on the first k layers of the network (for k={0, 1, …, n}) and learning independent weights for the latter nk layers, but i understand this would be a rather large experimental burden.	0
my major concern is with the experiments and evaluation.	0
to summarize, i like this idea, but more experiments are needed in order to understand this method merits.	0
concerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion.	0
" please see my detailed comments in the ""official comment"" the extensive revisions addressed most of my concerns quality ====== the idea is interesting, the theory is handwavy at best (addressed but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (addressed)."	0
weaknesses: ' there are several obvious additional experiments that, in my view, would greatly strengthen this work: 1. nearly all of the image categorization results (with the exception of those in table 4) are presented for the contrived scenario where the unsupervised representation is learned from the same training set as the one used for the final supervised training of the categorization model.	0
pros:  many experiments which try to study the effect cons: the described phenomenon seems to depend strongly on the problem surface and might never be encountered on any problem aside of cifar10  only single runs are shown, considering the noise on those the results might not be reproducible.	2
 pros: 1. extend the input from disentangled feature to raw image pixels 2. employ “obverter” technique, showing that it can be an alternative approach comparing to rl 3. the authors provided various experiments to showcase their approach cons: 1. comparing to previous work (mordatch & abbeel, 2018), the task is relatively simple, only requiring the agent to perform binary prediction.	2
"the contribution of the paper is:  some proposed methods to extract a colorinvariant representation  an experimental evaluation of the methods on the cifar 10 dataset  a new dataset ""crashed cars""  evaluation of the best method from the cifar10 experiments on the new dataset pros:  the crashed cars dataset is interesting."	2
concerning the experiments, it seems peculiar that the learning curves in figure 1 remain at a constant value for a long time at the beginning of the optimization before they begin to drop.	0
concerning the experiments, it seems peculiar that the learning curves in figure 1 remain at a constant value for a long time at the beginning of the optimization before they begin to drop.	0
in experiments, they show the advantage of bnns by conducting experiments based on blackbox and whitebox adversarial attacks without the need to artificially mask gradients.	0
positive aspects of the paper: the paper is a very strong empirical paper, with experiments comparable to industrial scale.	2
"the authors mention ""interpolating only between inputs with equal label did not lead to the performance gains of mixup,"" but this is not shown in the experiments."	0
however, the lack of sufficient theoretical justification is now well complemented by extensive experiments, and it will motivate more theoretical work.	0
as far as this reviewer can tell, the authors offer no experiments to show that a larger number of units for rnn or lstm would not have helped them in improving longterm forecasting accuracies, so this seems like a very serious and plausible concern.	0
again, the concern here is that the experiments are plausibly not being fair to all methods equally.	0
overall, the paper covers an interesting topic but could use extra editing to clarify details of the model and training procedure, and could use some redesign of the experiments to minimize the number of arbitrary (or arbitraryseeming) decisions.	0
the experiments suggests that the approach performs better than stateoftheart incremental learning approaches, and approaches offline learning.	0
 the paper is easy to read and well organized  the advantage of the proposed regularization against the more standard l2 regularization is clearly visible from the experiments  the idea per se is not new: there is a list of shallow learning methods for transfer learning based on the same l2 regularization choice [crossdomain video concept detection using adaptive svms, acm multimedia 2007] [learning categories from few examples with multi model knowledge transfer, pami 2014] [from n to n 1: multiclass transfer incremental learning, cvpr 2013] i believe this literature should be discussed in the related work section  it is true that the l2spfisher regularization was designed for lifelong learning cases with a fixed task, however, this solution seems to work quite well in the proposed experimental settings.	0
as a side note, it also feels a little disingenuous to describe the method in terms of gmms, but to perform all experiments with k=1 mixture components; in this setting the gmm degrades to a simple gaussian distribution.	0
overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.	0
on the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.	0
i have only minor concerns to this paper:  the experiments are designed to achieve comparable bleu with improved latency.	0
pros:  fairly well presented  wide range of experiments, despite underwhelming absolute results cons:  quasirnns are almost identical and already have results on smallscale tasks.	2
authors perform experiments on numerous tasks showing that sru performs on par with lstms, but the baselines for these tasks are a little problematic (see below).	0
as is, i cannot recommend acceptance given the current experiments and lack of theoretical results.	0
i agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks.	0
 pros :  interesting topic cons :  experiments are not convincing  writing & clarity could be improved sometimes the paper is a bit hard to read sometimes, because it does not refer to usual notions.	2
pros: insightful theoretical results, interpretation of experiments, large step toward understanding ghns cons: coherence/clarity	2
the paper contains a good amount of experiments, but in my opinion not quite enough to conclude that identity skip connections are inherently worse.	0
 as discussed in section 2, there are already many outlier detection methods, such as distancebased outlier detection methods, but they are not compared in experiments.	0
the experiments do not however demonstrate why the algorithm is performing better.	0
thus to me the experiments are still lacking.	0
shapeworlds dataset seems to be an interesting proofofconcept dataset however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups.	0
the experiments show that the generation component is quite effective, but this is an obvious missing step.	0
the authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach.	0
please cite some work in that case) (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at runtime, or speed of a single iteration, not necessarily speed to train) [as i am not primarly a neural net researcher, i had always thought pruning was done to decrease overfitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix] (4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (martingale or markov chain theory) this is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above).	0
the authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption.	0
i do appreciate the authors trying out their method on a real robotic platform, but perhaps the more honest assessment of the outcome of these experiments is that the approach didn't work very well, and more research is needed.	0
i think the transfer experiments should definitely be kept, but the authors should discuss the limitations to help future work address them, and present the transfer appropriately in the intro.	0
regarding the experiments: my main concerns are with regard to the completeness of the experiments.	0
cons  nontrivial loss of accuracy on the pruned network, which cannot be estimated for largerscale pruning as the experiments only prune one layer.	2
the experiments feel lacking.	0
cons  1. no validation / test curves for any experiments, which makes it hard to asses if one should use this method in practice or not.	2
it should be noted that the proof in appendix d assumes that the covariance of the noise is constant in some interval around the minimum; i think this is again a strong assumption and should be included in the statement of theorem 2. there are some detailed experiments showing the effect of the learning rate and batchsize on the noise and therefore performance of sgd, but the only real insight that the authors provide is that the ratio of learning rate to batchsize controls the noise, as opposed to the that of l.r.	0
cons: not clear justification and motivations the experiments are really lacking: no ablation study the results are only limited to single toy task.	0
the experiments are seriously lacking, an ablation study should have been made and the results are not good enough.	0
pros:  three simple techniques to use for mixedprecision training  matches performance of traditional fp32 training without modifying any hyperparameters  very extensive experiments on a wide variety of tasks cons:  experiments do not validate the necessity of fp32 accumulation  no comparison of training time speedup from mixed precision with new hardware (such as nvidia’s volta architecture) providing large computational speedups for mp computation, i expect that mp training will become standard practice in deep learning in the near future.	2
my first concern with the paper is that there are no experiments to demonstrate the necessity of fp32 accumulation.	0
though the effect is most obvious from the speech recognition experiments in section 4.3, mp also achieves slightly higher performance than baseline for all imagenet models but inceptionv1 and for both object detection models; these results add support to the idea of fp16 as a regularizer.	0
pros:  three simple techniques to use for mixedprecision training  matches performance of traditional fp32 training without modifying any hyperparameters  very extensive experiments on a wide variety of tasks cons:  experiments do not validate the necessity of fp32 accumulation  no comparison of training time speedup from mixed precision with new hardware (such as nvidia’s volta architecture) providing large computational speedups for mp computation, i expect that mp training will become standard practice in deep learning in the near future.	2
my first concern with the paper is that there are no experiments to demonstrate the necessity of fp32 accumulation.	0
pros: ' important problem ' elegant and simple solution ' nice results and decent experiments (but see below) cons: ' the assumption that the measurement process 'and' parameters are known is quite a strong one.	2
[significance] in the experiments, the proposed method is compared with the vanilla model (i.e., the model having no lowrank structure) but with no other baseline using different compression techniques such as novikov et al., 2015. so i cannot judge whether this method is better in terms of compressionaccuracy tradeoff.	0
pros  extensive experiments on nlp data.	0
pros:  experimental results are clearly presented  most details of experiments are explained  the basic outline of the paper explores an interesting question cons  many sentences do not have a clear point, and there are many spelling and grammar errors  despite claims to the contrary, the experiments are far from extensive; they explore two models, with variations in context for embeddings, with 3 optimizers, on just one task.	2
pros experiments seem well done.	0
regarding the cifar results, i may have read over it, but i think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this.	0
main concerns:  presentation: the experiments show that (1) dive outperforms ge, hyperscore and hfeature baselines; (2) which scoring function works best for dive; (3) dive outperforms sbow in many cases, but not always, though better on average; and (4) we can use dive for wsd (only shown qualitatively).	0
i have two major concerns regarding the experiments, beyond intelligibility.	0
i would recommend the authors to rerun these experiments but truncate the iterations early enough.	0
there was a concern or assumption in the original dtp paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the dtp propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments.	0
"pros: interesting idea, relevant theory provided, highquality experiments cons: no evidence that this is a ""breakthrough"" idea minor comments:  theorems seemed reasonable and i have no reason to doubt their accuracy  no typos at all, which i find very unusual."	2
pros:  very easy to follow idea and model  simple merge or rl and sl in an endtoend trainable model  improvements over previous solutions cons:  kmeans experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.	2
" tsp experiments show that ""in distribution"" dcn perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success."	0
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
it subsequently says it deals only with deterministic problems (which would reduce the problem further to a learning version of a multiagent classical planning problem), but in the experiments do consider stochastically moving preys.	0
there is however not much in terms of discussion nor analysis of the two experiments.	0
i find the contribution fairly significant but i lack some clarity in the presentation as well as in the experiments section.	0
they conduct experiments on imagenet1k with variants of resnets and multiple low precision regimes and compare performance with previous works pros: () the paper is well written, the schemes are well explained () ablations are thorough and comparisons are fair cons: () the gap with full precision models is still large () transferability of the learned low precision models to other tasks is not discussed the authors tackle a very important problem, the one of learning low precision models without comprosiming performance.	2
this is an interesting objective but since no interactive experiments presented in this paper, the rest of the experiments hinges on the definition of “pir” (positive interaction rate)using a model of human interaction.	0
in the experiments, the embedding dimension of mos is slightly smaller, but the number of mixture is 15. this will make it less usable, i think it's necessary to provide the training time comparison.	0
"additionally, the theoretical portion focuses on the effects of the three different sublearner types, but the experiments are ""intend[ed] to show that the value function is easier to learn with the madrl architecture,"" which is an entirely different goal."	0
this paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments.	0
"""we assess the feasibility of performing analogous experiments on real robotics hardware"": i assume this is a typo, but the paper does not actually contain any real robotics hardware experiments."	0
as there are not enough experimental details to judge, it's hard to figure out the problem, but this ppaper is clearly not publishable at any of the quality machine learning venues, for weakness in originality, quality of the writing, and poor experiments.	0
the ideas are simple, but they seem to work well in the experiments.	0
my major concerns are perhaps the fit of the paper for iclr as well as the thoroughness of the final experiments.	0
acm, new york, ny, usa, 14751484. doi: https://doi.org/10.1145/2939672.2939839 2. given that the experiments are conducted on tasks where there isn’t a large amount of training data, one concern is that the baseline model used by the authors might be overparameterized.	0
the authors claim experiments for cp are in appendix, but there are not additional experiments in the appendix 2.c.	0
this paper is well written, brings new ideas and perfoms interesting experiments, but its claims are somewhat bothering me, considering that e.g. your cifar10 results are somewhat underwhelming.	0
pros:  very thorough experiments across a number of domains cons:  methodological contributions are minor.	2
the experimental results are not very strong, but the authors claim that they are initial experiments and can be improved with just a little more experimentation.	0
"it also implies that this would be true for any type of noise, and support this later claim using experiments on cifar and mnist with three noise types: (1) uniform label noise (2) nonuniform but imageindependent label noise, which is named ""structured noise"", and (3) samples from outofdataset classes."	0
## pros / strengths  effort to assess momentum / adam / other modern methods  effort to compare to previous experimental setups ## cons / limitations  lack of wallclock measurements in experiments  only ~2 models / datasets examined, so difficult to assess generalization  lack of discussion about distributed/asynchronous sgd ## significance many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community.	2
the two experiments looks at (1) the training error, which the paper openly states does not explain why the proposed regularization works and (2) variance of the gradients throughout learning; the larger variance of gradients is speculated to be the cause, but this is almost expected, given that the method is designed to allow larger fluctuations and perturbations during training.	0
the experiments are fine, but not so conclusive.	0
pros methodology 1. inductive ability: can generalize to unseen nodes without any further training 2. personalized ranking: the model uses natural ranking that embeddings of closer nodes (considers node pairs of any distance) should be closer in the embedding space, which is more general than prevailing first and second order proximity 3. sampling strategy: the proposed nodeanchored sampling method gives unbiased estimates of loss function and successfully reduces the time complexity experiment 1. evaluation tasks including link prediction and node classification are conducted across multiple datasets with additional parameter sensitivity and missinglink robustness experiments 2. compared with various baselines with diverse model designs such as gcn and node2vec as well as compared with naive baseline (using original node attributes as model inputs) 3. demonstrated the model captures uncertainties and the learned uncertainties can be used to infer latent dimensions related works the survey of related work is sufficiently wide and complete.	0
the experiments demonstrate small but consistent gains with anr across a number of domains (language modelling on small datasets, plus image classification) and baseline models.	0
however, i have significant concerns regarding how the paper is written and final section of the proposed algorithm/experiments etc. introduction/literature review> i think paper significantly lacks literature review and locating itself where the proposed approach at the end stands in the given recent sr literature (particularly deep learning based methods) similarities to other techniques, differences from other techniques etc. there have been several different ways of using cnns for super resolution, how does this paper’s architecture differs from those?	0
i'd be less concerned about this if the results supporting the use of the cnn decoder were a bit more conclusive: while they are better on average across your smaller experiments, your largest experiment (2400d) shows them roughly tied.	0
the authors claim that the innovative of the graph residual convnets architecture, but experiments and the model section do not clearly explain the merits of gated graph convnets over graph lstm.	0
the main concern is that the experiments could be greatly improved.	0
3) in contrast to the thin experiments and (lack of) technical novelty, the introduction & related work writeups are overdrawn and uninteresting.	0
below are some detailed comments: pros  numerous public datasets are used for the experiments  good introductions for some of the existing methods.	0
it is useful to discuss this in the paper and may be have some experiments on linearly separable data but with updates in both layers.	0
in other words  all the experiments in the paper follow the assumption made, if authors claim is that the restriction introduced does not matter, but make proofs too technical  at least experimental section should show this.	0
list of pros & cons  informative and unique experiments that demonstrate emergent complexity coming from the natural curriculum provided by competitive play, for physicsbased settings  likely to be of broad interest  likely large compute resources needed to replicate or build on the results  paper is not anonymous to this reviewer, given the advance publicity for this work when it was released ==> overall this paper will have impact and advances the state of the art, particular wrt to curriculums in many ways, it is what one might expect.	2
there are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately gaussian.	0
pros:  task of reducing computation by skipping inputs is interesting  model is novel and interesting  experiments on multiple tasks and datasets confirm the efficacy of the method  skipping behavior can be controlled via an auxiliary loss term  paper is clearly written cons:  missing comparison to prior work on sequential mnist  low performance on charades dataset, no comparison to prior work  no comparison to prior work on imdb sentiment analysis or ucf101 activity classification the task of reducing computation by skipping rnn inputs is interesting, and the proposed method is novel, interesting, and clearly explained.	2
i will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.	0
 the gan experiments are interesting but come as a big surprise and are largely orthogonal to the other model; why not include this in your model description section?	0
the fact that the model works well on babi despite its simplicity is interesting, but it feels like the paper is framed to suggest that objectobject interactions are not necessary to explicitly model, which i can't agree with based solely on babi experiments.	0
clarity: the paper is well structured and written, but sections 14 could be significantly shorter to leave more space to additional and more conclusive experiments.	0
the paper shows the start of some interesting ideas, but needs revisions and much more extensive experiments.	0
it's ok not to have the theoretical answers to the questions but in that case the authors should provide ablation experiments.	0
i applaud the authors for mentioning the experiments are very preliminary, but that doesn't make them any less weak.	0
the experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly.	0
additional comments:  the experiments in general lack sufficient detail: were the attention masks trained supervised or unsupervised?	0
figure 3 would be okay if there were more experiments, but as it is the only quantitative result, more work should have gone in to it.	0
the regularization is motivated from the point of view of sampling the hidden states to be from the exponential family, but all the experiments provided seem to use a gaussian distribution.	0
section 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments.	0
h is never discussed after section 2.2. experiments on known datasets are interesting, but none of the results are competitive with current stateoftheart results (sota), despite what is said in appending d. for instance, one can find sota results for cifar100 around 16% and for cifar10 around 3%.	0
4) experiments there is some improvement of the proposed method  however overall, the improvements are marginal.	0
what is the advantage over other competitive schemes, etc. in summary, while there is a minor novelty in connecting two separate ideas (cpd and uml) into a joint uml setup, the paper lacks sufficient motivations for proposing this setup (in contrast to say kernelized kmeans), the technical details are unconvincing, and the experiments lack sufficient details or analysis.	0
the experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome.	0
this paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at section 3.3. more specifically, if you generate the parameters .theta according to eq. 7 and posit a prior over .theta then you will have a problematic variational bound as there will be a kl divergence, kl(q(.theta) || p(.theta)), with distributions of different support (since q(.theta) is defined only along the directions spanned by u), which is infinite.	0
in the introduction must energy is used on the importance of large data sets, but it appears that only fairly smallscale experiments are considered.	0
the notation did help make clear the later discussion of the experiments, but it was not clear to me that it was required in order to explain the experimental results.	0
pros: () the idea introduced is simple and flexible to be used for any cnn architecture () experiments on imagenet1k prove demonstrate its effectiveness cons: () experiments are not thorougly explained () novelty is extremely limited () some baselines missing the experimental section of the paper was rather confusing.	2
moreover it seems this axiom is even not used in the proof of theorem 2. concerning the experiments: the experimental setup, especially in section 3.3.1, is not well defined: on which layer of the network is the mask applied?	0
in the experiments, results of this method should be compared not against nns trained on the data directly, but against nns trained on dimension reduced version of the data (eg: first fixed number of pca components).	0
the experiments are nice as a preliminary investigation, but not enough in order to illustrate the benefits of signsgd over sgd.	0
i like the new experiments, but i am not impressed much with them to increase my score.	0
the authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming.	0
the experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (mirowski et al, 2016) achieved by training on static maps such as their “imaze”.	0
 pros: ' asynchronous modelparallel training of deep learning models would potentially help in further scaling of deep learning models ' the paper is clearly written and easy to understand cons: ' weak experiments: performance of algorithms are not analyzed in terms of wallclock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm ' weak theory: although the algorithm is claimed to be motivated by continuoustime formulation of gradient descent, neither convergence proof nor algorithm design really use the continuoustime formulation and discretetime formulation seems to suffice; the proof is straightforward corollary of lin et al. summary: this paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously.	2
experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm.	0
pros:  generating programs with neural networks is an exciting direction  novel task of generating ui code from ui screenshots  three new datasets of ui images and corresponding code  paper is clearly written cons:  limited technical novelty  limited experiments i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.	2
the experiments using resnet indicate it is not prohibitively expensive, but i am eager for more details.	0
although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well.	0
however, many related works are missing in the literature, for example, highway networks [1], deeplysupervised nets [2] and deep networks with stochastic depth [3], etc. significance: the paper lacks of theoretical justification as well as the experiments are not convincing.	0
this is a potentially important observation, and the experiments were well worth performing, but i don't find them fully convincing (partly because i was confused by the presentation).	0
"i'm not fully convinced by the second and third experiments, partly because i didn't fully understand the plots (more on this below), but also because it isn't clear to me what we should expect from the spectrum of a hessian, so i don't know whether the observed specra have fewer large eigenvalues, or more large eigenvalues, then would be ""natural""."	0
these sorts of ideas are explored in the empirical part of the paper, but i did not find the actual experiments in this section to be very compelling.	0
cons: 1) overall, some of the claims made by the paper are not fully supported by the experiments.	0
authors note that for higher dimensional spheres, adversarial examples on the manifold (sphere shell) could found, but not smaller d: “in our experiments the highest dimension we were able to train the relu net without adversarial examples seems to be around d = 60.” yet,in their later statement in that same paragraph “we did not investigate if larger networks will work for larger d.”, it is unclear what is meant by “will work”; because, presumably, larger networks (with more weights) would be harder to avoid adversarial examples being found on the data manifold, so larger networks should be less likely “to work”, if “work” means avoid adversarial examples.	0
the algorithm is simple but the experiments are not convincing.	0
pros:  it's novel, interesting, well written, and appears to work very well in the experiments provided.	2
update: the revised version of the paper addresses all my concerns about experiments.	0
pros: 1. the approach is simple (no pretraining, no reward shaping, just rl from scratch with terminal reward, uses lstm for keeping track of past state), computationally efficient (no computation over the full graph), and performs well in most of the experiments reported in the paper.	2
maybe this is a matter of the dataset fb15k itself but then having experiments on another dataset with hundreds of relation types could be important.	0
cons: • hard to replicate experiments without the deep computational pockets of deepmind.	0
= major comment = i'm concerned by the quality of your results and the overall setup of your experiments.	0
the experiments on invariances seem to be the highlight of the paper, but they basically do not tell me anything.	0
the presentation and experiments are okay overall but i have a few questions and requests below that i feel would strengthen the submission.	0
 you claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experiments.	0
the paper presents a number of interesting experiments and discussions about those experiments, but offers more exciting ideas about training neural nets than experimental successes.	0
while in some experiments smash offers excellent results, in others the results are lackluster (which the authors admit, offering possible explanations).	0
pros  ' strong related work section that contextualizes this paper among current work ' very interesting idea to more efficiently find and train best architectures ' excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e. last paragraph of section 4.1, and last paragraph of section 4.2) ' publicly available code cons  ' some very strong experimental results contrasted with some mediocre results ' the balance of the paper seems off, using more text on experiments than the contributions to theory. '	2
if this is truethat learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually 'establishing' that requires considerably more experiments.	0
there are some concerns about this network that need to be clarified: 1. sigma is never clarified in the main context or experiments 2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random.	0
thus, in reality, the experiments show that the pipeline generative model  classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models.	0
the proposed metric is interesting, but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice.	0
my main concern focus on the validity of the proposed model in harder tasks such as the atari experiments in kirkpatrick et.	0
pros  the paper is written in a clear and concise manner  it suggests an interesting connection between a traditional model and deep learning techniques  in the experiments they trained the network on 64 x 64 patches and achieved convincing results cons  please provide the value of the diffusion coefficient for the sake of reproducibility  medium resolution of the resulting prediction i enjoyed reading this paper and would like it to be accepted.	2
 curiosities:   i got the impression from the results (specifically the lack of discussion about message length) that in these experiments agents always issued full length messages even though they did not need to do so.	0
i know that there is no reference implementation of ftbo, so i am not asking for a comparison, but the comparison against fabolas is misleading for experiments (2) and (3).	0
weaknesses: 1. although “the key aim of xgan is to learn a joint meaningful and semantically consistent embedding”, the experiments are actually devoted to the qualitative style transfer only.	0
"the bottom line seems to be: ""my model and approach works better than the other guys' model and approach"", but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much"	0
overall: pros:  a nice idea with some novelty, based on a nontrivial observation  the experimental results how the idea holds some promise cons  the method is not presented clearly enough: the main component modeling the network activity is not explained (the hdda module used)  the results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)  experimental results are partial: results are not presented for multiple defenders, no ablation experiments after revision: some of my comments were addressed, and some were not.	2
cons:  my major concern is that the baselines evaluated in the experiments are quite limited.	0
i understand that in the experiments the authors used two models (either the average of random realization, or solving a different optimization for each realization), but none of them is an appropriate solution for a stochastic system.	0
the originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.	0
there is also a lack of detail regarding the human judgement experiments that make the significance of the results difficult to interpret.	0
i do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but i think for realworld relevance, variety in problem settings (i.e. datasets) is simply much more important.	0
however the paper is also very dense and quite hard to follow at times  in general i think the paper would benefit from moving some content (like the wakesleep part of the paper) to the appendix and concentrating more on the key results and a few more experiments as detailed in the comments / questions below.	0
some questions and concerns:  i could not quite figure out how many layers did each gcn have in the experiments and how impactful is this parameter  why is it necessary to replicate gcns for each of the transition matrix powers?	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
they then go on to explore the sparsity of the latent space my main issues with this paper are experiments: the proposed approach is tested only on 2 datasets (one synthetic, one real but tiny  2k instances) and some of the plots (like figure 5) are not convincing to me.	0
post rebuttal response: the authors have addressed the comments on the mnist experiments and show better results, however, as far as i can see, they did not address my concern about the comparisons on the image captioning experiment.	0
incomplete experiments: the authors only show experiments on videos containing objects that have already been seen, but no experiments with objects never seen before.	0
i realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner.	0
overall, the new experiments and discussion are a step towards a thorough analysis of zeroorder attacks, but they're not there yet.	0
 there are a separate number of optimization steps for the design and policy parameters within each iteration of the training loop, however the numbers used for the experiments are not listed.	0
in summary, while the paper presents a simple but possibly effective and very general cooptimization procedure, the experiments and discussion don't definitively illustrate this.	0
pros useful extension of an important technique backed up by behavioural experiments.	0
the paper also covers results of the experiments by not only pointing out their proposed network’s success, but by analyzing why certain earlier network models were able to achieve competitive learning results.	0
 interesting work, but i’m not convinced by the arguments nor by the experiments.	0
first unsupervised experiments on mnist data show improved mse of joint autoecnoders but are these differences really significant (e.g. from 0.56 to 5.52) ?	0
moreover, in the experiments, the authors indeed use the cycleconsistency loss by setting .lambda_{cyc} = 10. but the feature consistency loss may not be used by setting .lambda_{feature} = 0 or 0.5. from table two, it appears that whether using the featureconsistency loss does not have significant effect on the performance.	0
i find the paper fairly interesting but still have some concerns in the technical part and experiments.	0
al. the parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be rnn but the experiments uses a feedforward network).	0
weaknesses:  the primary downside is that the approach requires a specialized architecture to work well (all experiments are done with squeezenets).	0
it’s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.	0
the experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear.	0
the mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted mnist tasks, imagenet incremental inclusion of classes, imagenet unbalanced, and two language modeling tasks.	0
the experiments seem to be poorly done and does not convey any clear points, and not directly comparable to previous results.	0
strong points:  new method that generalizes existing methods weak points:  paper should be made more accessible, especially pages 1011  should include more data sets for graph classification experiments, e.g., larger data sets such as reddit'  paper does not include proofs, should be included in the appendix  review of literature could be extended some remarks: ' section 1: the reference feragen et al., 2013 is not adequate for kernels based on walks. '	2
baselines could include unsharing the weights in the tree, removing the max backup, having a regular mlp with similar capacity, etc. page 5, the auxiliary loss on reward prediction seems appropriate, but it’s not clear from the text and experiments whether it actually was necessary.	0
however, my main concerns with this paper are related to motivation and experiments.	0
for mnist and mnistfashion experiments, the motivation is mentioned to be similar to srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized.	0
globally, you experimental part is rather weak, we would expect a stronger methodology, more experiments also with more difficult benchmarks (halfcheetah and the whole gym zoo ;)), more detailed analyses of the results, but to me the value of your paper is more didactical and conceptual than experimental, which i really appreciate, so i will support your paper despite these weaknesses.	0
only the names of the datasets used in the experiments are given, but they are not described, or even better, shown in pictures (maybe in a supplementary).	0
it argues that this attention module can be added to any layer but experiments show only 1 layer and 1 attention map already achieve most of the improvement.	0
re: measurement/process noise the fact that the method assumes perfect measurements and, with the exception of the 3d experiments, no process noise is concerning as neither assumptions are valid for physical systems.	0
furthermore, the lack of systematic experiments and ablation studies casts doubts on the entire framework.	0
authors selectively cite and compare sener et al. only in svhnmnist experiment in sec 5.2.3 but not in the office31 experiments in sec 5.2.2.	0
on the downside, all these experiments concern predictive (discriminative) problems.	0
in terms of experiments, it is shown that the system is more effective than others but not so much 'how' it achieves this efficiency.	0
the experiments lack comparisons except the human evaluation, while the loglikelihood improvement is marginal.	0
my concern with this paper is about the experiments that are only based on simulated agents, as it is the case for learning.	0
the digital search application is interesting, however a detailed description with comprehensive experiments are needed for the publication of an application paper.	0
 pros:  new module  good performances (not stateoftheart) cons:  additional experiments the paper is well motivated, and is purely experimental and proposes a new architecture.	2
what the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, i do not think that this paper is relevant for the iclr conference.	0
(on the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)	2
2. it says in the experiments part that the authors have used 3 different s_{attack} values, but they only present results for s_{attack} = 0.5. it would be nicer if they include results for all s_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the s_attack value change.	0
pros:  very promising results with an interesting active learning approach to multitask rl  a number of approaches developed for the basic idea  a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)  paper is overall well written/clear cons:  comparison only to a very basic baseline (i.e. uniform sampling) couldn't comparisons be made, in some way, to other multitask work?	2
cons: lack of experiments and empirical results supporting the arguments.	0
the idea is interesting, however the experiments in this paper is seriously lacking.	0
the experiments only compare to reed et al. (2017)/kalchbrenner et al. (2017), with finn at al. (2016) as a nonstochastic baseline, but no comparisons to, e.g., vondrick et al. (2016) are given.	0
pros  experiments show that the autoencoder helps improve classification accuracy for small training set sizes on the shape classification task.	0
finally, it would have been very interesting to see experiments on real data concerned with sets.	0
likelihood estimation on the omniglot dataset questions and concerns: the model appears novel and is interesting, the experiments, however, are lacking in that they do not compare against other any recently proposed memory augmented deep generative models [bornschein et al] and [li et.	0
the experiments are lacking and the results are not good enough.	0
3 can be misleading because the gradient is not backpropagated through all paths in the defined cost  “in our experiments we used f(x) = sgn(x) p |x|”: this makes sense to me for eq. 9 but why not use f(x) = x in eq. 10?	0
it has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same.	0
my main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture.	0
to conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments.	0
they do have some constraint that the fraction of changed words cannot differ by more than delta, but delta = 0.5 in the experiments, which is really large!	0
on the whole, i like the line of inquiry and the elegant simplicity of the proposed approach, but the paper has some flaws (and there are some gaps in both motivation and the experiments) that led me to assign a lower score.	0
quality i appreciate this line of research in general, but there are some flaws in its motivation and in the design of the experiments.	0
strengths  the proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.	2
my feeling from reading the paper is that it is rather incremental over cai et al. i am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant.	0
 in the experiments, the baseline is based on lr, but this would not be fair because usually we cannot expect any linear relationship for molecular fingerprints.	0
the research questions seems straightforward, but it is good to see those experiments review some interesting points.	0
weak points   i have a critical question for clarification in the experiments.	0
however, i believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments.	0
 the paper suggests that using the forward kldivergence is important, but this does not seem to be tested with experiments.	0
 the experiments are not constructed to support the motivation / claim, but just to show that model performance improves.	0
 the numerical experiments are encouraging but a bit short.	0
i would be less concerned about the above points if i found the experiments compelling.	0
cons: the provided experiments are weak to demonstrate the effectiveness of the proposed method.	0
i have some concerns about the architectures and experiments presented in the paper.	0
for architectures: the attentionbased model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta pixelcnn model is a standard pixelcnn trained with the maml approach by finn et al. for experiments: the imagenet flipping task is clearly tailored to the strengths of the attentionbased model, and the presentation of the general omniglot results could be improved.	2
pros:  novel/original proposal justified both theoretically and empirically  well written, easy to follow  limited evaluation on a classification and regression task is suggestive of the proposed approach's potential  efficient implementation cons:  related work, in particular the first paragraph, should compare and contrast with the closest extant work rather than merely list them  evaluation is limited; granted this is the nature of the target domain presentation: while the paper is generally written well, the paper appears to conflate the definition of the convolutional and correlation operators?	2
although the authors observe empirically that orthogonalization weights are roughly shared across different batches, the paper lacks a convincing argument for why this can happen.	0
their method makes deep networks robust to blackbox attacks, which was empirically demonstrated.	0
[strong points] ' proposed randomized whitebox attacks are empirically shown to be stronger than original ones. '	2
proposed ensemble adversarial training empirically achieves smaller error rate for blackbox attacks.	0
overall i think your intuitions and ideas are good, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.	0
a list of pros: 1. interesting connection to dc function 2. attempt to analyze generalization error 3. faster speed of convergence empirically a list of cons: 1. the contribution in posing the objective as a dc function looks limited as it is very straightforward.	2
in the case of existing blackbox attacks, the authors argue (convincingly) that the method is both flexible and empirically effective.	0
interesting questions do arise, such as how to assess the value of new data and how to price datapoints, but these questions are never addressed (neither theoretically nor empirically).	0
i wonder how applicable this in practice  i frankly didn't see insights here that i can apply to other problems that don't fit into this particular narrowly defined framework  i do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.	0
the method relates closely to prior work on actiondependent baselines, but explores in particular onpolicy fitting and a few other design choices that empirically improve the performance.	0
"examples include , ""attentionbased convolutional neural network for machine comprehension"", ""a parallelhierarchical model for machine comprehension on sparse data"", and ""coarsetofine question answering for long documents"" this paper does not compare to the above style of approach empirically, but the hierarchical approach seems to have more advantages and seems a more straightforward solution."	0
all in all i find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.	0
 the method is introduced for general exponential families, but a) not empirically evaluated for more than the gaussian case and b) not a complete algorithm for e.g. the bernoulli case.	0
they empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks.	0
pros: 1. the spectra of the hessians with different model sizes, input data distributions and algorithms are empirically studied, which provides some insights into the behavior of overparameterized neural networks.	2
cons: on the downside, the limitations exposed are done so empirically, but the underlying theoretical causes are not explored (although this could be potentially because this is hard to do).	0
however, publishing this work is in my opinion premature for the following reasons:  the authors do not provide further evidence of why nonsaturating gans perform better or under which mathematical conditions (nonsaturating) gans will be able to handle cases where distribution manifolds do not overlap;  the authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory;  the authors do not provide practical recommendations how to setup gans and not that these findings did not lead to a bulletproof recipe to train them.	0
ib seems interesting but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done.	0
as far as i know, this paper is however the first to empirically study the robustness of ensembles against adversarial examples.	0
i understand that empirically it works better but i don't get the intuition.	0
 sec 5.1: while the qualitative example is useful (with a bit more text), i believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std maml and possibly compare to a std bayesian inference method from the hb formulation of the problem (in the linear case)  sec 5.2: the abstract clams increased performance over maml but the empirical results do not seem to be significantly better than maml ?	0
this paper then compare this objective with the mmd distance between the samples a & b. it points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the mmd distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the mmd has been observed in the past to perform poorly for face generation [dziugaite et al. uai 2015]).	0
i understand that since the learner algorithm is an nn, this is not the case  but more explanation is necessary here  does your method also reduces the empirical possibility to get stuck in local minima?	0
that weakens the paper and suggests an important area of future work, but i think the empirical evidence is sufficient to show that there's something interesting going.	0
the method is poorly written, severely lacks algorithmic novelty, and the proposed approach shows no empirical gains over random mini batch sampling.	0
i do not think the method is theoretically wellmotivated as presented, but the empirical results seem solid.	0
i feel the ideas interesting and valuable especially in light of strong empirical results, but the authors should do more to clarify what is actually happening.	0
their method makes deep networks robust to blackbox attacks, which was empirically demonstrated.	0
[strong points] ' proposed randomized whitebox attacks are empirically shown to be stronger than original ones. '	2
proposed ensemble adversarial training empirically achieves smaller error rate for blackbox attacks.	0
concerning the first objective the empirical results do not provide meaningful support that the generative model is really effective.	0
cons: while the reviewer is essentially fine with the idea of the method, the reviewer is much less convinced of the empirical study.	0
the method meets a relevant set of criteria that no other method seems to meet, but arguments set forth in the story need some revision and the empirical evaluation needs improvement, especially with respect to model fidelity.	0
the recommendations were reasonable although lacking empirical support (or pointers to the literature), so i would take them somewhat carefully, more as the current 'group think' than ground truth.	0
however there is no direct comparison either theoretical or empirical against them.	0
my main concern is about the performance against existing methods (no empirical results are provided), and while it does provide interpretability, i am not sure that other approaches (e.g. tessler et al. 2017) could not be slightly modified to do the same.	0
), but fails to add theoretical or empirical knowledge that furthers the field.	0
from the empirical side, the authors compare the proposed optimizers on many datasets and models, but concerningly only using the baselines' default hyperparameters.	0
the proposed technique is reasonable on its own, but the empirical results do not come with any measure of statistical significance.	0
i have two concerns regarding the empirical section, which may be resolvable fairly quickly: 1) are the embedding vectors l2 normalized before using them in each task?	0
sigir09 pros: 1. an interesting rl formulation for query reformulation cons: 1. the use of rl is not properly justified 2. the empirical result is not convincing that the proposed method is indeed advantageous  after reading the author response and checking the revised paper, i'm both delighted and surprised that the authors improved the submission substantially and presented stronger results.	2
this wouldn't be so bad if they were justified by empirical success rather than principled design, but i'm also a bit skeptical of the strength of the results.	0
instead, i would recommend that in future versions of this document a single network, with a specific router and set of decisions, and with a single algorithm, will be explained with clear notation endtoend beyond the clarity issues, i suspect also that the novelty is minor (if the state does not include any information about the current output) and that the empirical baseline is lacking.	0
one aspect in which the paper is lacking is the empirical analysis.	0
the empirical results look good in comparison to the competing methods, but i suspsect an author of those competitors could find a way to make their own method look better in those plots, too.	0
my main concern is that the result here is purely empirical, with no concrete theoretical justification.	0
the performance stated by the authors of resne(x)t weakens the empirical results of lmarchitecture.	0
"i had been thinking (or perhaps ""hoping"" is a better word) that the term ""optimal"" implied maximal in terms of some quantifiable measure, but it's more of an empirical ""optimality""."	0
the method provides equivalent accuracy and sparsity to published stateoftheart results on these datasets but it is argue that learning sparsity during the training process will lead to significant speedups  this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations.	0
the method provides equivalent accuracy and sparsity to published stateoftheart results on these datasets but it is argue that learning sparsity during the training process will lead to significant speedups  this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations.	0
positive aspects of the paper: the paper is a very strong empirical paper, with experiments comparable to industrial scale.	2
the empirical results are promising and the ablation studies are good, but it also makes me wonder a bit about where the benefit is coming from.	0
 this reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements: 1) you should include in your comparison queryby bagging & boosting, which are two of the best outofthebox active learning strategies 2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones, but many questions are still unanswered:  would any 77 split work just as well (ie, crossvalidate over the 14 domains)  do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains?	0
the empirical results demonstrate that sab performs slightly better than tbptt for most tasks in terms of accuracy/ce, but there is no mention of comparing the memory requirements of each.	0
" ""this is entirely possible though the ensemble representation"" typo, through > through  that instance normalization causes noisy audio is an interesting empirical result, but i'm interested in a principled explanation of why this would happen."	0
overall, i like the idea, so i am leaning towards accepting the paper, but the empirical evaluations are not convincing.	0
the authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach.	0
the idea of this combination is not surprising but the attendee of iclr might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.	0
" other comments page 6: ""it does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for bayesian approaches and in this context our results are directly relevant."""	0
cons: 0. the whole paper just presented strategies and empirical results.	0
the paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other stateoftheart methods.	0
overall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.	0
– it's hard to pin down exactly what this means, but it sounds like you're making an empirical claim here: semantic information is more important than nonsemantic sources of variation (syntactic/lexical/morphological factors) in predicting the flow of a text.	0
 summary the paper is wellwritten but does not make deep technical contributions and does not present a comprehensive evaluation or highly insightful empirical results.	0
for the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but i’m not sure the empirical evidence provided for the mnist and cifar10 datasets is sufficient to judge whether or not the method does help with mode collapse.	0
i'd really like to see a strong theoretical and/or empirical justification for it, and both are lacking.	0
given the lack of any theory on this, an empirical analysis is certainly valuable.	0
i wonder how applicable this in practice  i frankly didn't see insights here that i can apply to other problems that don't fit into this particular narrowly defined framework  i do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.	0
the method relates closely to prior work on actiondependent baselines, but explores in particular onpolicy fitting and a few other design choices that empirically improve the performance.	0
pros: simple, effective method that appears readily available to be incorporated to any onpolicy pg methods without significantly increase in computational time good empirical evaluation cons: the name stein control variate seems misleading since the algorithm/method does not rely on derivation through stein’s identity etc. and does not inherit novel insights due to this derivation.	2
figure 1 is interesting but it could use better labelling (words instead of letters) overall: pros: wellwritten, good empirical results, wellmotivated and intuitively explained cons: not particularly novel, a modification of an existing idea, more sensitivity results would be nice	2
however the paper does not provide any empirical evidence that their approach actually works better than previous approaches to minibatch discrimination.	0
that is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists.	0
in the current form, this work is somewhere between a theoretical paper and an empirical one, however for a theoretical one it lacks strictness, while for empirical one  novelty.	0
consequently for a strong empirical paper i would expect much more baselines, including these proposed by krueger et al. pros:  empirical improvements shown on two different classes of problems.	2
summary: worthwhile empirical goal, but the paper could have been easily written using half as much space.	0
this sounds like it's making an empirical claim that your paper doesn't support, but it's so vague that it's hard to tell exactly what that claim is.	0
pros:  clearly written  good results  interesting empirical analysis between utc time and geolocation contribution:  triangulation based meshify vs quadtree meshification was claimed to be one of the major contributions.	2
"pros:  tackling a hard problem of overparametrised models, without introducing common unrealistic assumptions of activations independence  very nice result of ""phase change"" dependend on the size of hidden layer in section 7 cons:  simplification with nontrainable second layer is currently not well studied in the paper; and while not affecting expressive power  it is something that can change learning dynamics completely # after the update authors addressed my concerns by:  making simplification assumption clearer in the text  adding empirical evaluation without the assumption  weakening the assumptions i find these modifications satisfactory and rating has been updated accordingly."	2
there's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution.	0
these results are interesting, but given the empirical nature of this paper i would have liked to see results on more interesting datasets (celeba, cifar10, really anything but mnist).	0
these empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility.	0
though, the proposed extension to handle outofvocabulary items is a simple and straightforward string matching algorithm, but nonetheless it gives noticeable increase in empirical performance on both the tasks.	0
ultimately, the approach is interesting but there is not enough empirical evaluations.	0
the paper claims to explain many empirical facts, but it is not exactly clear which are the conspicuous and fundamental facts that need explaining.	0
all in all i find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.	0
my concerns are mostly regarding the empirical studies: 1. one of my main concern is on the empirical results in table 1. the disentanglement metric score for betavae is suspiciously lower than what’s reported in higgins et al., where they reported a 99.23% disentanglement metric score on 2d shape dataset.	0
pros: good empirical results.	2
there are no theoretical results regarding this question in the paper, and the empirical justification is also lacking.	0
the experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome.	0
pros:  provide theoretical guarantee for the use of orthogonal random features in the context of psrnn cons:  empirical evaluation only on small scale datasets.	2
my main concern is with lack of sufficient depth in empirical evaluation and analysis of the method.	0
my other concerns of this paper include: 1. it looks like the training data uses empirical ctr of (t,c) as ground truth.	0
however, the paper falls short in lacking of theoretical justification and convincing empirical results.	0
these sorts of ideas are explored in the empirical part of the paper, but i did not find the actual experiments in this section to be very compelling.	0
this is a useful result; however, given the lack of other outstanding theoretical or empirical results, it almost seems like this paper would be better shaped as a theory paper for a journal.	0
the dataset seems interesting but i find the empirical evaluations unconvincing.	0
some empirical observations are made, but it is not discussed whether what is observed is surprising in any way, or just as expected?	0
pros  a new approach to sparsifying that considers different thresholds for each layer  a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task.	0
you can make an argument for why the one that was ablated was “more interesting”, but really this is an obvious empirical question that should be addressed.	0
this paper does not make that much progress on the problem in general—the methods here are quite specific to words and to nli—and the proposed methods yields only yield large empirical gains in a reduceddata setting, but the paper serves as a wellexecuted proof of concept.	0
pros: authors provide some empirical evidence for the benefits of using their technique.	2
the empirical results themselves seem reasonable, but the results are not actually better than simpler methods in the corresponding tasks, the interpretation is less confident.	0
however, publishing this work is in my opinion premature for the following reasons:  the authors do not provide further evidence of why nonsaturating gans perform better or under which mathematical conditions (nonsaturating) gans will be able to handle cases where distribution manifolds do not overlap;  the authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory;  the authors do not provide practical recommendations how to setup gans and not that these findings did not lead to a bulletproof recipe to train them.	0
empirically, the paper strangely does not actually evaluate a parallel version of hyperband, but only evaluates the 5 parallel variants of sha that hyperband would run, each of them with all workers.	0
some parts of the paper lack clarity and the empirical results need improvement to support the claims (see details below).	0
an empirical comparison could be helpful but not provided.	0
i think that such a paper could have merits if it would really push the boundary of the feasible, but i do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines).	0
cons:  the softmax version of the algorithm  obtaining the best empirical study  is not backed by the theory.	0
the assumptions that large fourier peaks happen close to origin is probably welljustified from the empirical point of view, but it is a hack, not a well established wellgrounded theoretical method (the authors claim that in their experiments they found it easy to find informative peaks, even in hundreds of dimensions, but these experiments are limited to the svm setting, i have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features).	0
given the proximity of the proposed method to the baseline with regularised logistic regression, lack of empirical advantage is an issue.	0
"and given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that ""there is some rigor behind"" their conclusion that lenet is better than fc networks for classification on mnist because its empirical intrinsic dimensionality score is lower. '"	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
empirically, autostacker appears better than randomforest, but that is not a big feat.	0
pros:  automl is a topic of high importance to both academia and industry  good empirical results cons:  cascading is not new  unclear algorithm: what exactly does the hillclimber function do?	2
 1. this is an interesting paper  introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm 2. from the initial part of the paper, it seems that the proposed privynet is supposed to be a metalearning framework to split a dnn in order to improve privacy while maintaining a certain accuracy level 3. however, the main issue is that the metalearning mechanism is a bit adhoc and empirical  therefore not sure how seamless and userfriendly it will be in general, it seems it needs empirical studies for every new application  this basically involves generation of a pareto front and then choose paretooptimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures	0
with this in mind, my other concern is that their empirical analysis are only focused on a single task from the nlp domain (language modeling).	0
pros:  well written and easy to read paper  interesting theoretical guarantees of the approximation cons:  a bit incremental  weak empirical evaluations  no support for the claim of efficient gpu implementation == incremental == while the theoretical justification of the methods are interesting, these are not a contribution of the paper (but of previous work by mussmann et al.).	2
the empirical results here are interesting but not particularly striking; the most salient feature perhaps is that the architectures and training algorithms are perhaps a bit simpler but the overall improvements over existing methods are not too exciting.	0
footnote 2 states that this was due to empirical observations of instability but it also necessitates the importance weight correction during execution time.	0
this paper has many strengths: 1) the writing is clear, and the paper is wellmotivated 2) the proposed algorithm is described in excellent detail, which is essential to reproducibility 3) as stated previously, the approach is validated with a large number of real android projects 4) the fact that the language generated is nontrivial (javalike) is a substantial plus 5) good discussion of limitations overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.	2
this mismatch is especially strange, given the authors emphasize in the introduction that they provide bounds on divergences which are the same as used during the training (see 2nd paragraph on page 2)  here the bound is on w2, but the empirical gan actually does a regularized training (with constrained discriminator).	0
the empirical study itself is more limited and the paper suffers from a few mistakes and missing information, but to me the good points are enough to warrant publication of the paper in a good conference like iclr.	0
as they are mainly concerned by the l_1 version of the wasserstein distance, rather than focussing on the bias, the authors could consider the formulation in terms of inverse cumulative distribution functions in the 1d setup and the fact that the empirical cdf is never invertible: even if the theoretical cdf is invertible (which naturally guarantees uniqueness of the optimal transport) the underlying mass transportation problem is not as wellposed as that related to its statistical counterpart (however, smoothing the empirical distribution may remedy this issue).	0
either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf.	0
another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but i suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.	0
they significantly outperform vinyals et al. 2015 by sorting up to n = 120 uniform random numbers in [0, 1] with great accuracy < 0.01, as opposed to vinyals et al. who used a more complex recurrent neural network even for n = 15 and accuracy 0.9. the empirical study on jigsaw puzzles over mnist, celeba, imagenet gives good results on kendall tau, l1 and l2 losses, is slightly better than cruz et al. (arxiv 2017) for kendall tau on imagenet 3x3 but does not have a significant literature to compare against.	0
the ideas are still interesting, but the empirical results are less convincing.	0
thus i think that although this paper is written well, the theory is mostly recycled and the empirical results in section 4 are known; thus it is below acceptance threshold due to lack of novelty.	0
what the paper really lacks in my opinion is a closer analysis of 'why' the proposed approach works, i.e., a qualitative empirical analysis (toy experiment?)	0
cons: lack of experiments and empirical results supporting the arguments.	0
"i certainly agree that the study of sm models is ""interesting"", but what would make this valuable would be a more direct analogy, a direct explanation of some empirical phenomenon."	0
overall the idea appears to be useful but needs more empirical validation (affnist, imagenet, etc).	0
however, i believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments.	0
the paper is interesting but lacks strong empirical results.	0
overall, the paper's lack of sufficient convincing empirical support prevents me from recommending its acceptance.	0
overall, while i like the and think the goal is good, i think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning.	0
clarity my main complaint about this paper is clarity  it is not difficult to read per se, but it is difficult to fully grok the details of the approach and the experimental setup.	0
in fact, we do not know what is given as input and what is expected at the output some clues are given in the experimental setup, but not in the model description.	0
i am not an expert on gans for domain adaptation, and thus i can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper).	0
summary an interesting paper with novel theoretical ideas, but insufficient experimental validation.	0
the claim that lda works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true.	0
also several other methods have recently been proposed to improve stability of gans, however no experimental comparisons is made with these methods (wgan, egan, lsgan etc.)	0
cons: the experimental evaluation is not very thorough: no uncertainty of the mean is stated for any of the results.	0
 the main strengths of the paper are the supporting experimental results in comparison to plain feedforward networks (fnns).	2
while the paper is very well written, i have major concerns regarding the experimental evaluation and the amount of novelty.	0
positives:  the output kernel update is well justified  experimental results are encouraging negatives:  the methodological contribution of the paper is minimal  the proposed approach to maintain the budget is simplistic  no theoretical analysis of the proposed algorithm is provided  there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multiclass problem, so we would want to see comparisons with some good multiclass algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)	0
a second concern is the experimental exploration.	0
i can't quite judge how strong their experimental evaluations are, but i think that learning a neural programmerinterpreter from just inputoutput pairs using rl techniques is new and worth being pursued further.	0
the experimental performance is promising indeed, but a lot of domain knowledge is involved in the experiment design.	0
2012 in summary, i think this paper has a number of promising ideas and experimental results, but given the significant issues in clarity and significance to real world problems, i don't think that the current version of this paper is suitable for publication in iclr.	0
"c an the authors give experimental evidences for their claim: ""as such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a nongreedy...""  pros  the idea of the paper is clearly presented, the algorithm is easy to follow."	0
2. while choosing the distance metric in transformed space, lab is used, but for the experimental results, l_2 is measured in rgb space  showing the rgb distance is perhaps not all that useful given it's not actually being used in the objective.	0
overall it is an interesting but long paper, the claims are a bit strong for cnn and need further theoretical and experimental verification.	0
pros:  the proposed modification to vin is simple, wellmotivated, and addresses the nondifferentiability of vin  experiments on synthetic data demonstrate a significant improvement over the standard vin method cons:  some important references are missing (e.g., maxent ioc with deeplearned features)  although intuitive, more detailed justification could be provided for replacing the maxoveractions with an exponentiallyweighted average  no baselines are provided for the experiments with real data  all the experimental scenarios are fairly simple (2d gridworlds with discrete actions, 1channel input features) the proposed method is simple, wellmotivated, and addresses a real concern in vins, which is their nondifferentiability.	2
"i found it a bit odd that adam was not used as a point of comparison in section 5, that optimistic adam was only introduced and tested for cifar but not for the dna sequence problem, and that the discriminator was trained for 5 iterations in section 5 but only once in section 6, despite the fact that the reasoning provided in section 6 seems like it would have also applied for section 5. this gives the impression that the experimental results might have been at least slightly ""gamed""."	0
evaluation pros: the paper’s primary contribution is experimental: sota results are achieved for nearly every benchmark image dataset (the exception being statically binarized mnist, which is only .28 nats off).	2
it clearly suggests that the entropy regularized wasserstein_2 distance should be used in numerical experiments but this point is not supported by experimental results.	0
however, i found the technical description lacking key details and the experimental comparisons inadequate.	0
i think the broad idea of using ea as a substep within a monotonically improving free energy algorithm could be interesting, but needs far more experimental justification.	0
however the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method.	0
the 'significance' network was described as critical to the performance, but there is no experimental result to show the sensitivity of the model's performance with respect to the architecture of the 'significance' network.	0
on the other hand, i'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different submodels are learning.	0
 the authors propose to tackle the tree transduction learning problem using recursive nn architectures: the prediction of a node label is conditioned on the ancestors sequence and the nodes in the left sibling subtree (in a serialized order) pros:  they identify the issue of locality as important (sequential serialization distorts locality) and they move the architecture closer to the tree structure of the problem  the architecture proposed moves the bar forward in the tree processing field cons:  there is still a serialization step (depth first) that can potentially create sharp dips to null probabilities for marginal changes in the conditioning sequence (the issue is not addressed or commented by the authors)  the experimental setup lacks a perturbation test: rather than a copy task, it would be of greater interest to assess the capacity to recover from noise in the labels (as the noise magnitude increases)  a clearer and more articulated comparison of the pros/cons w.r.t.	2
as a summary, the paper contains nice ideas and experimental results are promising, but has nonnegligible mistakes in theoretical parts which degrade the contribution of the paper.	0
2. cheap soft unitary constraint 3. efficient cuda implementation (not experimentally verified) cons  1. some experimental setups are unfair, and some other could be clearer 2. only small scale experiments (although this factorization has huge potential on larger scale experiments) 3. no wallclock time that show the speed of the proposed parametrization.	2
 another claim that is made is that complex numbers are key, and again the argument is the need to span the entire space of unitary matrices, but the same comment still hold  that is not the space this work is really dealing with, and no experimental evidence is provided that using complex numbers was really needed.	0
 in the experimental section an emphasis is made as to how small the number of recurrent params are, but at the same time the input/output projections are very large, leaving the reader wondering if the workload simply shifted from the rnn to the projections.	0
3. the method claims to learn an internal representation of a denser reward function for the sparse reward problem, however the experimental analysis of this is pretty limited (section 5.3).	0
al.: image pivoting for learning multilingual multimodal representations, emnlp 2017 for a good set of references) 3) this omission of related work also weakens the experimental section.	0
the experimental results shown in this paper are clearly compelling in exposing the weaknesses of current seq2seq rnn models.	0
6. i think the experimental sections suffers from the following shortcomings: i. it does not substantiate all the claims made in the introduction ii.	0
it's not very surprising that adding more tasks and data improves performance on average across downstream tasks, but it is nice to see the experimental results in detail.	0
however overall i don’t think the experimental section is adequate to convince that the proposed models work better than a fair baseline vae.	0
overall, i have a couple of concerns about novelty as well as the experimental evaluation for the authors to address.	0
however, an issue in the writing, usage of external component and lack of experimental justification of the design choices hinder the clear understanding of the proposed model.	0
lack of experimental justification of the design choices the proposed recurrent unit contains various design choices such as separation of three different units (control unit, read unit and memory unit), attention based input processing and different memory updates stem from different motivations.	0
3. it seems that in the experimental results, is at most 2. is it because of the data or because of the lack of efficient training procedures for the hierarchical structure?	0
b) can convolutional flow be seen as faster but ´more restricted version of the lstm implemented inverse autoregressive flow (full lower triangular jacobian vs k off diagonal elements per row in convolutional normalizing flow) q2) i miss some more baselines in the experimental section.	0
i further have concerns about the experimental procedure, with the only the outcomes of single runs presented for each experiment and different experiments run for different numbers of iterations without justification.	0
pros:  promising experimental results cons:  method not described well  method appears adhoc	2
cons  the authors mention in the introduction that this encoding can speed up inference by allowing efficient parallel sparsetodense matrix conversion, and hence batch inference, but do not provide any experimental confirmation.	2
the improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results.	0
the paper is fine and the work is competently done, but the experimental results never quite come together.	0
 it is a matter of taste, but since all experimental results except the ones on the copy task are tabulated, one could think of adding a table with the results now contained in figure 3. relation to prior work: the authors are aware of most relevant work.	0
[strong points] ' based on experimental results over a broad range of datasets, deep network models and their attacks. '	2
overall, the paper only has a few interesting observations, but there is no good and detailed experimental analysis that help explain these observations.	0
while the revised version contains more experimental details, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context.	0
the experimental results show that relation networks outperform the other two baselines, but still ~30% behind human performance.	0
there is nothing wrong with the fundamental idea itself, but given the experimental results it just is not clear that it is working.	0
my main concern about the paper is novelty/importance and lack of experimental comparison to other techniques attacking similar complexity problems.	0
comparison between the proposed methods is nicely done but comparison to other literature pruning/compressing models is not experimentally shown.	0
 the tensor factorization setup ensures that the embedding dimensions are aligned  clustering by weights (4.1) is useful and seems coherent  covariatespecific analogies are a creative analysis cons:  problem setup not novel and existing approach not cited (experimental comparison needed)  interpretation of embedding dimensions as topics not convincing  connection to randwalk (aurora 2016) not stated precisely enough  quantitative results (table 1) too little detail: ' why is this metric appropriate? '	0
 the discussion on the dag graphical model is lacking experimental analysis (where separate baselines models are needed).	0
the paper contains some interesting experimental results, but unfortunately lacks concise motivation and description of the method and quality of writing.	0
there are several big table in the experimental session but barely mentioned in the text so i cannot understand what these table means.	0
a more systematic set of experiments could compare learning the proposed weightings on the first k layers of the network (for k={0, 1, …, n}) and learning independent weights for the latter nk layers, but i understand this would be a rather large experimental burden.	0
to summarize, the following are the pros of the paper:  clarity and good presentation;  good overview of the related literature;  extensive experimental comparison and good experimental results.	0
while the following are the cons:  the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;  lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;  lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);  the novelty of the approach is somewhat limited, although the idea of the extensive experimental comparison is good.	0
the simplenet approach is interesting but not sufficiently backed with experimental results.	0
maybe so, but we won’t know unless the experimental protocol prescribes a sufficient range of lrs for each architecture.	0
it looks like the dataset is useful but the model development and experimental sections are weak.	0
related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful.	0
it seems the game is easy from a reinforcement learning standpoint, and this is not necessarily a bad thing, but then the experimental study should be more rigorous in term of convergences, error bars, and baselines.	0
my main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty adhoc to me and there is a need for more experimental study to show how the methodology work.	0
pros: theoretical results on the convergence of ot/monge maps regularized formulation compatible with sgd cons experimental evaluation limited the large scale aspect lacks of thorough analysis the paper presents 2 contributions but at then end of the day, the development of each of them appears limited comments: the weak convergence results are interesting.	2
4) table 1 mentions fitnets, but cites ba & caruana (2014) instead of romero et al. (2015) 5) the “experimental trick” you mention for setting alpha and beta, seems to be just validation, comparing different settings and picking the one yielding the highest improvements.	0
i get the impression that the experimental work might be of decent quality, but the manuscript fails to convey important details of the method, of the experimental setup and in the interpretation of the results.	0
"the contribution of the paper is:  some proposed methods to extract a colorinvariant representation  an experimental evaluation of the methods on the cifar 10 dataset  a new dataset ""crashed cars""  evaluation of the best method from the cifar10 experiments on the new dataset pros:  the crashed cars dataset is interesting."	2
ultimately, though, i feel that there are two main issues that make this research feel as though it is still ultimately in the earlier stages: 1) the very large focus on the perspective that this approach is unifying modelbased and modelfree rl, when it fact this connection seems a bit tenuous; and 2) the rather lackluster experimental results, which show only marginal improvement over purely modelbased methods (at the cost of much additional complexity), and which make me wonder if there's an issue with their implementation of prior work (namely the highsight experience replay algorithm).	0
cons: the description on the experimental setup seems to lack some important details.	0
the technical approach seems well motivated, plausible, and potentially a good contribution, but the experimental work has numerous weaknesses which limit the significance of the work in current form.	0
so, due to the weaknesses in the experimental work, this work seems a bit premature.	0
 the paper is easy to read and well organized  the advantage of the proposed regularization against the more standard l2 regularization is clearly visible from the experiments  the idea per se is not new: there is a list of shallow learning methods for transfer learning based on the same l2 regularization choice [crossdomain video concept detection using adaptive svms, acm multimedia 2007] [learning categories from few examples with multi model knowledge transfer, pami 2014] [from n to n 1: multiclass transfer incremental learning, cvpr 2013] i believe this literature should be discussed in the related work section  it is true that the l2spfisher regularization was designed for lifelong learning cases with a fixed task, however, this solution seems to work quite well in the proposed experimental settings.	0
pros:  combining deep learning with methods from information geometry is an interesting direction for research  method is a generic dropin replacement for improving any seq2seq architecture  experimental results show modest performance improvements over vanilla seq2seq methods cons:  missing references for prior work combining information geometry and deep learning  insufficient explanation of the method  only experimental results are a nonstandard routefinding task  missing references and baselines for prior work on deep learning on graphs the general research direction of combining deep learning with methods from information geometry is an exciting and fertile area for interesting work.	2
cons: 1. experimental results are neither enough nor convincing.	0
in the experimental parts, the authors claim they use both relu and sigmoid function, but no comparisons are reflected in the figures.	0
on the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.	0
i have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them.	0
studying the relation between predictive coding and deep learning makes sense, but i do not come to the same (strong) conclusions as the author(s) by considering the experimental results  and i do not see evidence for a sophisticated latent representation learned by the network.	0
i am not saying that there is none, but i do not see how the presented experimental results show evidence for this.	0
pros: simple, interesting idea works well on toy problems, and able to prevent divergence in baird's counterexample cons: lacking in theoretical analysis or significant experimental results	2
pros: new theoretical study for dl algorithms focus on adversarial learning cons i find the contribution a bit limited some aspects have to be precised/more argumented experimental study could have been more complete comments:  'about the proposed framework.	2
i am finally very concerned with the experimental section.	0
my main concerns are related to the contribution of the paper and experimental pipeline followed to perform the comparison.	0
but, in the experimental section results are shown only for a single value, alpha_new=0.9 the authors also suggest early stopping but again (as far as i understand) only a single value for the number of iterations was tested.	0
in my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.	0
the clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)	0
overall, the combination of recurrent skip connections and attention appears to be novel, but experimental comparisons to other skip connection rnn architectures are missing and thus it is not clear how this work is positioned relative to previous related work.	0
a more reasonable cs style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.	0
overall this is a nice paper with very promising results, but i believe addressing some of the above weaknesses (with experimental results, where possible) would make it an excellent paper.	0
please cite some work in that case) (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at runtime, or speed of a single iteration, not necessarily speed to train) [as i am not primarly a neural net researcher, i had always thought pruning was done to decrease overfitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix] (4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (martingale or markov chain theory) this is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above).	0
the idea of this combination is not surprising but the attendee of iclr might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.	0
it certainly studies an important, somewhat neglected aspect of adversarial examples, but mostly speculatively and the experimental results study the resulting algorithm rather than trying trying the verify the hypotheses on which those algorithms are based upon.	0
finally the experimental results are okay but perhaps a bit preliminary.	0
"pros:  interesting tasks that combine imitation and reinforcement in a logical (but somewhat heuristic) way  good simulated results on a variety of pickandplace style problems  some initial attempt at realworld transfer that seems promising, but limited  related work is very detailed and i think many will find it to be a very valuable overview cons:  some of the claims (detailed below) are a bit excessive in my opinion  the paper would be better if it was scoped more narrowly  contribution is a bit incremental and somewhat heuristic  the experimental results are difficult to interpret in simulation  the realworld experimental results are not great  there are a couple of missing citations (but overall related work is great) detailed discussion of potential issues and constructive feedback: > ""our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved."""	2
the experimental part has some weaknesses.	0
in general, having no comparisons with other models proposed in the literature as improvements over gan such as acgan, infogan, wgan weakens the experimental section.	0
in general, having no comparisons with other models proposed in the literature as improvements over gan such as acgan, infogan, wgan weakens the experimental section.	0
pros:  experimental results are clearly presented  most details of experiments are explained  the basic outline of the paper explores an interesting question cons  many sentences do not have a clear point, and there are many spelling and grammar errors  despite claims to the contrary, the experiments are far from extensive; they explore two models, with variations in context for embeddings, with 3 optimizers, on just one task.	2
on the negative side, the experimental results are presented on only 1 experiment with a dataset and task made up by the authors.	0
 in this manuscript, the authors connect psychological experimental methods to understand how the black box of the mind solves problems with current issues in understanding how the black box of deep learning methods solves problems.	0
overall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper.	0
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
the experimental results presented in this paper are quite good, but both mnist and modelnet40 seem like simple / toyish datasets.	0
i am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.	0
this paper offers a promising direction for language modeling research, but would require more justification, or at least a more developed experimental section.	0
pros:  important starting question  thoughtprovoking approach  experimental gains on small data sets cons:  the link between the intuition and reality of the gains is not obvious  experiments limited to small data sets, some obvious questions remain	2
about the technique details, this paper is clearly written, but some experimental comparisons and claims are not very convincing.	0
as there are not enough experimental details to judge, it's hard to figure out the problem, but this ppaper is clearly not publishable at any of the quality machine learning venues, for weakness in originality, quality of the writing, and poor experiments.	0
2. there is a lack of experimental details.	0
pros: ' active learning may be used for improving the performance of deep models for ner in practice ' all the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for ner cons: ' the novelty of this paper is marginal.	2
well written and very clear, but somewhat lacking in the experimental or theoretical results.	0
while the technique is novel as far as i know (eq. (1) in particular), many details in the paper are poorly explained (i am unable to understand) and experimental results do not demonstrate that the problem targeted is actually alleviated.	0
my overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out.	0
pros: promising/interesting idea cons: not fully developed thin experimental section	2
the experimental results are not very strong, but the authors claim that they are initial experiments and can be improved with just a little more experimentation.	0
the experimental results show that it can learn something but it is difficult to know how good the number is.	0
although the idea seems incremental, the experimental results do seem solid.	0
2) clarification of the experimental section needed: the graph .mathcal{g} is not properly define but we manage to understand that it is depicted in figure 2 a), you should make that clearer.	0
## pros / strengths  effort to assess momentum / adam / other modern methods  effort to compare to previous experimental setups ## cons / limitations  lack of wallclock measurements in experiments  only ~2 models / datasets examined, so difficult to assess generalization  lack of discussion about distributed/asynchronous sgd ## significance many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community.	2
the concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts, which could be impactful.	0
 quality: even though the experimental results are compelling, the paper lacks thorough analysis in understanding the effects of the regularizer.	0
in other words  all the experiments in the paper follow the assumption made, if authors claim is that the restriction introduced does not matter, but make proofs too technical  at least experimental section should show this.	0
main concerns: 1) the experimental validation of the proposed approach is not consistent: the description of the baseline method is not detailed in the paper.	0
the proposed approaches are experimental but does not require human inspection.	0
even more concerning is the fact that each variant is presented and tested using a different network architecture and a different dataset, with no experimental comparison among these variants.	0
however i am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.	0
given the lack of comparison with these, it makes your experimental work lacking.	0
pros: ' its a good illustration of deepneural network machinery at work, and well put together ' the experimental results show it works very well ' good experimental work (different data sets, different algorithms) cons: ' sloppy mathematical presentation	2
i'd encourage the authors to do a more detailed experimental study with more tasks, but i can't recommend this paper's acceptance in its current form.	0
lack of experimental validation.	0
i am not so familiar with smiles strings  but could it be that the experimental success reported here is mainly a result of the very specific structure of valid smiles strings?	0
it is ok to form the hypothesis and present an interesting research direction, but in order to make this as a main point of the paper, the author should provide more rigorous arguments or experimental studies instead of jumping to the hypothesis in two sentences.	0
criticism: the experimental evaluation is rather solid, but not perfect.	0
exploring the use of syntax in neural translation is interesting but i am not convinced that this approach actually works based on the experimental results.	0
the notation did help make clear the later discussion of the experiments, but it was not clear to me that it was required in order to explain the experimental results.	0
pros: () the idea introduced is simple and flexible to be used for any cnn architecture () experiments on imagenet1k prove demonstrate its effectiveness cons: () experiments are not thorougly explained () novelty is extremely limited () some baselines missing the experimental section of the paper was rather confusing.	2
overall it's not clear what this paper adds to existing body of work: 1. axiomatic treatment takes a bulk of the paper, but does not motivate any significantly new method 2. from experimental evaluation it's not clear the results are better than existing work, ie yosinsky http://yosinski.com/deepvis	0
moreover it seems this axiom is even not used in the proof of theorem 2. concerning the experiments: the experimental setup, especially in section 3.3.1, is not well defined: on which layer of the network is the mask applied?	0
as this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors’ overall points.	0
second, the latter half of the paper is concerned with details of the experimental results, without offering any insights as to the implications for deep learning.	0
on the whole i appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.	0
the proposed approach is interesting, but i feel that the experimental section does not serve to show its merits for several reasons.	0
in addition to my concerns about the experimental evaluation, i have concerns about the general approach.	0
a large amount of the paper hinges on being able to ignore the second term in (6), and this fact is referred to many times, but the theoretical and experimental justification for this claim is very thin.	0
"in the paper, the author argued ""we propose and evaluate the minimal changes..."" but i think the these type of analysis also been covered by [1], figure 5. on the experimental side, to draw the conclusion, ""weighted sum"" is enough for lstm."	0
they seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over spens but they have essentially the same testtime complexity as simple feedforward models while having accuracy comparable to full inferencerequiring energybased models.	0
pros: 1) the general task of learning distributions over network weights is interesting 2) to my knowledge, the proposed approach is new cons: 1) experimental evaluation is very substandard.	2
the paper presents a number of interesting experiments and discussions about those experiments, but offers more exciting ideas about training neural nets than experimental successes.	0
pros  ' strong related work section that contextualizes this paper among current work ' very interesting idea to more efficiently find and train best architectures ' excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e. last paragraph of section 4.1, and last paragraph of section 4.2) ' publicly available code cons  ' some very strong experimental results contrasted with some mediocre results ' the balance of the paper seems off, using more text on experiments than the contributions to theory. '	2
overall this paper examines interesting structured and randomized low communication updates for distributed fl, but lacks some important experimental comparisons.	0
these experimental results are lacking.	0
pros: 1. very well written paper with good theoretical and experimental analysis.	2
 weaknesses:   how stable are the reported trends / languages across multiple runs within the same experimental setting?	0
in the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why.	0
however, i am concerned with the experimental results.	0
in summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.	0
overall: pros:  a nice idea with some novelty, based on a nontrivial observation  the experimental results how the idea holds some promise cons  the method is not presented clearly enough: the main component modeling the network activity is not explained (the hdda module used)  the results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)  experimental results are partial: results are not presented for multiple defenders, no ablation experiments after revision: some of my comments were addressed, and some were not.	2
pros: new theoretical analysis for multisource problem paper clear smoothed version is interesting cons learning bounds with worst case standpoint is probably not the best analysis for multisource learning experimental evaluation limited in the sense that similar algorithms in the literature are not compared extension a bit direct from the seminal work of bendavid et al. summary: this paper presents a multiple source domain adaptation approach based on adversarial learning.	2
that said, my main concerns regarding this paper are that: (1) there's not much new here, and, (2) the experimental setup may be flawed, in that it would seem model hyperparams were tuned for the proposed approach but not for the baselines; i elaborate on these concerns below.	0
in the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned.	0
4. the experimental part is ok to me, but not very impressive.	0
 the proposed approach is straight forward, experimental results are good, but don’t really push the state of the art.	0
however, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
the paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough. '''''''''''''''	0
3) cons: image captioning experiment: in the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines.	0
the experimental evaluation is thorough, to the degree allowed by the poorly defined task of generating images from random noise.	0
experimental results including these types of models could, in principle, demonstrate that autostacker is applicable, but the current work does not show this.	0
overall, the paper provides a very thorough experimental examination of a practical blackbox attack that can be deployed against realworld systems.	0
 after reading the authors's rebuttal i increased my score from a 7 to a 6. i do think the paper would benefit from experimental results, but agree with the authors that the theoretical results are nontrivial and interesting on their own merit.	0
"novelty: previous papers like ""betavae"" (higgins et al. 2017) and ""bayesian representation learning with oracle constraints"" by karaletsos et al (iclr 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does."	0
the main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis: none of the stateoftheart approaches are implemented here (rnn, basic models on w2v aggregations, ...).	0
the experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear.	0
similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyperparameter searches.	0
globally, you experimental part is rather weak, we would expect a stronger methodology, more experiments also with more difficult benchmarks (halfcheetah and the whole gym zoo ;)), more detailed analyses of the results, but to me the value of your paper is more didactical and conceptual than experimental, which i really appreciate, so i will support your paper despite these weaknesses.	0
3. experimental setup part is long but not wellexplained and is not selfcontained particularly for the evaluation metrics.	0
on the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.	0
in figure 3 experimental evidence is provided, but the results are not convincing given that the weight alignment only improves by ~1° throughout learning (compared to >45° in lillicrap et al., 2017).	0
"on page 5 the authors write: ""however we observe experimentally that the dynamics almost always converges."""	0
the paper is promising, but i have several questions: 1) one major concern is that the experimental results are only on mnist.	0
i find the idea to use the multihead attention very interesting, but one should consider the increase in number of parameters in the experimental section.	0
although the formal discussion is concerned with markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the ppd) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used).	0
the authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations.	2
quite heavy machinery to solve a fairly simple problem, but their approach is practical and effective experimentally (though the gain over the simple slerp heuristic is often marginal).	0
the article is however poorly written and it reflects a severe lack of scientific methodology : the problem statement is too vague and the experimental protocol is dubious.	0
another concern is the experimental evaluation.	0
 pros:  new module  good performances (not stateoftheart) cons:  additional experiments the paper is well motivated, and is purely experimental and proposes a new architecture.	2
"added: 20news results still poor for hpd, but its probably the implementation used ... their online variational algorithm only has advantages for large data sets pros: ' interesting new prod model with good results ' alternative ""deep"" approach to a hdllda model ' good(ish) experimental work cons: ' could do with a competitive nonparametric lda implementation added: good review responses generally"	2
"the experimental section is interesting, but in the end a bit disappointing: although a new artificial dataset is proposed to evaluate sets, it is unclear how different are the findings from those in the ""order matters"" paper:  the first set of results (in section 4.1) confirms that the set encoder is important (which was also in the other paper i believe)  the second set of results (section 4.2) shows that in some cases, an autoencoder is also useful: this is mostly the case when the supervised data is small compared to the availability of a much larger unsupervised data (of sets)."	0
overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method.	0
the experimental results seem promising, but the presentation can be improved.	0
the weakness of the proposed approach is the lack of explanation and investigation (experimental or theoretical) of why does noisy work so well.	0
the experimental results are somewhat limited but the overall framework looks appealing.	0
pros   interesting idea  reads well  fairly good experimental results cons   kenneni seems like it couldn't be realistically deployed  lack of an intermediate difficulty task	2
pros it is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task cons not very novel experimental evaluation is wanting the focus of this paper is on integrating perception and reasoning in a single system.	2
the idea for disentangling identity from other factors of variation using identitymatched image pairs is quite simple, but the experimental results on faces and shoes are impressive.	0
the experimental comparisons are lacking in some respect, as the comparison with softmax as a metric learning method seems uncommon, i.e. using the logits instead of the bottleneck layer.	0
 since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on cl conclusion:  i feel that the motivation is good, but the proposed model is too handcrafted.	0
 it is stated in the conclusion that the advantage actorcritic used is beneficial, however no experimental comparison is shown.	0
the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.	0
"() pros / cons:  simple yet powerful method for text classification  strong experimental results  ablation study / analysis of influence of parameters  writing of the paper  missing discussion to the ""attention is all you need paper"", which seems highly relevant () typos: page 1 ""a support vectors machines"" > ""a support vector machine"" ""performs good"" > ""performs well"" ""the ngrams was widely"" > ""ngrams were widely"" ""to apply large region size"" > ""to apply to large region size"" ""are trained separately"" > ""do not share parameters"" page 2 ""convolutional neural networks(cnn)"" > ""convolutional neural networks (cnn)"" ""related works"" > ""related work"" ""effective in wang and manning"" > ""effective by wang and manning"" ""applied on text classification"" > ""applied to text classification"" ""shard(word independent)"" > ""shard (word independent)"" page 3 ""can be treat"" > ""can be treated"" ""fixed length continues subsequence"" > ""fixed length contiguous subsequence"" ""w_i stands for the"" > ""w_i standing for the"" ""which both the unit"" > ""where both the unit"" ""in vocabulary"" > ""in the vocabulary"" etc..."	0
however, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions.	0
cons:  the experimental evaluation is limited.	0
overall, i am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance.	0
the experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy.	0
moreover, authors also propose isru which is similar to tanh for rnn, but do not provide any experimental results.	0
overall, i think the problem is interesting but the technical description and the evaluation can be improved.	0
so far this approach seems more or less correct but in this case i would argue that subject b should not be considered for evaluation since its data is heavily used for hyperparameter optimization and the results obtained on this subject are at risk of being biased.	0
the paper is wellstructured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel.	0
evaluation pros: giving the vae discriminative capabilities is an interesting line of research, and this paper provides another take on treebased vaes, which are challenging to define given the discrete nature of the former and continuous nature of the latter.	2
pros:  novel/original proposal justified both theoretically and empirically  well written, easy to follow  limited evaluation on a classification and regression task is suggestive of the proposed approach's potential  efficient implementation cons:  related work, in particular the first paragraph, should compare and contrast with the closest extant work rather than merely list them  evaluation is limited; granted this is the nature of the target domain presentation: while the paper is generally written well, the paper appears to conflate the definition of the convolutional and correlation operators?	2
in summary, i'm both excited about the dataset and new architecture, but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset.	0
on one hand, the authors have introduced me to literature i was not aware of, but on the other hand, their actual novel contribution is a rather straightforward adaptation of ideas in the literature to policy gradients (that could be formalized in a more technically precise way) with an evaluation on a single type of game.	0
cons: the experimental evaluation is not very thorough: no uncertainty of the mean is stated for any of the results.	0
while the paper is very well written, i have major concerns regarding the experimental evaluation and the amount of novelty.	0
paper weaknesses:  the evaluation of the model is not great: (1) it would be interesting to combine bedroom and kitchen images and train jointly to see what it learns.	0
pros several interesting ideas for evaluating evaluation metrics are proposed the authors tackle a very challenging subject cons it is not clear why gans are the only generative model considered unprecedented visual quality as compared to other generative models has brought the gan to prominence and yet this is not really a big factor in this paper.	2
the use of learned representations needs more rigorous justification the evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for “discriminativeness” and seems like something that can be gamed.	0
 the authors implicitly contradict the argument of theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported.	0
however, my main concern is the evaluation.	0
i can't quite judge how strong their experimental evaluations are, but i think that learning a neural programmerinterpreter from just inputoutput pairs using rl techniques is new and worth being pursued further.	0
presentational comments (these don’t affect my evaluation, they’re mostly observations but they contribute to a general feeling that the paper is rushed and preliminary): ' bpe does not “learn”, it’s entirely deterministic. '	0
 interesting problem, but limited novelty and flawed evaluation the paper considers the problem of following natural language instructions given an firstperson view of an a priori unknown environment.	0
overall, this paper constitutes a reasonable firstpass at this problem, but there is significant room for improvement to address issues related to the stated contributions and flawed evaluations.	0
there are important shortcomings with the evaluation.	0
of course, the paper also invents some new evaluation metrics and then applies them on benchmark datasets, but this content only appears much later in the paper (well after the soft 8 page limit) and i admittedly did not read it all carefully.	0
the authors did a great job on the simulation framework, but table 1 falls short in terms of evaluation metric: to properly assess the performance of the method on simulated data, it would be good to have evidence that the type 1 error is calibrated (e.g. by means of qq plots vs null distribution) for all methods.	0
one idea for evaluation: comparison with ground truth makes sense for pnl, but not so much for general nonlinear because of unidentifiability.	0
final evaluation  i would like to see this idea published, but not in its current form.	0
the method meets a relevant set of criteria that no other method seems to meet, but arguments set forth in the story need some revision and the empirical evaluation needs improvement, especially with respect to model fidelity.	0
on the downside, the quantitative evaluation of method does not seem very thorough and the approach seems quite heuristical at times.	0
my two main criticisms are as follows 1. the evaluation of the method is generally subjective without clear use of baselines or demonstration of what would do in the absence of this work  it seems like it works, but i feel like i have a very poor grasp of relative gains.	0
i think a large part of this is my own unfamiliarity with the literature, but i also think that space has been prioritized to showing off the qualitative results at the expense of more careful description of the approach and the evaluation methods.	0
overall, i have a couple of concerns about novelty as well as the experimental evaluation for the authors to address.	0
the paper addresses an important problem, but does not seem ready for publication:  the evaluation only uses simulated data.	0
"also on p.6 i'm not entirely clear on how the ""network reduction"" is performed  it looks like finer scales are progressively dropped in successive blocks, but i don't think they exactly correspond to those that would be needed to evaluate the full model (this is ""lazy evaluation"")."	0
i think their method and their evaluation has some major weaknesses, but i think that it still provides a good baseline to force work in this space towards tasks which can not be solved by simpler models like this.	0
overall, i believe the idea is nice, and the initial analysis is good, but i think the evaluation, especially against other methods, needs to be stronger.	0
however, the evaluation metrics and procedure are poorly explained; what are adjusted mutual information (ami) and normalized mutual information (nmi)?	0
"weaknesses ' the title is misleading; ""blackbox function evaluation"" does not suggest what is intended, which is training on functionevaluation equations."	0
"weaknesses p2: it is mentioned that the framework is the first to combine symbolic expressions with blackbox function evaluations, but i would argue that neural programmerinterpreters (npi; reed & de freitas) are already doing that (see fig 1 in that paper where the execution trace is a symbolic expression and some expressions ""act(left)"" are blackbox function applications directly changing the image)."	0
as far as i understand, the treelstm learns to the return value of function evaluation expressions in order to predict equality of equations, but this should be clarified.	0
"p8 fig 4b; p9: relating to the question regarding plateaus in the function evaluation: ""in figure 4b […] the top prediction (0.28) is the correct value for tan with precision 2, but even other predictions are quite close"" – they are all the same and this bad, right?"	0
cons: 1. the authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction).	0
my major concern is with the experiments and evaluation.	0
significance: i think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published.	0
i am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3way outlier detection).	0
if the goal is to use summarization as an extrinsic evaluation of sentence embedding models, there needs to be better justification of this is a good idea when there are so many other issues in content selection that are not due to sentence embedding quality, but which affect summarization results.	0
many recent sentence embedding models are missed such as those from lin et al. (2017), gan et al. (2017), conneau et al. (2017), jernite et al. (2017) etc. the evaluation and discussion sections were mostly unclear and the results of poorly performing methods were not reported at all making the comparisons and arguments difficult to comprehend.	0
however, there are several issues both in the approach and the current preliminary evaluation, which unfortunately leads me to a reject score, but the general idea of combining different specifications is quite promising.	0
related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful.	0
" : there is no endtoend evaluation, except figure 7 that is not referenced in the text, that that has a weird evaluation protocol (""the probability of finding the correct nearest neighbor conditioned on the model being correct"")  : the evaluation context is supervised hashing, but the evaluation is flawed: when the same classes are used for evaluation as for training, there is a trivial encoding that consists in encoding the classifier output (see ""how should we evaluate supervised hashing?"""	0
◦ it is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to nonoptimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 ◦ fig 4. right looks like a reward signal, but is labelled proportion correct.	0
pros: theoretical results on the convergence of ot/monge maps regularized formulation compatible with sgd cons experimental evaluation limited the large scale aspect lacks of thorough analysis the paper presents 2 contributions but at then end of the day, the development of each of them appears limited comments: the weak convergence results are interesting.	2
"the contribution of the paper is:  some proposed methods to extract a colorinvariant representation  an experimental evaluation of the methods on the cifar 10 dataset  a new dataset ""crashed cars""  evaluation of the best method from the cifar10 experiments on the new dataset pros:  the crashed cars dataset is interesting."	2
cons: the authors claim they can learn tftf interactions and it is one of the main biological contributions, but there is no evidence of why (beyond very preliminary evaluation using the trrust database).	0
however, i think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.	0
it would be useful to have such numbers to make a better evaluation of the bnn performance in the blackbox attack setting.	0
"cons  none typos  1. section 2, paragraph 3 : ""is given in figure 1"" > ""is given in algorithm 1"" note  since i'm not familiar with the differential privacy literature, i'm flexible with my evaluation based on what other reviewers with more expertise have to say."	2
5. in section 5.1 (number of permutations), why is “test accuracy” adopted but not “auc” (which is the evaluation metric in section 4.1)?	0
the clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)	0
visual question answering is mentioned several times in the paper, however no evaluations are done in this task.	0
some quantitative evaluation is presented for texture synthesis and key invariance, but the main results seem to be qualitative analysis and examples included as supplemental material.	0
i liked the idea of the rhythm evaluation, but again, i have some questions about the specific implementation.	0
i have a few questions about the evaluation, but most of my comments are about presentation.	0
this model is a relatively simple extension of the neural statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation.	0
(1) one concern i have for this paper is about the evaluation.	0
they seem to optimize over this parameter, but this requires access to the out of distribution set prior to the final evaluation.	0
authors claim to have an efficient implementation but the paper lacks a proper quantitative evaluation.	0
 this is a minor point and did not have any impact on the evaluation but vae > vhe, reparameterization trick > resampling trick.	0
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
the evaluation domains are relatively simple, but it was nice to see that the authors also make an attempt to investigate the qualitative behaviour of their method.	0
the paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other stateoftheart methods.	0
overall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.	0
i am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.	0
the idea is interesting but the evaluation leaves doubts.	0
 summary the paper is wellwritten but does not make deep technical contributions and does not present a comprehensive evaluation or highly insightful empirical results.	0
2.2 the noise evaluation in section 5.3 is nice, but not related with the section 4. this is problematic because, it is not clear if the focus of the paper is on evaluating madrl and performance on the ms.pacman task, or experimentally demonstrating claims in section 4. recommendations: 1. shorten the paper to be within (or close to the recommended length) including appendix.	0
while the paper is very wellwritten i have certain concerns regarding the motivation, model, and evaluation methodology followed: 1) a stronger motivation for this model is required.	0
this would perhaps make a reasonable robotics paper if it had a realworld evaluation and if the claims were scoped more realistically, but asis, i don't think this work is ready for publication.	0
the paper discusses how you would use the model for planning (that you can learn v(h) and p(a|h)) and says this was done in section 3.3, but there does not seem to be any evaluation of this.	0
the lack of evaluation within a larger planning framework somewhat diminishes the impact as it is unclear how well this approach would work in practice.	0
however, given the weaknesses in the evaluation of this approach, it is difficult to say how significant the work is.	0
however i am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain).	0
pros: simple, effective method that appears readily available to be incorporated to any onpolicy pg methods without significantly increase in computational time good empirical evaluation cons: the name stein control variate seems misleading since the algorithm/method does not rely on derivation through stein’s identity etc. and does not inherit novel insights due to this derivation.	2
the main drawback of the paper is the lack of evaluation.	0
"pros:  tackling a hard problem of overparametrised models, without introducing common unrealistic assumptions of activations independence  very nice result of ""phase change"" dependend on the size of hidden layer in section 7 cons:  simplification with nontrainable second layer is currently not well studied in the paper; and while not affecting expressive power  it is something that can change learning dynamics completely # after the update authors addressed my concerns by:  making simplification assumption clearer in the text  adding empirical evaluation without the assumption  weakening the assumptions i find these modifications satisfactory and rating has been updated accordingly."	2
there's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution.	0
figure 3: this can be inferred from the text (i think), but i had to remind myself that “iw train” and “iw test” refer only to the evaluation procedure, not the training procedure.	0
i'm concerned since the evaluation is only done on 100 points. '	0
the quantitative evaluations seem promising, though difficult to interpret fully due to a lack of provided detail (see below).	0
many ideas are presented with some abstract motivation, but there is no comparative evaluation to demonstrate what happens when any one piece is removed from the system.	0
granted, being able to infer hidden states is of course an important problem, but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win starcraft competitions, meaning that, at least as the paper is currently framed, the critical evaluation metric would be showing that a defogger helps to win games.	0
criticism: the experimental evaluation is rather solid, but not perfect.	0
unfortunately, while the idea has merit, and i'd like to see it pursued, the paper suffers from a fatal lack of validation/evaluation, which is very curious, given the amount of data that was collected, the fact that the authors have both a training and a test set, and that there are several natural ways such an evaluation might be performed.	0
the two examples of fig 3 and the additional four examples in the appendix are nice for demonstrating some specific successes or weaknesses of the model, but they are in no way sufficient for evaluation of the system, to demonstrate its accuracy or value in general.	0
i am not very familiar with this type of systems, but it is clear to me that the evaluation is biased and does not prove the working hypothesis of the authors.	0
pros:  provide theoretical guarantee for the use of orthogonal random features in the context of psrnn cons:  empirical evaluation only on small scale datasets.	2
the lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work.	0
pros: it gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses, since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree.	2
overall it's not clear what this paper adds to existing body of work: 1. axiomatic treatment takes a bulk of the paper, but does not motivate any significantly new method 2. from experimental evaluation it's not clear the results are better than existing work, ie yosinsky http://yosinski.com/deepvis	0
my main concern is with lack of sufficient depth in empirical evaluation and analysis of the method.	0
cons: 1. the evaluation is performed only on a single environment in a restricted fashion.	0
in some cases the results are similar to pixtopix (also in the numerical evaluation) but the method allows for onetomany image generation, which is a important contribution.	0
pros:  wellmotivated and ambitious goals  human evaluation conducted on the outputs.	2
in addition to my concerns about the experimental evaluation, i have concerns about the general approach.	0
weaknesses ' the discussion of evaluation of the model is weak.	0
pros: 1) the general task of learning distributions over network weights is interesting 2) to my knowledge, the proposed approach is new cons: 1) experimental evaluation is very substandard.	2
the author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality.	0
pros: 1. the paper is wellwritten, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community.	2
i will be more than happy to modify my evaluation if these concerns are addressed by the authors.	0
"considering that their paper is titled as a work to use ""dependencies"" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation."	0
i think that such a paper could have merits if it would really push the boundary of the feasible, but i do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines).	0
i think probably the issue is the lack of good evaluation methods for generative models.	0
pros: new theoretical analysis for multisource problem paper clear smoothed version is interesting cons learning bounds with worst case standpoint is probably not the best analysis for multisource learning experimental evaluation limited in the sense that similar algorithms in the literature are not compared extension a bit direct from the seminal work of bendavid et al. summary: this paper presents a multiple source domain adaptation approach based on adversarial learning.	2
but one concern that i have is regarding the evaluation metrics used for it.	0
pros:  efficient model  proposed architecture is general enough to be useful for other sequencetosequence problems cons:  evaluation metrics for the morphological agreement task are unsatisfactory  it would appear that the baselines could be improved further using standard techniques	2
however, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.	0
the authors put lots of effort in the evaluation, but still:  it is unclear what is the average quality of the samples  a human study might help  it is unclear to which extent the images are copied from the training set.	0
the experimental evaluation is thorough, to the degree allowed by the poorly defined task of generating images from random noise.	0
my main concerns with this paper are novelty, reproducibility and evaluation. '	0
2) pros:  new quantitative evaluation criteria based on motion accuracy.	2
additionally, the authors present evaluation based on psnr and ssim on the overall predicted video, but not in a perstep paradigm.	0
"i note that the term ""hillclimber"" would suggest that some sort of improvement has to be made in each step, but the algorithm description does not show any evaluation step at this point."	0
i think that the fundamental issue of stochasticity of concern for repeatability and generalisability of these performance evaluation exercises is not in the stochastic optimisation search but in the use of a single holdout sample.	0
pros:  well written and easy to read paper  interesting theoretical guarantees of the approximation cons:  a bit incremental  weak empirical evaluations  no support for the claim of efficient gpu implementation == incremental == while the theoretical justification of the methods are interesting, these are not a contribution of the paper (but of previous work by mussmann et al.).	2
the large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising.	0
as far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to soa methods, but it is not really clear whether the outperformance is of statistical significance.	0
3. experimental setup part is long but not wellexplained and is not selfcontained particularly for the evaluation metrics.	0
however, i would have preferred to see more quantitative evaluation and less qualitative evaluation, but i understand that doing so is challenging in this domain.	0
in other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.	0
originality  4 contributions significance  important problem, very active area of research  comparison to very recent algorithms  but no ppo in the evaluation	0
from algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used.	0
my main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.	0
the authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations.	2
these are pretty images, but the evaluation criteria is unclear.	0
however, there are several weaknesses existed: 1. there is no quantitative evaluation and comparisons.	0
another concern is the experimental evaluation.	0
the experiments lack comparisons except the human evaluation, while the loglikelihood improvement is marginal.	0
i find it somewhat surprising that the proposed model is only slightly better in terms of loglikelihood than a pixelrnn, but much better in terms of human evaluation – given that both models were optimized for loglikelihood.	0
while many gan papers do not place an emphasis on quantitative evaluations, at this point, i consider the complete lack of such an evaluation as a weakness of the paper.	0
i would however strongly encourage the authors to pursue this research further: it seems very promising, and i think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!	0
), but considering that there is a considerable body of work on this problem, the paper provides not comparative evaluation of the proposed approach to published ones in the literature.	0
the results from mnist and celeba datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward mmd metric and potentially discuss their pros and cons.	0
pros it is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task cons not very novel experimental evaluation is wanting the focus of this paper is on integrating perception and reasoning in a single system.	2
the authors acknowledge this but do not offer ways to tackle the issue c) conditioning on the label histogram should make the problem easy: one is giving away the number of nodes and the label identities after all; however even in this setup the approach fails more often than not d) the graph matching procedure proposed is a rough patch for a much deeper problem  the evaluation should include a measure of the capacity of the architecture to : a) reconstruct perfectly the input b) denoise perturbations over node labels and additional/missing edges	0
the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.	0
weaknesses  i think this paper is missing some important evaluation.	0
cons:  the experimental evaluation is limited.	0
overall, i am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance.	0
i find the paper lacking on the evaluation front.	0
pros: 1. an interesting framework for babi qa by encoding sentence to ngrams cons: 1. the overall justification is somewhat unclear 2. the approach could be overengineered for a special, lengthy version of babi and it lacks evaluation using realworld data	2
i am very disappointed in the authors' choice of evaluation, namely babi  a toy, synthetic task long abandoned by the nlp community because of its lack of practicality.	0
with sufficient knowledge of related works from these areas, i find that the authors' proposed method lacks proper evaluation and sufficient novelty.	0
this may be sufficient for a first proofofconcept but a comparison against standard benchmark methods and datasets for semantic segmentation is missing.	0
 the paper provides an interesting data collection scheme that improves upon standard collection of static databases that have multiple shortcomings  end of section 3 clearly summarizes the advantages of the proposed algorithm.	0
"the authors repeatedly emphasize the ""collaborative"" aspect of mtd, saying that the turkers have to collaborate to produce similar dataset distributions, but this is misleading because the turkers don't get to see other datasets."	0
the authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing theorems 1 and 2. my current understanding is that, in practice, when the birthday paradox test gives a collision i have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity.	0
cons: 1) coreference eval: no details are provided for how the data was annotated for the coreference task.	0
so far this approach seems more or less correct but in this case i would argue that subject b should not be considered for evaluation since its data is heavily used for hyperparameter optimization and the results obtained on this subject are at risk of being biased.	0
the student model learns according to a standard stochastic gradient descent technique (adam for mlp and cnn, momentumsgd for rnn), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model.	0
cons:  i am wondering whether the dataset contains biases regarding (dx, dy).	0
… will produce samples from a distribution whose marginals along each of the projections w_k match those of the true distribution”.. i presume an infinite number of generator distributions could give rise to the correct marginals however not necessarily be converged to the data distribution.	0
the proposed celebroom dataset (a 50/50 mixture of celeba and lsun bedrooms) is a good dataset to validate the problem on but it is disappointing that the authors do not actually scale their method to their motivating example.	0
while this is definitely not the optimal way to approach the problem it is meant as a comment on the nonintuitive and poorly characterized properties of complex high dimensional data manifolds.	0
similar to memorygan, infogan also did not need any data annotations, but could learn the latent code flexibly.	0
i know it's hard to evaluate a gan model but i think the authors can at least show the nearest neighbors in the training dataset and the training data that maximizes the activation of the corresponding memory slot together with the generated samples to see the difference.	0
this probably does not match the assumption of many of the datasets being tested upon (cifar, mnist) but i don't consider that a fundamental issue.	0
from a methodology pointofview, the paper has limited novelty (transport operators, and learning thereof has been studied elsewhere), but there are some technical insights (likelihood model, use in data augmentation).	0
this is a very suitable assumption, e.g., for the swiss roll data set, but it is unclear to this reader why it is a suitable assumption beyond such toy data.	0
one could construct synthetic examples with data that are trivially separable (easy) but require unrelated or orthogonal classifiers.	0
"(iii) a task could be very ""difficult"" in the sense of high loss, but it could still be perfectly learned in the sense of finding the ideal ""groundtruth” classifier, but for a dataset that is highly nonseparable in the provided featurespace."	0
that said i found the intuitive story a little bit difficult to follow  it's true that in figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem  the gradients should still pull the generator 'towards' the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finergrained details.	0
"from the second paragraph alone: ""impede their wider applications in new data domain"" > domains ""extreme collapse and heavily oscillation"" > heavy oscillation ""modes of real data distribution"" > modes of the real data distribution ""while d fails to exploit the failure to provide better training signal to g"" > should be ""this failure"" to refer to the previouslydescribed generator mode collapse, or rewrite entirely ""even when they are their jensenshannon divergence"" > even when their jensenshannon divergence i'm sympathetic to the authors who are presumably nonnative english speakers; many good papers contain mistakes, but in my opinion the level in this paper goes beyond what is appropriate for published work."	0
"they have no choice but to have assumptions, because they want to abstract away the ""data"" part of the analysis while still being able to use certain properties about the rank of the features at certain layers."	0
"that being said, it's easy to voice such concerns, and i'm willing to believe that there might not exist a simple way to derive the same results with an approach more along the line of ""whatever your data, pick whatever small epsilon, and you can always have the desired properties by perturbing your data by that small epsilon in a random direction""."	0
in particular, the result of spectral clustering on the reuters database is not reported, but one could use other scalable versions of spectral clustering as a baseline.	0
unless the generator overfits the training data, but then it would not generalize.	0
i think that having less number of parameters is a good thing in this setting as the data is scarce, however i would like to see a more indepth comparison with respect to the number of features produced by the model itself.	0
pros:  the proposed modification to vin is simple, wellmotivated, and addresses the nondifferentiability of vin  experiments on synthetic data demonstrate a significant improvement over the standard vin method cons:  some important references are missing (e.g., maxent ioc with deeplearned features)  although intuitive, more detailed justification could be provided for replacing the maxoveractions with an exponentiallyweighted average  no baselines are provided for the experiments with real data  all the experimental scenarios are fairly simple (2d gridworlds with discrete actions, 1channel input features) the proposed method is simple, wellmotivated, and addresses a real concern in vins, which is their nondifferentiability.	2
although that experiment shows a promising, significant advantage over vin, the lack of baselines for the experiment with real data is disappointing.	0
worth noting that recently, (shwartzziv and tishby) demonstrated, not on largescale datasets but on small ones, that an optimal representation for a classification task must reduce as much uninformative variability as possible while maximizing the mutual information between the desired output and its representation in order discriminate as much as possible between classes.	0
the paper claims that the fusion method realizes a 'minimalistic' representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn't clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?). '	0
it feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it’s hard to imagine datasets that have exact but unknown matches (i welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.	0
the results shown in figure 2 don’t convince me, not just because they are qualitative and few, but also because i’m not sure i even agree that the proposed method is producing better results: for example, the discogan results have some artifacts but capture the texture better in row 3. i was also not convinced by the supervised second step in section 4.3. given that the first step achieves 97% alignment accuracy, it’s no surprised that running an offtheshelf supervised method on top of this will match the performance of running on 100% correct data.	0
pros:  the proposed idea is simple and easy to implement  the results show improvement in terms of visual quality cons:  i agree that the proposed prior should better capture the data distribution.	2
cons  1. it would be nice to have another experiment not based on text (speech recognition / synthesis, audio, biological signals, ...) to see how it generalizes to other kind of data (although i can't see why it wouldn't).	2
the authors conduct experiments on several real but relative smallscale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets.	0
 a representation learning framework from unsupervised data, based not on autoencoding (x in, x out), but on neighborencoding (x in, n(x) out, where n(.)	0
all in all, i do like the idea as a concept but i am wary about its applicability to real data where defining a good neighborhood metric might be a major challenge of its own.	0
the proposed algorithm assumes the gaussian likelihood with homoscedastic noise, but this is not the case for many realworld data (mnist and color images are usually modelled with bernoulli likelihood).	0
however, as an an applications paper, the bread of experiments are a little bit lacking  with only that one potentially interesting dataset, which happens to proprietary.	0
this is presented in the same figure and paragraph as the cifar results, but instead uses a single synthetic data point in dimension 5, and k=1.	0
the authors did a great job on the simulation framework, but table 1 falls short in terms of evaluation metric: to properly assess the performance of the method on simulated data, it would be good to have evidence that the type 1 error is calibrated (e.g. by means of qq plots vs null distribution) for all methods.	0
the paper essentially introduces a method to use offpolicy data, which is of course important, but does not cover the important scenario where we only have access to (state,action) pairs given by an expert.	0
the models use the same training data but are differentiated by a differente parameter initialization and by training on differently drawn minibatches.	0
i understand that this is perhaps on a different data split into m1/2 but why is there such a drastic difference?	0
through a series of experiments and a newly defined dataset, it exposes the shortcomings of current seq2seq rnn architectures.	0
this is much more supported by the data, but still i think very particular to this situation.	0
pros:  providing better parallelism opportunities for convolutional neural networks  simple approach to finding optimal global configurations that seems to work well  positive results with significant speedups across 3 different networks cons:  unclear if speedups hold on newer devices  useful to see how this scales across more than 1 machine  claim on overlapping computation with data transfer seems incorrect.	2
it's not very surprising that adding more tasks and data improves performance on average across downstream tasks, but it is nice to see the experimental results in detail.	0
i suspect that most of the embeddings included in the table also have many oovs in the rw dataset but still compute results on it using either an unknown word embedding or some baseline similarity of zero for pairs with an oov.	0
 summary of the reviews: pros: • a novel way to evaluate the quality of heterogeneous data sources.	2
cons: • since the data quality is a function of local variation, it is unclear about the advantage of the proposed data quality regularization versus using a simple moving average regularization or local smoothness regularization.	0
weaknesses: — given that the performance of stateonart on clevr dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models.	0
3. it seems that in the experimental results, is at most 2. is it because of the data or because of the lack of efficient training procedures for the hierarchical structure?	0
my biggest concern however is the applicability of this approach to highdimensional data.	0
i might not be familiar with some relevant results, but it appears to me that algorithm 1 will not converge to the same solution as full data gd would.	0
the best method very clearly depends on the taks and the amount of available data, but i found it difficult to extract an intuition for which method works best in which setting and why.	0
the paper addresses an important problem, but does not seem ready for publication:  the evaluation only uses simulated data.	0
at best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are.	0
"for example, there is this statement: ""we observe that our method (tr) converges to solutions with much better test error but worse training error when batch size is larger than 128. we postulate this is because sgd is easy to overfit training data and “stick” to a solution that has a high loss in testing data, especially with the large batch case as the inherent noise cannot push the iterate out of loss valley while our tr method can."""	0
moreover, some algorithms were used as benchmarks on some datasets but not others.	0
on the other hand, the adagrad algorithm guarantees a datadependent bound that is o(.sqrt{t}) but can also be much smaller.	0
table 2 contains almost no real comparison due to the lack of data points.	0
detailed comments p1 “, that explicitly approximate data distribution, the approximation of gan is implicit” the wording of this is pretty strange: by “implicit”, we mean that we only have 'samples' from the distribution(s) of interest, but what does it mean for an approximation to be “implicit”?	0
overall, the good inception scores aren’t too surprising given the model has several generators for each mode, but i think we need to see a demonstration on better datasets.	0
furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion.	0
"it may be true that ""one plot showing synthetic icu data would not provide enough information to evaluate its actual similarity to the real data"" because it could not rule out that case that the model has captured the marginal distribution in each dimension but not joint structure."	0
it appears that the analysis in this paper threw out more than 90% of the patients in their original dataset, which would present serious concerns in using the resulting synthetic data to represent the population at large.	0
the idea is to project the data onto a hypersphere not only in the input layer, but also other layers, which leads to better resistance against those attacks.	0
" it is very hard to get a clear winner from table 2. the ""dspherenet  wm  inputtruncated spherecorr"" seems to be the best on clean data, but then it loses robustness when learnt on clean and performs worse against pgd."	0
strengths: — figureqa can help in developing models that can extract useful information from visual representations of data.	2
weaknesses: — since the dataset is created synthetically, it is not clear if it is actually visual reasoning which is needed to solve this task, or the models can exploit biases (not necessarily language biases) to perform well on this dataset.	0
— the only advantages mentioned in the paper of using a synthetic dataset for this task are having greater control over task’s complexity and enabling auxiliary supervision signals, but none of them are shown in this paper, so it’s not clear if they are needed or useful.	0
overall: the proposed dataset seems reasonable but neither the dataset seems properly motivated (something where analysts actually struggle and models can help) nor it is clear if it will actually be useful for the research community (models performing well on this dataset will need to focus on specific abilities which have not been studied in the research community).	0
the paper briefly mentions supervising attention models using such boxes, but it isn’t clear how bounding boxes for data points could be used.	0
[weaknesses] 1: there are no novel algorithms associated with this dataset.	0
cvpr seems a better place to publish this paper, but i'm open to iclr accept dataset paper.	0
however, my major concern about this dataset is the synthesized question is very templated and less variational.	0
it might be acceptable is the authors are focusing on timeseries data; but in this case, it’s unclear why the authors are applying mil on it.	0
"the authors say ""... and after tuning our algorithm to emged this dataset, ..."", but this isn't enough."	0
"these two papers need to be cited: rudolph et al., nips 2017, ""sturctured embedding models for grouped data"": this paper also presents a method for learning embeddings specific for subgroups of the data, but based on hierarchical modeling."	0
pros and cons: pros : see above cons: my problem with the paper is lack of experiments on public datasets.	0
"as a side note, i would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on ""modified brownian motion”."	0
the method uses a learnable character embedding to transform the data, but is an endtoend approach.	0
pros  the proposed regularizers are able to separate out the classes inherent in the data, even if this information is not provided through class labels.	0
" many blackbox optimization problems are ""multifidelity"", in which it is possible to acquire data with different levels of cost and associated uncertainty."	0
pros:  a new gail formulation for saving on interaction data.	2
o snli data should be described: content, size, the task it is used for pro:  a novel idea of producing natural adversary examples with a gan  the generated examples are in some cases useful for interpretation and network understanding  the method enables creation of adversarial examples for block box classifiers cons  the idea implementation is basic.	2
 multiple base networks: the assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance.	0
 the same concern as above applies to the transferability and dataset decider experiments	0
i don’t understand the first sentence of the paper: “in the field of speech and audio processing, due to the lack of tools to directly process high dimensional data …”.	0
this is probably out of scope for iclr but to really test these methods, they should be trained/stored on a real small device and trained/finetuned using user data to see what would work best.	0
this reframing does point out a different and perhaps better path for ai, but it is not entirely new and this paper does not present a method for getting from sensed data to higher level concepts.	0
overall, this paper presents a different way of thinking about ai, one in which the amount of training time and training data required for learning is greatly reduced, however what is missing is a generalizable algorithmic strategy for implementing this framework.m	0
maybe they are effective precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish... i would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or, even better, from a dataset of randomly selected images that lack categorical coherence (e.g., photos randomly picked from the web, such as flickr pics).	0
my best guess is that it is the same as p_u(), the underlying data distribution, but makes parsing the paper hard.	0
"the paper lacks detailed explanation of the problem it is actually addressing by omitting the current systems' performance: simply stating: 1.1/page 2 ""thus it became essential and urgent to set up a larger scale training dataset to enhance the accuracy of the forecast results."""	0
the authors claim this is the largest dataset of such purpose, but they didn't demonstrate that the smaller datasets offered previously is indeed less competitive.	0
it looks like the dataset is useful but the model development and experimental sections are weak.	0
strengths:  open source data set for air quality monitoring that is significantly better than existing ones.	2
weaknesses:  the air quality data is measured at point locations (stations) which are interpolated to obtain spatial data.	0
2) the results do demonstrate clearly the advantage of the various choices and is useful 3) the theoretical connections between data angles and query times are quite interesting, cons: 1) unclear problem statement.	0
to me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?	0
"the presentation of this algo is a bit short and could deserve more space (in the supplementary) for the da application, the considered datasets are classic but not really ""large scale"", anyway this is a minor remark."	0
as written, the paper to me looks like two separate white papers: {beginning  to end of section 3}: as a theoretical white paper that lacks experiments, and {section 4}: experiments with some recent methods / datasets (this part is almost like a cute course project).	0
however, i think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.	0
of course many people have published on clevr although of its language limitations, but i was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverseengineer the way that the language was generated with a contextfree mechanism that is similar to how the data was produced. '	0
overall, i really enjoyed reading the paper, but i was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the kb is large.	0
speech data ' memorization of corrupted labels ' adversarial robustness (white box and black box attacks) ' gans (though quite a limited example, it is hard to generalize from this setting to the standard problems that gans are used for). '	0
the authors cite recent work by devries & taylor (2017) and pereyra et al. (2017), but the technique of combining multiple samples for data augmentation have been a popular approach.	0
i understand and appreciate the authors' argument as to why mixup should work, but it is not sufficiently convincing to me why a convex combination in euclidean space should produce good data distribution.	0
these models have mostly been evaluated on text, but i don’t see any reason they can’t be extended to sequential forecasting on time series data.	0
this idea however is difficult to be applied to deep learning with a large amount of data.	0
on the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.	0
they propose to learn a distancemetricfree model that assumes a markov chain as the generative mechanism of the data and learns not only the transition matrix but also the optimal ordering of the observations.	0
first, the authors do not generate the samples directly, but instead select them from the dataset  this is quite unconventional.	0
in total, the model seems clean and somewhat novel, but it has only been tested on unrealistic synthetic data, the framing with respect to related work is poor, and the contributions are overstated.	0
i agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks.	0
the goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers.	0
the clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)	0
shapeworlds dataset seems to be an interesting proofofconcept dataset however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups.	0
first, the visual data is composed of primitive shapes and colors in a black background.	0
2. it is interesting to visualize the results obtained with/without multitask learning in figure 6. cons: 1. the contribution is quite limited since the authors only apply multitask learning to the three mnistlike datasets and there is no technique contribution.	0
the authors argue that it is not a data augmentation technique, but rather a learning method.	0
i strongly disagree with this statement, not only because the technique deals exactly with augmenting data, but also because it can be used in combination to any learning method (including nondeep learning methodologies).	0
when combining multiple types of augmentation the results are better, but i'm wondering if this is because more augmented data is used overall.	0
"specifically, the authors say that for each image they produce 5 additional ""virtual"" data points, but when multiple methods are combined, does this mean 5 from each method?"	0
the idea is nice and simple, however the current framework has several weaknesses: 1. the whole pipeline has three (neural network) components: a) input image features are extracted from vgg net pretrained on auxiliary data; 2) autoencoder that is trained on data for oneshot learning; 3) final classifier for oneshot learning is learned on augmented image space with two (if i am not mistaken) fully connected layers.	0
it is also suggested that sum_i ||g_{1:t,i}|| = sqrt{sum_{t=1}^t g_{t,i}^2} might be much smaller than dg_infty, but this is very unlikely, because this term will typically grow like o(sqrt{t}), unless the data are extremely sparse, so we should at least expect some dependence on t.	0
something that concerns me is that, although your methods produce good results, it looks like the hyperparameters are chosen so as to overfit to the data (please do correct me if this is not the case).	0
pros:  promising results  good summary of adversarial methods cons:  poorly written  appears to overfit to the test data	2
"pros:  easy to follow line of argument  very interesting result of mapping ""solution"" of unregularised logistic regression (under gradient descent optimisation) onto hard max margin one cons:  it is not clear in the abstract, and beginning of the paper what ""convergence"" means, as in the strict sense logistic regression optimisation never converges on separable data."	2
" this paper claims to present a ""general deep reinforcement learning"" method that addresses the issues of realworld robotics: data constraints, safety, and lack of state information, and exploration by using demonstrations."	0
"pros:  interesting tasks that combine imitation and reinforcement in a logical (but somewhat heuristic) way  good simulated results on a variety of pickandplace style problems  some initial attempt at realworld transfer that seems promising, but limited  related work is very detailed and i think many will find it to be a very valuable overview cons:  some of the claims (detailed below) are a bit excessive in my opinion  the paper would be better if it was scoped more narrowly  contribution is a bit incremental and somewhat heuristic  the experimental results are difficult to interpret in simulation  the realworld experimental results are not great  there are a couple of missing citations (but overall related work is great) detailed discussion of potential issues and constructive feedback: > ""our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved."""	2
 the second gan solution trained on reverse codes from real data is interesting  in general the topic is interesting, the solution presented is simple but needs more study cons:  it related to adversarial learned inference and bigan, in term of learning the mapping z >x, x>z and seeking the agreement.	0
 the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.	0
" other comments page 6: ""it does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for bayesian approaches and in this context our results are directly relevant."""	0
" ""we can lower bound the loglikelihood of each ''dataset x'' ...""  3.2: ""in general, if we wish to learn a model for x in which each latent variable ci affects some arbitrary subset xi of the data (''where the xi may overlap''), ..."": which is just like learning a z for a labeled x but learning it in an unsupervised manner, i.e. the normal vae, isn't it?"	0
this is obviously possible with any regularization technique, but i think it is more of an issue here since parts of the data are not even used in learning.	0
the problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks.	0
the problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks.	0
in another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?	0
pros  extensive experiments on nlp data.	0
first, the authors used a specific corpus (with specific preprocessing) to train their vectors, but compared their results to those reported in other papers, which are not based on the same data.	0
"the term ""siamese kernel"" is not very informative: yes, you are learning new representations of data using dnns, but this feature mapping does not have the properties of rkhs; also you are not solving the svm dual problem as one typically does for kernel svms."	0
pros:  very easy to follow idea and model  simple merge or rl and sl in an endtoend trainable model  improvements over previous solutions cons:  kmeans experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.	2
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
the experimental results presented in this paper are quite good, but both mnist and modelnet40 seem like simple / toyish datasets.	0
the results are positive, though they are often below the performance of behavioral cloning (which only trains from the expert data but also uses the expert’s actions).	0
they do constitute a good starting place to test a model, but given the importance of regularization on those specific tasks, it is difficult to predict how the mos would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting.	0
pros:  important starting question  thoughtprovoking approach  experimental gains on small data sets cons:  the link between the intuition and reality of the gains is not obvious  experiments limited to small data sets, some obvious questions remain	2
on the positive side: having scalable models is important, especially models that can be applied to data with privacy concerns.	2
on the negative side: in the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data.	0
 your compression ratio is much higher for mnist but your accuracy loss is somewhat dramatic, especially for mnist (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354).	0
for the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but i’m not sure the empirical evidence provided for the mnist and cifar10 datasets is sufficient to judge whether or not the method does help with mode collapse.	0
 the authors of this paper proposed a datadriven blackbox visualization scheme.	0
"there are many methods that can operate on prestructured data only, but also have the ability to incorporate text data when available, e.g. ""universal schema"" [riedel et al, 2014]."	0
i can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model.	0
i don't mind the restriction of the setting under study to be adding a small dataset to a model trained on a large dataset, but i don't agree with the way the authors have stated things in the first paragraph of the paper because there are many realworld domains and applications that are necessarily of the small data variety.	0
interesting questions do arise, such as how to assess the value of new data and how to price datapoints, but these questions are never addressed (neither theoretically nor empirically).	0
a majority of the references included in the reference section lack some or all of the required meta data.	0
interpreting blackbox models via model extraction, 2017” (incomplete reference data)  “song han, huizi mao, and william j. dally.	0
understanding blackbox predictions via influence functions, 2017.” (incomplete reference data)  “h.	0
[ ============================== end of revision =====================================================] this paper concerns with addressing the issue of sgd not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net).	0
the author overstates that their model outperforms the stateoftheart models they compare to, but that is not true for the euadr dataset where in 2 out of 3 relation types the proposed model performs on par with the stateoftheart model.	0
the network allows efficient active incremental training, which significantly reduces the amount of training data needed to match stateoftheart performance.	0
acm, new york, ny, usa, 14751484. doi: https://doi.org/10.1145/2939672.2939839 2. given that the experiments are conducted on tasks where there isn’t a large amount of training data, one concern is that the baseline model used by the authors might be overparameterized.	0
however this is not sufficient, since learning offpolicy does not mean that an algorithm is indifferent to the behavior policy that has generated the data.	0
regarding the limited set of problems: of course any given work can only explore so many tasks, but for this to have general implications in nlp i would maintain that a standard (structured) sequence tagging task/dataset should have been considered.	0
this is not about the number of datasets, but rather than diversity of the output spaces therein.	0
however, for large datasets, the nucleusp norm is shown to be advantageous over l2 regularization only in one of the three datasets (svo)  this is fine (and not my concern) as i think it is important to show experiments where the algorithm is not advantageous too.	0
my main concern is the fact that the paper does not make any comparison with other methods for handling of discrete data in gans.	0
"it also implies that this would be true for any type of noise, and support this later claim using experiments on cifar and mnist with three noise types: (1) uniform label noise (2) nonuniform but imageindependent label noise, which is named ""structured noise"", and (3) samples from outofdataset classes."	0
al. maps data to the orthonormal hilbert space, but authors map to the d (formed by lr patches).	0
final proposed algorithm > splitting the data for high and low coherence makes sense however coherence is a continues variable.	0
the method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset), but is not practically useful yet since the regularization benefits of batch normalization are also taken away.	0
it is useful to discuss this in the paper and may be have some experiments on linearly separable data but with updates in both layers.	0
the assumption on data and structure of network is a bit strong, but this is the first result that achieves a number of desirable properties ``1. works for overparametrized network 2. finds global optimal solution for a nonconvex network.	0
this is somewhat achieved when the activation is replaced with standard relu, where the paper showed with a small number of hidden units the algorithm is likely to get stuck at a local minima, but with enough hidden units the algorithm is likely to converge (but even in this case, the data is still linearly separable and can be learned just by a perceptron).	0
however what i meant here is that with more complicated data and different loss functions it is hard to believe that this can still hold.].	0
experiments demonstrate that with the mnist database is not obtained an improvement in the error reduction but a reduction of the computational time.	0
as it is, i am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere.	0
the concerns i have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.	0
one model might be able to capture more diverse distributions, but lose a bit of quality, while another model might be able to create good samples when train on low diversity data.	0
vaes can obtain very good samples on celeba, a dataset with relative low diversity, but not so good samples on cifar. '	0
the embeddingenhancing method has low originality but is effective on this particular combination of model architecture, task and datasets.	0
 in this paper, the authors propose a method to train deep convolutional neural networks in an incremental fashion, in which data are available in small batches over a period of time.	0
the authors show results on different datasets, but fail to compare with stateoftheart models on this subject.	0
pros:  task of reducing computation by skipping inputs is interesting  model is novel and interesting  experiments on multiple tasks and datasets confirm the efficacy of the method  skipping behavior can be controlled via an auxiliary loss term  paper is clearly written cons:  missing comparison to prior work on sequential mnist  low performance on charades dataset, no comparison to prior work  no comparison to prior work on imdb sentiment analysis or ucf101 activity classification the task of reducing computation by skipping rnn inputs is interesting, and the proposed method is novel, interesting, and clearly explained.	2
"as a broad overview, the use of what the authors call ""word"" representations of notes is not novel (appearing first in bachbot and the performancernn); i suspect the model may be outputting sequences from the training set; and the dataset is heavily constrained in a way that will make producing pleasing melodies easily but heavily limits the possible outputs of the model."	0
 your results showing that human raters preferred your models are impressive, but you have made the task easier for yourself in various ways: 1) constraining the training data to pop music 2) making all of the training data in a single (major) key 3) effectively limiting the melody range to within a single octave.	0
 this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the blackbox mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.	0
in fig. 4, the face of leonardo dicaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as leonardo, or it is because gan has the magic to stitch characteristics of different face identities together?	0
"examples include , ""attentionbased convolutional neural network for machine comprehension"", ""a parallelhierarchical model for machine comprehension on sparse data"", and ""coarsetofine question answering for long documents"" this paper does not compare to the above style of approach empirically, but the hierarchical approach seems to have more advantages and seems a more straightforward solution."	0
similarly, it is nice that the present method works on less data, but the beauty of word embeddings is that they can be trained on any text  i.e. data is not a problem, and 'work' for any word type.	0
stripping away everything but nouns clearly allows cooccurrence semantic patterns to emerge from less data, but at the cost of the supervision mentioned above.	0
the paragraph just above section 4 says that the authors sample a batch of training data for this, but assume that the test point x_star has to be included in this batch.	0
however the smiles data set is a little mixed for active vs passive  authors should try to shed some light into that as well.	0
this might be true in the data space but the gan objective is optimized over the parameter space and it is therefore not clear to me their argument hold w.r.t to the network parameters.	0
this is fine as far as it goes, but the paper refs basri & jacobs 2016 multiple times as if it says anything relevant about this paper: basri & jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds.	0
there is nothing wrong about this, but perhaps other more complex (nonlinear) models to combine data could lead to more robust learning.	0
pros: one of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on chime3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions.	2
my concerns are mostly regarding the empirical studies: 1. one of my main concern is on the empirical results in table 1. the disentanglement metric score for betavae is suspiciously lower than what’s reported in higgins et al., where they reported a 99.23% disentanglement metric score on 2d shape dataset.	0
 the authors concentrate a lot on the celeba dataset, however i believe the comparison with betavae would be a lot clearer on the dsprites dataset (https://github.com/deepmind/dspritesdataset) (the authors call it 2d shapes).	0
i do, however, have two major concerns about the paper: 1. the proposed idea to add a mutual information constraint between the data x and latent code z is a very natural fix to the failure of regular vaes.	0
 summary this paper proposes a penalized vae training objection for the purpose of increasing the information between the data x and latent code z. ideally, optimization would consist of maximizing log p(x)  | i(x,z)  m |, where m is the userspecified target mutual information (mi) and i(x,z) is the model’s current mi value, but i(x,z) is intractable, necessitating the use of an auxiliary model r(z|x).	0
cons: 1) lacking motivation/intuition the main motivation for the approach, as far as i understand, is to learn cluster boundaries for nonlinear data  where kmeans fails.	0
"the paper isn't written too badly, although there are plenty of grammatical and spelling errors, some of which are listed below: ""a highly confident misclassification results"" ""a sufficient part or the whole training dataestes"" ""have less or none robustness toward"" ""can be expressed as a series transformations"" ""it is worthy mentioning"" ""we evaluate the robustness of network to"" ""method and it's variants"" ""this shed light on"" it may seem like i'm being pedantic, but it really does make for a better read to correct things like these."	0
the robustness of the new method is verified using 3 different types of attacks, e.g. attacks generated by gaussian noise, fast gradient sign methods and a blackbox attack, with stateoftheart cnn models trained on the ilsvrc2012 dataset.	0
the key realworld use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets.	0
the paper is not surprising but this seems like an effective technique for people who want to build effective systems with whatever data they’ve got.	0
unfortunately, while the idea has merit, and i'd like to see it pursued, the paper suffers from a fatal lack of validation/evaluation, which is very curious, given the amount of data that was collected, the fact that the authors have both a training and a test set, and that there are several natural ways such an evaluation might be performed.	0
''weakness'' in general, the paper has some major weaknesses in how the dataset has been constructed, details of the models provided and generally the novelty of the proposed model.	0
while the model on its own is not very novel, the paper does make an interesting computational observation that it could help to reason about vision even in textual dialog, but the execution of the dataset curation is not satisfactory, making the computational contribution less interesting.	0
in the introduction must energy is used on the importance of large data sets, but it appears that only fairly smallscale experiments are considered.	0
the second term in eq. 2 will pull z's closer but it can also wrongly place data points from different clusters nearby.	0
experiments on only one data set, i.e., mnist, are conducted pros: ' studying how to use unlabeled data to improve performance of gan is of technical importance.	2
typically images have different sizes, however in the dataset are described as having 299x299x3 size, are all the test images resized before hand?	0
why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem); (ii) the large amount of data required to learn the policy; and (iii) the significant risks associated with training with a physical vehicle; one can see the merits in employing a hierarchical action space, whereby decision making operates over highlevel actions, each associated with lowlevel controllers, but that the adopted formulation is not fundamental to this abstraction.	0
concerning the theoretical justification: it is not clear how axiom 2 ensures that the proposed measure only depends on points within the input data manifold.	0
my main concerns are the following: 1) one motivation of dfm is that in many applications data is a discretization of a continuous process and then can be represented by a function.	0
in the experiments, results of this method should be compared not against nns trained on the data directly, but against nns trained on dimension reduced version of the data (eg: first fixed number of pca components).	0
p.7 in the additional nli results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the esim model again draws into question whether enough value is being achieved from the world knowledge.	0
pros:  generating programs with neural networks is an exciting direction  novel task of generating ui code from ui screenshots  three new datasets of ui images and corresponding code  paper is clearly written cons:  limited technical novelty  limited experiments i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.	2
on the whole i appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.	0
these strategies are evaluated in a particular semisupervised transfer learning setting: the models are first trained on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting multiple times a large dataset), then they are used on a final target task with again few labeled data and large unlabeled samples but beloning to a different set of categories.	0
this work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data.	0
my other concerns of this paper include: 1. it looks like the training data uses empirical ctr of (t,c) as ground truth.	0
pros: 1. the spectra of the hessians with different model sizes, input data distributions and algorithms are empirically studied, which provides some insights into the behavior of overparameterized neural networks.	2
the paper starts with a very valid premise that many of the automatically generated cloze datasets for testing reading comprehension suffer from many shortcomings.	0
pros of this work: 1) this work contributes a nice dataset that addresses a real problem faced by automatically generated datasets.	0
e.g., the paper claims that neural approaches are much worse than humans on cloth data  however, they do not use stateoftheart neural reading comprehension techniques but only a standard lstm baseline.	0
it might be the case that the best available neural techniques are still much worse than humans on cloth data, but that remains to be seen.	0
authors note that for higher dimensional spheres, adversarial examples on the manifold (sphere shell) could found, but not smaller d: “in our experiments the highest dimension we were able to train the relu net without adversarial examples seems to be around d = 60.” yet,in their later statement in that same paragraph “we did not investigate if larger networks will work for larger d.”, it is unclear what is meant by “will work”; because, presumably, larger networks (with more weights) would be harder to avoid adversarial examples being found on the data manifold, so larger networks should be less likely “to work”, if “work” means avoid adversarial examples.	0
fair comparison of the data is a serious concern.	0
pros:  thorough experimentation with ablation studies; show success of method when using limited training data.	2
cons:  authors make the broad claim of world knowledge being helpful for textual entailment, and show usefulness in a limited datasize setting, but don't test their method on other datasets rte (which has a lot less data).	0
this paper does not make that much progress on the problem in general—the methods here are quite specific to words and to nli—and the proposed methods yields only yield large empirical gains in a reduceddata setting, but the paper serves as a wellexecuted proof of concept.	0
i have 2 main concerns: the data required for learning a good path function may include similar states to those visited by some optimal policy.	0
weaknesses: 1. the proposed dataset consists of 17,714 qa pairs in the dev set, whereas only 5,000 qa pairs in the test set.	0
3. the paper lacks analysis on how much of performance improvement is due to visual genome data augmentation and pretraining?	0
for quantization, it appears that the hd rotation is essential for cifar, but less important for the reddit data.	0
in section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity.	0
with n as number of data points and m as number of landmarks, from the description on page 4 it seems the complexity would be o(n  m^2), but the steps 1 and 2 on page 5 suggest it would be o(n^2  m^2).	0
"it shows that in this very simple setting, the ""evidence"" of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data)."	0
good cover of relevant work in sec 3. cons the paper emphasis on the fact the their modeling multimodal time series distributions, which is almost the case for most of the video sequence data.	2
other results presented in the paper are puzzling and require further experimentation and discussion, such as the trend that the learnability of shallow networks on random data is much higher than 10%, as discussed at the bottom of page 4. the authors provide some possible reasoning, stating that this strange effect could be due to class imbalance, but it isn't convincing enough.	0
"section 3 we use the dataset of visual attributes...: drop ""dataset"" i think the prelinguistic objects are not represented by 1hot, but binary vectors."	0
the method is correct but it is not clear whether the method can match the performance of stateoftheart methods such as graph convolution neural network of duvenaud et al. and structure2vec of dai et al. in large scale datasets.	0
 the paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target.	0
the concern that i have is that we continue to see these hyperparameter tuning papers that discuss how important the task is, butto the best of my knowledgethe last paper to actually improve sota using automated hyperparameter tuning was snoek et al., 2012., and there they even achieved 9.5% error with data augmentation.	0
"the bottom line seems to be: ""my model and approach works better than the other guys' model and approach"", but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much"	0
since the main contribution is to use an existing algorithm to tackle a practical application, it would be more interesting to tweak the approach until it is able to tackle a more realistic scenario (mainly larger scale, but also more realistic dynamics with traffic models, real data, etc.).	0
the need for a dynamical system could be argued to make sense for the camera task, perhaps, as video frames naturally form a time series; however, as already mentioned, for the mnist data, this is not the case, and the fact that the snn does not generalize here seems likely due to their under utilization rather than due to an inherent lack of capability.	0
the other issue concerned the validation of the approach on databases other than mnist.	0
perhaps this is due to a lack of available graph training data, but it doesn't seem to make a lot of sense.	0
i do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but i think for realworld relevance, variety in problem settings (i.e. datasets) is simply much more important.	0
p8: i think both the hjelm (bgan) and che (maligan) are using these weights to address credit assignment with discrete data, but bgan doesn’t use a mle generator, as is claimed in this work.	0
a lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the lowlevel statistics of natural images.	0
similarly, the representation analyzed in figure 7 is promising, but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the betavae paper.	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
pros: the paper compares different classifiers on three datasets.	2
( as proposed model is a deep model, the lack of comparison with deep methods is dubious) 7. in section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough.	0
the authors do a good job of positioning their study with respect to related work on blackbox adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty.	0
regarding the artificialness of their natural noise  obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial.	0
speculating, again: commercial mt providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale.	0
in this work the authors provide a way to measure privacy but there is no guarantee that if someone uses this method their data will be private, by some definition, even under certain assumptions.	0
not so much for how they explained the process of creating the dataset, that was very thorough, but the fact that this dataset was the only means to test the previous network models and their new proposed network model.	0
the paper would have been improved through testing of multiple datasets, and not just on there self generated dataset, but the contribution of their research on their network and older networks is still justification enough for this paper.	0
(3) why one billion word dataset is used in eval but not used for training?	0
first unsupervised experiments on mnist data show improved mse of joint autoecnoders but are these differences really significant (e.g. from 0.56 to 5.52) ?	0
i like this part because it perfectly makes sense not to let the generator hallucinate realworld effects on rather clean simulated data, but the other way around, remove all kinds of variations to produce a clean image from which the prediction should be easier.	0
if the real world data does not cover certain ranges, not because those values are infeasible or infrequent, but just because it so happens that this range was not covered in the data acquisition, the simulation could be used to fill in those ranges.	0
2. scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?	0
classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples.	0
 1. the idea is interesting, but the study is not comprehensive yet 2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper  stoecklein et al. deep learning for flow sculpting: insights into efficient learning using scientific simulation data.	0
notes to authors: i'm not familiar with 3be but the fact that it is used outside of its intended use case for the stock data is worrying.	0
the paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.)	0
semisupervised learning is interesting, but in the scenario of mt we do have enough parallel data for many language pairs.	0
the toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.	0
the authors demonstrate the method on just two datasets, and effectively they show results of training only for feedforward neural nets (the authors claim that “the entire spiking network endtoend works” referring to their pretrained vgg19, but this paper presents only training for the three top layers).	0
experiments results are good for given synthetic scenarios but less convincing for real data.	0
the results look aesthetically more pleasing than the baselines, but the reader does not learn much about how the method actually behaves in practice; when does it break down, how sensitive it is to various choices (network structure, learning algorithm, amount of data, how well the content and view can be disentangled from each other, etc.).	0
in other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.	0
" i believe that i understand the authors' intention of the caption of fig. 1, but ""samples outside the dataset"" is a misleading formulation."	0
in particular rainbow also uses a version of distributional multistep, which as far as i can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « offpolicyness » of the replay data), but still seems to work well on atari.	0
another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but i suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.	0
that being said, i am sympathetic to making simplifications to a dataset for the sake of scalability, but it shouldn't be presented as representative of sql.	0
only the names of the datasets used in the experiments are given, but they are not described, or even better, shown in pictures (maybe in a supplementary).	0
main comments the motivation to consider algebraic topology and dataset difficulty is interesting, but i think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings.	0
(the authors look at using cifar10, but project this down to 3 dimensions  as current methods for persistent homology cannot scale  which somewhat invalidates the goal of testing this out on real data.)	0
as also reported in the comments by t. kipf i found the lack of comparison to previous works on attention and on constructions of nn for graph data are missing.	0
from algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used.	0
1. the framework uses the class information, i.e., “only data samples from the normal class are used for training”, but it is still considered unsupervised.	0
better yet it would be to use methods like local outlier factor (lof) (breunig et al., 2000 – lof:identifying densitybased local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data).	0
of course you can still apply a static autoencoder to timesplit data, but what ends up happening is the model will use its capacity to try to explain the temporal signal in the data — a deeper model certainly has more extra capacity to do so.	0
"the comment that the posenet and vidloc methods ""lack a strainghtforward method to utilize past map data to do localization in a new environment"" is unclear. '"	0
cons:  the dataset used makes very strong simplification assumptions.	0
not problem per se, but it is not the most challenging sql dataset.	0
as the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix y).	0
cons:  data parallelism is a very commonly used technique for scaling.	0
the framework includes a few interesting ideas including using intermediate representation (ir) to express static computation graph and execute it as dynamic control flow, combining pipeline model parallelism and data parallelism by splitting or replicating certain layers, and enabling asynchronous training, etc. some concerns/questions are 1) the framework is targeted at devices like fpga, but the implementation is a multicore cpu smp.	0
it is not clear why msda cannot handle timeseries data but dauto can.	0
the paper explains that the simulator is obtained from log data, but this is not sufficient.	0
experiments are only presented with synthetic data but given the potential for the method and its novelty, i believe this can be accepted.	0
some other issues regarding quantitative results:  in table 1, there are 152 clusters for 10d latent space after convergence, but there are 61 clusters for 10d latent space in table 2 for the same mnist dataset.	0
3) cons: approximate posterior in nonsynthetic datasets: the variable z seems to not be modeling the future very well.	0
cons  the experiments are on toy datasets only.	2
"the experimental section is interesting, but in the end a bit disappointing: although a new artificial dataset is proposed to evaluate sets, it is unclear how different are the findings from those in the ""order matters"" paper:  the first set of results (in section 4.1) confirms that the set encoder is important (which was also in the other paper i believe)  the second set of results (section 4.2) shows that in some cases, an autoencoder is also useful: this is mostly the case when the supervised data is small compared to the availability of a much larger unsupervised data (of sets)."	0
"this is interesting (and novel compared to the ""order matters"" paper) but corresponds to known findings from most previous work on semisupervised learning: pretraining is only useful when only a very small supervised data exists, and quickly becomes irrelevant."	0
finally, it would have been very interesting to see experiments on real data concerned with sets.	0
the data is then partitioned within a batch based on this z value, and monte carlo sampling is used to estimate the variance of y conditioned on z, but it's really unclear as to how this behaves as a regularizer, how the z is sampled for each monte carlo run, and how this influences the gradient.	0
positive aspects:  emphasis in model interpretability and its connection to psychological findings in emotions  the idea of using tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative.	2
weaknesses:  a deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary.	0
pros: ' observations are replicated for several network architectures and datasets. '	2
likelihood estimation on the omniglot dataset questions and concerns: the model appears novel and is interesting, the experiments, however, are lacking in that they do not compare against other any recently proposed memory augmented deep generative models [bornschein et al] and [li et.	0
overall i liked this paper: the authors provide a frank view on the current state of neural program synthesis, which i am inclined to agree with: (1) existing neural program synthesis has only ever worked on ‘trivial’ problems, and (2) training program synthesizers is hard, but providing execution traces in the training data is not a practical solution.	0
extensive experimentation on established toy datasets (usps<>mnist, svhn<>mnist, svhn, gtsrb) and other more realworld datasets (including the visda one) cons: ' literature review on domain adaptation was lacking.	0
pros:  the paper is wellwritten and easy to read  the proposed method is a natural extension of the mean teacher semisupervised learning model by [tarvainen & valpola, 2017]  the model achieves stateoftheart results on a range of visual domain adaptation benchmarks (including top performance in the visda17 challenge) cons:  the model is tailored to the image domain as it makes heavy use of the data augmentation.	2
"this is the first time noise is really talked about, and it seems like maybe noise in the data is about alpha, but noise in the ""learning process"" is about tau?"	0
"the paper states ""n is the effective capacity of the model trained on these data"", but ""effective capacity"" is never defined."	0
" ""uncertainty"" is one of the supposed benefits of the mtgp layer, but it was not at all clear how it was used in practice, other than  perhaps  as a regularizer during training, similar to data augmentation."	0
as far as i can tell (i'm not an rl expert), the dr approach carries stronger consistency guarantees and reduced variance but is still only as good the data it is trained on, and clinical data is known to have significant bias, particularly with respect to treatment, where clinicians are often following formulaic guidelines.	0
pros:  interesting concept of combining algebraic structure with a data driven method  clear idea development and well written  transparent model with enough information for reimplementation  honest pointers to scenarios where the method might not work well cons:  the method is only intrinsically evaluated (tables 2 and 3), but not compared with results from other motion estimation methods	2
the lack of this property is because there are no data annotations.	0
in addition, there are some other works that used gan as a method for some version of data augmentation:  rendergan: generating realistic labeled data https://arxiv.org/abs/1611.01331 data augmentation in emotion classification using generative adversarial networks https://arxiv.org/abs/1711.00648 it is fair to say that their model shows improvement on the above tasks but this improvement comes with a cost of training of gan network.	0
in summary, the idea of the paper is very interesting to learn dataaugmentation but yet i am not convinced the current paper has enough novelty and contribution and see the contribution of paper as on more the application side rather than on model and problem side.	0
simulating document relevance from clicks is a good enough approximation, but why not also use datasets with real human relevance assessments, especially since so many of them exist and are so easy to access?	0
pros  greatly improves the data efficiency of recursive npi.	0
cons  requires access to a blackbox oracle to construct the dataset.	2
the lack of dl architectures for 3d data is due to complexity of representation of 3d data, especially when using 3d point clouds.	0
one thing i am bit concerned is that the results are based on a single dataset.	0
"at best, it seems to me that the new objective does in fact force the generator to concentrate efforts on learning over the full support of the data distribution, but the lower quality samples and sometimes somewhat bad interpolations seem to suggest to me that it is 'not' yet doing so very ""efficiently""."	0
since the authors make sure that their two models compensate for these transformation, the difference in saliency can be only due to underlying assumptions about the input data made by the saliency methods and therefore the discussion boils down to which invariance properties are justified for which kind of input  it is not by chance that the attribution methods that work are exactly those that extract statistics from the input data and therefore compensate for the input transformation: ig with black reference point and pattern attribution.	0
this is important, because the inception score is not a universal measure of gan performance: it provides a specific view on the ability of a generator to cover humandefined modes in the data distribution, but it does not inform on intraclass mode coverage and is blind to things like the generator collapsing on one or a few template samples per class.	0
1) in the abstract the authors mention that they achieve a bleu score of 32.8 but omit the fact that this is only on multi30k dataset and not on the more standard wmt datasets.	0
the lack of comparison to these different methods of training rnns/lstms with generated or mixture of ground truth and generated data is the biggest shortcoming of the paper.	0
pros: 1. an interesting framework for babi qa by encoding sentence to ngrams cons: 1. the overall justification is somewhat unclear 2. the approach could be overengineered for a special, lengthy version of babi and it lacks evaluation using realworld data	2
"ctc is a sequence training criterion  the reason that deep speech 2 is better on noisy test sets is not only the fact they trained on more data, but they also trained on ""noisy"" (matched) data  how is this an endtoend approach if you are using an ngram language model for decoding?"	0
 there is discussion as to what ivectors model (speaker or environment information)  i would leave out this discussion entirely here, it is enough to mention that other systems use adaptation, and maybe rerun an unadapted baselien for comparsion  there are techniques for incremental adaptation and a constrained mllr (feature adaptation) approaches that are very eficient, if one wnats to get into this  it may also be interesting to discuss the role of the language model to see which factors influence system performance  some of the other papers might use data augmentation, which would increase noise robustness (did not check, but this might explain some of the results in table 4)  i am confused by the references in the caption of table 3  surely the waibel reference is meant to be for tdnns (and should appear earlier in the paper), while pnorm came later (povey used it first for asr, i think) and is related to maxout  can you also compare the training times?	0
the paper does a good job of explaining the connection, but i think the presentation could be clarified.	0
this may be sufficient for a first proofofconcept but a comparison against standard benchmark methods and datasets for semantic segmentation is missing.	0
the proposed architecture is able to take advantage of this dataset bias, but would fail to do so on pascal voc, which has a much more intricate bias.	0
"the authors repeatedly emphasize the ""collaborative"" aspect of mtd, saying that the turkers have to collaborate to produce similar dataset distributions, but this is misleading because the turkers don't get to see other datasets."	0
in summary, i'm both excited about the dataset and new architecture, but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset.	0
cons:  i am wondering whether the dataset contains biases regarding (dx, dy).	0
the proposed celebroom dataset (a 50/50 mixture of celeba and lsun bedrooms) is a good dataset to validate the problem on but it is disappointing that the authors do not actually scale their method to their motivating example.	0
i know it's hard to evaluate a gan model but i think the authors can at least show the nearest neighbors in the training dataset and the training data that maximizes the activation of the corresponding memory slot together with the generated samples to see the difference.	0
some of the conclusions could be further clarified with additional experiments (e.g., sec 3.6 ‘while the reason that rms also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the imagenet dataset’).	0
"(iii) a task could be very ""difficult"" in the sense of high loss, but it could still be perfectly learned in the sense of finding the ideal ""groundtruth” classifier, but for a dataset that is highly nonseparable in the provided featurespace."	0
evaluation pros: the paper’s primary contribution is experimental: sota results are achieved for nearly every benchmark image dataset (the exception being statically binarized mnist, which is only .28 nats off).	2
the author's mention avoiding naturalistic labels such as omniglot characters (closer to the real version of concentration) due to the possibility the agent might memorise the finite set of labels, however by choosing a large dataset and using a nonoverlapping set of examples for the test set this probably could be avoided and would provide a more naturalistic test set.	0
 the mtfl experiments look most convincing (although this might be because i am not familiar with sota on the dataset), but still there is no control for the number of parameters, and the performance improvements are not huge  on cifar10  there is a marginal improvement in performance, which, as the authors admit, can also be reached by using a deeper model.	0
it feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it’s hard to imagine datasets that have exact but unknown matches (i welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.	0
a quick look at pascalvoc results indicate that deeplabresnet has iou of over 79 on this dataset, but the reported numbers in this paper are only around 73 iou.	0
however, as an an applications paper, the bread of experiments are a little bit lacking  with only that one potentially interesting dataset, which happens to proprietary.	0
it is only 0.4 improvement overall on the race dataset; although it outperforms gar on 7 out of 13 categories; but why is it worse on the other 6 categories?	0
through a series of experiments and a newly defined dataset, it exposes the shortcomings of current seq2seq rnn architectures.	0
i suspect that most of the embeddings included in the table also have many oovs in the rw dataset but still compute results on it using either an unknown word embedding or some baseline similarity of zero for pairs with an oov.	0
weaknesses: — given that the performance of stateonart on clevr dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models.	0
"secondarily, the paper asserts that ""our architecture can handle datasets more diverse than clevr"", but runs no experiments to validate this."	0
although drelu’s expectation is smaller than expectation of relu, but it doesn’t explain why drelu is better than very leaky relu, elu etc. 2. cifar10/100 is a saturated dataset and it is not convincing drelu will perform will on complex task, such as imagenet, object detection, etc. 3. in all experiments, elu/lrelu are worse than relu, which is suspicious.	0
from the empirical side, the authors compare the proposed optimizers on many datasets and models, but concerningly only using the baselines' default hyperparameters.	0
moreover, some algorithms were used as benchmarks on some datasets but not others.	0
pros:  the paper is clearly written  the method is new and somehow theoretically guaranteed by the proof of the proposition 1  the experiments are clearly explained with detailed configurations  the performance of the method in the model compression task is promising cons:  the “simple deduction” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis  it is confusing whether the output of the gate is sampled based on or computed directly by the function g  the experiments lack many recent baselines on the same dataset (penn treebank: melis et al. (2017) – on the state of the art of evaluation in neural language models; wmt: ashish et.al. (2017) – attention is all you need)  the experiment’s result is only slightly better than the baseline’s  to be more persuasive, the author should include in the baselines other method that can “binerize” the gate values such as the one sharpening the sigmoid function.	2
"[minor] typos: initilization > initialization, varitional > variational [major] expected an additional ""baseline"" in the expts  supervised but with the neural net policy architecture (nn approaches outperforming supervised on lyrl dataset was baffling before realizing that supervised is implemented using a linear crf)."	0
it appears that the analysis in this paper threw out more than 90% of the patients in their original dataset, which would present serious concerns in using the resulting synthetic data to represent the population at large.	0
i think my biggest concern is that the method was only tested on a single dataset hence it is not convincing enough.	0
experiments using the thor dataset are announced but are left underspecified (e.g., the movement actions), but no results or discussion are given.	0
paper weaknesses: i have the following concerns about this paper: (1) the paper performs the experiments on modelnet40, which is a toy dataset for this task.	0
weaknesses: — since the dataset is created synthetically, it is not clear if it is actually visual reasoning which is needed to solve this task, or the models can exploit biases (not necessarily language biases) to perform well on this dataset.	0
— the only advantages mentioned in the paper of using a synthetic dataset for this task are having greater control over task’s complexity and enabling auxiliary supervision signals, but none of them are shown in this paper, so it’s not clear if they are needed or useful.	0
overall: the proposed dataset seems reasonable but neither the dataset seems properly motivated (something where analysts actually struggle and models can help) nor it is clear if it will actually be useful for the research community (models performing well on this dataset will need to focus on specific abilities which have not been studied in the research community).	0
[weaknesses] 1: there are no novel algorithms associated with this dataset.	0
cvpr seems a better place to publish this paper, but i'm open to iclr accept dataset paper.	0
however, my major concern about this dataset is the synthesized question is very templated and less variational.	0
quality and significance  due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments.	0
"the authors say ""... and after tuning our algorithm to emged this dataset, ..."", but this isn't enough."	0
the wav2letter paper didn't compare with any baseline on librispeech (probably because librispeech isn't a common dataset, but at least the kaldi baseline is there).	0
pros:  clearly written, well executed paper  makes a strong point for the use of convolutional architecture for sequences  provides useful benchmarks for the community cons:  the claims on effective memory size need more context and justification 1: the lambada dataset: word prediction requiring a broad discourse context, paperno et al. 2016 2: the goldilocks principle: reading children's books with explicit memory representation, hill et al. 2016	2
strengths the authors present a new datasets for mathematical identities.	2
 multiple base networks: the assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance.	0
 the same concern as above applies to the transferability and dataset decider experiments	0
(as an intermediate step, the performance could also be measured with the dataset classifier trained in the same way but used as a soft weighting, rather than the hard version rounding alpha to 0 or 1.)	0
4: the automatically extracted topic can be very noisy, but the paper didn't mention any of the extracted topics on aqad dataset.	0
maybe they are effective precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish... i would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or, even better, from a dataset of randomly selected images that lack categorical coherence (e.g., photos randomly picked from the web, such as flickr pics).	0
"the paper lacks detailed explanation of the problem it is actually addressing by omitting the current systems' performance: simply stating: 1.1/page 2 ""thus it became essential and urgent to set up a larger scale training dataset to enhance the accuracy of the forecast results."""	0
the authors claim this is the largest dataset of such purpose, but they didn't demonstrate that the smaller datasets offered previously is indeed less competitive.	0
it looks like the dataset is useful but the model development and experimental sections are weak.	0
"the contribution of the paper is:  some proposed methods to extract a colorinvariant representation  an experimental evaluation of the methods on the cifar 10 dataset  a new dataset ""crashed cars""  evaluation of the best method from the cifar10 experiments on the new dataset pros:  the crashed cars dataset is interesting."	2
however, i think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.	0
on the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.	0
first, the authors do not generate the samples directly, but instead select them from the dataset  this is quite unconventional.	0
shapeworlds dataset seems to be an interesting proofofconcept dataset however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups.	0
that is, the authors first train a neural network (with a specific architecture; in this case, it is an allconvolutional network) on a combination of the datasets (mf; fn; nm; mfn) and then use the learned weights (in all but the output layer) to initialize the weights for taskspecific training on each of the datasets.	0
weaknesses:  while the proposed psvrt dataset addresses the 2 noted problems in svrt, using only 2 relations in the study is very limited.	0
" ""we can lower bound the loglikelihood of each ''dataset x'' ...""  3.2: ""in general, if we wish to learn a model for x in which each latent variable ci affects some arbitrary subset xi of the data (''where the xi may overlap''), ..."": which is just like learning a z for a labeled x but learning it in an unsupervised manner, i.e. the normal vae, isn't it?"	0
pros:  very easy to follow idea and model  simple merge or rl and sl in an endtoend trainable model  improvements over previous solutions cons:  kmeans experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.	2
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
for the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but i’m not sure the empirical evidence provided for the mnist and cifar10 datasets is sufficient to judge whether or not the method does help with mode collapse.	0
i don't mind the restriction of the setting under study to be adding a small dataset to a model trained on a large dataset, but i don't agree with the way the authors have stated things in the first paragraph of the paper because there are many realworld domains and applications that are necessarily of the small data variety.	0
the author overstates that their model outperforms the stateoftheart models they compare to, but that is not true for the euadr dataset where in 2 out of 3 relation types the proposed model performs on par with the stateoftheart model.	0
in general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being.	0
regarding the limited set of problems: of course any given work can only explore so many tasks, but for this to have general implications in nlp i would maintain that a standard (structured) sequence tagging task/dataset should have been considered.	0
this is not about the number of datasets, but rather than diversity of the output spaces therein.	0
for a given dataset, why should we not use adam or msgd (or other existing algorithms such as adagrad, rmsprop), but your algorithms?	0
"it also implies that this would be true for any type of noise, and support this later claim using experiments on cifar and mnist with three noise types: (1) uniform label noise (2) nonuniform but imageindependent label noise, which is named ""structured noise"", and (3) samples from outofdataset classes."	0
## pros / strengths  effort to assess momentum / adam / other modern methods  effort to compare to previous experimental setups ## cons / limitations  lack of wallclock measurements in experiments  only ~2 models / datasets examined, so difficult to assess generalization  lack of discussion about distributed/asynchronous sgd ## significance many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community.	2
it would be nice to demonstrate that this regularization works in at least one more problem, maybe imagenet, though maybe regularization is not needed there but just find one more dataset that needs regularization and test this on that.	0
the method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset), but is not practically useful yet since the regularization benefits of batch normalization are also taken away.	0
section 5.3: amortization error is an important contributor to the slack in the elbo on mnist, and the dominant contributor on the more complicated fashion mnist dataset.	0
these results are interesting, but given the empirical nature of this paper i would have liked to see results on more interesting datasets (celeba, cifar10, really anything but mnist).	0
the concerns i have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.	0
vaes can obtain very good samples on celeba, a dataset with relative low diversity, but not so good samples on cifar. '	0
even more concerning is the fact that each variant is presented and tested using a different network architecture and a different dataset, with no experimental comparison among these variants.	0
pros:  task of reducing computation by skipping inputs is interesting  model is novel and interesting  experiments on multiple tasks and datasets confirm the efficacy of the method  skipping behavior can be controlled via an auxiliary loss term  paper is clearly written cons:  missing comparison to prior work on sequential mnist  low performance on charades dataset, no comparison to prior work  no comparison to prior work on imdb sentiment analysis or ucf101 activity classification the task of reducing computation by skipping rnn inputs is interesting, and the proposed method is novel, interesting, and clearly explained.	2
"as a broad overview, the use of what the authors call ""word"" representations of notes is not novel (appearing first in bachbot and the performancernn); i suspect the model may be outputting sequences from the training set; and the dataset is heavily constrained in a way that will make producing pleasing melodies easily but heavily limits the possible outputs of the model."	0
 this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the blackbox mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.	0
in fig. 4, the face of leonardo dicaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as leonardo, or it is because gan has the magic to stitch characteristics of different face identities together?	0
my concerns are mostly regarding the empirical studies: 1. one of my main concern is on the empirical results in table 1. the disentanglement metric score for betavae is suspiciously lower than what’s reported in higgins et al., where they reported a 99.23% disentanglement metric score on 2d shape dataset.	0
 the authors concentrate a lot on the celeba dataset, however i believe the comparison with betavae would be a lot clearer on the dsprites dataset (https://github.com/deepmind/dspritesdataset) (the authors call it 2d shapes).	0
the robustness of the new method is verified using 3 different types of attacks, e.g. attacks generated by gaussian noise, fast gradient sign methods and a blackbox attack, with stateoftheart cnn models trained on the ilsvrc2012 dataset.	0
the paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used.	0
it seems to assume that you don’t have much unlabelled text (or you’d use glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well.	0
''weakness'' in general, the paper has some major weaknesses in how the dataset has been constructed, details of the models provided and generally the novelty of the proposed model.	0
while the model on its own is not very novel, the paper does make an interesting computational observation that it could help to reason about vision even in textual dialog, but the execution of the dataset curation is not satisfactory, making the computational contribution less interesting.	0
pros:  provide theoretical guarantee for the use of orthogonal random features in the context of psrnn cons:  empirical evaluation only on small scale datasets.	2
my concern with that is that the authors might be using the test set for model selection; it is not a priori clear that the setup that works better for ml should also be better for rl, especially as it is not the same across datasets.	0
pros:  generating programs with neural networks is an exciting direction  novel task of generating ui code from ui screenshots  three new datasets of ui images and corresponding code  paper is clearly written cons:  limited technical novelty  limited experiments i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.	2
on the whole i appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.	0
these strategies are evaluated in a particular semisupervised transfer learning setting: the models are first trained on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting multiple times a large dataset), then they are used on a final target task with again few labeled data and large unlabeled samples but beloning to a different set of categories.	0
the dataset seems interesting but i find the empirical evaluations unconvincing.	0
the paper starts with a very valid premise that many of the automatically generated cloze datasets for testing reading comprehension suffer from many shortcomings.	0
pros of this work: 1) this work contributes a nice dataset that addresses a real problem faced by automatically generated datasets.	0
"i'm not very familiar with squad dataset, but the results seems worse than ""reading wikipedia to answer opendomain questions"" table 4 which seems use a vanilla lstm setup."	0
cons: 1. you should elaborate more on the negative results on fb15k and why this performance would not transfer to other kb datasets that exist.	0
maybe this is a matter of the dataset fb15k itself but then having experiments on another dataset with hundreds of relation types could be important.	0
nell has indeed 200 relations but if i'm not mistaken, the nell dataset is used for fact prediction and not query answering.	0
cons:  authors make the broad claim of world knowledge being helpful for textual entailment, and show usefulness in a limited datasize setting, but don't test their method on other datasets rte (which has a lot less data).	0
weaknesses: 1. the proposed dataset consists of 17,714 qa pairs in the dev set, whereas only 5,000 qa pairs in the test set.	0
"section 3 we use the dataset of visual attributes...: drop ""dataset"" i think the prelinguistic objects are not represented by 1hot, but binary vectors."	0
(3) concerning sota results, i have to agree with anonreviewer3: one way to demonstrate success is to show competitive performance on a dataset (e.g., cifar) on which other researchers can also evaluate their algorithms on.	0
nevertheless, i agree with the authors that another way to demonstrate success is to show competitive performance on a 'combination' of a dataset and a design space, but for that to be something that other researchers can compare to requires the authors making publicly available the implementations they have optimized; without that public availability, due to a host of possible confounding factors, it is impossible to judge whether stateoftheart performance on such a combination of dataset and design space has been achieved.	0
strengths: 1. the new cartoonset dataset is carefully designed and compiled.	2
the experiment with multitask learning over mnist dataset looks interesting, but it is still a toy experiment.	0
i do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but i think for realworld relevance, variety in problem settings (i.e. datasets) is simply much more important.	0
similarly, the representation analyzed in figure 7 is promising, but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the betavae paper.	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
pros: 1) the visual quality of the results is very good, both on faces and on objects from the lsun dataset.	2
( as proposed model is a deep model, the lack of comparison with deep methods is dubious) 7. in section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough.	0
a major point of concern is that they do not use the public dataset proposed in finn et al. 2016, but use their own (smaller) dataset.	0
the incremental architectural changes, different dataset and training are responsible for most of the other improvements.	0
not so much for how they explained the process of creating the dataset, that was very thorough, but the fact that this dataset was the only means to test the previous network models and their new proposed network model.	0
the paper would have been improved through testing of multiple datasets, and not just on there self generated dataset, but the contribution of their research on their network and older networks is still justification enough for this paper.	0
the use of a synthetic dataset also weakens the paper, and means it is unclear if the results will generalize to real language.	0
(3) why one billion word dataset is used in eval but not used for training?	0
2. scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?	0
the toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.	0
"the real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets."	0
the authors demonstrate the method on just two datasets, and effectively they show results of training only for feedforward neural nets (the authors claim that “the entire spiking network endtoend works” referring to their pretrained vgg19, but this paper presents only training for the three top layers).	0
on the positive side, the method has been tested on 3 different datasets, outperforming the baselines (recent methods from 2016) on 2 of them.	2
p2 footnote 1: so if i understand this correctly, this work builds upon a dataset of over 65k recipes from kiddon et al. (2016), but only for 875 of those detailed annotations were created?	0
the approach seems to work well for a relatively clean setup such as the chair dataset, but for the other datasets the separation is not so apparent.	0
" i believe that i understand the authors' intention of the caption of fig. 1, but ""samples outside the dataset"" is a misleading formulation."	0
 tl;dr: the paper extends equilibrium propagation to recurrent networks, but doesn't test the algorithm on a dataset requiring a recurrent architecture.	0
cons: 1. it would have been better to see performance of the proposed method in other datasets (wherever possible).	0
that being said, i am sympathetic to making simplifications to a dataset for the sake of scalability, but it shouldn't be presented as representative of sql.	0
main comments the motivation to consider algebraic topology and dataset difficulty is interesting, but i think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings.	0
cons:  experimental results on only one dataset.	0
 in figure 1, the “g1” on the right should be “g2”;  section 2.2.1, “x_f” should be “x_f”;  the motivation of having “z_v” should be introduced earlier;  section 2.2.4, please use either “alpha” or “.alpha” but not both;  section 3.3, the dataset information is incorrect: “20599 images” should be “202599 images”; missing reference:  neural face editing with intrinsic image disentangling, shu et al. in cvpr 2017.	0
cons:  the dataset used makes very strong simplification assumptions.	0
not problem per se, but it is not the most challenging sql dataset.	0
this observation is not new, but relevant in the field of image retrieval, where in many applications the object of interest for a query is actually not present in the training dataset.	0
some other issues regarding quantitative results:  in table 1, there are 152 clusters for 10d latent space after convergence, but there are 61 clusters for 10d latent space in table 2 for the same mnist dataset.	0
3) cons: approximate posterior in nonsynthetic datasets: the variable z seems to not be modeling the future very well.	0
"the experimental section is interesting, but in the end a bit disappointing: although a new artificial dataset is proposed to evaluate sets, it is unclear how different are the findings from those in the ""order matters"" paper:  the first set of results (in section 4.1) confirms that the set encoder is important (which was also in the other paper i believe)  the second set of results (section 4.2) shows that in some cases, an autoencoder is also useful: this is mostly the case when the supervised data is small compared to the availability of a much larger unsupervised data (of sets)."	0
weaknesses:  a deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary.	0
likelihood estimation on the omniglot dataset questions and concerns: the model appears novel and is interesting, the experiments, however, are lacking in that they do not compare against other any recently proposed memory augmented deep generative models [bornschein et al] and [li et.	0
i am not an expert on this particular dataset, but to my eye the results look impressive.	0
simulating document relevance from clicks is a good enough approximation, but why not also use datasets with real human relevance assessments, especially since so many of them exist and are so easy to access?	0
cons  requires access to a blackbox oracle to construct the dataset.	2
the overall scheme is intuitive, but the model is experimented on two small datasets of few thousand of molecules, and compared to a stateoftheart deeptox, and also to some basic baselines (rf/svm/logreg).	0
in the herg dataset rlcnn is again weaker than the full cnn, but also seems to be beaten by several baseline methods.	0
the paper does mention the agents will be available, but leaves open wether the dataset will be also available.	0
one thing i am bit concerned is that the results are based on a single dataset.	0
the presented results using small networks on the mnist dataset only show that networks with isrlu can perform similar to those with other activation functions, but not the speed advantages of isrlu.	0
1) in the abstract the authors mention that they achieve a bleu score of 32.8 but omit the fact that this is only on multi30k dataset and not on the more standard wmt datasets.	0
this may be sufficient for a first proofofconcept but a comparison against standard benchmark methods and datasets for semantic segmentation is missing.	0
"the authors repeatedly emphasize the ""collaborative"" aspect of mtd, saying that the turkers have to collaborate to produce similar dataset distributions, but this is misleading because the turkers don't get to see other datasets."	0
some of the conclusions could be further clarified with additional experiments (e.g., sec 3.6 ‘while the reason that rms also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the imagenet dataset’).	0
this probably does not match the assumption of many of the datasets being tested upon (cifar, mnist) but i don't consider that a fundamental issue.	0
worth noting that recently, (shwartzziv and tishby) demonstrated, not on largescale datasets but on small ones, that an optimal representation for a classification task must reduce as much uninformative variability as possible while maximizing the mutual information between the desired output and its representation in order discriminate as much as possible between classes.	0
cons: experiments: only small datasets were used in the experiments, it would be more convincing if the author could use larger datasets.	0
it feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it’s hard to imagine datasets that have exact but unknown matches (i welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.	0
c) the paper should run some experiments on language applications where rnn is widely used d) i might be wrong on this point, but it seems that the gpu utilization of the method would be very poor so that it's kind of impossible to scale to large datasets?	0
the reviewer is concerned that these datasets may not be representative of real problems.	0
of course, the paper also invents some new evaluation metrics and then applies them on benchmark datasets, but this content only appears much later in the paper (well after the soft 8 page limit) and i admittedly did not read it all carefully.	0
another concern is practicality of the proposed method which seems to require maintaining explicit distribution over all examples which would not be practical for modern datasets where nns are typically applied.	0
pros: () the paper is well written and the method is well explained () the authors ablate and experiment on large scale datasets cons: () the proposed method is a simple extension of resnext () the gains are reasonable, yet not sota, and come at a price of more complex training protocols (see below) () generalization to other tasks not shown the authors do a great job walking us through the formulation and intutition of their proposed approach.	2
"secondarily, the paper asserts that ""our architecture can handle datasets more diverse than clevr"", but runs no experiments to validate this."	0
pros: 1. experimental study on retrofitting existing word vectors for esl and toefl lexical similarity datasets cons: 1. the paper is poorly written and the proposed methods are not well justified.	2
[strong points] ' based on experimental results over a broad range of datasets, deep network models and their attacks. '	2
from the empirical side, the authors compare the proposed optimizers on many datasets and models, but concerningly only using the baselines' default hyperparameters.	0
moreover, some algorithms were used as benchmarks on some datasets but not others.	0
overall, the good inception scores aren’t too surprising given the model has several generators for each mode, but i think we need to see a demonstration on better datasets.	0
pros and cons: pros : see above cons: my problem with the paper is lack of experiments on public datasets.	0
strengths the authors present a new datasets for mathematical identities.	2
 multiple base networks: the assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance.	0
 the implementation details are included, including the way of implementing image rotations, different network architectures evaluated on different datasets, optimizers, learning rates with weight decayed, batch sizes, numbers of training epochs, etc.  outperforms all baselines and achieves performance close to, but still below, fully supervised methods  plots rotation prediction accuracy and object recognition accuracy over time and shows that they are correlated ''paper weaknesses''  the proposed method considers a set of different geometric transformations as discrete and independent classes and formulates the task as a classification task.	0
the authors claim this is the largest dataset of such purpose, but they didn't demonstrate that the smaller datasets offered previously is indeed less competitive.	0
"the presentation of this algo is a bit short and could deserve more space (in the supplementary) for the da application, the considered datasets are classic but not really ""large scale"", anyway this is a minor remark."	0
as written, the paper to me looks like two separate white papers: {beginning  to end of section 3}: as a theoretical white paper that lacks experiments, and {section 4}: experiments with some recent methods / datasets (this part is almost like a cute course project).	0
cons  only two factors of variation are studied, and the datasets are fairly simple.	2
the method provides equivalent accuracy and sparsity to published stateoftheart results on these datasets but it is argue that learning sparsity during the training process will lead to significant speedups  this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations.	0
the method provides equivalent accuracy and sparsity to published stateoftheart results on these datasets but it is argue that learning sparsity during the training process will lead to significant speedups  this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations.	0
it could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with bnns (or why that's not a good idea).	0
pros: outperforms node2vec and deepwalk on 5 of the 6 tested datasets and achieves comparable results on the last one.	2
 this reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements: 1) you should include in your comparison queryby bagging & boosting, which are two of the best outofthebox active learning strategies 2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones, but many questions are still unanswered:  would any 77 split work just as well (ie, crossvalidate over the 14 domains)  do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains?	0
the clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)	0
that is, the authors first train a neural network (with a specific architecture; in this case, it is an allconvolutional network) on a combination of the datasets (mf; fn; nm; mfn) and then use the learned weights (in all but the output layer) to initialize the weights for taskspecific training on each of the datasets.	0
2. it is interesting to visualize the results obtained with/without multitask learning in figure 6. cons: 1. the contribution is quite limited since the authors only apply multitask learning to the three mnistlike datasets and there is no technique contribution.	0
" other comments page 6: ""it does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for bayesian approaches and in this context our results are directly relevant."""	0
pros:  very easy to follow idea and model  simple merge or rl and sl in an endtoend trainable model  improvements over previous solutions cons:  kmeans experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.	2
 please fix .cite calls to .citep, when authors name is not used as part of the sentence, for example: graph neural network nowak et al. (2017) should be graph neural network (nowak et al. (2017)) # after the update evaluation section has been updated threefold:  tsp experiments are now in the appendix rather than main part of the paper  kmeans experiments are lloydscore normalised and involve one cifar10 clustering  knapsack problem has been added paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problemspecific baseline (lloyd for kmeans, greedy knapsack solver) beats proposed method.	0
the experimental results presented in this paper are quite good, but both mnist and modelnet40 seem like simple / toyish datasets.	0
 your compression ratio is much higher for mnist but your accuracy loss is somewhat dramatic, especially for mnist (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354).	0
for the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but i’m not sure the empirical evidence provided for the mnist and cifar10 datasets is sufficient to judge whether or not the method does help with mode collapse.	0
in general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being.	0
this is not about the number of datasets, but rather than diversity of the output spaces therein.	0
however, for large datasets, the nucleusp norm is shown to be advantageous over l2 regularization only in one of the three datasets (svo)  this is fine (and not my concern) as i think it is important to show experiments where the algorithm is not advantageous too.	0
it is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets, cam attention visualization and also additional materials with highres attacks.	0
one possible explanation being that during test time, the approach does not use samples from w but rather a summary of them, say posterior means, in which case, it defeats the purpose of sampling from global parameters, which may explain why wai and whai perform about the same in the 3 datasets considered.	0
batch size and learning rate analysis was very informative but should be done on resnets and larger datasets to make the paper strong and provide value to the research community.	0
## pros / strengths  effort to assess momentum / adam / other modern methods  effort to compare to previous experimental setups ## cons / limitations  lack of wallclock measurements in experiments  only ~2 models / datasets examined, so difficult to assess generalization  lack of discussion about distributed/asynchronous sgd ## significance many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community.	2
pros methodology 1. inductive ability: can generalize to unseen nodes without any further training 2. personalized ranking: the model uses natural ranking that embeddings of closer nodes (considers node pairs of any distance) should be closer in the embedding space, which is more general than prevailing first and second order proximity 3. sampling strategy: the proposed nodeanchored sampling method gives unbiased estimates of loss function and successfully reduces the time complexity experiment 1. evaluation tasks including link prediction and node classification are conducted across multiple datasets with additional parameter sensitivity and missinglink robustness experiments 2. compared with various baselines with diverse model designs such as gcn and node2vec as well as compared with naive baseline (using original node attributes as model inputs) 3. demonstrated the model captures uncertainties and the learned uncertainties can be used to infer latent dimensions related works the survey of related work is sufficiently wide and complete.	0
the experiments demonstrate small but consistent gains with anr across a number of domains (language modelling on small datasets, plus image classification) and baseline models.	0
below are some detailed comments: pros  numerous public datasets are used for the experiments  good introductions for some of the existing methods.	0
these results are interesting, but given the empirical nature of this paper i would have liked to see results on more interesting datasets (celeba, cifar10, really anything but mnist).	0
the concerns i have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.	0
the embeddingenhancing method has low originality but is effective on this particular combination of model architecture, task and datasets.	0
the authors show results on different datasets, but fail to compare with stateoftheart models on this subject.	0
pros:  task of reducing computation by skipping inputs is interesting  model is novel and interesting  experiments on multiple tasks and datasets confirm the efficacy of the method  skipping behavior can be controlled via an auxiliary loss term  paper is clearly written cons:  missing comparison to prior work on sequential mnist  low performance on charades dataset, no comparison to prior work  no comparison to prior work on imdb sentiment analysis or ucf101 activity classification the task of reducing computation by skipping rnn inputs is interesting, and the proposed method is novel, interesting, and clearly explained.	2
also, the authors compare their approach (fig. 6) vs bayesian optimization and random search, which are approaches that are know to perform extremely poorly on high dimensional datasets.	0
h is never discussed after section 2.2. experiments on known datasets are interesting, but none of the results are competitive with current stateoftheart results (sota), despite what is said in appending d. for instance, one can find sota results for cifar100 around 16% and for cifar10 around 3%.	0
"(cons) 1. by biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims ""achieved state of the art results on three datasets."""	0
pros:  provide theoretical guarantee for the use of orthogonal random features in the context of psrnn cons:  empirical evaluation only on small scale datasets.	2
my concern with that is that the authors might be using the test set for model selection; it is not a priori clear that the setup that works better for ml should also be better for rl, especially as it is not the same across datasets.	0
pros  ' this paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. '	0
pros:  generating programs with neural networks is an exciting direction  novel task of generating ui code from ui screenshots  three new datasets of ui images and corresponding code  paper is clearly written cons:  limited technical novelty  limited experiments i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.	2
the paper starts with a very valid premise that many of the automatically generated cloze datasets for testing reading comprehension suffer from many shortcomings.	0
pros of this work: 1) this work contributes a nice dataset that addresses a real problem faced by automatically generated datasets.	0
cons: 1. you should elaborate more on the negative results on fb15k and why this performance would not transfer to other kb datasets that exist.	0
cons:  authors make the broad claim of world knowledge being helpful for textual entailment, and show usefulness in a limited datasize setting, but don't test their method on other datasets rte (which has a lot less data).	0
the method is correct but it is not clear whether the method can match the performance of stateoftheart methods such as graph convolution neural network of duvenaud et al. and structure2vec of dai et al. in large scale datasets.	0
i do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but i think for realworld relevance, variety in problem settings (i.e. datasets) is simply much more important.	0
the heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but i'm not convinced this will work well in all situations, and may even be suboptimal for the examined datasets.	0
it also was tested on graph classification datasets, but the results were not as good for some of the datasets.	0
similarly, the representation analyzed in figure 7 is promising, but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the betavae paper.	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
they then go on to explore the sparsity of the latent space my main issues with this paper are experiments: the proposed approach is tested only on 2 datasets (one synthetic, one real but tiny  2k instances) and some of the plots (like figure 5) are not convincing to me.	0
pros and cons: pros: the method is simple to implement, the paper lists for what kind of datasets it can be used.	2
pros: the paper compares different classifiers on three datasets.	2
the paper would have been improved through testing of multiple datasets, and not just on there self generated dataset, but the contribution of their research on their network and older networks is still justification enough for this paper.	0
2. scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?	0
as far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to soa methods, but it is not really clear whether the outperformance is of statistical significance.	0
3. the proposed attention mechanism is mainly demonstrated for singleclass classification task, but it would be interesting to see if it can also help the multiclass classification (e.g. image classification on mscoco or pascal voc datasets) 4. the localization performance of the proposed attention mechanism is evaluated by weaklysupervised semantic segmentation tasks.	0
the toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.	0
"the real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets."	0
the authors demonstrate the method on just two datasets, and effectively they show results of training only for feedforward neural nets (the authors claim that “the entire spiking network endtoend works” referring to their pretrained vgg19, but this paper presents only training for the three top layers).	0
on the positive side, the method has been tested on 3 different datasets, outperforming the baselines (recent methods from 2016) on 2 of them.	2
the approach seems to work well for a relatively clean setup such as the chair dataset, but for the other datasets the separation is not so apparent.	0
cons: 1. it would have been better to see performance of the proposed method in other datasets (wherever possible).	0
only the names of the datasets used in the experiments are given, but they are not described, or even better, shown in pictures (maybe in a supplementary).	0
3) cons: approximate posterior in nonsynthetic datasets: the variable z seems to not be modeling the future very well.	0
cons  the experiments are on toy datasets only.	2
weaknesses:  a deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary.	0
pros: ' observations are replicated for several network architectures and datasets. '	2
extensive experimentation on established toy datasets (usps<>mnist, svhn<>mnist, svhn, gtsrb) and other more realworld datasets (including the visda one) cons: ' literature review on domain adaptation was lacking.	0
the results from mnist and celeba datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward mmd metric and potentially discuss their pros and cons.	0
pros: (a) human evaluations applications to several datasets show the usefulness of maskgen over the maximum likelihood trained model in generating more realistic text samples.	2
simulating document relevance from clicks is a good enough approximation, but why not also use datasets with real human relevance assessments, especially since so many of them exist and are so easy to access?	0
the overall scheme is intuitive, but the model is experimented on two small datasets of few thousand of molecules, and compared to a stateoftheart deeptox, and also to some basic baselines (rf/svm/logreg).	0
1) in the abstract the authors mention that they achieve a bleu score of 32.8 but omit the fact that this is only on multi30k dataset and not on the more standard wmt datasets.	0
they apply the proposed method two wellknown benchmark datasets under a fully connected and a convolutional neural network, and demonstrate that in the former case a slight improvement in accuracy can be achieved, while in the latter, the method performs similar to the grouplasso, but at a reduced computational cost for classifying new images due to increased compression of the network.	0
the student model learns according to a standard stochastic gradient descent technique (adam for mlp and cnn, momentumsgd for rnn), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model.	0
pros:  good new implementation of an existing idea  significant perplexity gains on character level language modeling  good at domain adaptation cons:  memory requirements of the method  wordlevel language modeling experiments need to be run on larger data sets (edit: the authors did respond satisfactorily to the original concern about the size of the wordlevel data set)	2
positives:  the output kernel update is well justified  experimental results are encouraging negatives:  the methodological contribution of the paper is minimal  the proposed approach to maintain the budget is simplistic  no theoretical analysis of the proposed algorithm is provided  there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multiclass problem, so we would want to see comparisons with some good multiclass algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)	0
this is a very suitable assumption, e.g., for the swiss roll data set, but it is unclear to this reader why it is a suitable assumption beyond such toy data.	0
strengths:  open source data set for air quality monitoring that is significantly better than existing ones.	2
however the smiles data set is a little mixed for active vs passive  authors should try to shed some light into that as well.	0
experiments on only one data set, i.e., mnist, are conducted pros: ' studying how to use unlabeled data to improve performance of gan is of technical importance.	2
for example, the gan is trained on an image data set with many birds and cars but not many fire hydrants.	0
 1. this is an interesting paper  introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm 2. from the initial part of the paper, it seems that the proposed privynet is supposed to be a metalearning framework to split a dnn in order to improve privacy while maintaining a certain accuracy level 3. however, the main issue is that the metalearning mechanism is a bit adhoc and empirical  therefore not sure how seamless and userfriendly it will be in general, it seems it needs empirical studies for every new application  this basically involves generation of a pareto front and then choose paretooptimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures	0
pros:  good new implementation of an existing idea  significant perplexity gains on character level language modeling  good at domain adaptation cons:  memory requirements of the method  wordlevel language modeling experiments need to be run on larger data sets (edit: the authors did respond satisfactorily to the original concern about the size of the wordlevel data set)	2
positives:  the output kernel update is well justified  experimental results are encouraging negatives:  the methodological contribution of the paper is minimal  the proposed approach to maintain the budget is simplistic  no theoretical analysis of the proposed algorithm is provided  there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multiclass problem, so we would want to see comparisons with some good multiclass algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)	0
the authors conduct experiments on several real but relative smallscale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets.	0
pros:  important starting question  thoughtprovoking approach  experimental gains on small data sets cons:  the link between the intuition and reality of the gains is not obvious  experiments limited to small data sets, some obvious questions remain	2
pros: ' its a good illustration of deepneural network machinery at work, and well put together ' the experimental results show it works very well ' good experimental work (different data sets, different algorithms) cons: ' sloppy mathematical presentation	2
the key realworld use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets.	0
in the introduction must energy is used on the importance of large data sets, but it appears that only fairly smallscale experiments are considered.	0
strong points:  new method that generalizes existing methods weak points:  paper should be made more accessible, especially pages 1011  should include more data sets for graph classification experiments, e.g., larger data sets such as reddit'  paper does not include proofs, should be included in the appendix  review of literature could be extended some remarks: ' section 1: the reference feragen et al., 2013 is not adequate for kernels based on walks. '	2
in other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.	0
"added: 20news results still poor for hpd, but its probably the implementation used ... their online variational algorithm only has advantages for large data sets pros: ' interesting new prod model with good results ' alternative ""deep"" approach to a hdllda model ' good(ish) experimental work cons: ' could do with a competitive nonparametric lda implementation added: good review responses generally"	2
extensive experiments show that the proposed method without adversarial training is competitive with a stateoftheart defense method under blackbox attacks.	0
pros:  since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a wellmotivated problem  it is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision  the proposed penalized approximation of the problem is simple and straightforward to try  experiments indicate that the method works as advertised cons:  there are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines  experimental gains are very modest compared to an independentlytrained crf  novelty is a bit low, since the trick of differentiating through meanfield updates is wellknown at this point  efficiency: the number of variables in the crf scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems differentiating the minimumcost multicut problem is wellmotivated, and i think the multiplehuman pose estimation problem is a nice application that seems to fit this abstraction well.	2
overall, the paper explores an interesting direction, but i think the current experiments are too preliminary for acceptance.	0
it is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of blackbox models.	0
overall i think this could be a great paper, but it needs further justification of some of the architectural choices and more rigorous analysis/experiments before it will be ready for acceptance.	0
on the other hand, the experiments on gans just seem to say that the algorithm works but not much more beyond that.	0
in summary, the paper may lack technical novelty in some respect, but the experiments are convincing in terms of proofofconcept, and the approach is smart.	0
experiments:  the comparison to the state of the art is fine, but i suggest adding the results of chen et al, cvpr 2018, which achieves quite close accuracies, but still a bit lower.	0
there is some novelty in the proposed approach, but it is mitigated by the relation to the work of chen et al., cvpr 2018. the experiments show good results, but a more thorough evaluation of the influence of the hyperparameters would be useful.	0
pros:  maop seems successful on the tested games in the experiments  demonstrates that, with a sufficiently engineered method, selfsupervised learning can be used to discover different types of objects, and their dynamics.	2
first of all, the use of scg as a metric is ok, but experiments seem to only focus on early iteration comparisons.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
the authors briefly mention some of it in the “related work” section, but unfortunately there is no comparison being made in experiments (the only algorithm being compared against is a “vanilla” a2c that uses a single scalar reward).	0
more importantly, it seems that all experiments are performed on tasks where the underlying structure is known, however this is almost never the case in practice.	0
the experiments show interpolation is somehow manageable but extrapolation is difficult to achieve.	0
it seems that victim nodes are carefully selected and fixed throughout all the experiments, but it limits the generalization about the performance of the proposed algorithms.	0
however i find the whitebox experiments lacking as almost every method has 100% success rate.	0
in their experiments, a simple lstm seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)).	0
pros:  simple and clean proposal  easy to read cons:  limited insight  weak experiments	2
3.3 talks about weight perturbation, but no figure or experiments are shown until section 5.2 figures are often very far from the corresponding text.	0
overall evaluation: the proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand.	0
the main focus in experiments and the exposition is on 3d point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify).	0
i maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.	0
the paper tests the performance in different experimental settings, but the baselines used in the experiments are concerned.	0
in addition, this paper only shows the curves and numbers in the experiments, but it is better to discuss some cases in the qualitative analysis, which may highlight the contributions of the paper.	0
the work also lacks value in literature review, optimization and experiments.	0
pros: 1. the paper is well written 2. experiments are very detailed and thorough cons: 1. the proposed approach lacks novelty 2. experimentation lacks a user study which helps understand if and when gams are at least as interpretable as rulebased approaches.	2
specifically, i'd have liked to see experiments: ' with/without a context model q ' with a standard discriminator (single output or convolutional), but a microcoordinate generator ' with a macroblock discriminator, but a standard generator ' without coordinate conditioning, but different generator parameters for each coordinate these experiments would help better understand the strength of cocogan and how it fits in with other gan models.	0
"third, the paper lacks proofread and there are some typos and grammtical issues visual at several places, such as ""systems raises"", ""that uses reconstruction residual"", ""multitide"" ""see 3.1"", ""cnn:s"", etc. fourth, in the experiments, the linear operation is applied to different layers and different networks, what's the reasoning behind this?"	0
in the cifar10 experiments, they consider one label corruption setting and lack experimentation on uniform label corruptions.	0
the related works has thorough coverage on label corruption, but these works do not appear in the experiments.	0
i think the goal of the experiments should not be to compare against other 1bit algorithms (though to be precise, 1bit qsgd is a ternary algorithm) , but against the fastest lowcommunication algorithm.	0
the experiments are interesting but the authors should compare to more baselines with alternative priors (e.g. stick breaking vaes, vampprior, epitomic vaes, discrete vaes).	0
 thorough experiments studying the effect of sparsity on the representation weaknesses  no discussion/comparison to other vae approaches that incorporate sparsity into the latents: eptimoic vaes (2017), discrete vaes with binary or categorical latents are sparse (see: discrete vaes, concrete/gumbelsoftmax, vqvae, outputinterpretable vaes), stick breaking vaes, structured vaes for the betabernoulli process (singh, ling, et al., 2017).	0
regarding experiments, the toy experiment in section 5.1 is interesting, but it is not mentioned what network architecture is used in this experiment.	0
i am currently in the borderline mode, but would be very happy to change my evaluation if the focus of the paper is somewhat changed and additional experiments on improving generalization (or some other experiments, but making the results a bit more useful/surprising) are added.	0
"approaches such at least like ""modeling the intensity function of point process via recurrent neural networks"" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data."	0
the random graph experiments (table 3) show the effect of good structure in gcn, but i felt that it is not enough to demonstrate an improvement by gcns.	0
strengths: the description is clear and good analytic experiments are performed.	2
for semisupervised experiments, there is bigger improvement for miniimagenet (4% for both without distractors and with distractors) but less so for tieredimagenet (close to 0% for without distractors and with distractors).	0
their experiments showed that the equivariant network models(stdcnns, gcnns, hnets, et al.) are robust to geometric transformation but vulnerable to pixelwise adversarial perturbations.	0
the experiments have some weaknesses.	0
the experiments demonstrate wins over some autoencoder baselines, but the reported numbers are far worse than old unsupervised feature learning results on e.g. cifar10.	0
strengths:  interesting new objectives for representation learning based on increasing the js divergence between joint and product distributions  good set of ablation experiments looking at local vs global approach and layerdependence of classification accuracy  large set of experiments on image datasets with different evaluation metrics for comparing representations weaknesses:  no comparison to autoencoding approaches that explicitly maximize information in the latent variable, e.g. infovae, betavae with small beta, an autoencoeder with no regularization, invertible models like real nvp that throws out no information.	2
technical quality overall, the experiments are wellthought, but the following questions need to be explained: 1. in the introduction, the authors claim three contributions they made in this paper.	0
weaknesses:  in my view, there should be experiments in which the proposed method is compared with other approaches that improve generalization in deep rl like zhang et al., 2018 and justesen et al., 2018.	0
weaknesses:  the main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer.	0
concerning the experiments: in my opinion the picture of the dataset ant and cheeta is irrelevant and could be removed for more explainations of the method.	0
this insight and the experiments in the paper are interesting, but i am unsure if the paper as it is presented now passes the bar for iclr.	0
pros:  the method shows that for continuous adaptation certain representations can be kept instead of the original examples cons:  the method claims to present a life long learning strategy, yet it is not scalable to long time horizon (memory and inference costs rise linearly with time)  some experiments are not presented well enough to be understood.	2
specifically for the mnist experiments it seems that feature stored are of dimension 256, while the original image is of dimension 784 – this is lower, but no by an order of magnitude (x10).	0
pros: clear description and novelty of the method cons: insufficient experiments.	2
experiments: the experiments are presented in a subset of 5 classes from cifar10 (also used by weinshall et al.), but also in the full cifar10 and cifar100 datasets.	0
the authors should mention that the transfer scoring function was borrowed from weinshall et al, clarify the differences between their pacing functions from those in weinshall et al., etc. 2) the usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently.	0
... eq.(10) thus, the main technical contribution is found in the weighting by {h(x)h(x)^t}, but its impact on the performance is not evaluated in the experiments.	0
overall, i think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough.	0
experiments clearly show that it indeed is resilient to mode collapse, but i have would have been curious in seeing some more discussion regarding this point.	0
'this dataset was not available at the time of the submission, but for the revision it would make sense to also evaluate on the new xnli dataset of conneau et al. (emnlp 2018) for multilingual nli experiments.	0
the experiments look ok but are not groundbreaking and are not enough to make this paper more than a mere combination of existing methods.	0
"this style of analysis is sorely lacking and fits the theme of learning representations  the paper itself is well written and quite polished  the authors have taken great care to rule out any possible confounds through experiments that are not identical to the main claims or objective (specifically the study of section 3.3) to address:  the only concern i have (and it is somewhat significant), is the notion of ""generalization""."	0
in the experiments gcn does worse than glp, but in the paper it is shown that gcn is a special case of glp with the relu function removed from eq. 9. why does removing this function make it worse?	0
both max pooling and jaccard distance are not something new, but the author did a great job presenting the idea and proved it's effectiveness through extensive experiments.	0
 experiments lack ablations and proper comparisons.	0
 i may have missed this, but what exactly are the symmetry conditions that were used in the experiments?	0
 the experiments are very thorough and i appreciate the comparison to the tuned baselines, but i am missing some details in the paper: (a) did you tune the sghmc method in figure 2, as well?	0
(b) mitigating unwanted biases with adversarial learning (which the authors cite, but do not offer any comparison or differentiation) to improve the paper, these related work should be discussed in related work section, and (if applicable) compared to the proposed method in the experiments, rather than a very brief mention of one of them in section 3.3 and no comparison.	0
 my main concern on the whole argument of the paper is whether the benefits we see in the experiments come from the elimination of negative transfer, or just come from having more training labels from different tasks available.	0
 the definition of auxiliary tasks are described in the second last paragraph of 3.3, but it would be clearer if it is also mentioned how they are defined in the experiments section.	0
i can understand the point that training nnlm accelerates the experiments, but the author(s) should consider trying a simply lstm model after the best settings had been discovered (e.g. table 1).	0
strengths:  a new auxiliary learning algorithm  positive results on cifar data set weaknesses:  novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space  there is no attempt to provide a theoretical insight into the performance of the algorithm  the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance  experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario  the paper is poorly written and sentences are generally very hard to parse.	2
the experiments are promising in support of the theory, but they do not seem to address this data invariance property.	0
 the authors say they are only going to show experiments on one possible use case, but then make claims for other use cases.	0
 it is interesting observation that the recurrent network has a better tolerance to noise and adversarial attacks, but i am not convinced giving the sparse set of experiments done in the paper.	0
overall i think the current work lacks novelty, significance and solid experiments to be accepted to iclr.	0
overall, the experiments are interesting but it may be hard to generalize the findings to nonlinear settings.	0
 lacking experiments.	0
however in the experiments training seems to be done with a maximum of two traits.	0
the main motivation was to improve scalability of rl and hrl to large state spaces, but the experiments are on the four rooms domain and the first room of montezuma’s revenge, which is not particularly large scale.	0
experiments on blackbox attacks to inceptionv3 model show that the proposed bandit based attack can significantly reduces the number of queries (24 times fewer) when compared with nes.	0
[1] exploring the space of blackbox attacks on deep neural networks, by arjun nitin bhagoji, warren he, bo li and dawn song, https://arxiv.org/abs/1712.09491 (conference version accepted by eccv 2018) [2] autozoom: autoencoderbased zeroth order optimization method for attacking blackbox neural networks, by chunchen tu, paishun ting, pinyu chen, sijia liu, huan zhang, jinfeng yi, chojui hsieh, shinming cheng, https://arxiv.org/abs/1805.11770 ========================================== after discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments.	0
2. more experiments on targeted blackbox attacks: while untargeted attacks on imagenet is a relatively easy task, i was a bit skeptical on the attacking performance of simba in targeted attacks  since the selection of lowfrequency bases directly limits the search space of adversarial examples, as opposed to arbitrary random directions adopted in qlattack, boundaryattack, and optattack.	0
— experimental results the results are interesting proofsofconcept but a few more experiments/answers would be helpful:  it still appears that pr curve in the highprecision regime (fig 3b) has lower precision than frcnn/yolo.	0
in addition, the details about matrix factorization experiments are also rather lacking.	0
for low shot learning i think that the proposed method shows some promise, but it is difficult to draw hard conclusions from the experiments.	0
some experiments on a image classification context are also provided, but some competitive methods are not evaluated (e.g. pscn).	0
it carried out several good experiments as a good start, but several points are not comprehensively studied and analysed.	0
figure 4 provides some insights but it'd be good if some experiments were done to show clear wins over baseline methods that do not employ performance prediction.	0
 however the exposition (particularly the experiments) does not fully demonstrate this.	0
# strengths controlled toy experiments of deep rl generalization issues: the experiments on breakout quantify how badly a3c overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks).	2
"while the method for obtaining lowervariance gradients is interesting and appears useful, the application to learn optimizers is very much oversold: the paper states that the comparison is to ""well tuned handdesigned optimizers"", but what that comes down to in the experiments is adam, sgdmomentum, and rmsprop with a very coarse grid of 11 learning rates and 'no regularization' and 'no learning rate schedule'."	0
3. i also have concerns with the experiments.	0
the experiments are in very simplistic 2d grid world environments, but it makes the analysis and understanding of the 3d representations h much more simpler to follow.	0
the experiments are generally over enough tasks and compare against several baselines, and although the empirical wins are not that large i feel that they would be sufficient for publication if not for my other concerns.	0
more detailed comments:  my concerns about this work are both on modeling aspects and experiments.	0
the experiments on conditional image generation look interesting, but i wonder if the ground truth transformation for mnist can be simply described as in some linear transformation on the original image.	0
weaknesses: o) the experiments focus a lot on mujuco1m.	0
cons (not necessarily a negative) requires a very careful reading as the paper provides a lot of information (though as mentioned it is very well written) the quantitative evaluation is somewhat lacking in that there are no quantitative psychophysical experiments to compare this model to competing ones across different observers.	2
the work is original enough but might need some improvement or more explanation in experiments/result section.	0
as for the concerns from my original review: the transferability experiments reported in the author comments below are quite informative, and i'd encourage the authors to incorporate them into the paper (or an appendix if space is an issue).	0
figure 2 is a good start, but adding details about the layer sizes, types of pooling layers used, and the model training setup would help clarify the experiments.	0
experiments show that the loss functions created by dl2 are often solved quickly and correctly, but not always strengths  the framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection.	2
the paper claims interpretability but i don't see any experiments verifying interpretability.	0
if the doubts about the experiments are clarified and the methods are motivated better (or the strengths/weaknesses are better analyzed), i will vote for acceptance.	2
"the authors also want to support the pros of adversarial memory units by comparing against ""gradient"" memory units that are trained to decrease the loss with the experiment shown in figure 2. however, figure 2a seems problematic to me, so i am not sure whether the authors are doing their experiments correctly."	0
3. what the experiments have observed may not due to the clipping of negative reward, but could due to the scaling down of the reward.	0
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
in general, the paper is not showing stateoftheart results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.	0
the result section given in the paper is its weakness and requires a more indepth analysis:  the results given for omniglot are impressive  the experiments analyzing the impact of diversity and routing depth are interesting and offer interesting insight into the architecture  the results do not show learning behavior over epochs; this is not necessary, but would give an additional insight into the learning behavior of the architecture  the experimental settings are confusing: why are the different experiments performed with different datasets?	0
this makes it seem as if the authors cherrypicked the best results for the different experiments (this might not be the case, but the results on omniglot alone are good enough that negative results and a detailed discussion of them would not have hurt the paper, but enriched the discourse)  additional experiments that offer a transition from larger datasets to smaller ones would be interesting; seeing how the performance of the architecture behaves e.g. on cifar10 for 1k, 5k, 10k, 25k and 50k would have illustrated how well the architecture is able to generalize from different numbers of samples in summary, i think the paper analyzes a very important problem and has a lot of potential.	0
i also have concerns about the independence assumption in their sampling distribution (section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.	0
however, i feel the paper is lacking a convincing evidence (from what i could find the authors base all the conclusions on one set of similar experiments performed with one generator architecture on one data set) in order to be viewed as a significant contribution to the generative modeling field.	0
this also leads to my concern about the experiments.	0
while language modeling in terms of perplexity is not necessarily a focus of this paper, my concern translates also to the remaining experiments as they use the same weak baseline.	0
the experiments with regards to robustness to adversarial attacks i find convincing, however the overall performance is not very good (such as the accuracy on cifar10).	0
experiments: the experiments show that the proposed method is better able to reconstruct examples than does ali  a result is not necessarily surprising, but is interesting and worth further investigation.	0
the current experiments show that the method works better on lowdimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.	0
  update: i think the experiments are interesting and worthy of publication, but the exposition could be significantly improved.	0
i could think of a few more experiments regarding submodularbased models, possibly different settings of the ‘diverseclick’ data for a sensitivity analysis, and a more direct comparison to [ai, et al., sigir18], but this isn’t required to make the results sufficiently convincing.	0
a few more experiments would make this case stronger, but has realworld data.	0
(6/10) === pros ===  extends a widely used model (pointernetworks) to the reranking setting  discusses practical issues in getting this to work at scale  shows that it works in a realworld setting  contextualization within existing research shows good understanding of related work === cons ===  is a fairly direct application of pointernetworks with the innovation being in the details (i.e., is more of an ‘industry’ paper)  additional experiments around ‘diverseclicks’ settings (to see how smooth the performance curve) and submodular comparisons may have been interesting in summary, i think there is room for improvement (some outlined in the conclusion), but is an interesting finding with promise that i plan to try myself.	2
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
[cons in summary] 1. the motivations/claimed contributions are not well supported/illustrated by the proposed algorithm or experiments.	2
cons:  all of the experiments were done on toy datasets.	0
 the paper suggests with experiments that gqn generates predictions that are less consistent across samples, but it is not clear exactly which design decisions lead to this difference.	0
even the ‘focused experiments’ can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the roboschool examples are a bit more interesting, but also less conclusive).	0
it is easy to ask for additional experiments (i.e., other mechanisms of uncertainty such as the countbased discussed in related work, other settings in 2.2) — but the quality seems high enough that i basically trust the settings and findings.	0
(5/10) === pros ===  demonstrates that curiositybased reward works in simpler game environments  (implicitly) calls into question the value of these testbed environments  well written, with a large set of experiments and some interesting observations/discussions === cons ===  little methodological innovation or analytical explanations  offers minimal (but some) evidence that curiositybased reward works in more realistic settings  doesn’t answer the one question regarding observation representation that it set out to evaluate  the more interesting problem, rl  auxiliary loss isn’t evaluated in detail  presumably, the sample complexity is ridiculous overall, i am ambivalent.	2
as such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. florensa et al. 2017 and aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.	0
it is also pretty unexpected that the uniform policies are doing so poorly, worse than the standard policy for the pommerman experiments.	0
it appears in your transfer experiments that you do indeed train the gdm faster to adapt to the model dynamics, but it doesn’t appear to help your gats algorithm actually converge to a good level of performance.	0
cons: • the experiments on cifar 10/100 seems to be at par with a conventional crossentropy loss.	0
it's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.	0
regarding the experiments, and this is probably the weakest part of this paper, i would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided).	0
i can’t validate the sota claims, but it seems like the model is still improving: are there’s further experiments?	0
i understand that what i ask for is very difficult to answer, but experiments with more datasets and different types of queries (such as satisfiability) might have made me happier.	0
authors suggest to further scale loss functions using the cosine similarity but it only experiments with the simpler case of binary decision of using both gradients or only the main one.	0
the authors cite many interesting, realistic and practical setups (zhang et al., 2016; jaderberg et al., 2017; mirowski et al., 2017; papoudakis et al., 2018), but do not use any of these setups in their experiments.	0
however i felt the baselines provided in the experiments were insufficient, and i would recommend the authors improve these and resubmit to a future conference.	0
[weaknesses] 1. the authors performed multiple experiments regarding various tasks defined in this paper.however, i can hardly find any quantitative evaluation for the results.	0
the paper spends most of the time discussing the relationship between the network size and the generalization error, but it does not have experiments supporting the hypothesis that harder problems are more difficult to fit or to generalize (in the paper's terminology, large beta_g and large beta_p).	0
authors mention that hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for imagenet experiments?).	0
"pros  clarity of the ideas that are presented  interesting unifying perspective on sequence generation algorithms  insightful new interpretations of existing algorithms in terms of exploration cons  the example new algorithm is not very original  the associated experiments are incomplete ==> details 1. page 2, ""dayan & hinton (1997); levine (2018); abdolmaleki et al. (2018) study in a probabilistic inference perspective."""	2
"2. at the beginning of section 3.1, policy optimisation is a family of algorithms 3. page 7 in the setup of the experiments, ""we use the adam optimizer for sgd training"" is incorrect since sgd is not a family but a specific algorithm, which is different from adam."	0
pros:  interesting idea  experiments are interesting cons:  formal results are either trivial or could be improved in their statements  experimental guarantees only, up to what is hidden in the bigoh notations of theorem 2.2, 2.3. details: ' in theorem 2.2, you need to remove the , unless you point to the taylor theorem that guarantees that for the identity you claim before (5).	2
whitebox attack and blackbox attack the paper is well written, both theory and experiments are well explained.	0
so if the paper wants to claim blackbox robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.	0
also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)?	0
overall, the paper proposed a reasonable method, but the significance of the paper can be better justified by more solid experiments.	0
however, some parts in the paper are not wellsupported and many important elements to understand the experiments are lacking.	0
"one could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds."	0
concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case?	0
i do not have any other major comments for these sections as my main concerns are about the experiments section.	0
i would appreciate if the authors could address my concerns below, and i would be happy to modify my score accordingly: 1) my understanding is that the proposed method (the one named bliss in the experiments) only makes use of the proposed semisupervised framework (section 3.2) and is not followed by the iterative procrustes refinement (section 3.3), but this is not clear at all from the paper.	0
the paper has the following weak points: 1. the reliance on inpainting for almost all experiments is somewhat worrying.	0
the authors tried to add experiments on imagenet, but these experiments apparently didn't finish before the end of the rebuttal period.	0
the model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.	0
note that the use of four columns corresponding to different beam sizes is misleading… this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.	0
also, i think the authors could try to emphasize more on the shortcomings of rws discovered by the gmm experiments, and how defensive importance sampling fixes it.	0
i do not expect the authors to be able to modify this paper to be accepted at a machine learning venue, but the experiments for the blackbox functions appear more promising.	0
quality: the introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing. '	0
cons:  the experiments do not entirely support the utility of the method, even as proof of concept (e.g. not varying the pool size, no results on computation speed comparison, competitors are not optimized, mnist experiments have not converged).	0
none of these are proved by experiments: a. highdimensionality is not addressed b. computational efficiency is mentioned in the conclusions, but there are no experiments comparing run times, and no discussion about algorithmic complexity.	0
3. rl vs not rl: often times in discussion you mention “agents” and rl (e.g. entire section 1.1), but none of the experiments are about rl.	0
pros:  sgd is a central algorithm and further analysis laying out its properties is important  thorough experiments.	2
some of the figures in the experiments are a little cluttered and the lighter colors can be hard to see (e.g. fig 2(b)), but this is minor.	0
positives: mr algorithm might be good but no compelling experiments are presented.	0
this is not only a logical fallacy of circular reasoning between theory and experiments, but also a faulty result because the actual experiments use a unimodal prior (see issue #1).	0
issue #4 (experimental): the title suggests no weakening of the decoder is needed, but experiments are not performed with decoders of varying sizes despite all the issues above with incorrect theory and experiments, it could still be possible that feature injection with bow is helpful to prevent posterior collapsing of text vaes.	0
"apart from this, the paper claims that combining these tricks avoids ""weakening the decoder"" but uses a decoder with word dropout in all experiments, and also claims they decrease the number of hyperparameters, which is not shown in any rigorous way."	0
hence, this paper lacks enough novelty for publication, and it is not clear from the experiments that the specific method proposed in this paper is better than others in the sota.	0
second, the experiments are (as i understand it, but i may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed rl method.	0
empirically, the authors show that the proposed model achieves better results on 2 out of 4 domain adaptation experiments on digits, but without proper statistical testing it is not clear whether the improvement is significant or not.	0
in experiments, when training the shared encoder with both instances from source and target, the selection procedure of .hat{x}_s plays an important role in the final result, but the authors didn't describe how this part is being done.	0
cons: this is a nice idea and some preliminary experiments show positive qualitative results.	0
"it seems so, but this could be more explicit in the statements, even just by replacing ""an existing .mathcal g"" with "".mathcal g = .xi(s_g, b_g, l_g)"" in the statement of (e.g.) theorem 1. in your numerical experiments: you don't make it at all clear enough that you're plotting 'different loss functions' for the gans and the other methods!"	0
 experiments the results on the synthetic data seem promising, but the results on mnist and cifar10 are not impressive enough: ' the visual quality of figure.9 does not look very appealing.	0
the experiments are however interesting as the paper compares to the latest hyperparameters optimization strategies for neural nets on simple tasks (eg cifar10) and gets comparable results.	0
in their experiments, they show that they can outperform a jpegideal channel code model, but perform similarly to a vaeldpc (ldpc is a classic error correcting code) setup.	0
the same goes for experiments based on vqvae, the paper simply states that they were not able to get this working, but not what experiments were run to come to this conclusion. '	0
(applicable to both experiments) minor issues: in 4.2 the white and the white > the black and the white	0
concerning the experiments, i don't understand what is the split between training and testing data.	0
pro: ' the approach is proposed for irl, gail and bc cons: ' lack of positionning w.r.t pomdp litterature ' lack of details in the experiments, and lack of good experimental results ' low contribution in term of model [merel et al. 2017] learning human behaviors from motion capture by adversarial imitation [choi et al.] inverse reinforcement learning in partially observable environments	0
  using a generative model as the surrogate distribution for kernel twosample test is novel  an important and new application of deep generative models  strong experiments on synthetic and realworld time series data sets  very clear writing and explanation of the idea  reply sample segments from both directions (past and future) while in the practical setting, cpd is usually sequential and in one directional  lack theoretical understanding of the limit of the neuralgenerator in the kernel twosample test	0
you have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.	0
the method is obscure to me, but from my point of view, the experiments are done quite thoroughly and the results look good.	0
weaknesses: i had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3).	0
now coming to the experiments, the results are presented in a table that is poorly formatted.	0
"this is not merely because they are more commonly used than grus, but they are strictly more powerful  see ""on the practical computational power of finite precision rnns for language recognition"", published at acl 2018. given the results in this paper and actually the paper that first introduces the forget gate for lstms, it seems that performing these experiments solely with grus might lead to wrong conclusions about rnns in general."	0
it not only shows new theoretical results, but also conforms their validity in realworld experiments.	0
2. i don't think that it is really appropriate to call 3layer model a 'deep learning model', i would recommend to just name it 'neural network' also, i think that experimentally paper is pretty strong, but it would be nice to see the repository with algorithm code and experiments available.	0
# strengths sgspens are better across the experiments than other spen training methods, though i do not know why they are not compared against rspens on multilabel classification.	2
unfortunately, i do not think this paper as it stands currently is ready for publication at iclr for the following reasons: (1) the comparison and discussion with prior literature is lacking; (2) the experiments are rather weak; and (3) the significance and novelty of the method itself seems limited.	0
however, given the lack of experiments demonstrating this, it is hard to say how significant the approach is.	0
it however needs more thorough analysis or experiments that validate the ideas as also experiments on harder, largescale datasets.	0
however, i still have some concerns about the work: 1. in the experiments about image generation, it seems that the proposed method does not enhance the performance obviously when compared to gp and wgangp, why the combination of vgan and gp can enhance the performance greatly(how do they complementary to each other), what about the performance when combine vgan with wgangp?	0
unfortunately the paper falls short in two main areas:  novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on velickovic et al 2017)  impact: the results achieved in the experiments are very small improvements compared to the baseline of rgcn (~ 0.01 in two experiments and ~ 0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyperparams, ...) however, on a positive note, the paper has been written very well and i really liked the frank discussion on page 8 about results on mutag dataset.	0
"regarding cons:  the critical structural choices (such as the attention model in section 3.2) are presented without too much justification, discussion of alternatives, etc.  the experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results."	0
 the experiments show good results compared to existing algorithms, but not impressively so.	0
some experiments seem to follow this idea such as 'mdnconad{2, 4, 8, 16}' in table 2. but in these experiments the addition of conad offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16).	0
one main concern is that most of the experiments seem to have results as visual inspections of figures provided.	0
the approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack appletoapple comparisons.	0
" experiments fall short of really using the setting proposed  the paper focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties (including downsides, such as variance increase) detail: ' the paper claims to propose a ""theory"" for spread divergences (conditioning a fdivergence by a ""thirdparty"" conditional distribution on supports which makes supports match) still keeping the identity of indiscernibles. '"	0
also, appendix c lacks details to understand precisely how the experiments where done.	0
shortcomings of the paper: 1. the experiments seem rather weak.	0
however, i have have three main concerns: 1) unconvining experiments.	0
in the end, i understand many choices have to be made for any evaluating setting, but many choices are very arbitrary (end of section 3 and especially experiments) and there is a lot of tuning, so it’s unclear whether some of the observations happen just in a particular combination of choices, but are more general.	0
 as identified by the authors themself, lacking of supporting experiments on largescale dataset and realworld models.	0
i understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.	0
however the novelty is limited which means that numerical experiments should be quite extensive to demonstrate a clear impact on the field.	0
the experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but i am not entirely sure whether the given architecture is necessary, see comments below.	0
the experiments suggest that updating a small subset of parameters can improve results significantly in the 1shot regime, but the gap between normal maml and the subset maml is much smaller in the 5shot regime.	0
overall, i found the paper fairly confusing and poorly written, with unconvincing experiments.	0
i also found the adaptation to new tasks and zeroshot experiments very interesting but the setup was not described very concretely: in the transfer learning section, i hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multitask objective or if there would still be performance gain by pretraining a model on only one of those tasks.	0
experiments: the experiments are somewhat limited, but show the expected correlations (e.g. distortion vs predictiveness).	0
i also think that contributions compared to other works could be made more clear, as well as additional experiments and discussions of the shortcomings of this approach may be added.	0
my only concern is the experiments: 1) some of the benchmark datasets for the proposed task as well as some wellknown methods (see battaglia et al’18 and references in there) are missing.	0
the experiments are good but could be designed a bit better.	0
 learning deep embeddings in krein spaces quality: average originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: limited experiments  see detailed comments the underlying idea is interesting and probably novel.	2
however right now, the paper lacks a lot of details on how the experiments were run.	0
the choice of lstms is understandable but other experiments could have been done in order to make clearer why it has been chosen.	0
although the idea is interesting and the presented results seem promising, there are some key experiments lacking that may prevent this work from making its claims on robustness and detectability.	0
 pros seemingly reasonable approach to polyphonic music generation: figuring out a way to splitting the parts, share parameters appropriately, measuring entropy per time, all make sense the resulting outputs tend to have very shortterm harmonic coherence (e.g. often a ‘standard chord’ with some resolving suspensions, etc), with individual parts often making very small stepwise motion (i.e. reasonable local voice leading) extensive comparison of architectural variations positive results from listening experiments cons musical outputs are 'not' clearly better than some of the polyphonic systems described; despite the often small melodic steps, the individual lines are quite random sounding; this is perhaps a direct result of the short history i do not hear the rhythmic complexity that is described in the introduction the work by johnson (2015) (ref.	2
with respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references).	0
3) the experiments lack comparisons to several important baselines from selfsupervised learning community, and methods using soft labels for training (as mentioned in 2) above).	0
however it needs more experiments to confirm some of its claims about scalability and flexibility.	0
nevertheless i have some major concerns with the methodology, proposed evaluation metric and experiments.	0
strengths  i applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the gp adapter framework (gpgru) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)  it likewise outperforms both commonly used preprocessing (gruf) [2][3] and the much more complicated neural net architecture (gruhd) from [6] (across two datasets and tasks)  the simplicity of this approach probably lends itself to additional customization and innovation  the literature review seems quite thorough and does an especially nice job of covering recent work on rnns for multivariate time series and irregular sampling or missing values  the experiments are thorough and welldesigned overall.	2
more data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments).	0
 the mimiciii experiments omit the gpgru model, which weakens the results by leaving the reader to imagine how it might compare (i would expect it to outperform the proposed approach by an even wider margin than it did for uwave).	0
of course, the experiments use different cohorts and variables so they're not directly comparable, but it nonetheless diminishes the potential impact of the results presented here.	0
however, unlike as advertised, the paper does not address the domain shift issue in metalearning, and the experiments lack thorough evaluation as the paper considers itself as a metalearning paper and only compares to other metalearning approaches without much comparison to domain adaptation papers.	0
the experiments in the paper seem correctly executed and it is nice that there are multiple baselines but i'm not convinced that the comparison is very insightful.	0
it would be more insightful to see how the method performs on more challenging tasks where exploration is more important, but i understand that these experiments are computationally demanding.	0
to make up for the clarity and correctness issues with the mathematical justification of the approach in the paper as currently written, the results would have to be especially impressive compared to strong baselines, but the experiments section does not give enough information to make that evaluation.	0
overall, there are some interesting ideas here but the paper needs more work to polish the mathematics and to produce convincing experiments.	0
rule learning ilp tasks: i don’t know enough about learning logic rules tasks to comment on those experiments, but table 3 seems overwhelming and the concept of 10 runs is still unclear.	0
overall, i found that this work is empirical, and i’m not convinced by its experiments about the advantage of multiplediscriminator training, due to lacking of fair computational cost comparison with singlediscriminator training.	0
" [some experiments missing] the experiments section 4.6 uses a case of none and ""best wd"" to address some of my concerns."	0
this paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.	0
this is briefly mentioned in the paper, but i think it needs to be made more explicit, and lwf should be a baseline in the experiments to clearly indicate the benefit of keeping this data.	0
pros: 1. detailed proofs presented in appendix 2. they present 6 questions and answer them with effective experiments.	2
the proposed algorithm does not seem to do better than a 5 years old dqn algorithm… i know this was not the main contribution of the paper, which is theoretical, but then it’s not clear to see the added value of the experiments.	0
"in fact, authors explicitly claim that empirical section ""note that in these experiments, the purpose is not to achieve state of the art performance, but to exemplify how backdrop can be used and what measure of performance gains one can expect.""."	0
the authors do point to the appendix for their experiments section, however this is not a good idea.	0
i will list the issues and some suggestions i have, in order to help make this work better, hopefully good enough for acceptance: 1) the authors cited [1] a few times in the paper, but actually their approach of using a vae to compress frames into a latent, an lstm to predict the next latent, and a cmaes trained network for the policy is precisely what is proposed in [1] (which had experiments that trained on the actual environment, like in this work, and also the generated environment).	0
cons: 1. i don't think the experimental results are convincing enough for the reasons below: 1.1. all experiments are conducted over mnist with testing accuracy around 96%.	0
there are just too many seemingly arbitrary choices in the design of this benchmark and the lack of interesting findings in the baseline experiments highlights these issues.	0
5. macaque experiments: some experiments on macaques were performed for this article, but there is no mention of ethical guidelines and whether they were respected.	0
overall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could.	2
pros: clearly written; experiments on the datasets chosen do seem to suggest that the proposed methods have potential.	2
experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.	0
9. the loss weighting, gradient guiding, gradient seeding, and criterion weighting conditions are not clearly defined but need to be to understand the ablation experiments.	0
all experiments are constructed to show that the method can indeed achieve accuracy comparable to the full model but with a smaller training set.	0
however i find it hard to say anything about the proposed approach, whether it improves over previous ones or not, specially because the experiments are limited to toy problems.	0
lack of experiments: the currently presented experiments are all on rather simple data.	0
results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain.	0
pros:  clearly written  useful experiments for those seeking to select a differential bayesian filter, and learning (heteroscedastic) noise from data.	2
 experiments on realworld use cases rather than toy problems cons:  incomplete experiments according to footnote, thus results and conclusions might change after this review.	0
"section 1: ' ""our experiments show that ... "" this may be a matter of taste, but i did not expect to see the main conclusions already in the introduction."	0
"pros:  generative modeling of sequence data still in its infancy  potentially lower variance than policy gradient approaches  experiments are promising cons:  lots of grammatical errors and odd formulations questions:  equation 14: what does it mean to find the ""maximum entropy solution"" for the given optimization problem?"	2
cons: some details are missing in the experiments.	0
the experiments are another cause for concern: fan et al. should have been tested as a baseline with similar implementation (controlling for architecture and .lambda), and implementation differences in prior works of table 1 make it difficult to draw conclusions.	0
pros:  the paper is well written, very easy to read, well explained (and better formalized than [xiao and al.]);  the idea of deforming images is new (if we forget about [xiao and al.]) and simple;  experiments show what such a technique can achieve on mnist and imagenet.	2
i fail to understand how to process this sentence, but i have a feeling that it’s incorrect to make any claims to the performance of “rnnlike models” as such a model was not used as a baseline in the experiments in any way.	0
 the experiments are lacking in some respects.	0
 this paper shows through a set of experiments that the common belief that a large neural network trained, then pruned and finetuned performs better than another network that has the same size of the pruned one, but trained from scratch, is actually false.	0
cons:  it is still difficult to believe that most of the previous work and previous experiments (as in zhu & gupta 2018) are faulty.	0
pros:  nice connection between the optimization of the prior (or likelihood function) and ratedistortion theory cons:  lack of discussion on important related works  weakness of the experiments	2
however there are questions about the experiments, discussions around the experiments and the usefulness of the observation for training better models and/or giving additional insights to what we know.	0
the experiments are extremely lacking, not only are any of their cited alternatives compared, they don't compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.	0
i do, however, have significant concerns about the experiments.	0
this, however, does not guarantee robustness (see [1] for why such “unfriendly” landscapes can usually be circumvented) some concrete evaluation concerns and experiments that the authors can run to alleviate them:  figure 5 shows adversarial robustness even against eta = 0.25at this value of epsilon, the attacker should be able to create realistic images of other classes (even without pgd), so this suggests that the loss is somehow making examples hard to find rather than removing them.	0
experiments in section 4.5 lack detail and context.	0
 after reading the authors' response and their additional experiments, i still see this work as a very decent paper, but not a very exciting one.	0
 update: i still feel that the paper should have either strong theory, strong experiments, or some of each to be accepted, but that both are lacking.	0
3.the experiments lack details for reproducing the results or generalizing the gain to other problems.	0
pros: 1. the paper is clearly written and easy to follow; 2. the way the authors introduce timedependent gating into lstm is easy to follow and reimplement; 3. experiments on various tasks of long temporal dependencies do show improvement over the standard lstm cells; 4. the experiments on the adding task does show glstm is less sensitive to initialization than the phased lstm; 5. the experiments on setting curriculum training schedule to improve convergence on lstms are insightful.	2
however, the paper lacks the technical novelty and presents only limited experiments and analysis.	0
unfortunately, the experiments are seriously lacking in my opinion, as i believe 'the major focus' of those experiments should be comparisons to other denoising / regularization techniques.	0
the lack of experiments outside of one video action classification & captioning dataset (and one additional one for a transfer learning study) limit the empirical generality of the findings.	0
 the experiments in section 4 are of course very limited, but this paper makes a significant theoretical contribution, so i don't really see the need for extensive experiments.	0
if claiming sota results, compare to [1] or [2] which use different methods to the one proposed in these paper but outperform the baselines used in the experiments.	0
the experiments in the current paper are preliminary but encouraging.	0
pros: 1) overall nice but simple extension of tt/ tr framework 2) nice set of experiments which have shown improvement over standard tt/ tr framework for mtl.	2
"cons: very little can be deduced from these experiments:  ""increasing the batch size beyond a certain point yields no improvement in wallclock time to convergence, even for a system with perfect parallelism."""	0
they do mention that we can choose any labeling we want, but still strange that for the experiments this was not taken into account.	0
the experiments involve synthetic regression and classification datasets but there are no novel insights that advance what is already known about the hyperparameter optimization (e.g., see [3]).	0
first, the experiments are only done on a small scale dataset (cifar10), which is ok in general, but questionable when the proposed method explicitly targets big data regime and making the training faster.	0
i appreciated that the onetoone transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.	0
overall, this is a nice paper with a small, incremental idea and substantial experiments that show its practical value.	0
strengths:  the experiments are very thorough.	2
weaknesses:  the experiments are done on cifar10, cifar100 and subsets of cifar100.	0
the base approach is the same as presented in [1], but the changes to the learning procedure are adequately justified (and the experiments corroborate this).	0
the lack of discussion of this point and the omission of such experiments is highly suspicious.	0
(4) experiments: the intro states that training with 10 images reaches 94% accuracy, but this does not seem consistent with the results in table 1. the caption of figure 2 suggests that accuracy is between 12% and 94% which means the stated 94% is not representative or typical.	0
originality  the casting of the problem as domain adaptation is original but from the experiments it was not conclusive as to how much benefit we get.	0
pros a good idea, enough experiments that indicate the benefit of casting this as a domain adaptation problem.	0
cons i feel, the authors should have extended the experiments to imagenet which is a much larger dataset and validate the findings still hold, i feel the discussion section and comparison to other methods needs to be worked to be more thorough and to tease out the benefit of each of the various terms added to the loss functions as currently all we have is final numbers without much explanation and details.	2
 examplar experiments show the possibility of using the key idea to generate adversarial examples weaknesses:  the experiments are very limited and show just 5 examples of generated images on mnist and imagenet.	0
it seems it is working from the experiments but a better analysis is required on how such a bijection is learned and if there are any specific properties of such bijection such that it will work only in some setting.	0
my advice to improve this work would be to do more experiments and to show better that the lack of performance gain is due to overfit, and can be fixed with larger data or data augmentation.	0
"the evaluation further depends on a ""selective classifier"" which is not detailed, but critical to understanding the experiments."	0
for instance in the generalization experiments, the numbers presented seem to show some interesting (and somewhat surprising) trends, however the authors do not really pursue these or provide any insight as to why this is the case.	0
some weak points as well 1. application value is not so big, as there is no real application problem and the experiments are based on simulation.	0
2. especially ike the fact that these connections help discover flaws in existing algorithms (like the modecollapse issue in fun) cons: 1. the options experiments (and comparison to fun) are done on simple atari games, which do not benefit as much from hierarchical policies.	0
i) the proposed architecture is mainly adopted from the original transformer but it is highly related to the baselines used in the experiments.	0
the experiments are fine, but they left me with questions: 1. is it a surprise that the network learnt to compose objects and inpaint over occluded regions?	0
using this for universal attack is interesting, however the experiments are not that convincing: 1. to show this is a good way for universal attack, i think the authors should compare with previous work in (moosavidezfooli et al).	0
"(3) lacking solid experiments: in section experiment, the authors claim ""finally, we show the tradeoff for pruning resnet50 on the ilsvrc dataset."	0
overall feedback: this is a wellwritten paper, but i think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.	0
4) conclusion the problem tackled is a difficult one, but other papers that are not included in experiments have been tested on this task.	0
 the paper is written rather well, however i find the experiments incomplete and have some reservations about the method.	0
the experiments show the singlepath inference doesn't lose much accuracy but it saves memory and time.	0
these experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach.	0
there are many notational issues which i go into below, but the key issue is experiments and reproducability.	0
"this seems to preclude even basic architectural advancement like skip connections / resnet  the authors even mention this in section 3.1, and point to experiments on resnets in section 4.4, but the words ""skip"" and ""resnet"" do not appear anywhere else in the paper."	0
the experiments show improvements over existing methods but i can't tell whether the right baselines were used.	0
because of the above mentioned flaws (especially 3 and 4, and lack of experiments), i think the paper is below the standard of iclr conference.	0
"i quote: ""we refer to the report foll et al. (2017) for a detailed but preliminary formulation of our models and experiments."""	0
strengths: models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.	2
cons: the experiments do not push the limits of their method.	0
pros:  the idea makes sense and it seems gpu friendly in the sense that the flops reduction can be easily converted in a real speedup  results show that the joint use of two resolution can provide better accuracy and lower computational cost, which is normally quite difficult to obtain  the paper is well written and experiments are well presented.	2
 the appendix shows many interesting additional experiments cons:  the improvement in performance and speed is not exceptional, but steady on all models.	0
experimental protocol: i understand that such an approach is difficult to evaluate quantitatively but i am not sure what there is to learn from experiments reported in table 3, as there is no point of comparison on this task.	0
in particular, the proposed evaluation seems valid when applied to different crosslingual mapping methods over the exact same embeddings, but its validity beyond that is not obvious nor tested empirically in the experiments.	0
it finds that deep generative replay (dgr) that learns to generate imaginary new samples from previously seen training data, potentially augmented with soft labels seems to work best in these specific experiments but potentially doubles the computational cost.	0
cons: 1. details of each experiments are missing.	0
the experiments show that this method's performance is impressive compared to an lbfgs implementation provided by bollapragada 2018 , but as i recall that paper presented a variable/increasing batch method, while the authors' method uses fixed batches (as far as i can tell) so it is not clear that comparison on time alone is sufficient.	0
which would be fine but some key experiments are missing to make the paper empirically rigorous.	0
in terms of strengths, the ganbased approach is wellmotivated and it appears that the authors were thorough in their experiments on cora/citseer (e.g., with a number of ablation/sensitivity studies).	2
the idea is neat and the experiments suggests that it works, but what comes later in the paper is mostly rather straightforward so i doubt whether it is sufficient for iclr.	0
pros:  the background, model and experiments are clearly explained.	2
in general, i find the idea original, and of potential practical use, but i believe the contribution of paper to be limited and not well supported by experiments.	0
the idea in this paper is novel but experiments do not seem to be enough.	0
the experiments are on grid worlds but for such a novel problem like this i think they are at the right level because they allow the reader to understand the results.	0
pros: interesting and novel idea cons: unclear transfer learning model, insufficient experiments.	2
the need for effective ways to quickly generate adversarial attacks in rl is clear, but the authors' experiments don't seem to clarify that their proposed aproaches achieve this goal.	0
however, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, i feel that the paper still require improvement before it can be accepted.	0
experiments show the proposed approach outperform the two baselines but in some cases the confidence interval is quite large and prevent definitive conclusions (e.g. up to 0.05 in table 2).	0
strengths  ' seems original: i'm unaware of any other method connecting rule lists and prototypes and nns ' neat applications to healthcare limitations  ' interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rulelists shown is an applestoapples comparison ' prototypes themselves never evaluated ' many design choices inside method not justified with experiments  why highway networks  rcnns?	2
pros: ' findings provide us useful direction for future research (that dataparallelism centered distributed training is going to hit the limit soon) ' extensive experiments across 5 datasets and 6 neural network architectures cons: ' experiments are a bit too much focused on image classification ' error bars in figures could've provided greater confidence in robustness of findings	2
to ensure reproducability of your results i ask you to provide the respective codes e.g. on github (can be done anonymous)  repeating myself from the last review: there is a lot of work addressing that making a kernel psd may not be good idea  you provide experiments for a small number of data where your kernel is now psd but what is with the other data (where e.g. in pekalska and followers it was shown that making them psd is bad ... )  is your approach solving this  or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?	0
the experiments would be stronger by e.g. approximating the marginal heldout loss (perhaps using iwae or otherwise), since it seems almost guaranteed that more flexible variational families should achieve a tighter bound on the training set (it's possible that there were actually heldout metrics but i missed them, in which case please let me know).	0
cons experiments should be more thorough lack of clarity made it hard to understand at certain points	2
as a sidenote, i realize that this might sound like a standard request for 'show more experiments', but i think the paper would be more impactful if it contained one scenario in which its benefits over other approaches are clear.	0
 minor 'gripe': is defined as a set in eq. 2, but it is treated as a matrix or an operator (and also referred to as such); this should be more consistent  the discussion of the aggregation of multiple statistics in section 3.2 appears to be somewhat redundant in light of the discussion for eq. 4 and eq. 5 in the preceding section  in the appendix, more details about the training of the fcn should be added; all other parts of the experiments are described in sufficient detail, but the training process requires additional information about learning rates etc.	0
this section is unclear to me, i had initally assumed that the data would be fed in one pixel at a time, but due to the presence of the other experiments i presume this is not the case.	0
i do not think the paper sufficiently motivates why a monotonic decay should be good, and while the new sota on permuted mnist is great, i'm concerned that the first experiments are not reproducable, as detailed previously in this review.	0
two more interesting experiments are given and have convincingly superior results (at first glance) but i am not familiar with those domains.	0
the experiments could benefit from some more rigorous statistical analysis, but that would most likely require a higher number of repetitions, which is understandably hard to do in a largescale setting.	0
 cons  the experiments are conducted thoroughly in the imagenet, but the selection of the dataset is not appropriate.	2
pros:  available source code  large number of experiments cons:  the exposition could be improved, in particular the description of the plots is not very clear, i'm still not sure exactly what they show  not clear what the target audience of the first part (section 2) is, it is too technical for a survey intended for outsiders, and discusses subtle points that are not easy to understand without more knowledge, but at the same time seems unlikely to give additional insight to an insider  limited amount of new insight, which is limiting as new and better understanding of gans and practical guidelines are arguably the main contribution of a work of this type some suggestions that i think could make the paper stronger  i believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.	2
however, the paper lacks clarity and the experiments are not really convincing.	0
cons: ' experiments were underwhelming, and the choice of problems/parameters to tune was not the right one for the problem. '	0
parts of the paper could be clearer quality: ' i believe that although the idea is great, but the quality of the experiments could have been higher.	0
i was excited to see experiments with carla, but was underwhelmed when i realized that the only parameter of the simulator that the method controlled was the number and the type of cars in the scene, and the task of interest was a car counting task (for which not much detail was provided).	0
pros/cons summary:  the proposal yields good results in the provided experiments  minor contributions that are not convincing enough  muddled presentation of ideas  dubious or weakly motivated design choices  poorly written with plenty of typos  difficult to follow	2
cons:  the experiments evaluation is restricted to simplistic environments.	0
the experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.	0
 the experiments are very lacking.	0
although the idea seems to be interesting and novel, but not enough evidence to prove the efficiency, from both theoretical and numerical perspective, even though many numerical experiments are proposed.	0
pros: ' establishing a connection to other topic of research often facilitates productive collaboration between two fields ' provides a new perspective to understand prior work ' provides new useful algorithms cons: ' experiments were conducted on small models and small datasets ' unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear walltime would actually be reduced with these algorithms.	2
 the experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates.	0
lastly, the experiments are done on some standard but small benchmarks, and my personal experience with these datasets are that it is very easy to overfit, and most of the effort will be put in to prevent overfitting.	0
this could be fine for many 3d application, but results may lack an exhaustive comparison with other spherical and manifoldbased methods on the proposed experiments.	0
simulation results in the paper only demonstrate how the use of the loss changes the solution but there is no discussion or experiments on complexity of training models that use this approach.	0
concerning experiments, generalized zero shot learning (gzsl) experiments seem to significantly outperform other methods, whereas results on the standard zeroshot learning task perform as well as stateoftheart methods.	0
experiments are conducted on thumost14 and activitynet 1.3. i like the theoretical part of this paper but have concerns about the experiments.	0
while the idea for obtaining a variational upper bound on the generative mutual information is novel and clever, the experiments in the paper are lacking.	0
to do so they introduce two lagrange multipliers, beta and lambda in their notation (equation 11) but there are no experiments showing the effect of these two hyperparameters.	0
they have what should be both an upper and lower bound on the same quantity, the generative mutual information, but these are not shown separately for any of their experiments.	0
although the authors propose some method to balance the computation between distributed workers, which should be important for distributed optimization algorithm design, but not enough numerical experiments are proposed to prove the efficiency.	0
major concerns: 1. while the equivalence of one iteration of a prox algorithm and a single forward pass of the block is understandable, it is not clear what happens from making several iterations (10 in the case of fullyconnected layers in the experiments) of the prox algorithm.	0
"""coordinate conditioning"" is an interesting approach, but i think the paper still lacks convincing experiments for its main motivating use case: generating outputs at a resolution that won't fit in memory within a single forward pass."	0
i understand that the objective function, j_{comp}^{qed} is newly developed by the authors, but not intensively examined in the experiments.	0
my biggest concern of this paper is that it is not really using sequential evaluations to do automatic design of experiments on molecules.	0
the results appear significant but would greatly benefit from more thorough experiments.	0
quality: the idea that is introduced seems intuitive and reasonable, but the experiments does not have enough details to prove that this method works (i.e. no quantitative results presented).	0
there are also zero details on experiments (network architectures, hyperparameters, etc.) — at least the authors promise to share the code, but it is not available yet.	0
regarding more specific details, there are too many small presentation issues for me to list them all, but here are a few representative samples:  please use “tile” or “cell” to denote individual elements in the grid (not “grid” itself)  it is unclear how states are represented as inputs to the model for generals.io  the 77% win rate against flobot seems to be an important achievement of the paper since it is mentioned in the abstract and introduction... but not in experiments (!)	0
maybe, the idea is novel but experiments are only in simulation.	0
however, there is a lack of experiments and analysis to show the effectiveness of the search algorithm and of the architecture founded by the approach.	0
however the merits of this method are not yet clear from the experiments.	0
the paper proposes what seems to be a good idea, but it is not yet demonstrated by the current experiments.	0
based on the experiments, the proposed method achieves marginal improvement in terms of f1 score but sometimes also slightly lower performance than other gan based such as pgan, so the impact of this work to solve positive unlabelled data problem is not evident.	0
this is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. also no guarantees or bound has been given about the variance reduction of estimator and it is more based on the authors intuition.	0
for example these can include (1) repeat the experiments with the same setup, but with a feature representation learned in an episodic way.	0
from the experiments, it seems that .sigma_x needs to be carefully chosen for different problems, .sigma^2_x < 0.3 seems to not work very well for bbb  ncp for the 1d sine data, but for the flight delay data .sigma^2_x is set to 0.1 and seems to work well.	0
in conclusion, i find the idea interesting, but the experiments do not show that this architecture can do anything new.	0
 experimental results are not convincing:  the introduction motivates the need for understanding human instructions and the abstract says 'given a human instruction', but i believe experiments do not have any human instructions.	0
pros :  beautiful clean formalization  interesting experimentation cons :  policies/algorithms are not directly logically constrained but more guided by constraints  the obtained reward function is sparse, e.g., subgoals don't receive any additional rewards  experiments are difficult to reproduce (no link to source, missing a full description of neural networks used, etc.)  some details are not necessary for the overall understanding, while some essential elements are missing (what happens if a nonaccepting absorbing state is reached during the training phase, what is the concrete inputs of your neural networks ?	2
teacher forcing was used to train the decoder in part (1) of the model, but without ground truth, i would expect more discussions and experiments on how the gumbel softmax trick affect or help the performance of the output.	0
demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot.	0
the method is clearly presented and easy to follow, and the experiments do seem to support the author's claims, but their exposition misses several important details (or could be presented more clearly).	0
it would however contribute valuable information if the authors stated how the confidence intervals of the f1 score are calculated (are the experiments based on several runs of each network or how is it done).	0
[questions for authors] why are experiments only performed on cpus but not gpus?	0
section 3 refers to a 90/10 training/evaluation split but then it is unclear in what experiments that exact split is used.	0
no real theory is offered, and the results are not really experiments testing hypotheses, but simply reporting the results of their design choices on a various of models.	0
this is demonstrated by their detailed implementation that attempts to overcome the main practical algorithmic concerns for neural networks (which may be beneficial even in the implementation of other optimization algorithms for deep learning) and their indepth experiments that give concrete evidence towards a reasonable explanation of why svrg methods currently do not work for deep learning.	0
the experiments for neural networks presented here seem to corroborate this; in particular, the variance reduction introduces an additional cost but without much benefit in the main initial phase (epochs up to 150).	0
strengths:  interesting idea of combining multiple noise types along with learned noise  clearly written, includes details of architectures and experiments.	2
overall, i feel the idea of this paper is interesting, but the theory and experiments in the paper are not very strong.	0
in section 4, .sigma varies (1.0, 3.0, etc) in different experiments, but again there are no explanations.	0
"''detailed comments'' _paper strengths_  the idea to use a negative video example for unsupervised detection learning seems novel  the proposed method is simple and the needed data can be collected with widely available equipments  the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example  the authors collected realworld data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)  the authors compare to a number of nonlearning approaches from opensource implementations (the reviewer cannot judge whether any relevant technique is missing)  the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes _paper weaknesses_  the authors clearly reduced the horizontal margins of the standard iclr style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard iclr style template upon update of their manuscript  the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. makerbased approaches could be employed for detecting the object  the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multiobject examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is selfoccluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)  other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging nontraining scenes with heavier clutter, nonseen lighting changes and occlusions to support their robustness claim  the proposed method cannot operate inthewild (e.g. youtube data) as it makes very strong assumptions about the required input data  the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene  the authors make no comparison to other unsupervised detection approaches (e.g. to the selfcited jonschkowski et al. (2017)) to prove shortcomings of other methods on the newly generated dataset  as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for egomotion estimation) makes applications to nonvideo data impossible, also none of the experiments exploit the nontemporal property of the approach to show single frame detection on a more varied set of scenes  the experiment showcasing the proposed application to ""learning from demonstration"" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task  the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)  the reviewer cannot follow the references to objectcentric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an objectcentric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion  the ""random search optimization"" discussed in section 3.3 is not a valid method as it ""solves"" this problem of instable training by picking the best of n runs with varying random seeds  figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach _reproducibility_  given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work."	2
pros  clear paper, easy to read  interesting application of attention mechanism to multiagent rl  promising initial results cons  no comparison to related algorithms on tasks where they have already been evaluated externally  the amount of workers is still quite limited in the experiments	2
i would suggest the authors to: ' better describe their contribution i.e model architecture and how the model can be used to obtain a real policy ' use 'stronger' use cases for the experiments, and particularly existing use cases ' provide a deep quantitative and qualitative comparison with sota pro: ' simple method, no need of a simulator cons: ' not clear how to move from trajectory generation to a real policy ' small contribution ' too light experimental study without comparison with baselines and state of the art	0
 ==============updated===================== the authors addressed some of my concern, and i appreciated that they added more experiments to support their argument.	0
pros:  simple yet effective approach to achieve the goals laid out in the problem statement  clearly written  thorough experiments and benchmarks  strong results cons:  no discussion of limitations  minor questions regarding quantization and size limits disclaimer: reviewer is generally knowledgeable but not familiar with the subject area.	2
overall, behavior cloning seems to help a little bit based on the experiments provided, but this finding is very likely indicative of the particular problem setting and seemingly not really a game changer.	0
see my response below for the concern that remains about the absence of the estimate of the log likelihood for the vae experiments.	0
 edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with codeslam), which address my concerns.	0
"some sentences of concern include: > ""these experiments provide conclusive behavioural evidence in favour of the texture hypothesis"" > ""we conclude the following: textures, not object shapes, are the most important cues for cnn object recognition."""	0
overall, although the combination of the presented ideas has some merit, the lack of extensive experiments that would compare it with the state of the art is not convincing, and the overall effectiveness of this method is unclear at this point.	0
the experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments.	0
i acknowledge and support the author’s decision to have thorough and clean experiments on these small models and tasks, rather than having halfbaked results on imagenet, etc. the downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds “in general”.	0
now for the weak points: (a) the justification for the training loss was not completely clear to me, although i can see that it has a variational flavor (b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to nextstep probabilities via the chain rule of probabilities, so we don't have a clear way to compare the tdvae models with jumpy predictions against other more traditional models (c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).	0
it would have been more useful to report less experiments but analyze the results of each experiment in greater depth. '	0
 experiments are ok, but on pretty small datasets, and for single hidden layer nns.	0
i would have liked to see all 3.1,3.2 maybe in an appendix and a lot more details on the experiments and setup for example 3. at the end of section 3.2 authors mention the lack of correspondence principle in some quantum systems, i would be happy for a refinement in the aspect of quantum computing that is true and also in general quantum mechanics but there is a huge body of work and an entire field dedicated to just that, settling the difference in correspondence principle and it’s meaning (for example chaos and the semiclassical limit of quantum mechanics (is the moon there when somebody looks?)	0
 the discussion of monochromatization mentions that it can be used in combination with piling up, but the experiments don't explain if it was ever used without piling up or not.	0
pros: i like this paper: it is a intuitive idea, and the experiments explore exactly what one would hope to gain from the prior (i.e. better initialization, improved sample efficiency).	2
cons: the only potential issue with the paper is the use of the implicit prior, as it complicates variational inference, requiring the extension to the elbo described in section 3.2. as far as i can tell, all experiments use the implicit priors.	0
pros  highquality writing  very clear  complete experiments on a variety of tasks, some of which do not have optimal solvers  honest assessment of the model cons  the theoretical contributions are not groundbreaking (either the the tweak on reinforce or the model architecture)  the model is still far from obtaining meaningful results on tsp (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...) details  dai et al has been published at nips and is no longer an arxiv preprint  the comparison to alphago should either be expanded upon or scratched.	2
 i would love to see more experiments on sat instances with a moderate number of variables but from realworld applications.	0
i commend the authors for supplying further experiments to explain the pros and cons of the proposed algorithms.	2
 the experiments are lacking in some respects: o it would be useful to report results without the vib regularization.	0
in its current form, i do not recommend accepting this paper but i do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via rl experiments.	0
the architecture itself is not very different from commonly used dcgan variants  the authors say that using pggan is desirable, but not critical, and the use of labels from odena et al. many of my own experiments with gans were plagued by instability (especially at higher resolution) and mode collapse problems without special treatment (largely documented, such as adding noise, adjusting learning rates and so forth).	0
"sure, those works focus on video prediction, while this work focus on building a ""forward model""and is supposed to be for modelbased rl, but this work has not performed any modelbased rl experiments, so from my point of view, it is a videoprediction model contingent on an action input."	0
pros:  the idea of non expansive network is interesting and important  results indicate some advantages in fighting adversarial examples and label noise cons:  the results for fighting adversarial examples are not significant from a practical perspective  the results for copying with label noise are preliminary and require expansion with more experiments.	2
"i have some concerns regarding their method and the experiments which are brought up in the following: method: in a nonfullycooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; i think something like sending a message is a better option and more realistic (e.g., something like the work of mordatch & abbeel, 2017) experiment: the experiment ""starcraft explore"" is similar to predatorprey; therefore, instead of explaining starcraft explore, i would like to see how the model works in starcraft combat."	0
in some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in realworld deployment.	0
the ideas in the paper are somewhat interesting, but i have major concerns with the motivation (it is unclear) and the experiments (not convincing): motivation: the authors motivate their inclusion of a hebbian working memory from the perspective of trying to mimic the human visual system.	0
pros:  integration with scms is interesting  counterfactual variants of algorithms are clearly motivated and interesting  paper is generally wellwritten cons:  assumption that the agent is given a model with no mismatch is very strong  model class (noise variables  deterministic functions) seems potentially restrictive  questions about impact of approximate inference  experiments could have been more varied	2
pros:  nice motivation, and the connection with industrial recommendation systems where candidate nomination and ranker is being used is engaging  it provides a conditional generative modeling framework for slate recommendation  the simulation experiments very clearly show that the expected number of clicks as obtained by the proposed listcvae is much higher compared to the chosen baselines.	2
cons:  do the experiments explicitly compare with the nomination & ranking industry standard?	0
in the introduction (paragraph 2), the authors point out potential limitations of previous opponent modeling algorithms, but never compare with them in experiments.	0
i think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent rl), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.	0
my main concern about the paper is that, currently, the experiments do not include any strong baseline (the es currently is not a strong baseline, see comments below).	0
cons: analysis on the experiments is a little insufficient, as shown below.	0
hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multimodal predictions is supported by experiments and intuition for the method, but not so much theoretically.	0
minor nits: i appreciate the human evaluation experiments on mturk but they are very difficult to understand with the figure 5. please label the yaxis.	0
cons, and suggestions on experiments my main concerns are around experiments.	0
"critically, the current experiments show that the large version of the proposed architecture performs better on these 3 metrics than some other architectures, but do 'not' compare to anything we could unanimously agree performs ""true planning""."	0
the real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of n, but also because there is no comparison with the other approaches.	0
in summary, pros:  well written and interesting  a new loss with potential for improvement over other losses  fairly thorough experiments cons:  much of the analysis is not new  unclear if the proposed loss will improve the state of the art, and if not why update after author response: thanks for your response.	2
pros :  experiments are numerous and advanced  transition probabilities are not transitioninvariant compared to vin  do not need pretraining trajectories cons :  limitation and hypotheses are not very explicit questions/remarks :  d_{rew} is not defined  the shared weights should be explained in more details  sometimes .psi(s) is written as parametrized by .theta, sometime not  is it normal that the .gamma never appears in your formula to update the .theta and w?	2
in section 6.3, the authors show an experiments in this case, but only on a dense relu network with 2 hidden layers, and it is unknown if it works in general.	0
the experiments of this paper lack comparisons to certified verification methods.	0
however, and this is a critical weakness of the paper, no attempt is made to compare the proposed method with respect to any related work, beyond a short discussion in section 3. the experiments do include some baselines, but they are all very weak.	0
pros:  well written and interesting  good experiments, results, and analysis cons:  perhaps slightly more similar to previous work than is argued update after author response: thanks for your response; i think the revised paper largely addresses my comments and those of the other reviewers, and i continue to hope it is accepted.	2
this paper lacks experiments on cv or nlp applications.	0
note: i have another concern about the experiments.	0
it would be nice to have some more intuitive explanations at least of theorem 1. also, it is clear in the experiments the superiority with respect to arora in terms of iterations (and error), but what about computational time?	0
7. the authors mentioned partial observation, outliers and subclusters in the global information, but the authors do not specifically define what the global information should be rigorously, and the paper does not theoretically prove or explain via experiments how the global information is kept by trimap.	0
minor: 9. in the algorithm, the authors show different equations for different t and t’, but are not evaluated in experiments.	0
 strengths:  even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to mt is novel  the presentation is very clear  the choice of methods is well argued and justified  the experiments are well executed and analysed  thorough and varied analysis of the experimental findings i recommend this paper for the best paper award.	2
secondly, i like the domain shift experiments, but i have the following question.	0
my primary concerns with this specific paper are twofold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what i would expect to work on a new dataset (2) the tradeoffs aren’t well calibrated in the sense that i don’t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesn’t seem trivial to set tradeoffs).	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
i still believe experiments on more tasks would be great but will be happy to accept this paper.	0
therefore i would like to see experiments with the es cost function, but with inclusion of the pruning step, and experiments with the affunction but without the pruning step.	0
 the semisupervised classification experiment is definitely better to assess the representation learning capabilities, but knn suffers with the same issues with the euclidean distance as in the kmeans experiments, and the linear classifier may not be flexible enough for noneuclidean and nonlinear manifolds.	0
 pros: overall, this is a nice empirical paper with a reasonably extensive set of experiments.	2
the authors have some preselected percentage in experiments but it is nontrivial to establish that for different applications.	0
general presentation fairly clear and easy to read # cons ' would have been more impactful to focus experiments on realworld scenarios in which bandwidth is constrained and naturally contentious # other comments ' pg.	2
in conclusion, the authors observe that quantizing weight/gradients systematically lead to a slight decrease in performance but provides promising improvement in term of training speed review  the paper is well written, documented and wellsectioned, with well written theoretical guarantees and thorough experiments, including one on a large dataset.	0
overall, although this paper is relatively incremental and has underwhelming experiments, it is a thorough work that is worthy of being presented at iclr 2019, in the reviewer's opinion.	0
"other refs to mention: randomized relu (randomly set the leak threshold) https://arxiv.org/pdf/1505.00853.pdf noisy activation functions quality: decent (6.5/10), but i think the experiments, level of analysis, and quality of writing are more like a very good blog post than an academic paper (although i think that that line is becoming very blurry) clarity: decent (7/10), but there are many small grammar errors, especially to do with pluralization (e.g. ""activation functions .... its importance"" should be ""their importance"") and incorrect verb tense (e.g. ""was used"" should be ""is used"" or ""has been used""; past tense implies it no longer is used)."	0
the gains might be more significant in deeper networks, but this is difficult to assess without experiments.	0
the strengths of this paper are:  this work addresses an important problem and is well motivated  experiments on both simulated and on a real system are performed the weaknesses:  the related work section is biased towards the ml community.	2
 while i commend the authors for performing both simulation and realworld experiments, i find the that experiments lack a principled evaluation.	0
there are several reasons, but most important: 1) most of the experiments in this paper use of the order of 10^9 or even 10^10 steps.	0
"2. the experiments with ""don't care"" should go to the experiment section, and the endtoend results should be present but not the ratio of incorrect bits."	0
i am not entirely convinced by the practical impact of the experimental section of the paper (though the experiments are beyond toylevel and i do not doubt the results), but i also believe that this is not the main contribution of the paper, which is rather laying the mathematical groundwork for future work.	0
therefore, i am not sure about the practical impact of the experiments: the glossy statistics seem to be indicative of the margin for improvements in the negative loglikelihood  but whether all of these improvements are really desirable is unclear.	0
pros  well written paper with lots of indepth experiments  does well at teasing out the impact of each of the techniques and gives some intuitive explanations of why they matter.	0
pros:  neat motivation;  extensive experiments;  clear illustration; cons  there are still some experiment results missing, as the authors themselves mentioned in the kinetics section (but the reviewer thinks it would be ready);  in page 3 the training section and page 4, the first paragraph, it mentioned θ and φ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative metalearning style method.	2
quality/significance  i have one major concern about the interpretation of the experiments in this paper.	0
pros:  interesting approach basedon the bits back argument  good performance trade off demonstrated through experiments cons:  only a few baseline results, in particular, at high compression size	2
conclusion: despite my concerns on the first part of this paper, i think the very thorough experiments, clear presentation and the interesting results on learning rate warmups and model distillation merit its acceptance.	0
strengths:  the authors conduct experiments ensuring robustness of mc framework.	2
second, there are methodological concerns about the experiments.	0
3. experiments: the authors cite the hierarchical attention transfer work of li et al (https://www.aaai.org/ocs/index.php/aaai/aaai18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments.	0
of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in ying et al. 2018 and simonovsky and komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting.	0
for example, the abstract claims a novel binarization method, but what is described in the paper does not seem especially novel (e.g. zero the negative weights and replace positives with their mean, if negative's mean < positive's mean, else vice versa); but more importantly, the experiments don't explore/support why this approach is any better (or when worse) than other schemes.	0
2. it is nice to see many experiments, but without preexisting knowledge about the datasets and their tasks, i can only make relative judgements based on the provided comparisons against other methods.	0
the only concern i would have is about the causal claims in section 3.4. i’m not completely sure the ablation experiments are the correct way to “prove” causality, as opposed to somehow trying to intervene on the orientation selectivity directly.	0
ultimately, i think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced.	0
as the argument is that the proposed loss is better than the reconstruction one and that of hoffman et al. 2018 for lowresource supervised adaptation, it would be worth demonstrating this empirically in table 2. summary: the proposed objective functions are well motivated, but i feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at iclr.	0
the experiments in the paper with different networks for mnist show the promise, but i feel that they are not enough. '	0
this work in isolation appears to present an improvement over prior work in this subfield, but it is not obvious that the findings in these experiments will continue to be robust in more competitive settings.	0
quality: some concerns about details of experiments (see cons list and significance section for further discussion).	2
 the paper lacks experiments beyond toyish tasks like mnist and cifar10 and does not do a good job comparing to the broader established literature and contextualizing its results on certain tasks such as cifar10 (reporting ratios to a baseline instead of absolute values, for instance).	0
regarding experiments, on the positive side, the authors consider a representative set of methods.	2
it remains to be seen how difficult the proposed benchmark will be, but the authors perform experiments on a number of baselines and show that it is nontrivial and interesting.	0
" pros: ' original idea of using separate ""discriminator"" paths for unknown classes ' thorough theoretical explanation ' a variety of experiments ' very wellwritten, and clear paper cons: ' the biggest problem for me was the unconvincing results."	2
mnisttomnistm has better baselines (pixelda performed better on this task for example), office is not suitable for domain adaptation experiments anymore unless one wants to be in a fewdatasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for nnbased domain adaptation); the results on cell were not convincing, i don't know the dataset but it seems that baseline nn does better than da most of the times. '	0
"(1)the paper puts a lot of stress on the stability of the training process in the beginning but clear experiments supporting their claim related to improved ""stability"" are lacking."	0
========== concerns: ========== [a] the discussion of differences to the closely related gail methodology is left until the related work after experiments.	0
aside from some questions about the experiments, i'm mostly concerned about the positioning of the paper  specifically with respect to prior work.	0
yes, the authors conduct some experiments to show that their algorithms achieve good performance in some benchmark datasets, but a careful discussion (if possible, theoretical) of when such an assumption is viable and when it is an oversimplification is necessary (analogous assumptions are used in naive bayes or variational bayes for simplifying the likelihood, but those are much more flexible, and we know when they are useful and when not).	0
my main concerns are novelty of the proposed method, and fairness of experiments.	0
however, i still have concerns about novelty and experiments.	0
for the cifar experiments, it is very good that it performs well, but it'd be informative to see if soseleto can perform even better with 10k samples. '	0
i am not an expert on these kind of experiments, but i found the comparisons fair and rather extensive interesting about this work:  clean bayesian decisiontheoretic viewpoint.	0
has some odd consequences (question below), but clearly works better for fewshot classification experiments:  5.1: convincing results, in particular given the simplicity of the model setup and the inference network.	0
however i am concerned about its sample efficiency and comparing experiments.	0
weaknesses:  all the experiments except the last row of table 2 concern adaptation between two domains.	0
the authors leave out some details with regards to the experiments, but with code available this should be sufficient for reproducibility.	0
experiments performed in a simple blocks world show that the proposed approach is not only useful for prediction, but can also be used for planning object placements.	0
 the theory for optimization of vis with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (theorem 2, theorem 4) cons:  i'm a bit skeptical about the experiments on gans.	0
the experiments all use architectures that are quite dissimilar to what is commonly used in practice, and achieve much worse accuracy, so that a reader is concerned that the results differ qualitatively in other respects.	0
section 4 is titled experiments, but section 4.1 starts with defining the concept selectivity.	0
2) in the main paper, all experiments were aimed to address icnn and icrnn have good accuracy, but not they are easier to optimize due to convexity.	0
'pros'  original idea for modelling distribution of sequence data  theoretical convergence in the jensen shanon divergence sense  promising experiments 'cons'  no major cons to the best of my knowledge 'typos'  it would be very nice to have black and white / color blind friendly graphs  eq 10 too long  introduce j_m & j_g in sentence  coma at the end of eq 5, and maybe align generator and discriminator in some position (e.g. at the semi colon).	2
maybe this is obvious for ppl more knowledgeable in the field, but this paper fails to make that point by either pointing out relevant references or containing the necessary experiments.	0
pros  thorough analysis, even the negative experiments are well written and throw more light into the problem space.	0
a significant part of the theory in earlier sections is about the 50x2d method, but in experiments this doesn't seem to work as well.	0
"aside from the lack of clarity about notation/what symbols represent, i have two main issues with appendix b. first is the claim ""note that x and y are independent as well as yi and yj when i neq j""; i think it should be made clearer that x and y are independent only in this setting because the experiments are with random data, and it does not seem obvious to me that yi and yj are independent."	0
i understand why the loss under perturbation is interesting, but this should be discussed along with what are the results of your experiments.	0
clarity: 7/10 results are clearly presented, although more intuition and context would be helpful, and some sections are not well explained or contextualized originality: 2/10 the methods are not novel, to my knowledge, and there is little exploration or insight given for the extensive experimental results significance: 7/10 measuring and reporting results is valuable, and the reported results are interesting pros:  interesting results, good plots  overall well structured and explained cons:  plots could use more explanation and interpretation in the captions, and more investigation and insight from the experiments  some sections are not clearly worded and i think the objective/interpretation of the experiments would not be clear to someone even a little unfamiliar with the approaches cited specific comments/nits (in order reading through paper): 1. first paragraph of intro is kind of fluff/unnecessary.	2
the authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.	0
even more troublesome is the fact that in all experiments you're providing your algorithm with the correct classprior pr(y=1), but it's not clear if this is provided to pu as well.	0
a single downstream task is very briefly mentioned in the experimental section, but it is only very vaguely described, it is unclear what experiments have been performed and there is no evaluation whatsoever.	0
i liked the comparison of decoupled and coupled crf training, but i didn't get much out of the synthetic experiments.	0
however, i have concerns about both motivations and experiments.	0
 i am not an expert in this area but the experiments look convincing in general.	0
experiments show some improvement compared to (trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets.	0
i also think that the stanford 2d3ds experiments have some issues: unet and fcn8s are good baselines, but other prior work based on spherical convolution are omitted here.	0
however, the large scale experiments has some weaknesses.	0
finally, i found that the different benchmarks where relevant, but i would also suggest (for future work, or in the appendix) to additionally perform experiments on the wellknown random 3sat instances ( is fixed to 3).	0
i find this behavior really perplexing, but i trust that your experiments are correct.	0
thus the only nontrvial experiments are the ones on cifar10 (table 2), but the majority of the analysis is conducted on whiteonblack mnist and fashionmnist.	0
i guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.	0
in the appendix, the authors use a baseline value to alleviate this, but no discussion is provided in the main text or the experiments.	0
"the title of the paper in my opinion undersells the result which is not only that ""deep skinny neural networks"" are not universal approximators, but that the class of functions which cannot be approximated includes a set of practically relevant classifiers as illustrated by the figure on page 8. the presentation is extremely clear with helpful illustrations and toy but insightful experiments."	0
the multitask experiments used 4 different datasets to encourage diversity, but k=2 showed the best results.	0
cons: ' experiments are quite weak.	0
i appreciate the authors' efforts to clarify the intuition, but more technical details and experiments can be provided to support their arguments.	0
more specifically, the paper presents crl as a general method for learning compositional problems by decomposing them into simpler subproblems that are automatically discovered, but in practice, a far more limited version of crl is used in the experiments, and the suggested translational capabilities of crl, which are important for abstract subproblem discovery, are not properly validated: 1. in both experiments, the buildingblock functions are handcrafted to fit to the prior knowledge on the compositionally of the problem.	0
it presents a streamoriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm.	0
experiments:  the authors mentioned important related works in both “1 introduction” and “4 relationship with previous work”, but in table 1, they compared the mixfeat with only standard mixup.	0
cons: despite somewhat frequent usage, i would like to respectfully point out that permuted mnist experiments are not very indicative for a majority of desiderata of interest in continual learning, and i.m.h.o.	0
while the experiments in the paper did show the mutual information goes down as the clustering effect enhanced, it only means `clustering` and `compression` are correlated; but the paper claims `clustering` is the source of `compression`, i.e., `clustering` leads to `compression`.	0
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
i’m not asking for more experiments, but some discussion might be useful.	0
but there are also several weaknesses:  for all experiments, the way to obtain a goalconditioned policy in the first place is not described.	0
the authors conducted a good set of experiments, but are missing comparisons bayesian versions of maml.	0
however, the paper lacks a proper evaluation of the proposed method, and i don't think this paper is ready with the current set of experiments.	0
i’m not necessarily worried that experiments are not scaled up, i’m more concerned that the hypothesis and solution is only tested by means of change in performance.	0
"(2) but with replacing a known reward confusion"" > ""replaces a known reward confusion"" 4) experiments:  diverse experiments!"	0
"because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to hartford et al. the tasks are explicitly designed to exploit these additional parameters  so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect hartford et al. to fail (because it's designed for a different setting)."	0
 after reading other reviews and author comments, i have raised my rating to a 6. my main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments).	0
cons:  some more experiments would be good to substantiate the claim that analytical kl is better.	0
 '' review score incremented following discussion below '' strengths: well written and clear paper intuition is strong: not all sourcetarget class pairs are as beneficial to find adversarial examples for weaknesses: cost matrices choices feel a bit arbitrary in experiments cifar experiments still use very small normballs the submission builds on seminal work by dalvi et al. (2004), which studied costsensitive adversaries in the context of spam detection.	2
the experiments results are clearly presented but some of the details of the experimental setup are not always justified.	0
the experiments and ablations are thorough, but the empirical gains in terms of improving gan metrics are relatively minor.	0
the paper's significance lies in showing that cf is still a problem, but there is room for improvement in the analysis of the outcome of the experiments.	0
strengths:  substantial number of experiments (6 datasets), different domains  surprisingly simple methodological fix  substantial literature review  it has been argued that charlevel / pixellevel rnns present somewhat artificial tasks — even better that the authors test for a more realistic rnn application (reading comprehension) with an actually previously published model.	2
the authors state in the experiments that it is not the focus of the work, but i do believe that the comparison is meaningful to help understand whether the proposed framework is close to the stateoftheart in those standard metrics.	0
the authors could say that the discretecontinuous gap contributes to modecollapsing, but this is not too good either because it will require the paper to conduct experiments beyond text generation to show this.	0
"major concerns: my major concerns are threefold:  the authors do not provide enough details about some ""informal"" experiments which are sometimes important to convince the reader about the relevance of the suggested insights (e.g., line 3 page 5)."	0
the proposed technique (dubbed r2d2) is not particularly original (it is essentially “just” using rnns in apex), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach.	0
in the experiments in appendix g, it is claimed that a discriminator with the architecture specified in lemma 4.1 is used in gan learning, but either weight clamping or gradient penalty is used as well.	0
 “method more greatly resembles the original data than other ganbased methods” method does not resemble data  “due to their exact latentvariable inference, these architectures may also provide a useful direction for developing generative models to explore latentspaces of data for generating datasets for psychophysical experiments.” this is mentioned a few times, but never supported  acknowledgements should not be in the review version (can violate anonymity) 5) minor: why is a gaussian around the midpoint used for interpolations?	0
although the idea is interesting, the experiments are lacking.	0
cons (quibbles): experiments: the authors didn't compare the proposed method against topic model (vanilla lda or it’s derivatives discussed in related work).	2
 pros: ' the paper was wellwritten and explained the method and the experiments well cons: ' the problem seems illposed to me.	2
pros: ' interesting topic ' theoretical predictions match the practical experiments cons: ' nothing particular minor: ' fig 5a.	2
i appreciate the comparison and combination with a competitive method (gradient penalty) in section 5.3, but i wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining vib with gp leads to the best performance.	0
pros:  clearly written  technically correct cons:  technically straightforward  not convincing experiments  unclear, why the approach should work	2
in summary, i think this is a paper that may arise a lot of interest, although the different arguments are known and the experiments are poorly executed.	0
 the last paragraph of the related works section mentioned some related work with shortcomings as working only on lowdimensional data and features of specific types, yet the experiments are also mostly done on lowdimensional datasets.	0
such lack of clear argumentation occurs in several places  experiments isolating the contribution of the method with respect to traditional initializations are missing (for example: experiments on cifar10 and svhn showing the result of traditional initializations with all the bells and whistles (cutout, mixup) as the zeroinit gets.	0
my only concern there is that the only experiments performed are on mnist, which is known to be easily dealt with using the kind of feedforward architectures studied here.	0
i don't think that this is a dealbreaker, but i think that this section needs to be more prudent in the way that it concludes from these observations (the math and the experiments).	0
pros: thorough experiments, competitive baselines and informative ablation study.	2
other major concerns are: 1) the true utility of the model, and 2) the integrity of the experiments.	0
moreover, there are techniques like selfcritical sequence training (scst), which far outpeform mixer, and we haven't even discussed actorcritic baselines... in summary, the contribution over raml and spg in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly mixer is reported perform, and the omission of stronger baselines like scst and ac methods.	0
the hrl experiments also lack comparisons to any other hrl baseline.	0
 experiments show promising results for modifications of the proposed estimate cons:  having an unbiased estimate doesn't imply that its minimisation is a successful learning strategy.	0
this weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation  one of the mentioned motivations for unbiased estimates  being able to perform model selection on complementary labeled validation set  is not illustrated in the experiments questions:  i believe 1/(k1) normalisation factor in (5) is not needed  there seems to be a mistake in (9) (and its modifications later on)  i would expect either the subscript of the probability distribution in the last summand to be exchanged with in the loss, or a factor added  also, i think there are some mistakes in subscripts in (11)  what loss is the method from [ishida'17] optimising in the experiments?	0
then the experiments show such a definition is actually not correct, but rather a datadependent one.	0
 i did not replicate their experiments on gans, but the experimental numbers seem promising.	0
specific points on positives and negatives of the work follow: positives:  the paper shows a solid understanding of the literature in this domain and presents a strong motivation  the problem itself is addressed at a deep level with many nuanced (but important) considerations discussed  ultimately the results of the model seem convincing in particular with the accompanying psychophysical experiments negatives:  (maybe not a negative, but a question) at the extreme tradeoff between intrinsic structure and texture, the notion of a metamer seems somewhat obscured.	0
this is especially concerning since bett the method also requires the additional parameter lambda which affects performance in the experiments  it is not clear how to set it in practice, does e.g. crossvalidation etc. need to be used?	0
in the experiments it seems that authors just try two different values (0.1 and 0.5) but i assume this really should be a hyperparameter search for this.	0
cons: in general, i really like the task and a lot of the models and experiments, but the description of the real world experiments is severely lacking in information and results.	0
on the positive side, the paper is wellwritten and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets.	2
overall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is wellwritten and has some interesting results from its relatively simple approach, i've raised my rating to above acceptance threshold.	0
questions regarding experiments  caml is robust to the adaptation learning rate, but isn’t this true of any scheme that separates metalearned and adapted parameters into disjoint sets?	0
significance of this work  the significance of meta learning is good but based on the experiments authors conducted i am worried it has little significance.	0
the three tasks they chose are all toy problems and do not instill confidence in the validity of caml for either large scale experiments or in setups where distribution is changing but tasks remain same.	0
experiments are a bit too toy, but the authors did show significant improvements over dice with no control variate.	0
(5), (6) in my review i provided specific, objective criteria by which i have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the dice paper.	0
cons i think the main issue of this paper is the experiments can not fully support the advantage claim of the proposed method.	2
the authors are also to be commended for the mathematical elegance although the paper is very well written and extremely well structured, i struggled with the lack of experiments available in the paper.	0
my other concern is on the scale of the experiments.	0
4. the experiments showcase that the regularized estimator has a better path norm (and expectedly so) but almost similar (in case of mnist actually better) test accuracy.	0
questions/concerns about experiments:  does figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step, or 5 episodes, each from a separate run averaged together?	0
the experiments results show that the composition method does better than soft qlearning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms?	0
reviewer believes authors have a good line of research, but that it requires additional literature review and experiments before it is ready for publication.	0
the authors argue that crossentropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture.	0
a similar paper reporting alp to improve robustness in smaller datasets (cifar10) was submitted to iclr (https://openreview.net/forum?id=bylj6oc5k7) but was withdrawn after the authors performed additional experiments.	0
the reasoning is as follows: 1. different models that share the same final softmax layer will have highly correlated gradients in this final layer 2. this correlation can be carried all the way back to the input pertubations 3. the use of a multiway encoding results in a weaker correlation in gradients between models i found (2) to be a surprising assumption, but it does seem to be supported by the experiments.	0
"my concern is that the experiments were done on a narrow range of models, which only have ""weak"" adversarial training / defenses."	0
the experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments.	0
i don't think all papers need experiments, but this paper i think would have greatly benefited from one.	0
i would expect the experiments to be designed specifically to support the claims made, but little evidence is provided:  the authors claim that the method allows oneshot generalization to an unknown trajectory.	0
for experiments, i would really like to see experiment showing fourier coefficients at various stages of training of standard network on standard data and standard data but with randomized labels (or different complexity in some other way).	0
2. lack of experiments with different choices for features and parameter values.	0
the presented models are quite straightforward but exhibit good robustness against attacks listed in the experiments. '	0
cons:  lack of experiments on mobile networks like shufflenets and mobilenets  missing citations of some stateoftheart methods [1] [2].	0
pros:  the paper is wellwritten and clearly explains the technique, and figure 1 nicely summarizes the weakness of static channel pruning  the technique itself is simple and memoryefficient  the performance decrease is small cons:  there is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x)  in contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, realtime)  the experiments are limited to classification and fairly dated architectures (vgg16, resnet18) overall, the method is nicely explained but the motivation is not clear.	2
=========== update: authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.	0
strengths: theoretical content, experiments and methodology content (even a monte carlo approach) makes it a very complete paper.	2
these experiments and investigations are however based on a theoretical foundation which suffers from several issues.	0
pros:  this work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))a very interesting configuration of weak supervision, an increasingly popular and important area  the treatment is thorough, proceeding from establishing the minimum number of u datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments cons:  this is a crowded area (as covered in their related work section).	2
i do not feel that the paper is particularly novel, but the experiments are thorough.	0
my biggest concern of weather or not the rfmaugmented agent was capable of learning without a pretrained agent has been addressed with additional experiments and analysis (figure 8).	0
the architecture is general enough to work on other problems/tasks  which is good  but the authors focus on the binary vulnerability code dataset in the experiments.	0
the paper strikes me as a valuable contribution once the detail of the experiments are addressed, but personally i am not sure that whether the novelty of this paper is enough for the main conference track.	0
my main concerns are the following:  all the simulated experiments are able to demonstrate the effectiveness of the method, though they seem to be a bit too simplistic, e.g. known dynamics.	0
the presentation of the core idea is clear but imo there are some key missing details and experiments. '	0
in the experiments, this isn't discussed (though another issue is touched on a little bit: you wanted to find real stationary points to test, but you don't have exactly stationary points, but rather can get arbitrarily close).	0
strengths: attempts to solve a longstanding problem in modelfree rl (effective exploration in sparse reward environments) clear writing and structure, easy to understand (except for some minor details) novel, intuitive, and simple method building on ideas from previous works good empirical results (better than state of the art, in terms of performance) on some challenging tasks weaknesses: not very clear why (and when) the method works  more insight from experiments in less complex environments or some theoretical analysis would be helpful it would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) not clear how stable (to train) and robust (to different environment dynamics) the method is main comments / questions: the paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis.	2
better performance than behavioral cloning new way of using demonstrations cons: although both the method and the experiments look promising, there is a very simple yet competitive baseline missing.	0
the other experiments show some improvements over the baselines, however more experiments are necessary for claiming generality.	0
despite a solid theory developed, lack of numerical experiments reduces the quality of the paper.	0
" ablation study to empirically resolve the above concerns, it is necessary to present the empirical comparison with the ""static"" softmax."	0
this would be heavily dependent on the model structure (with more complex decision boundaries likely being harder to optimize) but they show empirically in 4 models that this method works well.	0
strengths:  this paper is interesting in the sense that it empirically shows that using regularization in training deep rl can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original.	2
it's good that the paper empirically confirms the intuition, but doesn't feel like a significant contribution on its own.	0
the paper illustrates empirically the convergence claims, but only under fixed hyperparameters, which completely illustrates the recent concerns about the reproducibility crisis in ml.	0
of course, it is quite interesting if a minor change makes a big difference in clustering performance (theoretically and/or empirically), but such result is not given.	0
"cons:  details are lacking about the ""anon"" dataset introduced in this paper (where do the photos come from and the labels, visualization of a few examples...)  there are not many technical issues discussed in the paper and that is fine as the main idea is relatively simple and its effectiveness is mainly demonstrated empirically, but i feel the paper is missing a discussion about the importance of the initial classifier trained to estimate the target prior probabilities for the source labels and whether it is crucial that it has a certain level of accuracy etc.  the approach in the paper implies a practitioner should have access to a very large target dataset and the computational and time resources to appropriately pretrain a complex network for each new target task encountered."	0
"pros: it's interesting to investigate and compare these different ""regularization"" techniques and compare them on different tasks empirically."	2
admittedly, this is a more vague statement which may be harder to analyze or empirically study, but it sounds to me more reasonable for explaining successful lowprecision training than the fact that we have certain tight bounds for quantized convex optimization.	0
however, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate 'why' certain combinations are more effective than others.	0
did they try other approximations but empirically find that these worked best?	0
in corollary 3.3. you characterize the convergence speed in a nice way, but i am missing the link to the behaviors observed empirically in e.g. fig. 2. what am i missing?	0
authors suggest some experimentally fixed values, but a proper analysis (at least empirically), would be useful for the readers.	0
i am not sure about why should it work (i explained in detail later), but it does work well empirically.	0
i understand that you make the claim that your improvement is orthogonal, but that seems like something that needs to be tested empirically.	0
 more broadly, but following from the above: the paper does not provide any real world examples, real or hypothetical, to give the reader an idea of whether the above uniformity assumptionor really any of these assumptionsare wellmotivated or empirically justified.	0
in summary, this manuscript proposes an interesting idea, but not sure empirically how useful it will be since for a complex network, this method may result in relatively big performance decrease.	0
in particular, the proposed evaluation seems valid when applied to different crosslingual mapping methods over the exact same embeddings, but its validity beyond that is not obvious nor tested empirically in the experiments.	0
which would be fine but some key experiments are missing to make the paper empirically rigorous.	0
the paper is very well written and easy to follow, but my impression is that, as an empirically driven paper, it could contribute from further experimentation as well as from linking key empirical findings to theoretical justifications.	0
detailed comments: i thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified.	0
the authors show empirically that the learned options do indeed decompose the stateaction space, but not the state space.	0
the authors argue that empirically, neural networks with the galu activation is as effective as that with the relu activation, but theoretically, the galu activation is easier to understand because of the separation of the nonlinearity and the learnable parameters.	0
this paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results.	0
 one of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straightup supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts.	0
as the argument is that the proposed loss is better than the reconstruction one and that of hoffman et al. 2018 for lowresource supervised adaptation, it would be worth demonstrating this empirically in table 2. summary: the proposed objective functions are well motivated, but i feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at iclr.	0
ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried eq 8 and it worked well empirically would be a lot better than what is there at the moment.	0
the authors empirically show that ghn  random search is not only efficient but also performs competitively against the stateoftheart.	0
it's unclear to what extent the theory would generalize not only to deep, nonlinear networks (which the paper addresses empirically) but also different structures in the task that are not well approximated by the svd.	0
" this paper does not even try to propose yet another ""vacuous"" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some ""prescriptive"" insights (per sanjeev arora) towards understanding generalization in deep neural nets."	0
main weakness is the complexity of the algorithms and many design choices wich are well argued for but not theoretically or empirically well founded.	0
other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: i am not sure whether these make the task easier or harder, but they certainly change it to some extent (the “despite this” above 5.1 suggests it would be harder, but this is not shown empirically).	0
this might be true, but it is empirically proven only by single dataset and single run.	0
the cost of the sparsification is essentially the application of the trained neural network to a small number of data points in order to compute the sensitivity scores pros:  the method works empirically, in that their empirical evaluations on mnist, cifar, and fashionmnist classification problems show that the drop in accuracy is lower when the neural net is sparsified using their corenet algorithm and variations than when it is randomly sparsified or the neural network size is reduced by using svd.	2
 despite the suggestions of the theory, the accuracy drop can be quite large in practice, as in the cifar panel of figure 1 i think the iclr audience will appreciate the attempt to provide a principled approach to decreasing the size of neural networks, but i do not think this approach is widely compelling as : (1) no true guaranteed control on the tradeoff between accuracy loss and network size is available (2) empirically the method does not perform well consistently (3) comparisons with reasonable and informative baselines are missing updated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method.	0
one may use evolutionary computing to empirically analyse such a encoding or one may come up an existence/nonexistence proof (i am not expert in the field however i guess ecoc field should have investigates similar problems) of such encoding.	0
pros:  this work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))a very interesting configuration of weak supervision, an increasingly popular and important area  the treatment is thorough, proceeding from establishing the minimum number of u datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments cons:  this is a crowded area (as covered in their related work section).	2
this is treated empirically, but would be stronger to have this show up in the theory somewhere.	0
 other prior work here has handled k classes with k u sets; could have extended to cover this setting too, since seems natural overall take: this learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.	0
" ablation study to empirically resolve the above concerns, it is necessary to present the empirical comparison with the ""static"" softmax."	0
the main weaknesses of the work lies in its motivation and in the empirical results.	0
this other work however reports better empirical results over the same benchmark.	0
the most interesting contribution is the query as kernel approach: however the concurrent submission “pay less attention with lightweight and dynamic convolutions” obtains better empirical results with a similar idea.	0
i have some questions about the empirical results but felt that the overall story was strong.	0
the empirical results are not overwhelming but at least show qhm as competitive with cm on tasks and architecture where sgd is typically dominant.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
there is no discussion or analysis but only empirical descriptions.	0
in addition to the lack of comparison to related multiobjective methods, the empirical evaluation is not very convincing since it is done on toy problems, with the most “realistic” one being a simple 2d task where the improvement brought by the proposed technique is not obvious (fig. 5).	0
overall, as empirical study, i think this work is interesting but i think paper should justify why we need this new benchmark.	0
despite these shortcomings, i believe this paper is another welcome push to introducing empirical bayes ideas into neural networks (though it’s not the first), and the empirical evidence seems to indicate that there is indeed something there to investigate further, so i give it a weak accept.	0
in summary, the paper has a novel idea, but has to be better developed in its empirical part.	0
 the sota result miniimagenet is the result of a bagoftricks approach that is not well motivated by the main methodology of the paper in section 2. major points:  the motivation for and derivation of the approach in section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data s. moreover, the approach is not fully bayesian as a point estimate of the hyperparameter .alpha is computed; instead, the approach is more similar to empirical bayes.	0
" the second motivation is not clear to me, and the claim that ""the training text collection should include many instances of sentences that have only minor lexical differences but found in completely different contexts"" needs more support, either theoretical or empirical."	0
as good as it sounds, unfortunately, the paper falls short of providing a convincing evidence (be it theoretical or empirical), and the way it tries to frame itself unique and different in relation to related works only indicate a lack of deep understanding of the existing literature.	0
the paper is full of empirical evidence that is guided by a simple observable that is very intuitive, however, it lacks a comprehensive discussion on the new quantity they propose that i consider a major flaw, but that i think (hope) that the authors can fix very easily.	0
although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as i will critique below.	0
many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims.	0
 pros:  superior empirical results are the key highlights of this paper.	2
given the lack of theoretical support for this idea, i would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.	0
i think that is fine, but then the empirical results should be extensive, and unfortunately they are not.	0
overall i find the above empirical observations and some of the other arguments in this paper interesting but i think there is a lot of scope for improvement in the paper.	0
justification for rating: this paper presents good empirical results, but without a clear identification of the source of improvement.	0
 the empirical results could make this paper compelling, but the comparisons are rather shallow, in that they compare on only a few datasets and don't compare extensively to many of the state of the art methods.	0
== summary == this paper has good empirical results, but i would really like to see a comparison against training a model on the union of the source and target domain.	0
"2. it try to link it to information theory but most of study is just empirical (which is fine, but avoid it can simplify the writing and make it more readable), e.g. "" according to information theory and the attention mechanism (bahdanau et al., 2014), it is clear that we.."" i agree with the intuition but how it can be ""if and only if""?"	0
pros:  the method is very simple  the empirical results, particularly on the synthetic noisy training data, seems to be encouraging.	2
 the ablation study argues that the method is robust to the hyperparameters, p, e, and h. cons:  i think the results remains to be highly empirical.	0
 i think the paper shows interesting results, but my concern is that it seems to be quite empirical.	0
"empirical results: i do not understand this statement: ""that 35% of random attacks were successful in this respect highlights the fact that black box adversarial attacks are definitely possible and highly effective at the same time"" why is 35% successful attack rate a positive result?"	0
i also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above).	0
i understand that the empirical evaluation presented do justify the methodology, but i am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented.	0
my main complaint about the paper is its lack of technical details and analysis of empirical results.	0
 the lack of clarity around particular definitions means that clarity is limited to the empirical results.	0
# summary pros: ' useful dynamic batching trick that can lead to speedups ' empirical evaluation compares to two existing techniques and breaks down individual components of runtime cons: ' no critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage ' some questionable statements and assumptions ' lack of formalization and clear definitions ' paper reads longdrawnout, subpar writing hurts readability	2
i think that it is interesting to think more carefully about how sparse reward states and state similarities can be used more efficiently but the ideas in the paper are not original or theoretically founded enough to have a lot of impact without the company of stronger empirical results.	0
i've seem several gan papers arguing that gradient penalty helps in cases beyond wgan, but most of them are just empirical observations... also the omega regulariser is computed on which x?	0
in conclusion, the paper has a novel idea i like, it is explained clearly, but the work has to mature a bit more in terms of empirical work and interpretation of results.	0
overall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).	0
the experiments are generally over enough tasks and compare against several baselines, and although the empirical wins are not that large i feel that they would be sufficient for publication if not for my other concerns.	0
from a highlevel perspective, the methodological innovation (a pointernetwork trained on sequence loss from logged data), setting (reranking a slate to consider interactions), and empirical analyses are largely ‘incremental’ — although i think nontrivial to put together and the paper itself is wellwritten and fairly convincing.	0
thus, as the focus is on details and empirical results over methodological innovation, this paper reads a bit like an industrytrack paper — but i find the results interesting overall and am marginally inclined to accept.	0
i feel like this paper may be important, but it is a little difficult for me (myself) to judge without clear empirical evidence of a strong method vs other methods.	0
"there may well be empirical reasons to prefer the performance of a vae as opposed to wakesleep, but the issue of whether it ""will converge on a good model"" is clearly an issue in vaes as well (see, e.g., the ""hacks"" necessary to achieve convergence to a nonspurious optima in [1])."	0
regarding the experiments, and this is probably the weakest part of this paper, i would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided).	0
 (w2) missing some experimental results / deeper insights : there were some notable empirical results that were missing or not provided, that raised some concerns in my mind.	0
i can certainly imagine that the irl approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.	0
"the authors present very interesting empirical results, but i am not convinced that the proposed notion of ""expressiveness"" properly explains the performance improvement in this set of tasks."	0
the reason of adding gan loss lacks either theoretical or empirical analysis.	0
graph laplacian can be used as an empirical estimate for the laplace beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function’s complexity in the intrinsic geometry of a manifold.	0
that makes the empirical results not very convincing to explain the key strengths of this work.	2
the main result from the theorem is that the error gap of norm constrained embeddings scales as o(d^0.5(lnn)0.5), but i did not see how this is related to the norms of the embedding vectors and how is this evidenced in the empirical studies?	0
concerns: the authors show various empirical results to highlight the performance of their approach, but i am still not sure where it is best to use sparse embeddings that are induced by the proposed approach vs. those of standard vae (or other of its sparse variants e.g., rectified gaussian priors by tim salimans).	0
maybe the proposed approach offers potential for tasks such as semisupervised learning or conditional generative modeling, but the current set of empirical results does not allow one to draw any conclusions there.	0
the paper has the following strong points: 1. it tells an interesting (and engaging) story about a largely empirical study, and while doing this never pretends to be more than it is.	2
there are additional concerns/questions regarding both theoretical part and empirical part: [1] section3.3: assumption that p_i(w_0^0) =p_i(w_1^0) = p_i is not reasonable when theoretically comparing .hat .kappa(w_1^0) and .hat .kappa(w_0^0).	0
pros:  interesting story  good empirical performance cons:  unclear whether the story is entirely correct if the authors can provide a convincing case for the interpretability of the sgdsgd results, i am happy to raise my score.	2
the biggest strengths are:  strong empirical robustness  analysis of combinations of methods and their interactions: different loss function, different architecture, different weight constraints, and adversarial training are all evaluated together and separately.	2
review: pros: the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.	2
empirically, the authors show that the proposed model achieves better results on 2 out of 4 domain adaptation experiments on digits, but without proper statistical testing it is not clear whether the improvement is significant or not.	0
in sum, the paper has some limitations in the empirical evaluation, but nonetheless the use of a gan promises significant gains in statistical power.	0
heuristic is good if enough empirical evidence is shown, but i do not think the experiment part is solid either.	0
"pros: it's interesting to investigate and compare these different ""regularization"" techniques and compare them on different tasks empirically."	2
"cons: many of the points made in the paper are not properly capturing the nuance in the ""conventional wisdom"", and although it's good to be reminded and the empirical results are interesting to look at, in fact these are not really new discoveries, and sometimes the conclusions are very misleading."	0
however i was not convinced that it gives empirical advantage.	0
i find the empirical evidence and support for the three modifications lacking in detail.	0
in general this work presents a simple and easy to implement solution to a common problem of cnns and even though it lacks more thorough theoretical analysis of this problem from the signal processing perspective (such as minimal size of the blurring kernel for fulfilling the nyquistshannon sampling theorem), it seems to provide ample empirical evidence.	0
here, some empirical results are lacking which analyze the speed/correctness of the identification of parameters for various choices of divergence/model noise.	0
strenghts:  the combination of components is novel  the method does not rely on task descriptors neither at training nor test time weaknesses:  the paper needs a major rewrite to improve fluency and to better organize and describe the proposed approach  the empirical validation is weak.	0
"also, i did not find mention to methods predicting parameters in the metalearning community but also others like: denil et al. ""predicting network parameters in deep learning"" nips 2013 empirical validation the empirical evaluation does show an advantage of the proposed approach on some simple streams composed by up to three tasks."	0
though it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study.	0
"but i don't see how this corresponds to ""reporting overfitting""; a model that simply reproduces exactly the empirical distribution of the training set would get an excellent cafd/fid score, but that's the usual sense of ""overfitting."""	0
my main concern is about the novelty of technical contribution which is mainly composed by two: 1) a prediction variance based importance sampling strategy for batch selection and 2) an empirical study the show the merits of approach.	0
empirical validation: the empirical validation is solid but limited.	0
i conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these.	0
while i strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend.	0
however, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate 'why' certain combinations are more effective than others.	0
my biggest concern is that empirical validation(experiment) is poor.	0
overall, i found that this work is empirical, and i’m not convinced by its experiments about the advantage of multiplediscriminator training, due to lacking of fair computational cost comparison with singlediscriminator training.	0
the empirical results show that it is not just a matter of reducing the training time but also of performance.	0
typically with papers proposing modifications of the training regime of the neural network one would expect one of three outcomes:  a well justified, mathematically sound method, well tested in simple cases and with some proof of concept results on proper tasks  a more heuristic, empirical driven research, where strong results on proper tasks  method, however justified, allows us to do something previously impossible, removing some limitations/constraints (like biologically plausible learning etc.) in its current form paper seems to lack any of these characteristics.	0
"in fact, authors explicitly claim that empirical section ""note that in these experiments, the purpose is not to achieve state of the art performance, but to exemplify how backdrop can be used and what measure of performance gains one can expect.""."	0
in sum, the paper has some interesting theoretical results but the empirical results are not convincing.	0
my major concern is that their empirical study cannot support their claim that ``the planning may occur implicitly, even when the function approximator has no special inductive bias toward planning''.	0
my concern is, given this is an empirical work, the number of datasets used in evolution is a bit small.	0
the authors claim that their invnet is approximately invertible but there is no guarantee for this, making empirical conclusions unclear.	0
pros:  theoretical guarantees, elegant approach  good empirical results compared to other models  desirable properties: rotationequivariance, lower computational complexity, fewer parameters, robustness and guaranteed stability to deformations cons:  somewhat incremental technical novelty: combination of two previously published methods (qiu et al. 2018 & weiler et al. 2017) comments: 1. i believe the related work section can be improved by explaining more clearly the connection between your work and the cited ones and emphasizing the advantages and limitations of rotdcf compared to other methods in particular, a reader should be able to precisely understand what is the novelty of this work is and what were the technical challenges in combining previously published ideas (such as dcf and sfcnn) 2. how do you determine the truncation in practice?	2
"given the limited technical novelty(can be described as oneliner ""store forward pass in 4/8 bit fixed point""), limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), i think this is a boarderline paper on the positive side, the empirical result could still be interesting to some readers in the iclr community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum."	2
however i feel both the theoretical analysis and empirical results will be more convinced to me if a more complete analysis is presented.	0
pros:  strong empirical results on two ctr tasks using the previous works of selfattention and topk restriction techniques.	2
pros:  this paper is clearly written and includes a thorough and welllaid out empirical component  the contribution to the video action classification and captioning space seems like a worthwhile one cons:  the novelty of this paper mainly seems to be with respect to video classification and captioning; other methodological aspects and empirical themes are interesting but fairly standard more generally.	2
the lack of experiments outside of one video action classification & captioning dataset (and one additional one for a transfer learning study) limit the empirical generality of the findings.	0
strengths:  the paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical bayes setup.	2
there is nothing inherently wrong with simple questions, in fact, the kind of questions posed in the present paper are quite valuable, however, it lacks detailed study and rigor of a strong empirical work.	0
empirical validation: the empirical validation is limited because of: a) lack of comparison to progressive nets, b) lack of simple baselines (e.g., how about replacing hnet with an inference process like task_id = argmin_i=1..t loss(cnet, task = i) ), c) unclear interpretation of the provided results (how can the accuracy on mnist be 100%?	0
significance: i think that the paper provides a warranted empirical study about recent approaches to learning with little or weakly labeled data, which is of general relevance in the field pros: the paper is wellwritten and easy to follow nice overview of recent approaches to fewshots/semisupervised learning.	2
detailed empirical analysis covering a range of training data regimes cons: only one benchmark is used results may also vary for different network architectures or application domains i in general found the paper interesting; however as mentioned, i find it hard to gauge the scope of the results presented here given that they are only provided for one benchmark and one network architecture.	0
in light of these considerations, i am updating the rating to 6 (to reflect the points that have been addressed), but i still do not believe that the method is superwell justified, despite an interesting formulation and strong empirical results (which are aspects the paper could still improve upon).	0
i am not sure about why should it work (i explained in detail later), but it does work well empirically.	0
their empirical values are different but this is beyond the study of bendavid(2010).	0
the empirical evaluation is not unreasonable, but also not strongly convincing.	0
my main concern is about these empirical verifications.	0
overall, the paper provides interesting idea but the empirical results may be biased due to illposed problem	0
 my biggest concern is in the empirical evaluation.	0
overall feedback: this is a wellwritten paper, but i think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.	0
weaknesses:  empirical differences are marginal, but authors claim to “significantly improve the state of the art […]” in the abstract, which i do not see as justified.	0
strengths: models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.	2
while the experiment is reasonably posed, in my view it lacks the crosslingual breadth and an empirical account of similarity.	0
in addition, the paper is build upon o the 51 model as in figure 2 and the graphical comparison between the empirical esd and the expected esd of the five models in table 1, and they lack any mathematical/rigorous definitionsee table 2. the simulations are performs over a particular data set and a particular setting, and i wonder if the observations would be different for a different data set and a different setting.	0
originality: the work appears original to me significance: tbd, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be sota).	0
despite this, there lacks some insightful analysis of the proposed approach, and the empirical studies should be enriched.	0
novelty aside, i believe there are major theoretical, algorithmic, and empirical concerns in the current work which i discuss below: theorem 1  the third condition is true for locationscale family of distributions e.g., gaussian.	0
the empirical results are compelling, but i have a strong technical concern about the convergence issue noted by the authors (which was also communicated to the authors in a previous conference’s review session).	0
especially the lack of a meaningful improvement over the compared baselines from the empirical studies makes me wonder whether the bbp is indeed fit for purpose or even necessary for this task.	0
i'm still concerned about how significant the contribution is (as it is a straightforward extension to sfs), but the empirical results are now quite strong.	0
as it stands, it's an excellent 'empirical' measure, and captures a very interesting problem, but i'd like to know how to make it even more theoretically grounded.	0
strengths:  interesting research at the intersection of modelfree and modelbased research  lots of effort went into properly evaluating a wide range of possible design choices  mainly well written weaknesses:  missing depths in providing a deep understanding of why the author's expectations and empirical findings are inconsistent  the authors use many tweaks and ideas to tune each component of their model making it difficult to draw conclusions about the exact contribution of each of these  error in the theoretical analysis (?)	2
weaknesses: based on empirical evaluation, the paper cannot make any claim about the generality of the obtained results.	0
i know what it means mechanically, but there is a vague set of explanations that do not leave an empirical or theoretical understanding of why this is a sensible choice.	0
overall, the paper doesn’t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but i think there are problems with both.	0
my only major comments are that i’m a bit skeptical about the lack of a more thorough (theoretical) analysis supporting their empirical findings (what gives me food for thought is that lstm helps that much on even fully observable games such as ms. pacman); and the usual caveats regarding evaluation: evaluation conditions aren't well standardized so the different systems (apex, impala, reactor, rainbow, ac3 gorilla, c51, etc.) aren't all comparable.	0
moreover to give such a statistical statement would require saying something about the spread of the results, such as the empirical variance, but none is given.	0
this paper in particular focuses on further empirical analyses related to previously observed instability of unsupervised mappingbased approaches for learning unsupervised crosslingual word embeddings, but remains focused on the task of unsupervised bilingual dictionary induction (ubdi).	0
the paper is very well written and easy to follow, but my impression is that, as an empirically driven paper, it could contribute from further experimentation as well as from linking key empirical findings to theoretical justifications.	0
pros well written good empirical results cons little novelty.	2
there is nothing in the model that says it can't have more than two vectors but just that the empirical evidence is only for form and meaning.	0
review: the paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.	0
the article would be ok if the empirical results were really strong, but unfortunately they are not entirely convincing: 1. the classification results are only for resnet architectures, it remains unclear whether results would hold also for other architectures.	0
 tables 1 and 2 are misplaced pros:  important topic (network quantization)  good empirical results  easy to apply cons:  combination of previously proposed methods  no convincing justification  no strong advantage over previous methods	2
these results seem solid (though a bit incremental), but if the primary contribution is empirical, then the paper would be a better fit for an nlp venue.	0
however we also need more empirical evidence to support.	0
the authors however do not present a detailed examination of empirical results of varying , and only suffice to determine it with crossvalidation.	0
my main concern with the work is the empirical nature of the nes iterative procedure.	0
pros:  this paper conducts comprehensive empirical studies.	2
detailed comments: i thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified.	0
i think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.	0
the authors show empirically that the learned options do indeed decompose the stateaction space, but not the state space.	0
for example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results.	0
since the proposed method is quite incremental over the prior work, a strong empirical section is must to justify the approach.	0
the authors argue that empirically, neural networks with the galu activation is as effective as that with the relu activation, but theoretically, the galu activation is easier to understand because of the separation of the nonlinearity and the learnable parameters.	0
this primarily comes from a lack of empirical comparison, and not much explanation as to why key competitors were excluded.	0
there are empirical results to back that claim, but i strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method.	0
although the paper contains interesting ideas and empirical results, i have several concerns about the current version.	0
also, i have some concerns about some of the details of the maxmargin losses (listed and discussed below), so i'm not sure how reliable the empirical comparison is.	0
i also have a number of concerns regarding the experimental section of the paper, which i find to be lacking both in details and the empirical comparison of the method to existing works.	0
some of this was considered with additional empirical results, but these also didn’t have sufficient clarity nor discussion for me to really understand the recommendation.	0
accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
 pros: overall, this is a nice empirical paper with a reasonably extensive set of experiments.	2
present work, however interesting with regard to its potential implications, strays away from providing such theoretical insights and suffices with demonstrating limited improvements in empirical tasks.	0
another complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc. therefore it is unclear whether observed differences are meaningful.	0
the strengths of the paper are a interesting theoretical analysis of convergence difficulties in adam, a proposal for an improvement, and nice empirical results that shows good benefits.	2
the empirical results seem good, but the generated audio does not match the quality of the stateoftheart.	0
===================================== overall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and wellmotivated approach with strong empirical results.	0
strengths: the key empirical observation that fully quantized models are more exposed to adversarial attacks is remarkable in itself and the explanation given by the authors is reasonable.	2
weaknesses: except for observing the empirical weakness of fully quantized models, the technical contribution of the paper seems to be limited to combining the lipschitzbased regularization and quantization.	0
not only it achieves state of the art results with convincing empirical evidence but also authors make a good job of providing details of their specific modelling techniques for training challenges.	0
ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried eq 8 and it worked well empirically would be a lot better than what is there at the moment.	0
should it not, it would provide an empirical justification for what is, in essence, a different restriction to that of the learned prior structure: it is conceivably actually the case that these encoder restrictions induce the desired decoder behavior, but this is distinct to learning a particular dependency structure in the generative model.	0
the authors empirically show that ghn  random search is not only efficient but also performs competitively against the stateoftheart.	0
the idea sounds great, but i am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical.	0
on the downside, the theoretical contribution is moderate, and the empirical studies quite limited.	0
pros:  empirical results seem strong.	2
pros:  nice approach for hierarchical deep rl  great use of her to improve subgoal learning  good empirical results showing benefit of approach over flat learning cons:  no empirical comparison to related work  subgoal testing phase seems a bit hacky.	2
pros and cons:  the paper is well motivated, not only through the text but also with empirical evidence (section 2).	0
the authors also show how to regularize the gradient updates to be conservative in function space in standard stochastic gradient style learning, but with rather inconclusive empirical results.	0
'pros:  a work in an area with very view contributions and a certain lack of theoretical results theoretical results that are actually used in the algorithmic implementation and that allow to define the regularisation parameter based on the size of the available samples improved empirical results 'cons: an incomplete stateoftheart section that does not cite several important contributions on the subject; lack of baselines due to the incomplete stateoftheart section; lack of clear comparison with lipton et al. both in terms of the proposed method and the obtained theoretical guarantees.	2
pros: 1. wellexecuted paper, with convincing empirical results on the newly collected dataset.	2
cons: the empirical results are not good.	0
# unclear evidence for the generalization of the model authors mention a possibility of generalization of the proposed techniques to other tasks, but without any empirical evidence.	0
it is well written in most of cases and easy to follow (however i got the impression that the paper was rushed in the last minute; there are some trivial typos and very low resolution images etc.) however, i have a huge concern about the empirical evaluations.	0
pros/cons summary:  interesting concepts that extend beyond empirical fixes.	2
though fundamental understanding can happen asynchronously, i reserve my concern that such empirical method is not substantial enough to motivate acceptance in iclr, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved.	0
main weakness is the complexity of the algorithms and many design choices wich are well argued for but not theoretically or empirically well founded.	0
due to the complexity of the algorithm and lack of access to authors code at review time, it is not feasible for me to validate empirical results.	0
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
although the empirical results show the proposed model outperforms several existing models, my concern is still on the originality of the paper.	0
pros: 1. the paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution.	2
the paper has good potential, but sufficient empirical evidence is needed to justify the proposed technique.	0
if the authors argue this is one of their main contributions, i find that lack of a more comprehensive empirical or theoretical study disconcerting.	0
the experiments and ablations are thorough, but the empirical gains in terms of improving gan metrics are relatively minor.	0
2. good empirical results cons: 1. one limitation of this work is that the goal set is known.	0
pros:  the proposed method is well motivated from empirical studies by visualizing parameters of three tasks, and the analysis on rare words are convincing.	2
unfortunately, the empirical evaluation in this work is lacking to formally confirm or reject my hypothesis.	0
the biggest weaknesses of this paperfor this audience, which skews empiricalconcern the extent to which the work addresses or provides insight about real neural networks.	0
the empirical covariance is psd, but when you apply a function to it elementwise it has no guarantee of conserving the psd property.	0
the second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.	0
this might be true, but it is empirically proven only by single dataset and single run.	0
overall, the paper is interesting with its nice empirical studies but stays somewhat superficial.	0
it is a good empirical paper demonstrating the practical use of an idea that is simple but reasonable, and in a way that is substantiated using proper cutting edge framework and baselines.	0
however, the reviewer has concerns about the motivation of the presented analysis and insufficient empirical results.	0
pros: ' extensive theoretical and empirical analysis ' simple idea that generalizes to multiple usecases, which implies robustness of the approach as a methodology cons: ' unimodal assumption is likely not realistic, which would result in misleading visualization of data ' visualization analysis focuses on how classrelationships are preserved rather than faithful representation of each data point, which is a wrong target ' synthetic experiment is conducted on a single, too simplistic one; more examples are needed to understand the capabilities of the model in more detail ' the bias of knowledge distillation is not controlled	2
7. the improved empirical performance of the clipped relu / sste is unsurprising but why does the vanilla relu ste perform so poorly on cifar10 with resnet20 with 2 bit quantization?	0
the cost of the sparsification is essentially the application of the trained neural network to a small number of data points in order to compute the sensitivity scores pros:  the method works empirically, in that their empirical evaluations on mnist, cifar, and fashionmnist classification problems show that the drop in accuracy is lower when the neural net is sparsified using their corenet algorithm and variations than when it is randomly sparsified or the neural network size is reduced by using svd.	2
 despite the suggestions of the theory, the accuracy drop can be quite large in practice, as in the cifar panel of figure 1 i think the iclr audience will appreciate the attempt to provide a principled approach to decreasing the size of neural networks, but i do not think this approach is widely compelling as : (1) no true guaranteed control on the tradeoff between accuracy loss and network size is available (2) empirically the method does not perform well consistently (3) comparisons with reasonable and informative baselines are missing updated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method.	0
authors provide nice theoretical motivation, yet empirical results seem incremental and do not fully support the effectiveness of this approach.	0
the empirical evaluation lacks any comparison to baselines and serves for little more than as a sanity check of the developed theory.	0
"however, the claims need to better match the empirical evidence, and for a paper that has ""better understanding"" in the title, i'd like to gain a better understanding of the differences to kaiser et al. (2018) that make vqvae fail for them, but not in the present case."	0
so it would have been nice to see argumentative (or even empirical) comparisons with popular models such as vhred for dialogue [serban et al., 2017], as many of these models are not intrinsic to either mt or dialogue (the only aspect specific to dialogue in vhred is context, but it can be set to empty and thus vhred could have been used as a baseline in the paper.)	0
the appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the loglikelihood terms are mixed together.	0
 the paper’s title and the openreview submission name should probably match update following author and reviewer discussion: i agree with others regarding the weakness of the empirical comparison to pseudocounts in particular, but still believe that the paper deserves to be accepted due to the fact that (1) some of the results are really good, and (2) this is a simple original idea that has the potential to drive further advances (hopefully addressing the empirical and theoretical limitations of the current work)	0
2) another concern is that the efficiency of the proposed method relies too much on the empirical result that the number of flat extreme ray is small.	0
strengths: attempts to solve a longstanding problem in modelfree rl (effective exploration in sparse reward environments) clear writing and structure, easy to understand (except for some minor details) novel, intuitive, and simple method building on ideas from previous works good empirical results (better than state of the art, in terms of performance) on some challenging tasks weaknesses: not very clear why (and when) the method works  more insight from experiments in less complex environments or some theoretical analysis would be helpful it would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) not clear how stable (to train) and robust (to different environment dynamics) the method is main comments / questions: the paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis.	2
pros:  a simple idea with good empirical results that would be of interest to the community cons:  (extremely) unclear presentation which hinders the message of the paper.	2
pros:  simplicity and effectiveness of the method  extensive experimental results under different settings cons:  it's not clear why the method works besides some notyetvalidated hypotheses.	2
pros:  since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a wellmotivated problem  it is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision  the proposed penalized approximation of the problem is simple and straightforward to try  experiments indicate that the method works as advertised cons:  there are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines  experimental gains are very modest compared to an independentlytrained crf  novelty is a bit low, since the trick of differentiating through meanfield updates is wellknown at this point  efficiency: the number of variables in the crf scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems differentiating the minimumcost multicut problem is wellmotivated, and i think the multiplehuman pose estimation problem is a nice application that seems to fit this abstraction well.	2
i’m worried about the computational efficiency of this method, but this paper neither discusss the computational complexity nor illustrate the results in the experimental section.	0
in particular, i found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and i’m not happy with lack of baselines in the experimental comparison (again, see below).	0
you mention this (paragraph 4.1) but do not take experimental steps to measure it.	0
i am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence.	0
b) i have to admit that i am not extremely familiar with common experimental evaluations used for derivativefree methods but the datasets used in the paper seem to be rather small.	0
writing is very clear and method is well motivated cons: ' i found the experimental validation a bit limited  the presented results are nice and for the problem quite comprehensive but i would have wanted something a bit more complicated than change point detection.	0
strengths: 1. the paper looks at a new angle to study and characterize cnn models in general, and vqa models in particular by looking into the psycholinguistic literature experimental setup studied with human subjects.	2
the experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and nonred items.	0
the proposed modifications are mostly fairly straightforward conceptually, but appear to work well, and this reviewer feels the paper has huge value in its experimental contributions extending and clarifying certain aspects of wavenet training and distillation.	0
"for example, the value seems to not really be in the ""proposing"" a texttowave neural architecture for speech synthesis (which, aside from important experimental tweaks, is essentially tacotron 2 training all parameters from scratch) but in showing that it works well experimentally."	0
6. it has been shown experimentally that calculating flow after the third resnet block produces the best results, but it is not clear why this is the case.	0
however, experimental results are lacking.	0
"i have few concerns  it would be good to have the ""average"" columns in the tables reporting the experimental results."	0
pros and cons () interesting idea () diverse experimental results on six datasets including benchmark and realworld datasets () lack of related work on recent catastrophic forgetting () limited comparing results () limited analysis of feature regularizers detailed comments  i am curious how we can assure that svm's decision boundary is similar or same to nn's boundary  supportnet is a method to use some of the previous data.	2
thus considering the lack of novelty and experimental validation, i recommend rejecting this paper.	0
3. the nonnegative variant sounds interesting, but the experimental results are quite limited.	0
the paper tests the performance in different experimental settings, but the baselines used in the experiments are concerned.	0
while the novelty and good structure of the paper are reasons to accept it, i have doubts concerning the soundness of the results due to the experimental setup.	0
reasons to accept the paper  novelty  works in an unsupervised setting  well written/structured reasons to reject  doubts concerning the experimental setting  (minor) related work is not complete  (minor) not all common performance measures are reported  [1] wang, peifeng, shuangyin li, and rong pan.	0
cons: the experimental section needs to be extended and the results are limited to simulations on cifar100 and evaluation on lab experimental data.	0
 the paper is mostly experimental but the message delivers clearly the paper’s objective  the direct visualisation is interesting  the paper suggests interesting problems related to the technique cons:  to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of wasserstein distances (wasserstein > sinkhorn ?	0
there's at least a few confusing details, noted below:  the experimental results for imagenet comparing against other models are directly taken from those reported by lu et al. that's fine, but it does mean that it's hard to make comparisons against 'any' other paper than the lu paper.	0
however, the proposed method is incremental from relation network (learning2compare sun te al. 2017) and lacks theoretical and experimental justification why the proposed model works well.	0
"approaches such at least like ""modeling the intensity function of point process via recurrent neural networks"" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data."	0
"pros:  a simple idea  encouraging experimental results cons:  confusing read  no clear intuition is given  restricted to lowdimensional datasets  strong baselines needed  the plots are too small to see (impossible to see when printed) other comments:  the authors are using the term ""feature vector"" to refer to a data point."	2
overall, the direction in which the paper is taking us is very interesting, and i can imagine a rewritten version of the present paper in the previous years' iclr workshop format to be a very good candidate but for a paper that has no theory, that is very light on its experimental descriptions and details, that is very light on relevant research review, and that is very loaded with vague and imprecise descriptions, i can not recommend the submission for iclr as it stands.	0
overall the combination of ideas is novel but the experimental results are limited in scope.	0
this paper has limited its focus on meta learning for fewshot text classification according to the title and experimental setup, but the authors do not properly define the task itself.	0
the main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation: ' for instance, the formalization of „assumption 1“ is unclear.	0
however, experimental evidence is lacking.	0
i do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.	0
overall, i find the paper to be incremental and lacking good experimental results and comparisons.	0
as is, the experimental results are not compelling enough to justify this (lack of clear quantitative improvement over state of the art).	0
the proposed method seems promising for volumetric data as the authors note, but this also needs to be demonstrated experimentally.	0
overall i think this paper introduces an interesting problem and some valid approaches, but can be significantly improved both in the methods it presents and the experimental setup.	0
significance: the concept of converting a nondifferentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.72%) on a single system (faster rcnn), that has already been pretrained (so not clear if this method can learn from scratch).	0
in my opinion, a more rigorous and thorough experimental exploration would increase the value, but the paper demonstrates that training with alternative fdivergences is feasible.	0
while i think that the paper contains many interesting ideas, it lacks a good motivation and, more importantly, i find the experimental design (as well as some results) to be very weak.	0
in summary, the paper presents promising experimental results, but lacks a theoretical justification or convincing intuition for the proposed approach.	0
pros:  good and promising experimental results.	2
 it would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5. conclusion: the experimental results seem promising however the motivation for the approach is not clear.	0
 the presentation of the approach makes sense, and experimental results using several different gans methods and competing regularization methods are extensive and good in general cons  i didn't find major issues of the paper.	2
strengths: the performance improvements over competing methods on movingmnist and kth presented in the experimental section are significant.	2
 adding a sentence explaining the intuition behind using satlu in equation (1) might be helpful to summarize my feedback: i think experimental results and analysis are strong, but the presentation is strongly lacking!	0
while the findings are certainly interesting, the method lacks experimental validation in certain aspects.	0
[first update] i find the authors' problem statement appealing, but share concerns with reviewer 1 about the privacy guarantees offered by the proposed method, and with reviewer 3 about need to clarify the experimental evaluation.	0
i do have concerns regarding the experimental evaluation:  the “demos” baseline approach should be explained in the main text!	0
to conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements.	0
the general idea and motivation are generally appealing but the experimental validation is a mess.	0
however currently it is still missing some important experimental results as mentioned above, and not ready to be published as a high quality conference paper.	0
i agree with the other two reviewers that the work is somewhat incremental, but the differences are well explained, the experimental results are interesting (particularly the differences of parameter vs representationbased sparsity, and the plots in appendix showing neuron importance over tasks), and the progression from sni to slnid is wellpresented.	0
"pros:  proposed regularizer is wellexplained and seems to work well, ablation study is helpful cons:  the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text  some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work specific comments / nits: (in reading order) 1. i think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative rl; i think ""forethinking"" or something like that that is an actual word would be better, but i can't think of a good word... maybe frugal?"	2
— experimental results the results are interesting proofsofconcept but a few more experiments/answers would be helpful:  it still appears that pr curve in the highprecision regime (fig 3b) has lower precision than frcnn/yolo.	0
lowshot results i appreciate lowshot learning as a testbed for this sort of disentangled image generation, but unfortunately the experimental results are not very convincing.	0
1.the paper explains the necessity and effectiveness of the method from the theoretical and experimental aspects, but the paper does not support the innovation point enough, and the explanation is too simple.	0
3. the lack of analysis on domainclass dependency of each dataset makes the analysis of experimental results weak.	0
while the paper is clearly written and is interesting, the contributions are rather incremental and the experimental section does not show a clear improvement wrt the stateoftheart, neither a clear conclusion on the impact of learning the edges.	0
to me it makes a lot of sense, but in the experimental part i could not clearly see if the improvement in performance is due to this representation of the binarized bn.	0
in a follow up response to anonymous public comments, some new tests using cifar10 data are presented, but to me, proper evaluation requires full experimental details/settings and another round of review.	0
experimental protocol: the experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of gan, number of iterations) vary significantly depending on the task (cf. table 3 for instance).	0
as there is no strong technical contribution beyond the experimental observations in the current submission, i suggest the authors try to address the gan shortcomings both mentioned in reviews and their reply, instead of just observing / reporting them.	0
== quality of results == the experimental results seemed overall positive, but i felt that they could have been stronger.	0
cons  the main paper feels quite empty, especially the experimental validation parts with limited number of baselines.	2
however the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper.	0
cons: while this paper has some nice results, there are some aspects of it that concern me, specifically related to hyperparameter tuning and experimental rigor: there are three methods given for using an lm to make a prediction: full, fullnormalized, and partial.	0
this, along with my concerns about the experimental rigor expressed above, limits the potential impact of the paper.	0
reviewer 1 and reviewer 2 both raised serious concerns about the types of sat instances that were used to evaluate the experimental setup, as well as about the use of z3 as a baseline for solving random sat instances.	0
given this additional information, i've lowered my score for the paper from an 8 to a 5. i do think that the approach is interesting, but have reservations about the experimental evaluation and the claims made by the current submission.	0
i am afraid that the paper lacks the necessary rigor in the experimental section.	0
cons experimental validation is lacking in many aspects.	2
in sum, while the proposed model seems novel, its motivation is unclear and it is difficult to assess the effectiveness of the proposed method due to lack of experimental validation.	0
conclusion: overall a great direction and interesting approach but requires more careful experimental setup and evaluation and discussion of related work for acceptance.	0
the result section given in the paper is its weakness and requires a more indepth analysis:  the results given for omniglot are impressive  the experiments analyzing the impact of diversity and routing depth are interesting and offer interesting insight into the architecture  the results do not show learning behavior over epochs; this is not necessary, but would give an additional insight into the learning behavior of the architecture  the experimental settings are confusing: why are the different experiments performed with different datasets?	0
i may have missed something, but i do not understand how the model can contain the uncertainty shown in the experimental results if the agent's initial position is provided to the current or past position predictor.	0
the experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine.	0
overall, i like the new ideas in this paper but i think a few more experimental settings are required before they should be published.	0
i think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.	0
however, it is poorly written, does not properly discuss the related works and does not present a convincing method or experimental results.	0
the idea of using polynomial transformation is not new, as the author(s) note(s), but in this setup it is backed up with neither with intuitive arguments nor with decent experimental results.	0
i have several concerns regarding the way the paper using indices, and the experimental result.	0
cons:  low originality and missing comparison to related work  unconvincing experimental section minor remarks: ' the title is misleading in the sense, that the spatial part of the spatiotemporal representation is not related to actual space, and further, that the backpropagation is not spikebased ' the figures appear blurry, and in the case of fig. 2 is very hard to read. '	0
third, the experimental results seem nice, but the lack of comparisons blurs the advantage of the proposed method.	0
 the experimental results in general do a fair job illustrating the likely issue (though i would have liked to see more rigor and depth here as well as detailed below) weaknesses / things that concerned me:  (w1) lacking rigor / depth: one of my big concerns with this work is that the analysis to demonstrate the inherent flaws of irgan if fairly shallow and not detailed enough.	0
 (w2) missing some experimental results / deeper insights : there were some notable empirical results that were missing or not provided, that raised some concerns in my mind.	0
however, as pointed above, i think this paper lacks enough experimental analysis and comparison.	0
"pros:  interesting idea  reasonable approach by combining existing building blocks cons:  too much focus on stft vs. cqt  too little focus on getting wavenet synthesis right  too limited experimental validation (too restricted choice of instruments)  poor resulting audio quality  feels too much of combining black boxes as a result i rate the paper ""not good enough"" in its current form."	2
on the positive side, the experimental validation seems well done.	2
it is good that the experimental section lists many appropriate baseline models and multiple evaluation metrics, but it is not clear how they are used.	0
pros:  interesting idea  experiments are interesting cons:  formal results are either trivial or could be improved in their statements  experimental guarantees only, up to what is hidden in the bigoh notations of theorem 2.2, 2.3. details: ' in theorem 2.2, you need to remove the , unless you point to the taylor theorem that guarantees that for the identity you claim before (5).	2
the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesn’t really show how the proposed work is adding value to the field.	0
it lacks the experimental comparison with previous methods but only include discussion in texts.	0
however, there are several important controls missing from the analysis, several claims which are unsubstantiated, and experimental details are lacking in a few places.	0
on one hand, it’s a thorough and wellwritten experimental paper, something which is really important but is also clearly underappreciated in the machine learning community.	0
perhaps this could be an application paper that brings existing methods to a slightly different (attention) domain, but not only such paper is less suitable for iclr, but also it would require strong experimental results.	0
cons:  a little more experimental evidence would be welcome.	0
i can certainly imagine that the irl approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.	0
in summary, experimental results are good to verify the goodness of a model, but the theoretical support or intuitions are more important to understand the benefits that we can gain by making this change to the standard lstm.	0
"my concern is that this also is reflected in the experimental results: in the food gathering game, since killing other agents incurs ""a small negative reward"", it is never in the interest of the team to kill other teammates."	0
the technical contribution is somehow limited, but it is substantially validated by a very well organized and convincing experimental evaluation.	0
results are only preliminary in the paper, but this statement needs a more thorough experimental backing.	0
the experimental details are lacking (learning rates?	0
concerning experimental results, no statistical significance test is performed, so it is not clear to me if the shown improvements are actually significant.	0
my main concern is related to the experimental evaluation of the method.	0
it's surprising that this works, but there are experimental results to back it up.	0
in addition to the limited technical novelty, i have a few other concerns as well, including some on the experimental evaluation:  realvalued node embeddings obtained from shallow/deep graph embedding methods can be used with 'overlapping' versions of kmeans.	0
in summary, i think the paper lacks both in terms of technical novelty as well as experimental evaluation and therefore doesn't seem to be ready.	0
pros:  the experimental results of this paper are its main strength.	2
issue #4 (experimental): the title suggests no weakening of the decoder is needed, but experiments are not performed with decoders of varying sizes despite all the issues above with incorrect theory and experiments, it could still be possible that feature injection with bow is helpful to prevent posterior collapsing of text vaes.	0
3. the careful experimental analyses not only show the insight of dynamic halting in qa task but demonstrate the act is very useful for algorithmic tasks.	0
clarity: fair there were some experimental details that were poorly explained but in general the paper was readable.	0
3. lack of quantitative evaluation: the experimental evaluation shows qualitative performance.	0
experimental results:  table 2: i think this table since it includes most models, but it still misses recrep (without delta = 0) and stoccon.	0
one key concern and flaw in their experimental work, which was not addressed, nor even raised, by the authors as a potential issue, is that their padam approach got one extra hyperparameter (p) to tune its performance in their grid search than the competitor optimizers (adam, amsgrad, momentum).	0
in conclusion, this is a well written paper, but the novelty is not apparent and the experimental results are weak, and so i am not convinced this is suitable for iclr.	0
pro: ' the approach is proposed for irl, gail and bc cons: ' lack of positionning w.r.t pomdp litterature ' lack of details in the experiments, and lack of good experimental results ' low contribution in term of model [merel et al. 2017] learning human behaviors from motion capture by adversarial imitation [choi et al.] inverse reinforcement learning in partially observable environments	0
"# weaknesses the main weakness of this submission lies in its experimental evaluation, especially the absence of any dynamic objects in the tested environment (""static world carla"", section 1)."	0
" typos in section 3 (""trailanderror""), section 4 (""autonmous"", ""knowledge to"") # recommendation although the theoretical benefits of the method are wellmotivated and clear (offpolicy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple carla test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work."	0
first, adding more details as suggested above and clarifying the experimental protocol seem like a must, but can be easily addressed by an update to the text.	0
the experimental results do not seem to be any better compared to baselines when measured in terms of data efficiency, but the proposed method requires fewer “reward computations”.	0
due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter .lambda=0.00011 on the cifar data set.	0
# cons ' biggest criticism is that adpsgd from lian et al 2018 is not included in experimental comparisons even though the paper is referenced.	2
the same experimental setup has been extended to use cifar10 as an additional, more realistic dataset, the use of potentially more powerful lstms as well as grus, and several runs to have more statistically significant results  which addresses my main concerns with this paper originally (i would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying).	0
while i believe the authors did a reasonable job following this through, i have some concerns about the experimental setup.	0
however, the paper currently shows potentially interesting experimental/theoretical results but does not do a comprehensive job of either side.	0
weaknesses:  the arguments for why the experimental evidence actually supports the existance of an approximate number system (ans) could be made more clear.	0
for example, the section on “ratios andweber fraction” argues that “these curves align well with the trend predicted by weber’s law”, but does not explain how the experimental data would present if the alternative hypothesis (pairingbased strategy) was being used.	0
pros:  the experimental section is quite thorough and the results seem overall good.	2
this work is lacking in the experimental section due to some missing details and few inconsistencies.	0
 the experimental section lacks performance and comparison in a controlled environment, e.g., on synthetic data with more samples to show statistical significance.	0
i am not entirely convinced that ad and mhp is a killer combination, but the experimental results are ok, nothing to complain here (except the usual bla: make it larger, more, etc), but honestly they really fine (maybe compare also again against more related work, e.g., ruff et al icml 2018).	0
o it’s stated in figure 2 and table 6 that 2 classes are either blank or nocrystal but is that a known fact (purposely chosen) or no pattern images for crystalline structures due to inadequate experimental settings to uncover the crystalline nature of the analyzed structure?	0
post rebuttal: i feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results.	0
my primary concerns are from the experimental sections of the paper.	0
[a] sequence level training with recurrent neural networks, iclr 2016 [b] adversarial feature matching for text generation, icml 2017 (2) evaluation: my main concern lies in the experimental evaluation, with detailed comments listed below.	0
the increments presented are reasonable and justified, but the experimental results, specifically on the larger datasets, warrant further investigation.	0
in principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated.	0
how are the representations to be used and what type of users is it intended to serve (expert/patients etc) pros and cons  interesting problem modeling could be better motivated experimental platform is limited for interpretability studies == hsu, w.n., zhang, y. and glass, j., 2017. unsupervised learning of disentangled and interpretable representations from sequential data.	2
 update: i've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental setup and multitask/singletask differences.	0
 the experimental validation is lacking in many aspects.	0
 it is an interesting idea for reducing the number of parameters, but i don't think the experimental section is adequate to judge it.	0
i believe the guidelines mention that more pages can be used if there are extensive results, but i don't think the experimental results warrant the extra page.	0
"in summary: pros  claims sota results on two good benchmarks for zeroshot learning  approach is original cons  paper lacks a lot of methodological and experimental details some minor details:  ""we found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset."""	2
 strengths:  clear explanation of the problem  clear explanation of the model and its application (pseudocode)  clear explanation of training and resulting hyperparameters weaknesses:  weak experimental settings:  (a) comparison against 'easy to beat' baselines.	2
however, there are some concerns about the experimental evaluations, 1. although the quantitative evaluations for 2point and 4point interpolations are important, it is hard to assess these interpolations in a semantically meaningful way.	0
however, the algorithm lacks justification in general and experimental results are not very persuasive.	0
however, the experimental performance of the proposed algorithm is somewhat lacking in comparison, perhaps because the authors focus on kernelised equivalents of mlps instead of cnns as zhang et al. my rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layerwise training, and absence of experimental comparison with several baselines (see details below).	0
cons:  in the experimental section, the methods used to learn the policies, dqn and ddpg, should be briefly explained or at least referenced.	0
the experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.	0
"strengths:  practical proposal to use graph regularization for neural network regularization  the proposal to construct graphs based on the current batch makes sense from an algorithmic point of view cons: experimental results are a bit weak  the most significant results seem to be obtained for ""implementation robustness"", but it is unclear why the proposed approach should be particularly good for this setting since the theoretical motivation is to prevent overfitting."	2
in conclusion, my opinion is borderline but only leans towards acceptance because the experimental results are strong.	0
while i strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend.	0
more data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments).	0
however i concern about the experimental parts, which are only evaluated on small settings.	0
this is also lacking from the description of the experimental protocol, which does not address the datasplits (how many classes were used for each) and size of the unlabelled test set.	0
further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches).	0
there is no good experimental comparison to that work, but it is unlikely it will perform worse.	0
pros:  the model is interesting and the motivation is quite clear  analysis is quite nice  writing is quite clear and decent cons:  extremely lacking experimental validation  there are literally no baseline models, no numbers or any kind of quantitative analysis.	2
weaknesses:  the claim that reusing the same training data used for training the teacher model in model compression can lead to overfitting of student model is not very obvious and needs more experimental evidence in my opinion.	0
cons my main concerns with this paper are regarding the experimental evaluation  i do not feel these are sufficient to justify the strength of the attack method proposed.	2
here are my broad concerns: 1. even though the datasets used are small (mnist/fashion mnist), the experimental validation of adversarial attacks is only performed on 100 test examples.	0
pros:  useful analysis that will help direct research in this area  shows that this approach works for models that have a high communication to computation ratio  provides a useful approach that works for a number of models cons:  positive experimental results are on models that are typically not used in practice e.g. alexnet and vgg16  speedups shown on lstms don't see worthwhile to scale, and in practice a modelparallelism approach may scale better corrections:  typo in notes for table 1 last sentence rcg => rgc  typo in first sentence in section 3.2: redsycn => redsync  section 3.3, #2 last sentence: maybe overdrafts => overshadows ?	2
my main concern is that i am not entirely convinced that the experimental results support the main claim made by the authorsthat the proposed adversarial approach is a viable solution to mitigating annotation artifacts.	0
what i am concerned is whether the experimental gains observed actually result from mitigating biases.	0
pros:  intuitive idea for a common problem  solution elegantly has the form of a modified policy gradient  convincing experimental results  selfcritique of core idea, and extension to address its main weakness  nicely written text, does not leave a lot of questions cons:  while the core idea is nicely motivated and described and good to follow, section 2.3 feels very dense and too short.	2
while the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate.	0
pros: important problem setting, good experimental results.	2
another main concern is in the author’s experimental setup.	0
pros:  clearly written and easy to understand what authors are trying to say  interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks  nice suggestion of choosing activation function for deep networks (proposition 4)  elu/selu/softplus/swish all satisfy this suggestion cons:  novelty may be not strong enough as the standard analysis tool from [1] was mostly used  experimental setup may suffer from some critical flaw few comments/questions:  p3: is m_{relu} = 2 correct, from relu eoc, shouldn’t it be ½?	2
cons: 1. i don't think the experimental results are convincing enough for the reasons below: 1.1. all experiments are conducted over mnist with testing accuracy around 96%.	0
when looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing.	0
as a whole, the authors argue that their method allows a better propagation of qvalues uncertainty but provide little theoretical or experimental evidence that would back this claim.	0
another major concern is the lack of experimental evaluation.	0
questions for authors:  the experimental results only show that using higher order interactions results in a better function approximation (explanation), but explanations for level > 2 do not seem to be that good (table 5).	0
overall, i think this is an interesting improvement to disentangled representations learning, but suffers a bit from early experimental results.	0
however it does not provide enough technical details which would allow this work to be reproducible and the experimental section does not verify almost any of the design choices which would allow the reader to asses the main factors which lead to the obtained network performance .	0
my biggest concerning is regarding the experimental results of this work.	0
pros:  clear mathematical foundations and fair experimental results.	2
authors suggest some experimentally fixed values, but a proper analysis (at least empirically), would be useful for the readers.	0
overall, the paper is easy to understand, but i lean towards rejecting this paper because i am not convinced by the experimental evidence.	0
i think the main idea of the paper is interesting but the presentation lacks a number of details in the description/parametrization and the experimental design.	0
the paper has many flaws:  the value of the theoretical results is unclear  the paper contains many statements that are either incorrect or overly sweeping  the experimental setup and results are questionnable theoretical results: ''proposition 1: pretty trivial, not much value in itself ''proposition 2: pretty obvious to the experienced reader, but nonetheless a valuable if narrow result.	0
given the handannotated nature of much of the input knowledge (the rule templates), this introduces an important concern that the experimental wins will not be robust in more realistic settings where different knowledge may be required.	0
[significance] # concern about experimental settings the experimental setting for nmt looks unnormal in the community.	0
there are some other problems with the presentation, including the fact that contrary to what is suggested in the introduction, the model seems to have access to the ground truth size of the blank (since positional encodings are given), making it all but useless in a real world application setting, but it is really difficult to evaluate the proposed task and the authors' conclusions without a much more detailed description of the experimental setting.	0
at this point, i think the paper provides a nice idea with a theoretical analysis  but it doesn't provide enough experimental evidence that this works.	0
cons: some experimental results can be difficult to reproduce.	0
this is a compelling idea but it needs more support than the theoretical or experimental sections give.	0
the experimental section compares the algorithm against a wellknown and strong baseline, but without any information about the variance of the results and only for a deep network.	0
however, the experimental part lacks a quantitative evaluation as well as a comparison to stateoftheart methods, which makes the assessment of the proposed model difficult.	0
pros the proposed experimental studies can be interesting to the community many interesting illustrations are provided.	0
cons the conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: deep models less transferable than shallow ones, averaging gradient is better i find the experimental studies a bit limited and i would expect larger studies which would have improve the interest of the paper.	2
throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice.	0
in general, i think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results.	0
pros:  core ideas seem promising  leveraging mathematical structure is a great strategy for constructing algorithms with desirable properties cons:  very limited experimental results  not clear if improvements are significant  hierarchy construction seems to be too limited to work in any reasonably sized problem  no evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method	2
the experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted.	0
overall, the problem investigated in this paper is very interesting and is of practical importance, the experimental results are preliminary but encouraging.	0
however, while it is interesting and useful, i do i have concerns both on the novelty and experimental comparisons in the current version.	0
this wouldn't be too much of a problem if the experimental results were strong, but unfortunately it isn't the case.	0
in summary, it is not a bad paper, but the experimental results are not sufficient to conclude that much.	0
2) experimental results the experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of stateoftheart competitors among the very many related methods published recently (e.g. [28], just to mention a small subset).	0
the mathematical shortcomings of the paper could be compensated by amazingly good experimental results.	0
pro: ' interesting algorithm for structured prediction (base on reward) ' interesting results on some (toy) usecases cons: ' lack of discussion on the positive/negative point of the approach w.r.t sota, and on the influence of the reward function ' lack of experimental comparisons ' only toy (but complicated) problems with limited training sets	0
cons: ' the experimental treatment is insufficient; in particular, a more carefully considered experimental justification is needed with respect to other detection strategies. '	0
the authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results.	0
pros  considering correlations between features from different encoder layers is a good idea  the improved encoder / decoder architecture is significantly more efficient than the cascaded approach of [li et al, nips 2017] cons  somewhat incremental  limited experimental evaluation  qualitative results not clearly better than existing methods  missing citation for multiscale losses limited experimental evaluation one of the key claims of the paper is that “our method with interscale (fig.7(f)) or intrascale feature transform (fig.7(g)) are more similar to the target style than those of singlescale style transfer without considering interchannel correlation” (figure 7 caption); this claim is substantiated primarily by qualitative results in figures 4, 7, and 8. personally i don’t find the results with interfeature correlations to be much better than those with only intrafeature correlations or the results from prior work.	2
the interscale loss seems intuitively like a good idea, but i don’t think the paper presents sufficient experimental evidence to justify it.	0
my main concern is about the experimental evaluation:  the authors should test their approach on carlini & wagner's attack which allows for explicit control of logit differences and thus could entirely defeat the background check.	0
however i have comments about their implementation and experimental results: 1. they are claiming to have a very high distortion while their model can still perform well.	0
so this raises a concern about novelty (although the experimental results are new).	0
the authors simply say such methods do not work, but show no evidence in the experimental section.	0
"the experimental results do show that the proposed reward augmentation leads to better performing policies, but the claims in the experimental section need to be less strong (""outperforms the baseline by a large margin""  figure 4  overlapping error bars; ""state of the art""  figure 5  again, error bars, and no improvement in some domains.)"	0
" the observed behaviour (sensitivity to mode collapse, word swap, word removal) of the ""reverse ppl"" metric is pretty much expected, but i agree some experimental results are still interesting."	0
the experimental results are neat, but not too impressive.	0
it would be more convincing to have a dedicated discussion of the practical advantages of the simplicity claimed by this method, discussing e.g. training/testing time, memory footprint of the models, convergence properties, stability, etc. comparison: the chosen baselines, i.e. munit and drit are experimentally shown to perform poorly on the considered task.	0
experimental protocol: i understand that such an approach is difficult to evaluate quantitatively but i am not sure what there is to learn from experiments reported in table 3, as there is no point of comparison on this task.	0
the experimental setup used to evaluate the approach is however limited.	0
moving on to experimental results, i think this is another area where i have a few concerns.	0
feedback: my main concerns with the paper are: ' the experimental section is fairly thin.	0
at a high level, the issues include a lack of convincing experimental verification of the method, a generally contradictory and confusing description of the methods, and frequent factual errors or mischaracterizations.	0
cons 1. unclear presentation of technical contributions, experimental results do not support the key contributions of faster attack generation 2. i am also unconvinced of the relevance of blackbox attack algorithms given the nascent stage of deeprl  since these agents are just being developed and their abilities need to improve significantly before they become deployable (and blackbox adversarial attacks are a real concern), i feel this work is premature and will need to be redone once more capable/robust agents can be trained for practical rl settings ### in light of the revision, i have revised my score given the rewriting of section 3 that addresses the second con i raised above.	2
interesting and original paper and ideas being developed, but might be a tiny bit weak in term of results, both theoretical and experimental ?	0
the principal issues i see are as follows:  the originality of the contributions is not clear  missing theoretical discussion  the experimental setup is terse and slightly confusing concerning the originality of the paper, the differences to gama et al., 'diffusion scattering transforms on graphs' are not made clear.	0
concerning the experimental setup, i think that the way (average) accuracies are reported at present is slightly misleading.	0
overall, i would say that this is a potentially strong paper, but that experimental evaluation does need work.	0
"3. the reformulation in sec 4, especially the formula for the forgetpolar input p_k, looks heavily handcrafted, without experimental supports but statements such as ""we ran numerous simulations"", which is not convincing enough."	0
5. lacking of explanation on specific experimental settings.	0
most of the weak points of this paper lie in the experimental section.	0
however, reviewers have a few concerns in terms of experimental design.	0
revision: although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:  missing experimental evidence for the efficiency of the nn search algorithm.	0
cons: my primary critique of the paper is that there is very little experimental investigation of the crucial details of the algorithm.	0
in addition to this lack of experimental focus, the only quantitative result is the parzen window estimates in table 1. the proposed method does best the others but the other reported results are quite oldall from 2015 or earlier.	0
clarity: ' the writing of the paper was clear for the most part, however the experimental section could have been clearer.	0
"the authors make an argument for why using successor features would be more ""stable"", but i found the experimental evidence to support this claim to be underwhelming."	0
in the current form however, it is really lacking in experimental evidence to support the main claims/contributions.	0
while i find this idea interesting and of potential practical use, i have concerns about novelty and the experimental results and overall i recommend rejection.	0
"the paper also lacks experimental results, and the main conclusion from these results seems to be ""mnist is not suitable for benchmarking of adversarial attacks""."	0
the experimental results also show a limited efficiency improvement according to table 1. although this is a debatable drawback compared with the novelty/contribution concern, it worth to reconsider the motivation of the proposed method given the fact that the automl framework is extremely expensive due to the drl design.	0
pros:  using data to learn exploration strategy in tis manner is a novel idea for bandits  good experimental results  well written paper cons:  practical impact may be minimal.	2
fig. 4 does address this concern by illustrating their point experimentally.	0
2. detailed experimental analysis along with some user studies cons: 1. an important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained.	0
my principal complaint is the general lack of experimental evidence.	0
there is no evidence to demonstrate the effect of constraining the mutual information between x and z. in short, the paper offers what appears to be a very clever idea, but does very little to experimentally explore its effects.	0
"it is unfortunate that the paper lacks intensive experimental comparisons with ""model assumption approaches""."	0
cons:  hard to tell whether this approach works since the metrics for evaluation are not specified and there are no quantitative results in the experimental section (only 1 qualitative example per task)  the work is not reproducible due to the lack of details (see more explanations below)  the theoretical analysis is a standalone piece of the paper, without any discussion about the implications, or making connections to the previous sections.	0
i do not aim to question the experimental settings, which seem reasonable, but this raises some flags to me: the proposed evaluation is not completely realistic, as (some) training corpora are indirectly aligned, and the system could be somehow exploiting that.	0
this is an interesting idea, and one i certainly wouldn’t have thought of on my own, but i think it is currently lacking sufficient experimental support to warrant publication.	0
the idea presented in the paper is interesting, but (1) the experimental results are not entirely satisfactory for the moment and (2) only one aspect of the idea is exploited in the paper, which can be made more interesting and impactful while studying both attack and defense setups.	0
significance: the approach can have its impact for optimizing deep networks with no gradient, but more exhaustive experimental testing would be required.	0
pros:  method is fairly straightforward  modeling relationships between labels is an important problem cons:  missing references to key prior work in this space  minimal comparison to prior work  confusing experimental setup  paper is difficult to read missing references this paper is far from the first to consider the use of a semantic hierarchy to improve classification systems; see for example: deng et al, “hedging your bets: optimizing accuracyspecificity tradeoffs in large scale visual recognition”, cvpr 2012 deng et al, “largescale object classification using label relation graphs”, eccv 2014 (best paper) jiang et al, “exploiting feature and class relationships in video categorization with regularized deep neural networks”, tpami 2017 none of these are cited in the submission.	2
although the paper tackles efficiently an important problem, i am concerned about the experimental section and think it would be improved by taking the following points into account: • the proposed approach is compared only with two other algorithms.	0
the presentation of the paper is excellent  clear, wellmotivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.	0
the paper has the following strengths: 1. the experimental results on eleven different architectures (of varying depth and breadth) are convincing, and are consistently better than layerwise max for choosing fractional bit length.	2
3. the experimental comparison with layerwise quantization is somewhat lacking.	0
() lack of comparison with recent i2i models () lack of experimental results and ablation studies () unclear novelty 2. major comments  the novelty of this paper is not clear.	0
 experimental results are not convincing:  the introduction motivates the need for understanding human instructions and the abstract says 'given a human instruction', but i believe experiments do not have any human instructions.	0
the experimental result of the paper is strong but the algorithm and also a couple of statements seem flawed.	0
the paper however does not motivate its contributions sufficiently, and does not provide enough experimental results to justify their method.	0
the experimental justification provided is lacking, as analysis is largely restricted to simple models and does not include stateoftheart attacks.	0
i would suggest the authors to: ' better describe their contribution i.e model architecture and how the model can be used to obtain a real policy ' use 'stronger' use cases for the experiments, and particularly existing use cases ' provide a deep quantitative and qualitative comparison with sota pro: ' simple method, no need of a simulator cons: ' not clear how to move from trajectory generation to a real policy ' small contribution ' too light experimental study without comparison with baselines and state of the art	0
in the same vein, technical details concerning the selection of the training algorithm hyperparameters are also missing from the experimental section.	0
the experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x).	0
the only major concern at this point, is the experimental evaluation on small fullyconnected networks.	0
my main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.	0
experimental validation raises some concerns.	0
 lack of strong experimental evidence.	0
the paper is clear and the main idea is rather interesting, but the presented experimental validations are arguably weak.	0
in the begining of section 4, the authors mention that the mechanisms of discern naturally induce a form of curriculum (which may be debated), but this aspect is not highlighted clearly enough in the experimental study.	0
this could be somewhat forgiven if the experimental results were strong, but unfortunately they are too limited, and marred by overlysimplistic baselines that aren't properly tuned.	0
strengths: 1. snas unites the strengths and avoids the weaknesses of enas and darts 2. snas provides a nice theory, which is verified through their experimental results.	2
the presentation needs work in several areas, and the experimental results require more explanation, but otherwise this seems like a solid paper.	0
 (minor) i do not think that the size of the search space a very meaningful metric pros:  good exposition  interesting and fairly elegant idea  good experimental results cons  tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture.	2
overall, the descriptions are clear and easy to follow, but the experimental results need clarifying.	0
overall the paper proposes a bnstyle structure on vae latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.	0
while the research topic of this paper is interesting, i recommend rejections because i have concerns about novelty and the experimental results.	0
the experimental validation of the fix is carried out similarly to godard et al 2017, with identical losses, on the same kitti dataset, using the same splits, but also on the virtual kitti dataset.	0
given this i still have concerns on clarity and still am unhappy with the lack of confidence intervals in the main experimental section.	0
# comments and questions: ## weaknesses in the experimental evaluation: i find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradientbased policy search methods).	0
overall, i find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.	0
on overall, although the proposed method seems a direct application of arora et al.,2016a, i find their extension novel and quite interesting, but the paper needs more experimental results to validate the idea.	0
major comments:  the weakness of this paper is lack of experimental comparisons with other prominent studies.	0
 descriptions of training details are reasonable, and the experimental results across several datasets are extensive cons  the network structure may not be novel, though the performance is very nice.	2
it would be great if authors can provide more insights on why it works well (though not the best, but still reasonable), besides only demonstrating the experimental results.	0
i also have a number of concerns regarding the experimental section of the paper, which i find to be lacking both in details and the empirical comparison of the method to existing works.	0
however, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) metalearning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works.	0
2) pros:  novel highlevel controller that takes in frontview visual information  novel multipolicy low level controller  interesting experimental section 3) cons: numerical comparison to previous methods:  the only issue i found with this paper is that there is no comparison with other methods.	2
the experimental results were solid on the task for which the model's extra assumptions paid off, but that's a niche comparison.	0
b) i find the experimental evaluation acceptable but a bit poor.	0
however, there are some major concerns about method analyses and experimental evaluations, 1. data embedding based on triplets has been presented in (van der maaten and weinberger, 2012).	0
 strengths:  even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to mt is novel  the presentation is very clear  the choice of methods is well argued and justified  the experiments are well executed and analysed  thorough and varied analysis of the experimental findings i recommend this paper for the best paper award.	2
weaknesses: 1) as an experimental study, it would be valuable to compare the performance of curiositybased learning versus learning based on welldefined extrinsic rewards.	0
significant part of the paper is dedicated to proof of convergence, however i feel that convergence proofs are not interesting to iclr audience unless the method is shown to be useful in practice, hence experimental section must be strong.	0
this point is not really addressed in the experimental evaluation as benchmarking is performed against a robust and an adaptive policy but not explicitly against the (arguably) most closely related work in yu et al. it could be argued, of course, that yu et al. essentially use adaptive policy generation but they do explicitly learn dynamics parameters based on recent history of actions and observations.	0
pros: ———  interesting work  accessible  effective  thorough evaluation (though potentially missing a key benchmark) cons: ———  potentially missing a key benchmark (and therefore seems somewhat incremental)  only limited insight offered by the authors in the discussion of the experimental results  some more details needed with regards to the experimental setup	2
my major concern is about your experimental evaluation.	0
overall, this is a reasonable paper but experimental section needs much more attention.	0
from experimental viewpoint, they have lower performance than almost all competitors on clean data, but they are beating them when there is whitebox as well as the backbox threats.	0
another complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc. therefore it is unclear whether observed differences are meaningful.	0
review: the paper is well written, with a clear description of the properties a good benchmark should have, an analysis of the current solutions and their shortcomings and an extensive experimental evaluation of the cnn divergence metric.	0
the experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that relu variants do not perform much differently on smaller datasets, and only make a difference in very deep networks.	0
pros:  a simple, straightforward idea  a good topic  progress in modelbased rl is always welcome cons:  unclear how this is significantly different from other related work (such as imagination agents)  experimental setup is poorly executed.	2
pros: ' available source code ' good experimental results ' easy to read ' interesting idea of encoding how active the various possible operations are with special weights cons ' tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train darts models, which they then show to transfer to two other data sets, respectively ' shared with most nas papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures ' theoretically not very strong, the derivation of the bilevel optimization is interesting, but i believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested	2
i am not entirely convinced by the practical impact of the experimental section of the paper (though the experiments are beyond toylevel and i do not doubt the results), but i also believe that this is not the main contribution of the paper, which is rather laying the mathematical groundwork for future work.	0
 after reading the authors' rebuttals, they have addressed part of my concerns but i still think the current form is not below the acceptance threshold due to its weak experimental results and unclear technical details.	0
terms  that is they sum over the permutations of subsets of h of length k. in the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in deep sets), but this suggests the natural question: is it the fact that you’re explicitly modelling higherorder interactions that improves performance?	0
 overall the paper is well organized and easy to read  the proposed idea is smart: when starting from a synthetic domain there may be several hidden extra information that are generally neglected but that can instead support the learning task  the experimental results seem promising still, i have some concerns  if the main advantage of the proposed approach is in the introduction of the priviledged information, i would expect that disactivating the related pi loss we should get back to results analogous of those obtained by other competing methods.	0
weaknesses:  there is a possible flaw in the choice of experimental settings.	0
there is only one idea proposed by the authors based on the experimental results – fixing the deeper layers during the warmup phase, but the practical implications of this idea are not discussed.	0
results:  page 9, the “impact of attention mechanism” is discussed but no experimental result is shown to support these claims.	0
pros: 1. the idea makes sense and the experimental results show solid cons: 1. some questions around generalization are not clearly answered.	2
2. lacks extensive experimental validation.	0
my reason for my rating is mainly because of (1) lack of experimental validation.	0
some of the experimental results show promise, but the ppo ant result raises some questions.	0
the experimental results could always be stronger, but no longer have any holes in them.	0
in general, a good experimental paper with a somewhat incremental conceptual contribution.	0
my concern about the experimental results on missing data imputation is that strong competition such as gondra et al’17 and yoon et al’18 that report better results on uci than classical approaches are not included.	0
i like the idea of learning a latent structure dag for vaes, but this paper introduces a rather weak way to try to achieve this, and the experimental results are not convincing.	0
my main concern, apart from clarity, is that there is no experimental comparison with any other method.	0
however, experimental section could better highlight the benefits afforded by the model and scalability concerns need to be addressed.	0
"> defend against ""studying on cot and adversarial attacks.."" > could be better formulated references: there are some inconsistencies (e.g.: initials versus first name) pros ====  paper is clear and wellwritten  it seems to me that it is a new original idea  wide applicability  extensive convincing experimental results cons ====  no theoretical guarantee that the procedure should converge  the training time may be twice longer (to clarify)  the adversarial section, as it is, does not seem relevant for me"	2
while this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective.	0
however, this discussion comes very late and includes a design decisions (2/3) that i find poorly justified in text and completely unjustified experimentally.	0
nevertheless, i have some concerns about the claims of the paper and the experimental process.	0
i have concerns about both theoretical and experimental contributions 1. the proposed regularizer for relaxing quantized constraint looks similar to binaryrelax (yin et al. 2018 binaryrelax: a relaxation approach for training deep neural networks with quantized weights.	0
simple idea and relatively easy to implement cons: ' clarity could be improved, especially in the experimental section ' the motivation for the svhn 04 to mnist 59 is not clear.	0
however, this paper lacks in terms of experimental evaluation and has some technical flaws.	0
this is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm.	0
my main concerns come from experimental results.	0
the experimental validation is not extensive, but the proposed method is well motivated and as far as i can tell original.	0
there are however some issues related to the experimental evaluation that remains unsatisfactory.	0
concerning the experimental section:  the mixture of gaussians experiment is a good illustration of how the choice of cost functions influences the solution.	0
however, the experimental evidence presented in this paper is a bit lacking.	0
strengths  very clearly written  a nice overview of existing methods and correct positioning of the author's contributions in the context of these works  a good experimental setup involving multiple languages weaknesses  i am not sure how to interpret the results in table 2 and table 3 (see questions below).	2
cons 1. the authors do not compare their approach to that of https://arxiv.org/pdf/1711.00455.pdf , both in terms of conceptual novelty and in terms of experimental results.	2
quality: i am concerned about the quality of the experimental evaluation of the method.	0
pros and cons:  clearly written  clearly motivated  nice review of literature  quite incremental (close to pathsgd / weight normalization), and missing actual comparison with weight normalization, which seems to be the direct competitor of enorm (see detailed comments)  some flaws in the experimental setup (see detailed comments), particularly in the fullyconnected experiment.	0
conclusion: all in all, i find that this work is a bit too incremental, missing some important comparisons with other techniques and its experimental setup could definitely be improved.	0
the paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method.	0
clarity: 7/10 results are clearly presented, although more intuition and context would be helpful, and some sections are not well explained or contextualized originality: 2/10 the methods are not novel, to my knowledge, and there is little exploration or insight given for the extensive experimental results significance: 7/10 measuring and reporting results is valuable, and the reported results are interesting pros:  interesting results, good plots  overall well structured and explained cons:  plots could use more explanation and interpretation in the captions, and more investigation and insight from the experiments  some sections are not clearly worded and i think the objective/interpretation of the experiments would not be clear to someone even a little unfamiliar with the approaches cited specific comments/nits (in order reading through paper): 1. first paragraph of intro is kind of fluff/unnecessary.	2
 i think the theory is presented for a model with the twoclasses only but used for multiple classes in the experimental sections.	0
a single downstream task is very briefly mentioned in the experimental section, but it is only very vaguely described, it is unclear what experiments have been performed and there is no evaluation whatsoever.	0
however, since the novelty is not so much in the graph convolution method, or in the use of graph methods for treating spherical signals, but in the combined application of the particular graph method proposed to the domain of omnidirectional images, i would expect a more thorough experimental study of the merits of the method and architectural choices.	0
overall, i find that the paper introduces some interesting points but is too limited experimentally in its current form to allow for a fair evaluation of the merits of the method.	0
my two main points where i think the paper could improve are:  more experimental results, in particular, how strong are the negative effects of mirl if we have actions that are important, but have a lower probability in the stationary action distribution?	0
concerning the experimental part of the paper, sections 4 & 5 are wellexplained but, in section 6, the solution decoding method, inspired from pca is a bit confusing.	0
al. 18) to architectures other than lstms but still the experimental results on the sentiment task uses an lstm as the model.	0
my concerns are as below:  how is the accuracy computed for the experimental data?	0
i appreciate the authors for providing so much detailed experimental results to the community, but this paper lacks novelty in general.	0
although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, i think that the paper is in dire need of a thorough cleanup before being published.	2
the two main flaws in the paper are the lack of details and missing important experimental comparisons.	0
overall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like iclr.	0
"so the authors should choose between two options: ' either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than ""with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either"". '"	0
my concerns come from both theoretical and experimental aspects: the linearprogramming problem eq.(4)eq.(7) has been studied in existing literature.	0
pros: in the experimental result, the proposed durr outperforms dncnnb, a current stateoftheart.	2
significance: i didn't carefully check all experimental details but the experimental results look quite nice and promising.	0
the experimental evaluation seems valid but could be easily strengthened (see comments).	0
"because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to hartford et al. the tasks are explicitly designed to exploit these additional parameters  so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect hartford et al. to fail (because it's designed for a different setting)."	0
i think the proposed method (and the fact that it works in simple cases) warrants acceptance, but i think more experimental work would make this a great contribution.	0
the experiments results are clearly presented but some of the details of the experimental setup are not always justified.	0
 most the experimental improvements are incremental.	0
3. it lacks thorough experimental analysis.	0
 the paper claims that the model was run on a real robot, but there is no experimental evaluation of the results, only a reference to videos.	0
"i have increased my rating to ""6: marginally above acceptance threshold""  it could have been much better to at least give some hints to overcome the cf for the proposed setting, but i guess giving extensive experimental comparisons could be valuable for a publication."	0
my main issue is with the experimental setting that is somewhat lacking.	0
update post rebuttal  the experimental setting that is a little lacking.	0
cons: 1) experimental evaluation is very limited.	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
however, i still have a concern over the static nature of the experimental environments.	0
ultimately, however, this paper's significance is not evident to me, mainly because the proposed method lacks thorough experimental validation.	0
weaknesses: 1. some experimental details are missing.	0
"it is doubtful whether the importance of activation of neurons based on ""current data"" is sufficiently verified in sequential learning (in the experimental section, avg performance for importance weight sometimes appears to come with performance improvements but not always)."	0
this weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation  one of the mentioned motivations for unbiased estimates  being able to perform model selection on complementary labeled validation set  is not illustrated in the experiments questions:  i believe 1/(k1) normalisation factor in (5) is not needed  there seems to be a mistake in (9) (and its modifications later on)  i would expect either the subscript of the probability distribution in the last summand to be exchanged with in the loss, or a factor added  also, i think there are some mistakes in subscripts in (11)  what loss is the method from [ishida'17] optimising in the experiments?	0
 despite the suggestions of the theory, the accuracy drop can be quite large in practice, as in the cifar panel of figure 1 i think the iclr audience will appreciate the attempt to provide a principled approach to decreasing the size of neural networks, but i do not think this approach is widely compelling as : (1) no true guaranteed control on the tradeoff between accuracy loss and network size is available (2) empirically the method does not perform well consistently (3) comparisons with reasonable and informative baselines are missing updated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method.	0
 i did not replicate their experiments on gans, but the experimental numbers seem promising.	0
while the authors demonstrate some experimental improvement in the test cases they tried, the lack of provable guarantees for their approach limits the theoretical appeal of their paper.	0
my main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be.	0
(3) the experimental evaluation is against previous work which tried to solve a different problem (black box based attacks).	0
the paper is technically solid and relatively easy to follow and the results are good, but comparisons with previous work (descriptive and experimental) are rather weak.	0
strengths: ' the paper presents some strong experimental results.	2
the more concerning part is the experimental evaluation.	0
this includes: situating this work more clearly against existing similar works which use logic in this way, clearly defining the novel contributions of this work as compared to those and others, overall making the methodology more clear and specific (including experimental methodology), and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., hrl multitask learning, metalearning) to emphasize the need/importance of this work — i am aware that at least 1 hrl work is mentioned, but this work is not really contrasted against it to help situate it.	0
cons: task does not necessarily require causal knowledge (predict the node with the highest value in this restricted linear setting) very limited experimental setting (causal graphs with 5 nodes, one of which hidden, linear gaussian with / 1 coefficients, with interventions in training set always 5, and in test set always 5) and lukewarm results, that don’t seem enough for the strong claims.	0
cons: 1. the experimental evaluation is quite weak.	0
it is claimed that the paper compares against “two baselines, cpo and the lagrangian method, on several robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost.” then it is stated in the experimental section “however since backtracking linesearch in trpo can be computationally expensive, and it may lead to conservative policy updates, without loss of generality we adopt the original construction of cpo to create a ppo counterpart of cpo (which coincides with sppo) and use that as our baseline.” this seems directly to contrast to the earlier statement which states that it is unclear how to modify the cpo methodology to other rl algorithms.	0
this comparison should be both for shortterm tasks such as block pick and place (finn et al, pathak et al, sermanet et al.) and also for longterm tasks as shown in (duan et al. 2017 and also in neural task programming/neural task graph line of work from 2018)  compare highfidelity performance it is used as a differentiator of this method but without experimental evidence.	0
cons:  this cooperative game theory formulation is not really explored in details and has little practical consequences to the experimental setting.	0
the experimental section is unconvincing and lacking in details.	0
4) the experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.	0
finally, the experimental section poorly describes how all the pieces of the system affect the final predictions.	0
however, the experimental comparisons are limited and baselines are lacking.	0
 i found the experimental validation to be quite rich but not done in a systematic enough manner.	0
i have some concerns over the experimental metric and section 2.2.3, but even if that is clarified, it is not clear how impactful this paper would be.	0
 important experimental details lack adequate descriptions  tables and figures are not written with adequate details details of negative feedback: major:  unclear baselines and questionable improvement on sota:  previous work (the neural density functions of ostrovski et al or the cts scheme of bellemare et al.) used significantly fewer (~100 million and ~150 million respectively vs ~2 billion) frames of experience in solving montezumas revenge, which makes this method’s benefit somewhat incomparable to previous methods given the sampling regime it operates in.	0
 the fact that the forward dynamics does worse than vanilla ppo (and the previous results in ostrovski et al and bellemare et al) on montezuma's revenge brings the strength of the used baseline into question overall, the experimental details are greatly lacking:  the way that the value function is trained (i.e. the objective function) is never explained in the paper.	0
the proposal is clever, but there are some philosophical hurdles to overcome and the experimental results offer little quantitative evidence to support this idea.	0
the experimental part of the paper is well written, but the formulation part is difficult to follow.	0
however the experimental results are very promising, and the approach should be modular and slotable into existing deep rl methods.	0
3: they assume the generator is invertible, which enables the analytic evaluation of the q. but no supporting evidence or design architecture for the statement above is provided.	0
i am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence.	0
b) i have to admit that i am not extremely familiar with common experimental evaluations used for derivativefree methods but the datasets used in the paper seem to be rather small.	0
there is some novelty in the proposed approach, but it is mitigated by the relation to the work of chen et al., cvpr 2018. the experiments show good results, but a more thorough evaluation of the influence of the hyperparameters would be useful.	0
in addition to the lack of comparison to related multiobjective methods, the empirical evaluation is not very convincing since it is done on toy problems, with the most “realistic” one being a simple 2d task where the improvement brought by the proposed technique is not obvious (fig. 5).	0
evaluation: pros:  the idea of the proposed approach is interesting: using variational inference for binary weight neural networks.	2
overall evaluation: the proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand.	0
 the evaluation tasks are rich but not clearly stated.	0
cons: the experimental section needs to be extended and the results are limited to simulations on cifar100 and evaluation on lab experimental data.	0
however, the synthetic regression task is a nice proofofconcept, but thorough regression evaluation could perhaps include the boston housing prices dataset or some uci datasets.	0
i am currently in the borderline mode, but would be very happy to change my evaluation if the focus of the paper is somewhat changed and additional experiments on improving generalization (or some other experiments, but making the results a bit more useful/surprising) are added.	0
evaluation  section 3.1 claims that the metrics are strongly correlated, but that is not true for mnist or cifar, and is somewhat true for fashionmnist.	0
it is unclear to me why their evaluation setup is different, but some clarification about this would be helpful.	0
strengths:  interesting new objectives for representation learning based on increasing the js divergence between joint and product distributions  good set of ablation experiments looking at local vs global approach and layerdependence of classification accuracy  large set of experiments on image datasets with different evaluation metrics for comparing representations weaknesses:  no comparison to autoencoding approaches that explicitly maximize information in the latent variable, e.g. infovae, betavae with small beta, an autoencoeder with no regularization, invertible models like real nvp that throws out no information.	2
in the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.	0
2. multiple evaluation metrics and baseline models are considered weaknesses 1. the proposed method is simple and lacks novelty.	0
if yes, the protocol in machado et al. (2018) is for training time but numbers in table 1 in this paper are for evaluation time.	0
5. table 1 shows results achieved on mnist but not fashionmnist; was the evaluation performed on mnist only?	0
cvpr 2017 [b] wang et al., deep metric learning with angular loss, iccv 2017 after rebuttal: the authors still did not address my concern about testing on only one task with only one evaluation metric.	0
(2) they also perform human evaluation of personalitycaptions and found that humans tend to prefer these captions over the neutral captions pros:  the paper introduces a first largescale dataset for personality captions that has 215 traits and ~200,000 images.	2
this paper does include a somewhat similar adversarial evaluation (section 4.3) but adds extra information to nli examples.	0
i also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above).	0
i understand that the empirical evaluation presented do justify the methodology, but i am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented.	0
specifically, the evaluation question is: “which sentence has an opposite sentiment of the original sentence and at the same time preserves the content of it?” what happens in cases where a preserves the content but does not style transfer and b transfers style but does not preserve content.	0
my only concern is about the evaluation on metaqa, which seems a not widely used dataset in our community.	0
 lack of evaluation against eot adversary.	0
[first update] i find the authors' problem statement appealing, but share concerns with reviewer 1 about the privacy guarantees offered by the proposed method, and with reviewer 3 about need to clarify the experimental evaluation.	0
i do have concerns regarding the experimental evaluation:  the “demos” baseline approach should be explained in the main text!	0
to conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements.	0
# summary pros: ' useful dynamic batching trick that can lead to speedups ' empirical evaluation compares to two existing techniques and breaks down individual components of runtime cons: ' no critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage ' some questionable statements and assumptions ' lack of formalization and clear definitions ' paper reads longdrawnout, subpar writing hurts readability	2
the evaluation on wikisql is solid but doesn’t strongly support the claim about nondeterministic oracle.	0
the result on the subset of atis helps, but it is less solid than the evaluation on wikisql because the dataset is too small (only 126 examples in the test set) and there are no external baselines.	0
this may be true, but they only perform evaluations on tasks where the primary goal is accuracy.	0
 section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.	0
the evaluation shows that this approach not only makes sense but (significantly) outperforms, under same conditions, specialized program synthesis programs.	0
in a follow up response to anonymous public comments, some new tests using cifar10 data are presented, but to me, proper evaluation requires full experimental details/settings and another round of review.	0
pros:  originality of the approach cons:  experiments could have been more convincing:  should compare against at least one other stateoftheart domain adaptation method  results on dependency parsing (the most challenging task they consider) were mostly negative  evaluation on other more recent multidomain nlp tasks would have been nice (e.g. multinli)  abstract and intro could provide better description of the conceptual contribution, as well as motivation	2
in terms of evaluation, the toy distributions show that the method seems to converge to the right target but does not compare to either vanilla hmc, anicemc or l2hmc which all guarantee asymptotic convergence.	0
i understand that this evaluation is used as there is no standard way of measuring diversity of a subset of items, but it is also clear that „no“ baseline can be competitive.	0
cons (not necessarily a negative) requires a very careful reading as the paper provides a lot of information (though as mentioned it is very well written) the quantitative evaluation is somewhat lacking in that there are no quantitative psychophysical experiments to compare this model to competing ones across different observers.	2
my main concern about the paper is the results section: authors perform both largescale offline comparison (imagenet) and a small subset human evaluation.	0
given this additional information, i've lowered my score for the paper from an 8 to a 5. i do think that the approach is interesting, but have reservations about the experimental evaluation and the claims made by the current submission.	0
i have the following concerns/questions: 1) the authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to “generate a priority ordering of architectures for evaluation”.	0
however i have some questions about the evaluation and practical application of this scheme.	0
overall, the main shortcoming of the paper is lack of performance evaluation, comparison to other methods and clarifying advantages or novel results over other methods.	0
5. evaluation 5.1. the paper only evaluates on the novel classes, but it is unclear what happens if the model encounters known “classes”, i.e. has to answer with known answers from the training set.	0
conclusion: overall a great direction and interesting approach but requires more careful experimental setup and evaluation and discussion of related work for acceptance.	0
overall '' ' pros:  extensive, interesting evaluation  novel cpc|action algorithm ' cons:  no theoretical analysis/justification for claims  there are several subtleties that i am not sure are sufficiently discussed in the paper (see my questions above)	2
 the evaluation of the task seems to be challenging (inception score may not be appropriate) but since this is probably the first paper to generate layouts, i would not worry too much about the actual accuracy.	0
i think this topic is very interesting and important, given there is still an unfortunate lack of wellbehaved and widely accepted evaluation metrics in the field of unsupervised generative models.	0
overall, i found that the paper pursues interesting and promising ideas, but is currently not fully satisfying in terms of evaluation and discussion.	0
there are some nontrivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.	0
however, given the lack of meaningful evaluation i am not convinced that the proposed method substantially advances the field of fast style transfer to warrant publication at iclr.	0
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
weaknesses: 1. table 4 on evaluation results, while comprehensive, lacks some explanation.	0
table 2 and 5 clearly show that arae has the best performance on bleu and human evaluation scores, although it lacks generation diversity.	0
as such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. florensa et al. 2017 and aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.	0
[weaknesses] 1. the authors performed multiple experiments regarding various tasks defined in this paper.however, i can hardly find any quantitative evaluation for the results.	0
however there is no evaluation of this assumption.	0
it is good that the experimental section lists many appropriate baseline models and multiple evaluation metrics, but it is not clear how they are used.	0
a more pressing concern is the evaluation of prior work.	0
this is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension  varying the value of epsilon and observing the change in robustness.	0
the evaluation is also lacking in breadth, ignoring other similar defenses such as (cisse et al., 2017) and (gouk et al., 2018).	0
strengths:  the paper has a valuable goal  some of the evaluations are interesting.	2
questions: (1) i have some concerns in terms of human evaluation.	0
the weakness of the paper in my opinion is the statistical analysis of the results, the lack of in depth evaluation of the extrinsic reward prediction and the rather poor baseline comparison.	0
my main concern is the limited technical novelty and evaluation:  the main idea of the architecture is extending 2d convolutions in image compression networks to 3d convolutions, and use skip connections for multiscale modeling.	0
the technical contribution is somehow limited, but it is substantially validated by a very well organized and convincing experimental evaluation.	0
however, currently the paper is lacking in terms of quantitative evaluation.	0
the paper unfairly dismisses prior work by making factually incorrect claims, e.g. section 2 claims “indeed, papers like (hernandezlobato & adams, 2015; gal & ghahramani, 2016; lakshminarayanan et al., 2017; kendall & gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of rmse.	0
my main concern is related to the experimental evaluation of the method.	0
in addition to the limited technical novelty, i have a few other concerns as well, including some on the experimental evaluation:  realvalued node embeddings obtained from shallow/deep graph embedding methods can be used with 'overlapping' versions of kmeans.	0
in summary, i think the paper lacks both in terms of technical novelty as well as experimental evaluation and therefore doesn't seem to be ready.	0
"and not just a little worse, but requiring almost 10x the number of ""generations"", with a generation of the authors' method performing 50 evaluations and a generation of sgd performing 1 evaluation."	0
summary: this is a simple and intuitively appealing idea, but i find the evaluation to be quite lacking because the tasks already use a language specification (such that actrce seems to be vanilla her in application) and there are no comparisons to previous work.	0
i also have concerns about their evaluation.	0
3. lack of quantitative evaluation: the experimental evaluation shows qualitative performance.	0
in sum, the paper has some limitations in the empirical evaluation, but nonetheless the use of a gan promises significant gains in statistical power.	0
the results on synthetic data seems promising, but there is insufficient evaluation being performed on real and larger dataset where the mode collapse problems are more likely to happen.	0
there are a few visualizations, a few human evaluations, but the conclusion is a bit vague.	0
however, the games that are used for evaluation are very small, in fact i believe they have fewer states than the number of parameters in their network (the number of network parameters is not provided but i assume >1000).	0
you have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.	0
"# weaknesses the main weakness of this submission lies in its experimental evaluation, especially the absence of any dynamic objects in the tested environment (""static world carla"", section 1)."	0
" typos in section 3 (""trailanderror""), section 4 (""autonmous"", ""knowledge to"") # recommendation although the theoretical benefits of the method are wellmotivated and clear (offpolicy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple carla test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work."	0
for example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (i imagine the invariance would be somewhat bad but it's important to confirm).	0
one of my biggest concern is regarding the experiment and evaluation section.	0
evaluation appears to be a common concern to the work on saliency maps.	0
due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter .lambda=0.00011 on the cifar data set.	0
therefore, this result does not identify a failure of adversarial training as the authors seem to suggest but rather a failure of the original evaluation of madry et al. (2017).	0
however i find the current evaluation severely lacking.	0
overall, while i think the paper contains interesting ideas, i find the current evaluation lacking.	0
cons:  it is not clear why the evaluation seem to only be done for the transductive learning settings.	0
otherwise it feels like the problem setting was invented to justify the analysis in this paper: “the tail wagging the dog” as the saying goes… 2. model completion yet still be an interesting analytical tool for deep networks, but this requires a different evaluation.	0
the appendix gives more details, but it seems a bit out of place even then because the evaluations don't seem to use it. '	0
overall, the contribution is somewhat incremental and the evaluation/discussion should focus more on the sense selection module.	0
this idea is simple and straightforward, but the evaluation is not convincing.	0
"also, i did not find mention to methods predicting parameters in the metalearning community but also others like: denil et al. ""predicting network parameters in deep learning"" nips 2013 empirical validation the empirical evaluation does show an advantage of the proposed approach on some simple streams composed by up to three tasks."	0
" ""investigations, we "": don't capitalize  ""the averaging 'has' a smoothing effect""  ""our motivation are""  ""contributed it to""  ""available 'to the' adversary""  ""crafting adversarial perturbations""  ""directly evaluation""  ""be fixed 100"" pros:  transferability and robustness of adversarial examples is a very important problem  interesting insights, esp."	2
the construction and evaluation of examples that are more resilient to certain image transformations  experimental results are convincing cons:  contribution overall may be a bit limited  grammatical errors and odd formulations all over the place	0
i think this evaluation will be overoptimistic insofar as if 10:3010:45 and 11:0011:15 are in the train set, but 10:4511:00 is in the test set, it will be relatively easy to predict 10:4511:00. i would suggest considering train/test/validate splits based on larger chunks, e.g. leave the data in 15 minute blocks, but randomly select hours (4 blocks) to put in train/test/validate.	0
comparison of gan models have been difficult due to a lack of adequate evaluation technique.	0
 con: o the results so far are interesting, and in places promising, but not so clearly good that this idea doesn't need further evaluation of its usefulness.	0
 the stronger results on long dependencies in targeted syntactic evaluation look promising, but maybe you need a bigger hidden size so you can also do as well on short dependencies?	0
therefore, there is no quantitative evaluation in terms of nll of the more interesting cases where epsilon is between 0 and 1. the model lacks therefore one of the attractive properties of pixelcnns: the tractable likelihood computation.	0
the paper says that the method has been evaluated on cifar10, but the reported evaluation on cifar10 is much less thorough than the one on celeba.	0
[a] sequence level training with recurrent neural networks, iclr 2016 [b] adversarial feature matching for text generation, icml 2017 (2) evaluation: my main concern lies in the experimental evaluation, with detailed comments listed below.	0
the rl evaluations aren’t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard rl methods are working with worse variational approximation distributions.	0
overall thoughts: using datasetspecific features for evaluation metrics makes a lot of sense, but i don't feel totally satisfied by this paper's investigation of the specific proposal of a vae, and am particularly worried about whether the metric just ends up preferring models similar to that vae.	0
quality, novelty and significance: the paper is written well, but clarity about the evaluation procedures is lacking in the main manuscript.	0
the experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.	0
"therefore, i set my confidence level to ""2: the reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper""."	0
nevertheless i have some major concerns with the methodology, proposed evaluation metric and experiments.	0
results demonstrate that the proposed icl approach achieves the lowest values, in terms of fsm, but any interpretation can be done if it is not correlated with well established evaluation metrics.	0
what if the best suffix cannot be found using dynamic programming (when the evaluation metric is not editdistance, but a sentencelevel reward)?	0
cons:  my main concern with this paper regards the evaluation metrics used.	0
(i understand why the authors chose to do so but as an algorithmic paper, resorting to an evaluation based on visual appeal is almost always unsatisfactory.)	0
another concern is that the evaluation of domain adaptation does not have much varieties.	0
however, unlike as advertised, the paper does not address the domain shift issue in metalearning, and the experiments lack thorough evaluation as the paper considers itself as a metalearning paper and only compares to other metalearning approaches without much comparison to domain adaptation papers.	0
to make up for the clarity and correctness issues with the mathematical justification of the approach in the paper as currently written, the results would have to be especially impressive compared to strong baselines, but the experiments section does not give enough information to make that evaluation.	0
cons my main concerns with this paper are regarding the experimental evaluation  i do not feel these are sufficient to justify the strength of the attack method proposed.	2
i think the results are interesting, especially the ones compared with human evaluation (fig. 1), but it's might be better to explain on which aspect each of the feature contributes to the improvement.	0
how is this possible?, i guess is because the evaluation of each configuration is done in a small validation subset, this, however is not stated by the authors.	0
another major concern is the lack of experimental evaluation.	0
# paper strengths  the paper deals with a topic of interest for the autonomous driving community  the authors identify a flaw in the iou accuracy evaluation metric for lane detection # paper weakness  the paper could be written better.	2
pros:  multiple encoderdecoder stages could be beneficial for lane segmentation cons:  lacking evaluation and comparison to baseline methods  missing details on proposed network architecture, making it hard to reproduce  unclear what colors in figures for qualitative evaluation represent: are individual lanes also distinguished?	2
i find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking. '	0
the human evaluation is commendable but there are insufficient details about how you carried out the experiment.	0
4) lack of details regarding some important aspects in the paper a) “note the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedure” the authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which i think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.	0
i find the proposed idea simple and elegant but the evaluation lacking, and as such i’m a bit hesitant to outright recommend accepting the paper:  evaluation is not very extensive or detailed.	0
weakness: my main concern is the lack of motivation for embedding words on the hyperboloid and the choice of evaluation metrics.	0
the evaluation results in table 1 and table 2 show that the hyperbolic embeddings only performs better than the euclidean embeddings in low dimensions but worse on higher dimensions (>50), while higher dimension embeddings generally encode more semantics and thus are used in downstream tasks.	0
this, however, does not guarantee robustness (see [1] for why such “unfriendly” landscapes can usually be circumvented) some concrete evaluation concerns and experiments that the authors can run to alleviate them:  figure 5 shows adversarial robustness even against eta = 0.25at this value of epsilon, the attacker should be able to create realistic images of other classes (even without pgd), so this suggests that the loss is somehow making examples hard to find rather than removing them.	0
below are some questions concerning this evaluation.	0
strengths  strong results  testing on both baglevel / singlelevel relation extraction  insights via multiple ablations — variation of the number of layers, exploring densely connected data subsections with cycles to identify examples for multihop inference  evaluation on new, humanannotated test set issues  evaluation only on one task, and one dataset (although, with more detail).	2
my rating is thus based on the lack of novelty and poor quality of evaluation justifying the actual novel aspects of the paper.	0
the paper is lacking a comprehensive evaluation and comparison to latest work on graph neural networks.	0
however, the experimental part lacks a quantitative evaluation as well as a comparison to stateoftheart methods, which makes the assessment of the proposed model difficult.	0
the first matter that concerns me in the evaluation of this work is that the networks under study, in addition to being nonstandard, are much shallower than the common neural networks that are used for evaluation of outofdistribution samples.	0
while i have some questions about the evaluation and minor concerns about the presentation i would overall recommend acceptance of the submission.	0
weak points: ' the evaluation is limited to only mnist datasets. '	0
i have various other concerns about the claims, the approach, and the evaluation.	0
i have several questions and comments about this paper:  one difference of the evaluation setup between this paper and yan et al. is that in yan et al., they are trying to infer more detailed hyperparameters of the architecture (e.g., the number of neurons, the dimensions of each layer, the connections), but within a family of architectures (i.e., vgg or resnet).	0
finally, while the human evaluation results are interesting, the lack of standard deviations make it hard to measure the significance of the results, especially when only 100 sentences from each model is rated.	0
in summary, my evaluation is as follow: 'pros'  pretty original problem formulation  generally well written paper 'cons'  lack of comparison with simple baselines in basic dataset distillation setting  use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated  possibly a mistake in the theoretical analysis of the linear case indeed, i found the paper to be generally quite clear and enjoyed reading it.	0
"the evaluation further depends on a ""selective classifier"" which is not detailed, but critical to understanding the experiments."	0
the empirical evaluation is not unreasonable, but also not strongly convincing.	0
pros  considering correlations between features from different encoder layers is a good idea  the improved encoder / decoder architecture is significantly more efficient than the cascaded approach of [li et al, nips 2017] cons  somewhat incremental  limited experimental evaluation  qualitative results not clearly better than existing methods  missing citation for multiscale losses limited experimental evaluation one of the key claims of the paper is that “our method with interscale (fig.7(f)) or intrascale feature transform (fig.7(g)) are more similar to the target style than those of singlescale style transfer without considering interchannel correlation” (figure 7 caption); this claim is substantiated primarily by qualitative results in figures 4, 7, and 8. personally i don’t find the results with interfeature correlations to be much better than those with only intrafeature correlations or the results from prior work.	2
pros  has quantitative evaluation table 1 includes quantitative results for different approaches, which is essential for proper evaluation.	0
while the latter is focused on single word settings and is thus solving an easier problem, what would happen if the alzantot et al. method was applied to each while the idea is interesting but incremental, the evaluation of the approach is weak.	0
my main concern is about the experimental evaluation:  the authors should test their approach on carlini & wagner's attack which allows for explicit control of logit differences and thus could entirely defeat the background check.	0
 some qualitative evaluations are also presented, but it's unclear what insight one is expected to gain from looking at them.	0
evaluation  the writing of the paper is in general ok, but reading the introduction that categorizes the ssl by three streams seem somehow unnatural to me.	0
 my biggest concern is in the empirical evaluation.	0
overall feedback: this is a wellwritten paper, but i think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.	0
human evaluation has been done before, the video colorization evaluation may be somewhat new, but i do not think it will generalize to tasks other than colorization.	0
i believe that this is an interesting research direction, however paper in its current form seems as a small incremental improvement over sota, and could be significantly improved by for example:  providing more comprehensive evaluation (including estimating accuracy to lower std errors)  adding other baseline solutions (such as sobolev training, cdni, or deep supervision)  considering any form of convergence/dynamics analysis of the proposed approach	0
although i like the idea and it seems a very interesting direction for generalisation to new goals, i do think the execution, the particular instantiation and (lack of) indepth evaluation with (at least some of the) existing methods in literature  including uvfas [1] and the different ways sfs have been used for generalisation [2,3,4]  is unfortunately letting it down.	0
this is not to say that evaluation by proxy should be banned, but rather that lowresource setups should be more extensively controlled for.	0
in addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf  the authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.	0
the evaluation does not address the concerns of false alarms on models trained to be robust to adversarial examples.	0
there are some interesting evaluations but i am not sure these are as rigourous as they could be, in particular (but not limited to) the survival game.	0
also, they should state what parameters they use for each of the environmental factors, e..g minimum distance etc. the survival game is poorly described, as are choices for the evaluation of it.	0
in particular, the proposed evaluation seems valid when applied to different crosslingual mapping methods over the exact same embeddings, but its validity beyond that is not obvious nor tested empirically in the experiments.	0
pros: 1. this paper investigates an important problem, aka, how does the methods compare to each other with the same evaluation protocol.	2
(w5) eval setting : my last concern was with the overall evaluation setup.	0
general evaluation ( pro/  con, more specific comments/questions below):  the paper is very wellwritten  the bbp presentation is light but very accessible.	0
strengths  ' seems original: i'm unaware of any other method connecting rule lists and prototypes and nns ' neat applications to healthcare limitations  ' interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rulelists shown is an applestoapples comparison ' prototypes themselves never evaluated ' many design choices inside method not justified with experiments  why highway networks  rcnns?	2
weaknesses: based on empirical evaluation, the paper cannot make any claim about the generality of the obtained results.	0
overall, i would say that this is a potentially strong paper, but that experimental evaluation does need work.	0
 pros  the proposed method shows better performance than srgan in terms of psnr, ssim, and human evaluation.	0
strengths:  clearly written and well presented  learning to generate clarification questions is an important topic  interesting combination of seqgans, mixer and selfcritical baseline for policy gradient updates  a range of good baselines for this novel task setup weaknesses:  minor: automatic evaluations are kind of useless here and the datasets are rather artificial for this task  major: generating clarification questions cannot be the end goal in and of itself (see above explanation) other comments:  section pretraining, paragraph question generator: i do not understand the reference to answer generator in this paragraph.	2
 the technical approach (combining vae vectors to make new shapes) is not particularly novel[ overall: the paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.	0
my only major comments are that i’m a bit skeptical about the lack of a more thorough (theoretical) analysis supporting their empirical findings (what gives me food for thought is that lstm helps that much on even fully observable games such as ms. pacman); and the usual caveats regarding evaluation: evaluation conditions aren't well standardized so the different systems (apex, impala, reactor, rainbow, ac3 gorilla, c51, etc.) aren't all comparable.	0
these sort of papers would benefit from a more formal/comprehensive evaluation by means of an explicit enumeration of all the dimensions relevant for their analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. however only some of then are (partially) provided.	0
cons:  the experiments evaluation is restricted to simplistic environments.	0
although the proposed method achieves the good performance on arc, i have concerns about both of the model design and evaluations.	0
the experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.	0
 the evaluation methodology is a good idea but is quiet trivial.	0
i think the method is reasonable and results are promising, but i'd like to see more focused evaluation on the semantics captured by the proposed model (compared to the models without gnn).	0
overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues).	0
to my understanding, the understanding is only qualitative by observing a few examples, and the decoupling does not refer to the method, but the evaluation.	0
these tasks could be faulty for other reasons, but just because we have no better technique than random encoders currently, doesn’t make these evaluation tasks not worthwhile.	0
 critically, the evaluation with human experts lacks the necessary rigor.	0
cons:  hard to tell whether this approach works since the metrics for evaluation are not specified and there are no quantitative results in the experimental section (only 1 qualitative example per task)  the work is not reproducible due to the lack of details (see more explanations below)  the theoretical analysis is a standalone piece of the paper, without any discussion about the implications, or making connections to the previous sections.	0
indeed, it is difficult in general to quantify the results of generative models, but most other gan papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset.	0
i do not aim to question the experimental settings, which seem reasonable, but this raises some flags to me: the proposed evaluation is not completely realistic, as (some) training corpora are indirectly aligned, and the system could be somehow exploiting that.	0
pros: 1. releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly; 2. the authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores; 3. the authors implemented an extended list of representation learning algorithms and compared them on the dataset; cons: 1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework.	2
4. there is one evaluation in figure 2, which suggests that the adnet model generated sentences have higher transfer strength but this evaluation is not conclusive in the sense that you have too much drop in content.	0
"it's hard to tell from the text, but it appears that this is an ""rlstyle"" evaluation setting, where we only care about rapid convergence rather than generalization."	0
6. the rating classifier (clf) is intriguing, but it's not clearly explained and its effect on the evaluation of the performance is not clear: one of the key metrics used in the evaluation relies on the output rating of a classifier, clf, that predicts reader ratings on reviews (eg on yelp).	0
however, such human evaluation is lacked in this paper.	0
section 3 refers to a 90/10 training/evaluation split but then it is unclear in what experiments that exact split is used.	0
the idea introduced in this paper is interesting but the paper is poorly written and organized which makes its through evaluation difficult.	0
i have concerns with respect to the evaluation, the relation of the paper compared to the stateoftheart in automatic program repair (apr), and the problem definition with respect to livevariable analysis.	0
overall interesting but i would recommend evaluating in standard similarity learning for nlp and other tasks (perhaps more than one) there are specific similarity evaluation sets for word embeddings.	0
this would not be a definitive test, but it would be more convincing than the current evaluation.	0
cons:  invalid evaluation to report only on the validation set, not test set.	0
the edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better.	0
 edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with codeslam), which address my concerns.	0
the only major concern at this point, is the experimental evaluation on small fullyconnected networks.	0
my main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.	0
strengths:  first (i think) work on generic whitebox attacks against object detectors  interesting (and aesthetically pleasing) textures resulting from the attacks weaknesses:  evaluation was limited to one dataset and two object detectors.	2
these should be optimized on an evaluation set, but the authors only mentioned that they split the dataset into training and holdout (test) set (section 4.1).	0
cons:  while the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clipart scene generation or tangram graphic design generation.	0
evaluation/clarity/originality/significance: 3.5/4/3/4 remaining concerns:  the poor performance of the baselines may indeed be due to lack of hindsight, but this should really be debugged and addressed by the final version of the paper.	0
the paper is well written but the evaluation and technical novelty is weak.	0
however, i have some concerns about presentation and a number of specific questions about model implementation and evaluation.	0
 not critical, but it would be nice to know if the contributions here stack with the pragmatic inference procedure in fried et al.  while, as pointed out on openreview, it is not required to include spl evaluations, i think it would be informative to do sothe preliminary results with no beam search look good!	0
# comments and questions: ## weaknesses in the experimental evaluation: i find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradientbased policy search methods).	0
overall, i find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.	0
my main concerns are about the evaluation and comparison of standard neural models.	0
the human evaluation is neat but is inconclusive–in a glaring act of omission, the authors do not link to samples generated by their model, while they include samples generated by the competition.	0
minor nits: i appreciate the human evaluation experiments on mturk but they are very difficult to understand with the figure 5. please label the yaxis.	0
the next major concern is regarding the evaluation of the quality of embeddings.	0
in contrast, the quantitative evaluation is more concerned with if the embeddings being consistent with the given hierarchy.	0
minor concerns:  “such as crfs or iterative evaluation” i would include a citation on this type of work.	0
however, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) metalearning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works.	0
even though doing all these evaluations may be a tad too much, but definitely, quite a few of those could have been done to make the approach look convincing and enticing.	0
"1) about the new criterion: 'pros: ' a) as clearly pointed out by the authors, using standard non temporal clustering comparison metrics for temporal clustering evaluation is in a way ""broken by design"" as standard metrics disregard the very specificity of the problem."	2
b) i find the experimental evaluation acceptable but a bit poor.	0
i can accept that the model will be rather insensitive to hyperparameters alpha and beta, but i've serious doubt about the number of clusters, especially as the evaluation is done here in the best possible setting.	0
 the authors propose an alternative approach to training seq2seq models, which addresses concerns about exposure bias and about the typical mle objective being different from the final evaluation metric.	0
it's not alone in this category (e.g., paper compare to theoretically optimal baselines if they can), but it is interesting to see another example of this kind of evaluation.	0
i think the contribution is significant because of the kind of evaluation, but i'm not sure it will ultimately have a large impact.	0
strengths:  motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step  cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation  maze navigation shows incremental benefits over nonmodulated plasticity  thorough experimentation  clippingtrick is a neat observation weaknesses:  evaluation: only on toy tasks (which includes ptb), no real world tasks  very incremental improvements on ptb over a very simple baseline (far from sota)  evaluated models (feedforward nns and lstms) are very basic and far from current sota architectures  no qualitative analysis on how modulation is actually use by the systems.	2
this point is not really addressed in the experimental evaluation as benchmarking is performed against a robust and an adaptive policy but not explicitly against the (arguably) most closely related work in yu et al. it could be argued, of course, that yu et al. essentially use adaptive policy generation but they do explicitly learn dynamics parameters based on recent history of actions and observations.	0
pros: ———  interesting work  accessible  effective  thorough evaluation (though potentially missing a key benchmark) cons: ———  potentially missing a key benchmark (and therefore seems somewhat incremental)  only limited insight offered by the authors in the discussion of the experimental results  some more details needed with regards to the experimental setup	2
my major concern is about your experimental evaluation.	0
review: the paper is well written, with a clear description of the properties a good benchmark should have, an analysis of the current solutions and their shortcomings and an extensive experimental evaluation of the cnn divergence metric.	0
 while i commend the authors for performing both simulation and realworld experiments, i find the that experiments lack a principled evaluation.	0
from my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.	0
just a concern or something that i quite did not understand about one of the arguments the authors use to justify the evaluation carried out: the authors claim that they want to avoid the subjectivity introduced by humans (citing gonzalezgarcia et al. 2017), and prefer to avoid user studies, presenting a more objective approach in their evaluation.	0
i do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper).	0
i concerns with one aspect of the evaluation.	0
cons:  my biggest issue is that there is no clear evaluation of the runtime benefit of the second viterbi decompressor.	0
motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).	0
on the downside, this paper does not present any baseline evaluation, party due to the fact that the proposed problem is new.	0
strong points:  novel, multiagent in nature, approach to oneclass classification  proposed method build a complex system, which can be used in much wider class of problems than just classification (due to joint optimisation of classifier and comparator)  extensive evaluation on 4 problems  nice ablation study showing that most of the benefits come from pure c/gc game (on average 68.8% acc vs 65.2% of just c, and 69.8% of entire system) but that h/gh players do indeed still improve (an extra 1%).	2
" this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
in the evaluation, the authors present in figure 4 some qualitative examples but i would have expected to see some quantitative evaluation of this.	0
however, this paper lacks in terms of experimental evaluation and has some technical flaws.	0
there are however some issues related to the experimental evaluation that remains unsatisfactory.	0
there are certainly interesting links between such models and kwik [1] algorithms (which are also supposed to be able to respond 'null' to queries), however they are not mentioned in this paper, which focuses mainly on evaluation methodologies.	0
there are quite a few details that are also unclear such as what the authors mean by 'clean example' etc. however the authors do not explain their attack very well, their definition of the performance metric is not sufficiently formal, and their evaluation methodology is weak.	0
in summary, the paper presents an interesting idea but the execution needs some improvement (especially, in terms of evaluation) before the paper is ready to be accepted to the conference.	0
i found some of the results and analysis interesting, but overall felt that the work can be made much stronger by more quantitative evaluations.	0
concerns:  the evaluation is not thorough enough: only two attacks are considered (fgsm and pgd, with the former being strictly weaker than the latter)  defensegan is similar in defense mechanism but the authors do not attempt to use the attacks of athalye et al 2018 (icml 2018) in their evaluation.	0
rationale for my evaluation: the method is somewhat incremental, however, this increment could be quite practically important.	0
evaluation: the paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing.	0
quality: i am concerned about the quality of the experimental evaluation of the method.	0
significance: given the lack of a rigorous evaluation framework and the lack of novelty of the proposed methods, i believe the significance of the contribution is very low.	0
strengths  thorough analysis with a good set of questions weaknesses  some peculiar evaluation and presentation decisions  introduces 'yet another' synthetic visual reasoning dataset rather than reusing existing ones i think this paper would have been stronger if it investigated a slightly broader notion of generalization and had some additional modeling comparisons.	2
another, more general comment concerns the actual evaluation task: as prior work, it seems that the authors optimise and evaluate their embeddings solely on the (intrinsic) word translation task, but if the main goal of this research is to boost downstream tasks in lowresource languages, i would expect additional evaluation tasks beyond word translation to make the paper more complete and convincing.	0
a single downstream task is very briefly mentioned in the experimental section, but it is only very vaguely described, it is unclear what experiments have been performed and there is no evaluation whatsoever.	0
overall, i find that the paper introduces some interesting points but is too limited experimentally in its current form to allow for a fair evaluation of the merits of the method.	0
the paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.	0
i guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.	0
the model and architecture is pretty convincing but the paper lacks more indepth analysis, comparison and evaluation of the model.	0
my main concern is that there seems to be a larger pool of work in semantic navigation than what the evaluation includes.	0
it is well written in most of cases and easy to follow (however i got the impression that the paper was rushed in the last minute; there are some trivial typos and very low resolution images etc.) however, i have a huge concern about the empirical evaluations.	0
con: due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits.	0
although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, i think that the paper is in dire need of a thorough cleanup before being published.	2
i am overall positive about the paper: the proposed idea is simple, but is wellexplained and backed by rigorous evaluation.	0
overall, i find the paper somewhat lacking in terms of evaluation.	0
however, the paper lacks a proper evaluation of the proposed method, and i don't think this paper is ready with the current set of experiments.	0
the experimental evaluation seems valid but could be easily strengthened (see comments).	0
the first part of the paper is easy to read (section 13), but section 4 is hard to follow, for the following reasons: ' section 4.1 presents the metrics used in the evaluation, which is nice.	0
as far as evaluation between distributed schemes (e.g., amb, fmb etc) is concerned, shouldn't one define the regret with respect to s itself?	0
comments: i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the numerical evaluation.	0
however, i am concerned with two issues here, which are related to the motivation and performance evaluation, respectively.	0
unfortunatley this leads to a number of weaknesses:  the implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.	0
the latter appears to be of less significance in this context, but i found robust offline policy evaluation underrepresented in the related work.	0
however, the methodological contributions are limited and i have some important concerns about their evaluation.	0
 the paper claims that the model was run on a real robot, but there is no experimental evaluation of the results, only a reference to videos.	0
it seems the authors are trying to do a bit of everything, but then the evaluation is insufficient.	0
unfortunately, the empirical evaluation in this work is lacking to formally confirm or reject my hypothesis.	0
however, human evaluation is lacked, which i think is essential for this line of work.	0
the authors also seem to use the proposal not to improve gan learning but rather as a tool for evaluation, in order to see whether the lack of diversity in gans comes either from failure of properly evaluating the wasserstein distance or from insufficient optimization in learning.	0
it provides evaluation of the vanilla group equivariant networks in a similar configuration, but due to design choices in the training and test set, it is not possible to compare it against other algorithms and other steerable bases such as those from [2].	0
generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation.	0
cons: 1) experimental evaluation is very limited.	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
the improvements gained with wasserstein and gradient penalties are interesting but results on more datasets and longer samples are needed, especially section 4.2 would benefit from quantitative and human evaluation.	0
"the main criticism i have would be that the human evaluation is rather simple (rating 15), i would have expected more finegrained categories, especially ones that relate to how much knowledge the system uses (i appreciate the ""wiki f1"" metric, but that is an automatic metric)."	0
interpretability is highlighted in the abstract and introduction as an important feature of the proposed approach but the evaluation of interpretability is limited to a few anecdotes from the authors’ review of the results.	0
 '' evaluation: the paper dismisses existing generative methods early in the evaluation phase but the justification for doing so is not entirely clear to me: firstly, if the inception score is used as an objective criterion it would seem reasonable to include the values in the paper.	0
cons: 1. the current evaluation is contrived.	0
the goal is to predict the future state of an agent in a multiagent setting, but it is not clear from the evaluation as how the presence of multiple agents influence the behavior of an individual.	0
 'pros:'  mostly clear and wellwritten paper  technical contribution (learning many diverse skills) is principled and welljustified (although the basic idea builds on a large body of prior work from related fields as also evidenced from the reference list)  extensive emperical evaluation on multiple tasks and different scenarios (mainly toy examples) showing promising results.	2
'cons:'  the main paper assumes detailed knowledge of the actor critic setup to fully follow and appreciate the paper (a few details provided in the appendix)  p(z): it is not entirely clear to me how the dimensionality of z should be chosen in a principled manner aside from brute force evaluation (as in 4.2.2; which does not go beyond a few hundreds).	0
i interpret the proposal not so much as a claim to a stateoftheart algorithm (even though the results are impressive) but as a very reasonable baseline in the evaluation of attack efficiency  one might even wonder why it has not been common practice thus far to evaluate against such kinds of algorithms by default.	0
this weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation  one of the mentioned motivations for unbiased estimates  being able to perform model selection on complementary labeled validation set  is not illustrated in the experiments questions:  i believe 1/(k1) normalisation factor in (5) is not needed  there seems to be a mistake in (9) (and its modifications later on)  i would expect either the subscript of the probability distribution in the last summand to be exchanged with in the loss, or a factor added  also, i think there are some mistakes in subscripts in (11)  what loss is the method from [ishida'17] optimising in the experiments?	0
the evaluation focuses on simulated robots, however gathering nonexpert data on real robots would be very expensive, so the approach would not make a lot of sense here (even if we replace trpo a more sample efficient rl method).	0
on the positive side, the paper is wellwritten and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets.	2
(3) the experimental evaluation is against previous work which tried to solve a different problem (black box based attacks).	0
overall, it has potential, but the proposed method would probably require clearer evaluation.	0
pros: the paper considers a wide variety of pooling strategies and deformation techniques for evaluation.	2
pros: the paper considers a wide variety of pooling strategies and deformation techniques for evaluation.	2
its main weakness is the evaluation (particularly the lack of human evaluation.)	0
detailed comments / questions  in the paragraph between eqns 2 and 3, the authors seem to suggest that teacher forcing is an added heuristic  however this is just the correct evaluation of the mle objective.	0
in conclusion, i suggest a reject of this paper due to the lacks of comprehensive study and evaluation.	0
the empirical evaluation lacks any comparison to baselines and serves for little more than as a sanity check of the developed theory.	0
the more concerning part is the experimental evaluation.	0
the fact that the authors find the approach of kannan et al. (2018) to offer an increase over the robustness of madry et al. (2017) thus raises concerns about the reliability of the evaluation.	0
my concerns focus on the remaining evaluations:  figure 5 (right) is difficult to understand.	0
 overall, the evaluation is heavy on qualitative results (many of them on simple gridworld tasks) and light on quantitative results (the only quantitative result being on a simple gridworld which is poorly explained).	0
both of which sound promising, but i felt the actual evaluation fails to show or even assess either of these rigorously.	0
the quantitative, and in some cases qualitative, evaluation lacks considerably.	0
this is a design choice that might work well for gridworld/navigation type of domains but there are mdp where the evaluation under this particular policy can yield uninformative evaluations.	0
therefore my main concern is that the performance evaluation is not suitable to achieve meaningful results.	0
cons: 1. the experimental evaluation is quite weak.	0
this seems is similar to work by achiam 2017. while this work seems like an interesting framework for encompassing several classes of constrained policy optimization settings in the lyapunovbased setting, i have some concerns about the evaluation methodology.	0
summary:  very minor contribution, a manuscript that is lacking important details and does not relate it's technical section to existing work, with very thin evaluation.	0
the qualitative evaluation shows that part prediction is plausible but the results for future frame prediction are somewhat unclear as there are no baseline comparisons for this aspect of the task.	0
i appreciate the extensive evaluation, but its organization can also be improved, considering that some important information are, again, in the appendix.	0
the surrogate loss to learn the features coupled with a linear policy evaluation algorithm appear to be novel, but does not warrant, in my opinion, the novelty necessary for publication at iclr.	0
the technical descriptions are difficult to follow in places, it makes some incorrect statements about speech and speech synthesis and its evaluation is lacking in a number of ways.	0
the final evaluation is on a difficult continuous control task, in which gas solve the task, but have much poorer sample complexity than es.	0
it is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of blackbox models.	0
however, it lacks novelty and is limited to tabular data as presented.	0
"the approach only seems help in the ""rare label, small data"" regime, which limits the applicability of the method but is still worthy of consideration."	0
i have the following two major concerns: (1) using deep neural network as a prior in signal denoising is definitely an important and also challenging problem, only when the neural network is learnt from data.	0
pros:  nice demonstration of improved data efficiency over existing modelbased methods.	2
7.1. maybe beyond the scope of this work, but it would be interesting to understand how much training data different models need to obtain this capability.	0
a comparison of methods on an important dataset is no doubt useful, but i don't think iclr is the right venue.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
" review of ""data interpretation and reasoning over scientific plots"" paper summary: the paper is concerned with algorithms that can interpret data shown in scientific plots."	0
pros: 1. the authors investigate a method which can handle data with temporal and spatial dependencies which is an important problem.	2
the paper introduces lid as the metric to be used within the introduction, but for readers unfamiliar with it, the series of snippets “model of distance distributions” and “assesses the number of latent variables” and “discriminability of a distance measure in the vicinity of x” are abstract and lack concrete connections/motivations for the problem (sample based comparison of two highdimensional data distributions) the paper is addressing.	0
 the sota result miniimagenet is the result of a bagoftricks approach that is not well motivated by the main methodology of the paper in section 2. major points:  the motivation for and derivation of the approach in section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data s. moreover, the approach is not fully bayesian as a point estimate of the hyperparameter .alpha is computed; instead, the approach is more similar to empirical bayes.	0
 this paper proposes the use of unamortized black box variational inference for data imputation (given a fixed vae with a factorized decoder), where the choice of variational distribution is a standard flow model.	0
i would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant.	0
the paper talks about oneshot metalearning over a stream of data, but doesn't make it clear how those functions are learned.	0
pros and cons () interesting idea () diverse experimental results on six datasets including benchmark and realworld datasets () lack of related work on recent catastrophic forgetting () limited comparing results () limited analysis of feature regularizers detailed comments  i am curious how we can assure that svm's decision boundary is similar or same to nn's boundary  supportnet is a method to use some of the previous data.	2
strengths:  i think this kind of method could be useful for data of very high dimensionality, when it is not possible to train everything end to end.	2
for example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here.	0
cons: the experimental section needs to be extended and the results are limited to simulations on cifar100 and evaluation on lab experimental data.	0
my overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the sasnet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.	0
"2. section 4.2: ""...experiment, but both mahalanobis distance and lof fail to detect both cars and horses using 50 % r2"" the comparison and the observation with r2 50% does not convey anything beyond obvious since it is already known that a factor as low as 50% looses almost half the information (variance in data)."	0
forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.	0
"my main concern is, however, the combination of these two assumptions:  labeling function is fixed  the distribution of data is of a certain form (i.e., theorem 4.1 reads like: for every parameter p and p there ""exists"" a distribution such that ...) isn't this too restrictive?"	0
however, the synthetic regression task is a nice proofofconcept, but thorough regression evaluation could perhaps include the boston housing prices dataset or some uci datasets.	0
i don't know exactly how the data is processed, but on mnist, an accuracy of less than 90% means it is quite bad (i can get >90% with pca  logistic regression).	0
pointing out that unsupervised disentangling is hard despite recent breakthroughs, and that supervised disentangling needs a large number of carefully labeled data, they propose a “weakly supervised” approach that does not require explicit factor labels, but instead divides the training data in to two subsets.	0
cons: the problem that this work solves seems somewhat artificial, and the training data, while less burdensome than having explicit labels, is still difficult to obtain in practice.	0
unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means.	0
it is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected).	0
"approaches such at least like ""modeling the intensity function of point process via recurrent neural networks"" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data."	0
the study is also interesting although the lack of publicly available datasets limits the extent of it and the strength of the results.	0
perhaps i don't know this data well enough but it seems like using the temporal aspects of the observations to define a slate makes the resulting data closer to a subset selection problem than an ordered list.	0
"pros:  a simple idea  encouraging experimental results cons:  confusing read  no clear intuition is given  restricted to lowdimensional datasets  strong baselines needed  the plots are too small to see (impossible to see when printed) other comments:  the authors are using the term ""feature vector"" to refer to a data point."	2
concerns or suggestions: (1) the training data is just one data point, it is not a sequence of data.	0
“autoencoder not only decreases the dimensionality of input data but also finds optimal abstraction of input data for better discrimination between subjects.” optimal abstraction of input data for better discrimination between subjects?	0
"note: the authors cite a paper in the introduction ""hierarchical attention transfer network for crossdomain sentiment classification"" (li et al 2018) which also achieves state of the art results on the amazon reviews dataset, but do not compare against it."	0
"first of all, node splitting by general binary splitters are called ""multivariate trees"", but interestingly this does not always bring the good prediction performance on current quite highdimensional data."	0
smolyanskiy improves on giusti’s work by (1) gathering additional trail image data using three cameras mounted to face forward but with lateral offsets and (2) using this additional data to train a 6 output neural network (“trailnet”) which predicts both view orientation and lateral offset.	0
my main considerations are: 1. uncertainty about generalizability 2. uncertainty about usefulness to practitioners or theorists (admittedly, this is hard to predict, but no clear usecase is available at this point) 3. a lot of data, but no clear central finding of the paper	0
in the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.	0
[pros & cons] () this paper tries to extend dual learning from word level to hidden state level; () multiple languages are involved in this framework; () experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used wmt datasets; () the paper is not easy to follow.	0
pros:  the method is very simple  the empirical results, particularly on the synthetic noisy training data, seems to be encouraging.	2
the result in table 5 shows that odd can outperform erm for real world datasets, but the improvement seems to be marginal.	0
 privacy concerns arise when data is shared with third parties, a common occurrence.	0
cons experiments are only limited to small scaletraditional graph datasets.	2
however, while the key of this paper is to determine a good clipping factor, the authors use uniform density function to represent the middle part of both gaussian and laplacian distributions where the majority of data points lie in, but exact computation for the tails of the distributions at both ends.	0
here are some specific concerns: 1) i could not find a link to an opensourced version of the dataset(s).	0
this leads to a network with >50 fc layers at time step 25 – not a reasonable and scalable network for life ling learning page 6:  the results show that the feature transformer method achieve accuracy close to cumulative retraining, but this is not too surprising, since feature transformer indeed does cumulative retraining: at each time step, it retrains the classifier (a 2 stage mlp) using all the data at all times steps (i.e. cumulative retraining).	0
i tried hard to discover a 'detailed accuracy number on a benchmark dataset with unchanged setting' but found only case 4. by ‘unchanged’ i mean it is not a subpart of the whole dataset, or using a rarely seen nn architecture.	0
experiments: the experiments are presented in a subset of 5 classes from cifar10 (also used by weinshall et al.), but also in the full cifar10 and cifar100 datasets.	0
i have several concerns here: 3.1) if the particles are parametric, the solution may greatly depend on the choice of x. as the empirical distribution has a finite support, it would be dominated by other points unless the data points are reweighted.	0
cons:  results are proven for particular settings rather than relying on realistic data distribution assumptions.	0
(v) while i found the experiment with eigensimilarity a nice contribution, there is a lot of alternatives: seeing whether there is a linear transformation from one language to another (using procrustes, for example), seeing whether the sentence graphs can be aligned using gans based only on jsd divergence, looking at the geometry of these representations, etc. did you think about doing the same analysis on the representations learned without the translation task, but using target language training data for the tasks instead?	0
the proposed method seems promising for volumetric data as the authors note, but this also needs to be demonstrated experimentally.	0
suppose the explicit likelihood estimator is a single gaussian, but the real distribution has multiple modes, fitting such the generated data and the training data on this likelihood will not help to avoid missing modes.	0
in fact, i wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar).	0
i really like the use of synthetic data to show superior expressive power, but i am unsure whether this can be contributed to deepwalk or the use of the proposed pooling operator (or both).	0
however the bounding of the condition number of the data matrix is interesting and guarantees are nearoptimal.	0
pros 1. positive signals have been shown on multiple datasets.	0
informally, it is expected from the transformed output of data record, one should be able to learn about a desired hidden variable, but should not be able to learn anything about a sensitive hidden variable.	0
this experiment would significantly strengthen the submission, but would still leave open the possibility that a clever adversary could extract more sensitive information than expected from the perturbed data.	0
"i found the following excerpt from the introduction especially compelling: ""it is important to design collaborative systems where each user shares a sanitized version of their data with the service provider in such a way that userdefined nonsensitive tasks can be performed but userdefined sensitive ones cannot, without the service provider requiring to change any data processing pipeline otherwise."""	0
"from the beginning of section 5: ""initially, we assume that the secret algorithm is not specifically tailored to attack the proposed privatization, but instead is a robust commonly used algorithm trained on raw data to infer the secret."""	0
i also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.	0
[1] understanding blackbox predictions via influence functions [2] sketching as a tool for numerical linear algebra [3] randomized algorithms for matrices and data [4] understanding machine learning from theory to algorithms	0
this work depends hugely on its own humandesignated oraclelike map, which provides drivingrelated features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status).	0
the authors focus on a setup where both target and external training data come from the same distribution but differ in class labels, where each class in the target data is a set of finergrained classes in the auxiliary data.	0
strengths:  a new auxiliary learning algorithm  positive results on cifar data set weaknesses:  novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space  there is no attempt to provide a theoretical insight into the performance of the algorithm  the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance  experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario  the paper is poorly written and sentences are generally very hard to parse.	2
the model appears to perform well with synthetic data though very poorly with natural sentences.	0
similarly, an example is defined to be adversarial if it has two characteristics: it 1) lies far from the training data but is classified with high output probability, and it 2) is classified with high output probability although it lies very close to another example that is classified with high output probability for the other class.	0
they do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set.	2
the experiments are promising in support of the theory, but they do not seem to address this data invariance property.	0
pros importance of the issue exposition and relation to previous work experimental results (although these were for smaller data sets) appendices really helped aid the understanding cons real world usefulness clarity	2
for example, the main gradient estimation method is the same as nes (ilyas et al. 2017); datadependent priors using spatially local similarities was used in chen et al. 2017. but i have no concern with this and the nice thing of this paper is to put these tricks under an unified theoretical framework, which i really appreciate.	0
2. the treat model is not the common data poisoning setting, but 'model poisoning' (the attacker can send an arbitrary parameter vector back to the server).	0
while the results seem promising, there are several issues that may potentially weaken the queryefficient claims made in this paper, especially due to the lack of sufficient attack comparisons (on smaller datasets) and inconsistent threat models when compared to existing works.	0
b. online training 1. at step t, take action , observe , 2. update model (or simply store the data points) 3. use the model to get an estimate of the features it is probably time consuming to do b at each step t, but i can imagine the authors being able to do it all with stochastic value iteration.	0
for lowshot learning on omniglot, the proposed method is outperformed by [antoniou et al, 2017] at all values of k. more importantly, i’m concerned that the comparison between the two methods is unfair due to the use of different dataset splits, as demonstrated by the drastically different baseline accuracies.	0
however this has been done by a number of authors:  vlachos and clark (2014): http://www.aclweb.org/anthology/q141042  berant and liang (2015): https://nlp.stanford.edu/pubs/berantliangtacl2015.pdf while the idea of using such oracles for structured prediction tasks in nlp was first proposed by daume iii et al. 2009: https://arxiv.org/abs/0907.0786 furthermore, it has been applied for rnn decoding in nlp, see: https://arxiv.org/abs/1511.06732 apart from this, some further comments:  the subset of sql tackled in this paper is less expressive than what has been done in previous work on atis and geoquery datasets.	0
 section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.	0
in a follow up response to anonymous public comments, some new tests using cifar10 data are presented, but to me, proper evaluation requires full experimental details/settings and another round of review.	0
"the new result is better than prior work (compared to the ""factorized"", the finetuned lm is 2.1% better on the full test set, and 0.3% better on the noveltybased test set), but also uses a lot more unlabeled data than the prior work (if i understand the prior work correctly)."	0
or, if you're concerned about some of the overlyspecific language in more domainspecific wikipedia articles, then you could restrict the dataset to be the 100k most frequentlyvisited wikipedia articles or something like that.	0
the authors mention that their models train faster and don't require labelled data, but some more details of what exactly was compared would be helpful.	0
 minor: of of data , by bymuzellec cite overall, comparing strengths and shortcomings of the paper, i vote for the paper to be marginally accepted.	2
this might be true for most real world dataset, but i believe the degree of clusteredness may vary by dataset.	0
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
in general, the paper is not showing stateoftheart results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.	0
weaknesses: 4. related work: 4.1. the paper mentions the zeroshot vqa work from teney & hengel, however, given that is one of the most related works, i expect a more thorough discussion of the similarity in dataset and method.	0
however, i feel the paper is lacking a convincing evidence (from what i could find the authors base all the conclusions on one set of similar experiments performed with one generator architecture on one data set) in order to be viewed as a significant contribution to the generative modeling field.	0
for example, deepsecure, assumes the noncollusion & semihonest model which means that the data is protected by splitting it on several servers, each server will try to use the information it can see to infer the private information but multiple parties do not collaborate in trying to get to private information and moreover, each party follows the designed protocol.	0
this is fine as a jumping off point for the research, but for the model to be taken seriously as a valid solution to problems involving such a scale of data that current models cannot even cope, then it needs to show its worth on problems involving such a scale of data that current models cannot cope.	0
my second concern is the proposed ctu mappings are taskspecific, and need to be handdesigned given the data.	0
the author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fmri?	0
what would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet?	0
from a highlevel perspective, the methodological innovation (a pointernetwork trained on sequence loss from logged data), setting (reranking a slate to consider interactions), and empirical analyses are largely ‘incremental’ — although i think nontrivial to put together and the paper itself is wellwritten and fairly convincing.	0
i could think of a few more experiments regarding submodularbased models, possibly different settings of the ‘diverseclick’ data for a sensitivity analysis, and a more direct comparison to [ai, et al., sigir18], but this isn’t required to make the results sufficiently convincing.	0
a few more experiments would make this case stronger, but has realworld data.	0
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
the collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.	0
the strategy this paper use for indexed data is to encode all data in a blackbox, which can be inefficient since the order of temporal data or the geometric structure of spatial data is not handled in the model.	0
my worry is that when putting all those informative data into a blackbox may not be the most efficient way to use them.	0
a gans adversary bases its comparisons on individual data points, rather than on distribution comparisons or on groups of points like mmd, etc. i understand the reasoning behind the choice of generative models (gms), but it is choosing gans out of the set of gms in this particular case that i am referring to.	0
 this paper is concerned with the socalled conditional generation, which was descried as the task of sampling from an unknown distribution conditioned on semantics of the data.	0
the key idea in this paper (using principal shared directions of perturbations, computed on a small subset of data points) has unfortunately already been proposed and tested in classical (nonequivariant) neural networks  see for example fig 9 in moosavidezfooli, 2017, cited in the paper, and published in cvpr 2017. the present paper proposes however a few additional bits of information with a nice theoretical analysis, while the previous works were mostly based on heuristics.	0
does it suggest the overconfidence is even a concern for familiar data/ iid data?	0
it's wellknown that adam is good at training but might perform badly on the test data.	0
pros:  the problem of designing generative models for 3d data is important.	2
 using the fft to invert seems slow (the theoretical flopcount is good, but it's still superlinear, and requires global data movement, so not good for a distributed implementation).	0
i am more concerned about the situation where the capacity of the model is challenged by the size of the dataset.	0
conclusion: my two biggest concerns are: 1) the algorithm is not tested on large data seta 2) the algorithm is not tested with the models of limited capacity.	0
overall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models.	2
my second concern is the results are all on synthetic data, and most shapes are very simple.	0
it also lacks generality as only one (proprietary) dataset is used.	0
weaknesses:  experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on nonsynthetic data.	2
2.2 made things much more intuitive but i fail to see how the indicator variable annotations (action, scifi, etc.) can possibly come out of the data.	0
however for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones.	0
it's concerning that in fig. 1 convergence on a toy dataset takes more than 2000 iterations.	0
two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical.	0
i’ve seen work using a reducedmnist dataset, which is probably created by random subsampling, but still more relevant than many of the aspects of embeddings cited in this section (the paragraph about arithmetic operations for instance).	0
pros: 1. first approach to combine nns, rule learning, prototype learning 2. provides an interpretable method for predictions on longitudinal medical data 3. experimental results seem to suggest that the proposed approach is resulting in accurate and interpretable models.	2
cons: 1. the various pieces in the method (rule learning, prototype, nns, data reweighting) seem to be somewhat haphazardly connected.	0
this however is conceptually not nice since there is no uniformity across the dataset.	0
a related approach for dimensionality reduction is also proposed, but i think this only applies to euclidean data, so i am a bit confused about that part.	0
the paper also claims that when sigma=0 there are trivial solutions that are unstable as data changes, but that when sigma=threshold (assuming this is what is meant by end of convexity) there are nontrivial solutions that are less sensitive to data changes and hence the most generalizable.	0
review: pros: the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.	2
2. data augmentation is naturally expected to be random, but the proposed method seems to learn a deterministic parameter for the augmenting transformation, which looks unnatural and limited.	0
"cons:  details are lacking about the ""anon"" dataset introduced in this paper (where do the photos come from and the labels, visualization of a few examples...)  there are not many technical issues discussed in the paper and that is fine as the main idea is relatively simple and its effectiveness is mainly demonstrated empirically, but i feel the paper is missing a discussion about the importance of the initial classifier trained to estimate the target prior probabilities for the source labels and whether it is crucial that it has a certain level of accuracy etc.  the approach in the paper implies a practitioner should have access to a very large target dataset and the computational and time resources to appropriately pretrain a complex network for each new target task encountered."	0
[cons & details] (1) as stated in the abstract, “…, but that the improvement diminishes as the size of data grows, indicating that powerful neural mt systems are capable of implicitly modeling roleword interaction by themselves…” (1) the main concern is that, considering ril does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline.	2
5. lack of variety in data: the data looks simple with one or few objects on plain background.	0
on the negative side:  it is unclear whether the data augmentation techniques is applied only at training time or also at test time.	0
pros: they improve generalization to unseen data.	2
cons: their models are considerably more complex, and they do not analyze their data in enough detail to suggest whether their complexity is necessary, or perhaps could be reduced.	0
 the authors claimed that their system can recognise codeswitching but actually randomly mixing data from different languages are not codeswitching.	0
the results on synthetic data seems promising, but there is insufficient evaluation being performed on real and larger dataset where the mode collapse problems are more likely to happen.	0
 experiments the results on the synthetic data seem promising, but the results on mnist and cifar10 are not impressive enough: ' the visual quality of figure.9 does not look very appealing.	0
the only reason i can think of is data augmentation, but this has nothing to do with the necst model.	0
concerning the experiments, i don't understand what is the split between training and testing data.	0
my main concern with this manuscript is the handling of missing data.	0
you have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.	0
the experimental results do not seem to be any better compared to baselines when measured in terms of data efficiency, but the proposed method requires fewer “reward computations”.	0
you mention complexity of data and model several times in the paper but never define what you mean by that.	0
you present a number for c2 in section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1).	0
due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter .lambda=0.00011 on the cifar data set.	0
on cifar10, for a certain combination of logit squeezing and gaussian data augmentation, the authors report results suggesting that their approach outperforms madry's model both in terms of accuracy on clean samples and robustness against adversarial samples (white and blackbox).	0
there are several concerns: 1. bayesian optimization might itself create a bias in the input data distribution, since it selectively pick some parameter configuration over the others.	0
providing benchmark data for tasks such disentanglement is important but i am not sure generating data is sufficient contribution for a paper. '	0
my main concern is the heuristic nature of the method without any successful real data application.	0
for example, the section on “ratios andweber fraction” argues that “these curves align well with the trend predicted by weber’s law”, but does not explain how the experimental data would present if the alternative hypothesis (pairingbased strategy) was being used.	0
specifically it is required that the log likelihood of but classified as class b with the same log likelihood of class b for g(z_b) is the same as the log likelihood of class a for the input x. such that all other classes have a log likelihood that is at least epsilon lower than both the one of class a and class b. the proposed approach is compared to a set of other interpretability methods, which were gradcam, lime, pda, xgem on mnist and fashion mnist data.	0
 the experimental section lacks performance and comparison in a controlled environment, e.g., on synthetic data with more samples to show statistical significance.	0
"i'm confused about ""although the proposed feature space is almost resistant to concept drifts, we incorporate incremental learning so that the model can adapt to new data without forgetting its existing knowledge, ""  if the system has changed the existing knowledge may be wrong and need to be forgotten?!"	0
i acknowledge that the difference from previously published methods is not that large, but i still think it has value as it's getting quite close to being a practical method for generating fake training data for speech recognition.	0
cons:  the paper forgets the state of the art for comparisons (optimal transport, data processing inequalities)  the paper formally shows little as most results are in fact buried in the text and it is hard to tell the formal from the informal.	0
"they will find it, along with a huge number of other useful properties, in series of ieee t. it papers, among which pardo and vajda's ""about distances of discrete distributions satisfying the data processing theorem of information theory"", van erven and harremoes, ""re ́nyi divergence and kullbackleibler divergence"" (for the kl / rényi divergence, but you have more references inside), etc. . '"	0
it seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak.	0
since vb is often concerned with the limiteddata regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is.	0
in addition, it is assumed that the confusion matrix c is known or estimated from data but it's not clear to me how this can be done in practice.	0
results are provided on relatively new tasks so it's hard to compare fully to previous methods, but the authors do make an attempt to provide comparisons on synthetic graphs and intrusion detection data.	0
your method may generalize, but how do we know that if you've only tested it on one small dataset?	0
i think this evaluation will be overoptimistic insofar as if 10:3010:45 and 11:0011:15 are in the train set, but 10:4511:00 is in the test set, it will be relatively easy to predict 10:4511:00. i would suggest considering train/test/validate splits based on larger chunks, e.g. leave the data in 15 minute blocks, but randomly select hours (4 blocks) to put in train/test/validate.	0
the experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but i am not entirely sure whether the given architecture is necessary, see comments below.	0
they synthesize their mnist dataset but corrupting a subset of mnist digits with noise and treating actions as rotations.	0
the authors evaluate their method by examining reconstructions of the mnist digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding).	0
this is especially concerning since in the linear case, the nlc can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered.	0
overall thoughts: using datasetspecific features for evaluation metrics makes a lot of sense, but i don't feel totally satisfied by this paper's investigation of the specific proposal of a vae, and am particularly worried about whether the metric just ends up preferring models similar to that vae.	0
i found the paper difficult to understand, but as far as i understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.	0
how are the representations to be used and what type of users is it intended to serve (expert/patients etc) pros and cons  interesting problem modeling could be better motivated experimental platform is limited for interpretability studies == hsu, w.n., zhang, y. and glass, j., 2017. unsupervised learning of disentangled and interpretable representations from sequential data.	2
strengths:  i appreciated the quantitative comparison on the lowd toy datasets (2d and 3d mogs), reporting modes captured, and jsd.	2
my only concern is the experiments: 1) some of the benchmark datasets for the proposed task as well as some wellknown methods (see battaglia et al’18 and references in there) are missing.	0
there are e.g. datasets (protein data  see tino) get very bad classification models if the negativ contributions are removed  accordingly they are not just noise but contain valuable information to the problem  it would be good to add a few sentences at the beginning of the paper to (super brief) review the core idea/concept of siamese networks  the english is sometimes a bit bulky and it would be good to check it by a native speaker e.g ' proposal do not require'  proposal does not require ' unlike the our'  follow by a spellchecker 'embeddng', 'interms', 'simalry'  'followed by removing it by flipping'  with flipping the contribution is not 'removed' but mapped to the positive part of the spectrum  how does your approach compare to a classical embedding of the indefinite kernel matrix into a pseudo euclidean space (see e.g. tino or pekalska)?	0
 just a hint: you may find the work of michael biehl on matrix learning in prototype networks interesting  your matrix l is called omega or lambda there  if i am not misleading there was a paper at aaai a few year ago about negative locality sensitive hashing where such an embedding is discussed ( not in the same way as you do  but related)  in some way eq 4 prunes down to a eigenvalue decomposition of your matrix m  in metric learning m could be e.g. the covariance matrix of the data and decomposing it into e ' v ' e'  will give you exactly this  you allow for quite some flexibility in learning your projection matrix in eq 5  how do you make sure that this is not oversimplifying the problem?	0
 e.g. if i place this into a supervised learning context it could easily happen that the mapping fits in perfect alignment to the training data but may be much less effective (or even useless) if it is applied on test data  w.r.t.	0
 'here, we show that our distance ...'  this is basically equivalent what is shown already in the book by pekalska ( i am not sure if it is original proposed/shown by them but it least it is there  maybe from 2001 or so)  'in other words, the numbers p  and p − are entirely decided by the data'  well yes, but i would slightly reformulate this.	0
weaknesses: 1. while the paper is well motivated, i believe that the presence of multisensory expectation learning depends heavily on the type of multisensory data.	0
so i have concerns about how the model would work in these cases depending on the data used.	0
"i understand that paper was just for generation purpose not specific to the dialog modeling, but it makes the claims in the paper misleading such as: ""unlike vae conversation models that impose a simple distribution over latent variables, dialogwae models the data distribution by training a gan within the latent variable space""."	0
pros:  provides insights on why adversarial training is less effective on some datasets.	2
"in summary: pros  claims sota results on two good benchmarks for zeroshot learning  approach is original cons  paper lacks a lot of methodological and experimental details some minor details:  ""we found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset."""	2
adversarial examples are also likely to be on the data manifold, but form a small enough set that it doesn’t affect standard generalization.	0
i think this work is promising and interesting to the probabilistic modeling community, but needs some cleanup and some more compelling presentation (non image data?	0
finally, for folks familiar with healthcare data and mimiciii specifically, the results are underwhelming: yes, the proposed approach beats (the authors' own implementations of) baselines, but it underperforms other published results on the mimiciii 48hour mortality task ([1][2] report aucs of 0.87 or higher).	0
strengths  i applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the gp adapter framework (gpgru) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)  it likewise outperforms both commonly used preprocessing (gruf) [2][3] and the much more complicated neural net architecture (gruhd) from [6] (across two datasets and tasks)  the simplicity of this approach probably lends itself to additional customization and innovation  the literature review seems quite thorough and does an especially nice job of covering recent work on rnns for multivariate time series and irregular sampling or missing values  the experiments are thorough and welldesigned overall.	2
more data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments).	0
given that the paper is concerned with irregular sampling (not missing data), i would expect statistics on sampling rates, not missingness...  why derive your own mimiciii subset and tasks rather than use one of several preexisting benchmarks (both of which include more variables and tasks) [1][2]?	0
build a better/more dataefficient machine translation system — this could be an interesting goal and suitable for the paper, but this doesn’t seem to be the framing that the authors intend.	0
thus, the paper is concerned with unsupervised version of the metalearning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).	0
this is also lacking from the description of the experimental protocol, which does not address the datasplits (how many classes were used for each) and size of the unlabelled test set.	0
weaknesses:  the claim that reusing the same training data used for training the teacher model in model compression can lead to overfitting of student model is not very obvious and needs more experimental evidence in my opinion.	0
weaknesses:  the authors don't compare how good this technique is in comparison to simple data augmentation.	0
little is known about the newly collected datasets, it is not clear how to interpret the relative improvements in validation loss compared to the original metrics of allamanis et al 2016, and the paper lacks necessary comparisons to othef pretrained embeddings, so though the overall claim that pretrained embeddings should be used for this task seems to hold up, it is not clear that this is a complete argument for the method chosen by the authors.	0
the idea is simple, but the approach improves the zeroshot translation performance of the baseline model, and seems to be better than either pivoting or training on direct but outofdomain parallel data.	0
this is briefly mentioned in the paper, but i think it needs to be made more explicit, and lwf should be a baseline in the experiments to clearly indicate the benefit of keeping this data.	0
the idea is to learn a representation that separates background (dimensions that do not vary across data points, but may be subject to change in a data transformation) and foreground (dimensions that vary between data points under the same background) and then apply a 'fixed' linear transformation in the learned representation space.	0
cons: 1. i have noticed that zügner et al.(kdd'18) present an adversarial attack method on gcn for graph data.	0
 imvlstm outperforms many baselines including popular interpretable models on three different datasets, and the interpretation part is not super rigorous, but convincing enough.	0
 the authors claimed to have a universal phonetic model but actually the model was trained only with english data.	0
however one of the claims does not seem supported by data: 1. the authors claim repeatedly that using the prediction error framework is computationally more efficient than alternatives but they do not show this.	0
pros:  it provides insights on why adversarial training may not improve robustness on the test data set.	2
authors claim the proposed strategy is efficient, but they didn’t provide the results on the imagenet1k data set.	0
 the paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones.	0
and it seems the simplest model is working best because, for the size of the dataset (small, unlike standard cv data), the nonsimple models are overparameterized and more nonlinear (for the lack of a better phrase) to train?	0
experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.	0
3) some writing issues: it would be better to 'clearly' demonstrate the final accuracy of different models (i.e. resnet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text.	0
contextfree explanations are generated in two ways: 1) when a local explanation shows same polarity among all valid data points, and 2) by negating the local explanations’ polarity at a data point, finetuning the model on the resulting modified function approximation, and regenerating the local explanations for other data points; if the polarity is reversed for all other data points, then the local explanation is also a global explanation strengths:  broadens the application of nid to provide hierarchical explanations and contextfree explanations  experiments on contextfree explanations show promising results, for instance, on the sentimentlstm model and in supplementary a. would be great to see more results on this front.	2
lack of experiments: the currently presented experiments are all on rather simple data.	0
pros:  clearly written  useful experiments for those seeking to select a differential bayesian filter, and learning (heteroscedastic) noise from data.	2
"pros:  generative modeling of sequence data still in its infancy  potentially lower variance than policy gradient approaches  experiments are promising cons:  lots of grammatical errors and odd formulations questions:  equation 14: what does it mean to find the ""maximum entropy solution"" for the given optimization problem?"	2
of course, it would be nice if additional datasets could be included as well, but this of course depends on the computational resources the authors have available.	0
 deepström networks quality: good originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: a number of open issues in the presentation  see detailed comments the paper is based on some concepts to link nonlinear data representations by means of nonlinear kernel mappings, with the deep learning approach.	2
to learn the weightings on the fly in your adaptive approach will address the complexity issue but may in parts degenerate performance  now it is a question what the user is more interested on  you mainly ignore the point how the landmarks have to be selected  random is (often) not a very reasonable choice  see respective work around this in your datasets the problem may not show up because you have images only which have very strong characteristics and are intrinsically very low dimensional.	0
pros: this is a crucial line of research direction that aims to make dnns applicable to many realworld problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search.	2
my concern is, given this is an empirical work, the number of datasets used in evolution is a bit small.	0
"""(2) limited context and/or smaller training corpus of documents: in settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of tms is challenging."""	0
the setting is certainly interesting, and the various data creation strategies are reasonable, but the paper suffers from two main flaws.	0
i understand that there are no ground truth cts factors for mnist/fashionmnist, but this makes me think that a dataset such as dsprites (aka 2d shapes) where the factors are known and has a mix of discrete and continuous factors would have been more suitable.	0
pros:  tests on several datasets  seems fairly generally applicable.	2
on the other hand, my main concern at the moment is that the results seem to be informative only for low dimensional data and networks of small width.	0
strengths  strong results  testing on both baglevel / singlelevel relation extraction  insights via multiple ablations — variation of the number of layers, exploring densely connected data subsections with cycles to identify examples for multihop inference  evaluation on new, humanannotated test set issues  evaluation only on one task, and one dataset (although, with more detail).	2
third, training the networks with the batchsize of bxm, but excluding the original data samples in the given batch would be another interesting experiment.	0
the second concern is that the set of chosen datasets for indistribution and outofdistribution are far from challenging: the datasets are so different that a linear classifier based on the local image statistics can separate them.	0
they make it sound like the authors introduce a new task and dataset for evidence scoring, but instead, they merely train on squad with existing annotations.	0
originality: pros: the suggested model outperforms others on three qa datasets.	2
cons: the way achievements are achieved is largely by using more data, making the comparison somewhat unfair.	0
this seems fairly obvious but i haven’t seen this property of gans being exploited to distinguish between gan generated and realdata this property/ shortcoming of the generator is not surprising at all and has been acknowledged before.	0
the results are good, but are on either synthetic data, or using ground truth bounding boxes.	0
 the authors mention two types of language models (word and character level), and also 4 text datasets to train the lms on, but do not provide results for all combinations.	0
the results seem to suggest that with very little labeled and no additional unlabeled data siamese networks perform the best, but when additional unlabeled data is available, vat outperforms gan in the regime with low amount of labeled.	0
significance: i think that the paper provides a warranted empirical study about recent approaches to learning with little or weakly labeled data, which is of general relevance in the field pros: the paper is wellwritten and easy to follow nice overview of recent approaches to fewshots/semisupervised learning.	2
detailed empirical analysis covering a range of training data regimes cons: only one benchmark is used results may also vary for different network architectures or application domains i in general found the paper interesting; however as mentioned, i find it hard to gauge the scope of the results presented here given that they are only provided for one benchmark and one network architecture.	0
"the intro does a good job of describing the two concepts, and making the contrast between local and global coherence, but when i was reading 3.1 i kept thinking this was describing cohesion (""t that follows s in the data""  sounds local, no?)."	0
the experiments involve synthetic regression and classification datasets but there are no novel insights that advance what is already known about the hyperparameter optimization (e.g., see [3]).	0
and finally, the authors claim that most modern datasets can fit to the accelerator memory if reduced 10x, but in my experience the network and it’s activations (which are stored during training) occupies most of the gpu memory even on high end accelerators, not leaving enough space to store a large dataset even after data reduction.	0
first, the experiments are only done on a small scale dataset (cifar10), which is ok in general, but questionable when the proposed method explicitly targets big data regime and making the training faster.	0
the latter sees the same number of updates but with less data, which should only decrease the test performance.	0
i’m also very surprised not to see the moment of training mode transition on the plot (i.e. the moment when the model switched from restricted dataset to the full one), the lack of it can indicate an implementation error.	0
i infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.	0
the paper concludes that related data is required but how related data can be collected remains unexplored.	0
the collected dataset, which is a contribution of the paper is also poorly explained.	0
positive aspects: the problem of training privacy preserving deep models over distributed data has been a significant and important challenge.	2
 top pros:  well motivated approach with good examples from clinical setting  sound proof on why information theoretical approach is better than mle based approaches  experiments on diversified data sets to show their approach's performance, with good implementation details.	2
top cons:  fairly strong assumption on the existence of mutually independent senior experts in the labeling process  hardtocheck assumption for theorem 3.4 for real world problems, on the sufficiency of senior expert's info to predict the true class label the paper is in general well written, and builds upon existing work on crowdsourced data mining and cotraining.	0
as pointed out by r2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (i am not suggesting that it is, but that possibility should be considered and explained in the paper).	0
on the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the fig. is very confusing.	0
i think it would have been interesting to see how the model performs in a semisupervised task (i.e. where some small fraction of the data has labels), but perhaps this is better suited for future work.	0
in summary, my evaluation is as follow: 'pros'  pretty original problem formulation  generally well written paper 'cons'  lack of comparison with simple baselines in basic dataset distillation setting  use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated  possibly a mistake in the theoretical analysis of the linear case indeed, i found the paper to be generally quite clear and enjoyed reading it.	0
cons i feel, the authors should have extended the experiments to imagenet which is a much larger dataset and validate the findings still hold, i feel the discussion section and comparison to other methods needs to be worked to be more thorough and to tease out the benefit of each of the various terms added to the loss functions as currently all we have is final numbers without much explanation and details.	2
i don’t think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.	0
 the use of a pretrained, domainspecific feature extractor is briefly mentioned but no details (what is the architecture, on which data it’s been trained, etc.) are provided.	0
i do think a lot of the gains here are due to clever design choices in their experiment (for instance using different types of raw data which help more on certain tasks, removing the first principal component, etc.) but putting everything together to get very competitive results with across all these tasks with an interesting approach and an accompanying analysis is a nice contribution.	0
cons  some of their gains are due to choice of dataset for training or removing the first principal component  advantages that other comparable models may or may not have.	2
i can see this method as a way to measure the representativeness of datapoints, but i would see it as a component of al, not an al alone.	0
"), but then all al that rank datapoints with some scoring function are ""discriminative""."	0
the proposed algorithm is evaluated on several benchmark data sets using several benchmark neural network architectures and the results indicate a reduction in communication costs are compared to a nonsparsified variant of federated learning strengths: • the algorithm jointly leads to compression of a neural network and reduction in communication cost of federated learning.	2
this sounds about right, but lacks data support.	0
my advice to improve this work would be to do more experiments and to show better that the lack of performance gain is due to overfit, and can be fixed with larger data or data augmentation.	0
at the bottom of page 3 in the related work, a concrete application used in prior work is mentionedwhere crowd workers are shown single labels and vote y/n, leading to a mix of standard (if y) and complementlabeled (if n) datahowever this mixed setting is not considered explicitly in this paper.	0
additionally, it is briefly mentioned that collecting complementary labeled data is faster, but again no concrete examples are given to support this.	0
the relation to dataset shift was very interesting, but it’s unclear what’s the exact connection between the proposed algorithm and the dataset shift.	0
the application of imitation learning to sports data is not new, but also not saturated either.	0
therefore, the overall analysis should be much more focused on models which achieve the same test performance but use require less data to achieve this performance.	0
pros:  while we focus on achieving better and better results with machine learning, we tend to forget about the private nature of the data we often have to work with.	2
however, the interesting and relevant use case is that in which the data is not randomly split, but rather complete families are held out from the training set for use in testing.	0
 summary: the authors address the task of cross language document classification when there is no training data available in the target language but data is available a closely related language.	0
these cases does not only exist in mathematical books, but also exists in practice, especially when the data is located on a lowdimensional manifold embedded in a highdimensional space.	0
but if that’s the case, that’s a very interesting data point that the additional structure actually helps considerably beyond the incremental advantage exemplified here.	0
significance:  clustering is an important problem and the authors show some results, but on only three datasets: mnist, newsgroups, reuters.	0
main concerns:  what is the stateoftheart on mnist and their other datasets?	0
3. the argument for why the proposed feature smoothing method works is presented in theorem4.3 in section 4.2, but the theorem seems to rely on the assumption that one can add data around the true decision boundary.	0
in any case, the lack of data augmentation may account for this disparity, but can easily be remedied.	0
3. long sentences are difficult to follow: “in real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”    the problem of largescale video understanding is an important and interesting problem to tackle.	0
the use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover humansemantic segmentations.	0
it finds that deep generative replay (dgr) that learns to generate imaginary new samples from previously seen training data, potentially augmented with soft labels seems to work best in these specific experiments but potentially doubles the computational cost.	0
strengths:  authors present a good exploration of how linear cnns memorize data when they do downsampling.	2
the argument that you do not want to use “auxiliary sources of labelled data or leverage domainspecific prior knowledge” is indeed necessary for fair comparisons, but also points to a limitation of the current approach.	0
despite more emphasis being put on mentioning the existence of bidirectional variants of gans, i still feel that the paper does not adequately address the following question: “what does gaia offer that is not already achievable by models such as ali, bigan, alice, and iae, which equip gans with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates?” to be clear, i do think that the above models are inadequate for the paper’s intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but i believe this is a question that is likely to be raised by many readers.	0
figure 2 does present latent space interpolations which help get a sense of the extent to which interpolates also belong to the data distribution, however in the absence of a comparison to competing approaches it’s impossible for me to tell whether the proposed approach yields more convex latent spaces.	0
" the necessity for latent spaces to ""respect the highdimensional structure of the [data] distribution"" is stated as a fact but not welljustified."	0
in the case of face images a reasonable argument can be made, but in a dataset such as cifar10 how should we linearly interpolate between a horse and a car?	0
significance: the paper acknowledges that methods do indeed exist for the various structured data types, but claim that they are complex, and that the proposed method is a simple general alternative.	0
the proposed method is simple and general, but it does require that the information lost when converting the structured data to the set is encoded as features in the set elements, e.g. the sequence index is added.	0
"the paper notes ""novel stylizedimagenet dataset"" and shows that models can learn shape/texture features both but there is not much detail/explanation on why ""stylized"" is the novel approach and also the methodology of constructing data by replacing with painting from adain style transfer (huang & belongie, 2017) is not discussed/explored."	0
i believe otherwise the results are incomplete since fig. 4 shows the biases of these models on in dataset but doesn't show if these biases are removed by training on sin.	0
also, there are a couple more papers that felt relevant to this work but are not mentioned:  estimating accuracy from unlabeled data: a bayesian approach, platanios et al., icml 2016. i believe this is related in how noisy labels are modeled (i.e., section 3 in the reviewed paper) and in the idea of correlation/consistency as a means to detect errors.	0
however, i have the following two main concerns 1. the major attack on distillation from an ensemble is that it needs to pool data across all sources which has cost and privacy concerns.	0
"however i'm not entirely convinced this ""data pooling"" is really necessary."	0
cons:  idea is not that novel relative to all the recent work on learning to supplement training data for fewshot learning (hariharan 2017, wang 2018, gao 2018, and schwartz 2018).	0
i agree that this is a useful area to study and direction for the community to go, but as the introduction of this paper states, this is the most interesting when the parties have control over logically separate components of the modeling pipeline and also when joint training of the components is being done, potentially on disjoint and private datasets.	0
a list of additional datasets is provided in table 5, but only the performance metric is listed, which is meaningless if it is not accompanied with figures for size, latency and speedup.	0
the only takeway about the additional datasets is that the proposed lbpnet can match or outperform a weak cnn baseline, but we don't know if the latter achieves stateoftheart performance (previous figures of the baseline cnn suggest it doesn't) and we don't know if there's significant gain in speed or size.	0
for the more realistic kinetics dataset, the proposed method is competitive with h264 and h265, but only in a very limited range of bit rates.	0
pros: ' findings provide us useful direction for future research (that dataparallelism centered distributed training is going to hit the limit soon) ' extensive experiments across 5 datasets and 6 neural network architectures cons: ' experiments are a bit too much focused on image classification ' error bars in figures could've provided greater confidence in robustness of findings	2
supportnet is compared to five methods (all data: network is retrained with new and old data, upper bound for performance, icarl: stateoftheart method for incremental learning, ewc: only ewc regularizer added, fine tune: only new data, random guessing: random guess to assign labels) on six datasets (mnist, cifar10, cifar100, enzyme function prediction, hela subcellular structure classification, breast tumor classification).	0
pros: (1) the authors propose a sensible approach, which is also novel to be best of our knowledge, using svm to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting.	2
(2) the authors claim that icarl suffers from overfitting on real training data (section 4.1) however table 2 shows icarl only on the enzyme function prediction which is also the dataset where the difference in performance between icarl and supportnet is the largest.	0
many authors have already plugged distances into an exp to make it a similarity  but this is changing the data representation (this is widely discussed by pekalska).	0
to ensure reproducability of your results i ask you to provide the respective codes e.g. on github (can be done anonymous)  repeating myself from the last review: there is a lot of work addressing that making a kernel psd may not be good idea  you provide experiments for a small number of data where your kernel is now psd but what is with the other data (where e.g. in pekalska and followers it was shown that making them psd is bad ... )  is your approach solving this  or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?	0
the authors state that iterations of the fixed point procedure converge rapidly in practice, but it seems like it's only been evaluated on these synthetic data sets.	0
it is inspired by a neuroscience application, but uses only simulated data.	0
it seems scattering performs less well than sota methods, but has the advantages of not requiring any training so potentially good candidates for low data regimes application.	0
regarding the methodology for training the classifier, i am not familiar with these datasets but using just a 1/10 of the data to train classifier seems a bit extreme ?	0
this section is unclear to me, i had initally assumed that the data would be fed in one pixel at a time, but due to the presence of the other experiments i presume this is not the case.	0
2. similarly, the distribution q sometimes refer to the original (unknown) input data distribution, but the same notation is also used to refer to this gaussian distribution centered on each test example.	0
i also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience i lump the 2 nondata ones together): (1) dataset: the dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation.	0
the authors justify this dataset by pointing out that existing realworld datasets underrepresent rare emotions (e.g., afraid), but that’s just a reflection of how these emotions are distributed in the real world.	0
(2) improvement in empathetic dialogue generation: the paper shows improvements across the board compared to a transformer baseline, but the question the authors do not satisfactorily address is whether their explicit (and i would say sometimes adhoc) treatment of empathy (e.g., using emotion classifier, etc.) is crucially needed to get better empathetic dialogues, since the authors did not control for training data size and model capacity.	0
the fact that generated outputs improve in all aspects (not only empathy, but in attributes completely unrelated to empathy such as fluency and relevance) suggests that the improvement is due to more data or capacity (e.g., perhaps yielding better encoder).	0
overall, the paper doesn’t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but i think there are problems with both.	0
these sort of papers would benefit from a more formal/comprehensive evaluation by means of an explicit enumeration of all the dimensions relevant for their analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. however only some of then are (partially) provided.	0
but even for the one category, there were no previous methods considered, and the only comparison was between random parameters and the learned ones, where we only see marginal improvement, and what i perceive to be particularly low iou for the car (although it'd help to know what's the sota there for comparison) for both vision applications i could help but wonder why the authors did not try to simply train on the validation set to give us another datapoint to evaluate the performance of the method: this is data that 'is' used for training the outer loop, so it does beg the question of what is the advantage of having hte inner loop.	0
the method is compared to nesterov accelerated gradient (nag) for the datafitting problem (fig 1) but not on the other tasks (figs 2 to 5).	0
therefore, i have concerns that the proposed framework may only be significant on the arc dataset.	0
there is another technique, which (i think) consists in flipping the images and their roles, but i do not understand why it should improve learning  besides providing more training data.	0
pros:  using data to learn exploration strategy in tis manner is a novel idea for bandits  good experimental results  well written paper cons:  practical impact may be minimal.	2
currently, several variants of data augmentations are used, and discussion may lack an explicit comparison with the stateoftheart of spherical and spectral methods.	0
pros:  the proposed intrafid is interesting but is missing two baselines (intrafid for two batches of real data and intrafid for two batches of a baseline gan without the proposed technique) which would help calibrate and contextualize the newly introduced metric.	2
one of the central claims is that the proposed method improves the stability of qlearning, but it is unclear how many random seeds were tested in figure 2 and table 2. it appears that only the data from one training run was used, and the reported standard deviations are computed using the last 10% of episodes in that single training run.	0
an example on mnist data illustrate these properties, but it is still not clear what are the visual clues as the criterion to select a good dr method and what are the global structures.	0
authors discussed the results in figure 4 for six realworld datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.	0
the surrogate model is expected not to give accurate prediction everywhere due to the limited number of data but produces uncertainty of its prediction as an dictator.	0
pros: the work contributed a dataset where the task has relatively objective criteria for success.	2
however if this is considered as a dataset description paper and the right expectation is set in the openings, it may still be acceptable.	0
later in the descriptions (e.g. 4.1 on baseline methods) the notion of training set is mentioned, but up to then there is no mentioning of how training and testing (novel scenes) data are created.	0
indeed, it is difficult in general to quantify the results of generative models, but most other gan papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset.	0
specifically, the resulting model is able to fit real data but not data with random labels.	0
the authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue.	0
this doesn't feel intuitively true from the example dialogue, but the nlg system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's nlu/nlg system is quite weak and maybe there's a problem with the datacollection procedure.	0
the results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the datacollection methodology was faulty leading to lots of failures.	0
results: the second concern i have is that, assuming my reading of the results as described above is correct, that the sqa method quite severely affects accuracy on the clean test data, e.g. increasing the error rate on cifar by 72% (from 12.33% to 17.06%).	0
cons:  hard to understand because it was left unclear what is evaluated, at least to readers who are not familiar with a possibly existing implied convention;  the method seems to harm accuracy on clean data a lot, which is the main use case of such a system.	0
the variance term (and the bias) depends on the distribution p(theta|s) of the model parameters given data s. this would be the posterior distribution in bayesian settings, but the paper considers the frequentist framework so this distribution encodes all the uncertainty due to initialisation, sampling and the nature of sgd optimizer being used.	0
based on the experiments, the proposed method achieves marginal improvement in terms of f1 score but sometimes also slightly lower performance than other gan based such as pgan, so the impact of this work to solve positive unlabelled data problem is not evident.	0
this is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. also no guarantees or bound has been given about the variance reduction of estimator and it is more based on the authors intuition.	0
the paper performs some sensitivity analysis with resepct to variance selection, but the study is again on one dataset.	0
my main concern about this work stems from not knowing how sensitive the recovered uncertainties are to the ood data generating mechanism and the parameters thereof.	0
from the experiments, it seems that .sigma_x needs to be carefully chosen for different problems, .sigma^2_x < 0.3 seems to not work very well for bbb  ncp for the 1d sine data, but for the flight delay data .sigma^2_x is set to 0.1 and seems to work well.	0
though adversarial training might help towards increasing standard accuracy in certain data regimes such as data scarcity, but when sufficient data is available there exists a tradeoff between the two goals.	0
the main hypothesis is that dependency graphs allow for a way to interpret the model across samples, but it doesn’t show any conclusive results about the data or models that wasn’t previously known.	0
perhaps this would be less of an issue if the authors had worked with e.g. imagenet, but for these smaller datasets it would definitely be worth to be on the safe side.	0
table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that vgg16 was also used for cifar and caltech, however other statements seem to say otherwise).	0
this work is based on cryptonet, but uses a different data representation.	0
c. table 2 compares the nonpersona hredgan with phredgan on udc, but the authors do not provide a comparison between these two on the tv dataset in table 1. d. the comparison between phredgan_a and phredgan_d is inconsistent for the two datasets (table 1 and table 2).	0
results on three canonical datasets using blackbox and whitebox adversarial attacks suggest that exl can be helpful in defending against bb attacks, and is easily combined with other adversial training approaches such as pgd to further improve robustness.	0
novelty/impact  creation of a new dataset on a new and interesting problem  useful comparison of modern networks on the task  gapnet  lacking technical novelty, insight, and performance is unconvincing  demonstrates that endtoend learning outperforms cell centric approach  was this really surprising or even new information?	0
2) pros  combining vision, memory, and motor control  allows the highlevel controller to operate at a coarser time scale  the set of lowlevel movement primitives can be extended by using more mocap data 3) cons  no comparison to earlier work  highly unnatural motions even though it makes use of mocap data  sample inefficient: more than 1 billion timesteps to train the highlevel controller 4) comments showing that the agent can provide suitable solutions for these tasks using raw vision input is indeed interesting, however it is not clear what the main contribution of the paper is as the authors fail to compare their results with earlier work.	2
the article motivates the work as combining raw multimodal sensor datasets, but no real tasks are shown.	0
at least in the appendix… my major concern is that both datasets are not dealing with real background noise.	0
i’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.	0
if you consider the colored regions to be ground truth labels for the points then the colored points are not adversarial examples, but are perturbations that truly switch the underlying label of the data point.	0
the goal of adversarially robust machine learning is not to be resilient to arbitrary magnitudes of noise, but specifically small perturbations that do not change the underlying class of a data point.	0
the test accuracy plots are indeed different, but this could be because robust learning is a statistically more complex problem and needs more data as has been studied in prior work [schmidt et al, 2018].	0
adversarial examples can have more entropy than original examples, but data with higher entropy do not necessarily contain adversarial examples.	0
"''detailed comments'' _paper strengths_  the idea to use a negative video example for unsupervised detection learning seems novel  the proposed method is simple and the needed data can be collected with widely available equipments  the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example  the authors collected realworld data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)  the authors compare to a number of nonlearning approaches from opensource implementations (the reviewer cannot judge whether any relevant technique is missing)  the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes _paper weaknesses_  the authors clearly reduced the horizontal margins of the standard iclr style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard iclr style template upon update of their manuscript  the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. makerbased approaches could be employed for detecting the object  the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multiobject examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is selfoccluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)  other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging nontraining scenes with heavier clutter, nonseen lighting changes and occlusions to support their robustness claim  the proposed method cannot operate inthewild (e.g. youtube data) as it makes very strong assumptions about the required input data  the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene  the authors make no comparison to other unsupervised detection approaches (e.g. to the selfcited jonschkowski et al. (2017)) to prove shortcomings of other methods on the newly generated dataset  as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for egomotion estimation) makes applications to nonvideo data impossible, also none of the experiments exploit the nontemporal property of the approach to show single frame detection on a more varied set of scenes  the experiment showcasing the proposed application to ""learning from demonstration"" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task  the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)  the reviewer cannot follow the references to objectcentric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an objectcentric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion  the ""random search optimization"" discussed in section 3.3 is not a valid method as it ""solves"" this problem of instable training by picking the best of n runs with varying random seeds  figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach _reproducibility_  given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work."	2
_conclusion_  overall, the reviewer appreciates the effort that went into the work but sees considerable need for improvement concerning the motivation and possible applications given the strong assumptions that are made about the input data.	0
not so much for it being of strong interest to the majority of iclr attendees, but due to the fact that it deals with data of origin (finance) and properties (highorder markov chain dependencies) that have never been considered in the past.	0
3. in b.1 in the appendix, the street view house numbers dataset is mentioned, but no results appear in the main text, why not?	0
results are comparable to the bl model on existing artificiallybalanced data but significantly better on more natural unbalanced data with a large number of negatives.	0
strong points of the paper are: ' this gives a principled design of the objective function based on the mutual information between the input data point and output representation. '	2
that's only true if you care about data noise, but the endresult is still point estimation for the parameters with uncalibrated probabilities.	0
this lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results.	0
now for the weak points: (a) the justification for the training loss was not completely clear to me, although i can see that it has a variational flavor (b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to nextstep probabilities via the chain rule of probabilities, so we don't have a clear way to compare the tdvae models with jumpy predictions against other more traditional models (c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).	0
pros:  this investigation gives a significant amount of insights on gan stability and performance at large scales, which should be useful for anyone working with gans on complex datasets (and that have access to great computational resources).	2
bcrl outperforms ddpg and dqn when learning from fixed data, but bcrl is slightly worse than behavior cloning at learning to reproduce an expert policy that does not take exploration actions.	0
"cons: ' the learning setting is more like ""fuzzy"" behavior cloning from noisy data than offpolicy rl."	0
 experiments are ok, but on pretty small datasets, and for single hidden layer nns.	0
strengths:  first (i think) work on generic whitebox attacks against object detectors  interesting (and aesthetically pleasing) textures resulting from the attacks weaknesses:  evaluation was limited to one dataset and two object detectors.	2
2) presentation of the model in section 3 lacks sufficient explanation and heavily relies on high level remarks on how the model is developed, a more detailed explanation (even including how the wavelet coefficients are computed) could be far more useful than the schematic view of the network architecture presented in figure 1. overall, i think this paper provides a novel contribution for modeling event data specifically for medical data.	0
my only remaining concern is on the lack of reporting average performance on the test data (which used to be the norm until recently for papers submitted to ml conferences).	0
"quality: the writing of the paper needs more polishing; i saw grammatical errors here and there: for example, at the first paragraph of page 2, ""alternating"" should be alternative and ""synthetical"" should be synthetic... clarity: i have not been able to fully understand why the proposed (uniform) sampling variant of bn is better than previous effort at making bn less computationally expensive in a gpubased training environment by reading the paper: 1. the authors argue that the ""summation"" operation is the one that makes bn expensive; however, the authors have not demonstrated enough evidence of this argument 2. if ""summation"" operation is what makes bn expensive, then in a gpubased environment, can we simply divide the data into smaller batch, and train on each gpu using a smaller batch (this is way, each gpu is essentially calculating the statistics based on a subsample) 3. the authors discussed microbn, which ""alleviate the diminishing of bn's effectiveness when the amount of data in each gpu node is too small..."" this seems to show that in practice, training with bn does not suffer from having a large batch, but instead suffers from having too small batch size on each gpu node."	0
it is based not upon data poisoning attacks, but model poisoning attacks.	0
the ltvae not only learns the location of each cluster to best represent the data, but also their number and the hierarchical structure of the underlying tree.	0
the idea of using a structure on the latent prior of a vae to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.	0
this can help us not only better understand the models, but also the dataset (vqa) and the task in general.	0
cons:  the proposed model requires rich annotation of training data since all the components of the systems are trained in a supervised fashion.	0
in some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in realworld deployment.	0
we can optimize eqn3 very well on any dataset collected from our policy; but i don't expect deploying the vae to production with c(r') as the desired user response will give us anything meaningful.	0
yes, they won’t use image data, but at least they can use the same amount of crowd label information, which would make a nice comparison with the presented related work and proposed nn: this is what you can get using just image data during test (crowd layer, maxmig, and others from the current paper), this is what you can get using just crowd labels during test (majority voting or, preferably, more advanced pure crowdsourcing aggregators), and this is what you can get using both image and crowd labels during test (the proposed forecaster and aggnet, for example) questions out of curiosity: i).	0
# strengths  improvements over prior neural link prediction methods  clearly written paper  interesting analysis of existing neural link prediction methods # weaknesses  as the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, i believe a more careful ablation study should have been carried out.	2
the real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of n, but also because there is no comparison with the other approaches.	0
weaknesses ' in practice it seems as though stability may depend on not only choice of model architecture but also the data themselves.	0
i would argue that a good interpolant should not cross over a hole in the data manifold; none of the presented interpolants can satisfy this as they only depend no the start and end points, but not on the actual distribution of the latent points.	0
so if the data does not fit the prior or are not iid, then the proposed interpolants will most likely perform poorly.	0
for this methods the boundary when this effect occurs would be interesting to explore; figure 2 goes into this direction which is a quite nice study but the boundary is not explored further; by using for instance the english ner data.	0
the english data set was used but only to exploit it for the transfer learning.	0
 motivated by the observation that most of previous dimensionality reduction methods focus on preserving local pairwise neighboring probabilities and lack in preserving global properties, this paper proposes a method called trimap to optimize a loss function preserving similarities among triplets of data points.	0
however, there are some major concerns about method analyses and experimental evaluations, 1. data embedding based on triplets has been presented in (van der maaten and weinberger, 2012).	0
pros 6. the basic idea (“we should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data”, p.	0
my primary concerns with this specific paper are twofold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what i would expect to work on a new dataset (2) the tradeoffs aren’t well calibrated in the sense that i don’t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesn’t seem trivial to set tradeoffs).	0
accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?	0
"this suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations compared to what it would learn given all the data."""	0
3. the cifar10 results are bit concerning, and one can't help but wondering if the technique really only helps when the data has simpler shared structure.	0
from experimental viewpoint, they have lower performance than almost all competitors on clean data, but they are beating them when there is whitebox as well as the backbox threats.	0
the experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that relu variants do not perform much differently on smaller datasets, and only make a difference in very deep networks.	0
the tinyimagenet numbers seem a little off; i don't have personal recent experience with this dataset but this blog post claims 56.4 with normal relus in a vggish architecture.	0
 because all questions are generated via templates in particular (and plots also follow a certain generation process, albeit noisy), i am concerned that it would ultimately be easy to invert the process to crack the dataset.	0
few comments/questions come to mind:  for the critic loss l_d in equation (1) , the authors mention that the .gamma based second term (that should ensure that the critic outputs 0 for noninterpolated inputs and expose the critic to realistic data even if the ae reconstruction is poor) does not seem to be crucial in your approach but stabilized the adversarial training.	0
pros: 1. a novel algorithm that promotes the interpolation ability of ae 2. a new synthesized line benchmark to verify the interpolation ability of different ae variants 3. strong results on downstream classification and clustering tasks cons: 1. the interplay of the adversarial network (between ae and critic) isn’t very clear and can be improved 2. eq. 1, should x be x_1 or a new data other than x1 and x2?	2
for example, adp and adpt runtimes were very close on wikitext103 dataset but very different on billion word corpus (table 3 and 4).	0
 sec 3.2  first line: wrong figure reference  you refer to fig 2  but probably mean fig 1  page 3 bottom: by appending the encoded color and radius data we have a feature with shape 64x64xn > i don't quite see how this is true.	0
conclusion: overall, i think this paper has solid contributions; the proposed meshconv operator is simple but effective to handle spherical data; the experiment results demonstrate its advantages over existing methods on broad applications, which are convincing.	0
so this might be a poor choice for finding anomalies that represent malicious behavior (e.g., network intrusion detection, adversarial examples, etc.), but good for finding natural examples from a different distribution (e.g., data entry errors).	0
one of the motivations for defining the loss in the feature space is the lack (or difficulty to train) auxiliary classifiers on large amounts of data.	0
i have remaining reservations on significance, but move rating up a notch to reflect the extensive improvements and the authors' confirmation that they will release the data.	0
however the current paper proposes a method for generating such steps/data for lef reactions.	0
this dataset seems to be best suited to evaluating intent classification and slot filling (intentslot), but the current work fails to improve over what goo et al. 2018 report on this data.	0
" mayo clinic data also has two competing risks, but table 2 only shows the prediction performance for one risk, with the justification ""liver transplant prediction is not in our interest""."	0
pros:  natural, novel extension to gradientbased metalearning  state of the art results on two competitive fewshot benchmarks  good analysis  clear writing cons:  realistic, highdim data is only from the image domain minor questions for the authors:  relation networks are computationally intensive, although in fewshot learning the sets encoded are fairly small.	2
experimental evaluations on a standard large benchmark dataset of uspto show improved prediction performance compared to previous methods of jin et al, 2017 and schwaller et al, 2018. comment:  given that a chemical reaction can be regarded as a multistep chain of bond breaking and forming, thus the method part seems a quite natural extension of the past effort but also sounds rather incremental even though the performance gain exists.	0
"it uses the ideas of the semantic consistency loss and training on adapted data from cycada, but ""fills out"" the model by applying these techniques in both directions (whereas cycada only applied them in the sourcetotarget direction)."	0
table 3 helps to show that the model can scale up to the highresource setting, but it would also be nice to see the reverse: comparisons against existing work run in the limited data setting, to better understand how much limited data negatively impacts the performance of models that weren't designed with this setting in mind.	0
the second one also makes use of domainspecific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other.	0
however, the paper spends lots of effort explaining representations, but only a few sentences explaining about how the proposed representations/data structures can help find a somehow generic value iteration solution, which allows to efficiently compute/retrieve a particular solution once a .delta vector is specified.	0
not only synthetic data but also several popularlyused data and models are being conducted and compared.	0
my concern about the experimental results on missing data imputation is that strong competition such as gondra et al’17 and yoon et al’18 that report better results on uci than classical approaches are not included.	0
closely related previous work (neyshabur 2017) scaled to 128x128 resolution on a much more difficult dataset  imagenet dogs but the authors did not compare in this case.	0
the authors appear to draw the line for the fid score of real data at 0. but since it is being estimated with only 10k samples there will be sampling error resulting in nonzero fid score.	0
" this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
2. the paper: `the sparse recovery autoencoder' (sra) by wu et al. https://arxiv.org/abs/1806.10175 is related in that it learns both the sensing matrix and a decoder and is also focused on compressed sensing, but for nonimage data.	0
i have some questions: (1) one of the motivation proposed by gu et.al. 2018 is that spelling based sharing sometimes is difficult/impossible to get (e.g. distinct languages such as french and korean), but monolingual data is relatively easy to obtain.	0
2. but based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances.	0
mnisttomnistm has better baselines (pixelda performed better on this task for example), office is not suitable for domain adaptation experiments anymore unless one wants to be in a fewdatasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for nnbased domain adaptation); the results on cell were not convincing, i don't know the dataset but it seems that baseline nn does better than da most of the times. '	0
although the theoretical flops can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together.	0
"there is a claim that ""soseleto has superior performance to all of the techniqiues which do not use unlabelled data"", however i'm not sure whether these techniques were used as prescribed and if the comparison was fair."	0
maybe you could have the same noisy dataset but with a small portion of random points having the wrong label.	0
 the paper builds upon deep image prior (dip)  work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc. the paper proposes a new architecture for the dip method which has much less parameters, but works on par with dip.	0
intuitively the objective wants f_0 to generate samples which, when mapped back to the x domain, have high logprobability under g, but its samples cannot be differentiated in the case of discrete data.	0
they bounds are loose, but not vacuous, and follow the same order of difficulty on a handful of datasets as the true generalization error.	0
cons:  the datasets used in the paper are quite simplistic.	0
cons or unclear points: 1) why the paper does not include all biological datasets (6 datasets in total, only 4 used in this papaer) presented in (verma & zhang, 2018) in the experiment section.	2
 data augmentation is a useful technique, but can lead to undesirably large data sets.	0
of course this now involves training the entire cnn on the augmented dataset, rather than just the last layer, but how relevant is the two stage training approach that the authors propose?	0
"this is especially important on the real robot case, where the authors correctly mention that the wam arm cannot be expressed exactly by the manipulator equations; this makes it all the more important to try identify system parameters via a datadriven approach, not with the hope of finding the exactly ""correct"" manipulator equations, but with finding some that are good enough to outperform the ""analytical"" model that the authors mention."	0
'pros'  original idea for modelling distribution of sequence data  theoretical convergence in the jensen shanon divergence sense  promising experiments 'cons'  no major cons to the best of my knowledge 'typos'  it would be very nice to have black and white / color blind friendly graphs  eq 10 too long  introduce j_m & j_g in sentence  coma at the end of eq 5, and maybe align generator and discriminator in some position (e.g. at the semi colon).	2
the epistemic uncertainty, coming from the lack of data, can be gained through monte carlo sampling of the dropoutmasked model during prediction.	0
paper weaknesses:  it would be more interesting if the dataset was created using multiple types of probe objects.	0
the authors claim that their approach, dymon is adapted to the many challenges of biological highthroughput data sets: noise, sparsity and lack of temporal resolution.	0
strengths  thorough analysis with a good set of questions weaknesses  some peculiar evaluation and presentation decisions  introduces 'yet another' synthetic visual reasoning dataset rather than reusing existing ones i think this paper would have been stronger if it investigated a slightly broader notion of generalization and had some additional modeling comparisons.	2
pros:  possibly, simple yet effective solution to handle time series data with missing values.	2
 strengths:  the paper identifies a valid limitation of the maml algorithm: with a limited number of gradient descent steps from a single initialization, there is a limit to the ability of a fixedsize neural network to adapt to tasks sampled from a diverse dataset.	2
"aside from the lack of clarity about notation/what symbols represent, i have two main issues with appendix b. first is the claim ""note that x and y are independent as well as yi and yj when i neq j""; i think it should be made clearer that x and y are independent only in this setting because the experiments are with random data, and it does not seem obvious to me that yi and yj are independent."	0
"""conducting memorization tasks, rather than inferencing with unseen data"" > saying ""generalization tasks"" would be more accurate; maybe this is being too picky but they can inference with unseen data just fine wihtout requiring more parameter precision; it's only if we care about the generalization performance that they require more parameter precision."	0
that cqt outperforms stft on musical data seems to be a well established result already, but this offers further validation.	0
i) try not use gan, but use mixup (linear interpolation of samples) as data augmentation, and go through the studentteacher training.	0
a fundamental distinction between parametric and nonparametric tests for cpd in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structuree.g. a first order autoregressive model introduces a simple markov structurewhereas nonparametric kernel tests typically imagine samples to be iid (before and after the changepoint).	0
for this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons x, y, z. unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal.	0
the experiment on generalization is using the test set for training to make use of small amount of data, but this makes things completely not comparable.	0
the method is shown to be effective on spherical mnist, modelnet, stanford 2d3ds and a climate prediction dataset, reaching competitive/stateoftheart numbers with much less parameters.. less parameters is nice, but the argument could be strengthened if the authors could also show impressive results in terms of runtime.	0
the author reported 1000 points for modelnet which is ok for that dataset but definitely too small for indoor scenes.	0
"in section 4.2 you use phrases like ""concatenated bilingual data"" but i couldn't find an explicit statement that you were cotraining on both language pairs."	0
 the paper ‘learning discrete wasserstein embeddings' describes a new embedding method that, contrary to usual embedding approaches, does not try to embed (complex, structured) data into an hilbertian space where euclidean distance is used, but rather to the space of probability measures endowed with the wasserstein distance.	0
pros: 1. wellexecuted paper, with convincing empirical results on the newly collected dataset.	2
with respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using).	0
the observation is that when two boxes are disjoint in the model but have overlap in the ground truth, no gradient can flow to the model to correct the problem (which is happens in case of sparsedata.	0
" ""deferent augmentation modes"" > different  ""inspite"" > in spite  ""reprort"" > report  ""utlize"" > utilize  ""computer the convolution"" > compute # conclusion although the altaz convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper."	0
 section 3.3: your goal here seems similar to the goal of clark and gardner (2018), trying to correctly calibrate confidence scores in the face of squadlike data, and similar to the goals of adding unanswerable questions in squad 2.0. i know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.	0
the core idea of mixing the latent spaces of two data samples is interesting and the results seem to indicate that it improves generalization, but i have two main criticisms of this work.	0
# lack of explanation for the relevance to active/transfer learning removing the bias in the training data seems to be relevant to active learning setting which finds informative points from the training data in general ml.	0
# lack of further analysis of between bias and performance you propose an architecture that can remove the bias of data.	0
# little or ambiguous impact of the result the accuracy difference in table 1 seems to be very minor and difficult to understand it due to lack of additional information about (1) how much bias each target dataset has, (2) a statistical test, and (3) real examples where the adversarial model can only answer.	0
my concerns are as below:  how is the accuracy computed for the experimental data?	0
as mahe trains mlps per data sample and searches through all interactions for finding contextfree explanations, this raises concerns.	0
pros: the proposed method allows to accurately impose an energy constraint (in terms of the proposed model), in contrast to previous methods, and also yields a higher accuracy than these on some data sets.	2
in such setups, the scarcity of actionlabelled data is not a real concern.	0
clearly it does still depend on generation of synthetic data, but that is more a different task (as described in section 2).	0
the domain adaptation part evokes data augmentation strategies of sussillo 2016 but that is not compared.	0
p.2, “(gans) have been shown on images to provide better approximations of the data distribution than other generative models”: this statement is earthier too strong (all other models) or does not say much (some other models) p.2, “however, this means that they are unable to learn localized features or exploit weight sharing.”: i see the point about no weight sharing in the generator, but feature learning p.3, “the key difference with the work in this paper is that pointnet and pointnet are not generative models, but are used in supervised problems such as classification or segmentation.”: yet, the kind of operation that is used in the pointnet is quite similar to what you propose?	0
2  those more familiar of the graph convolution literature will be more familiar with gcn [kipf et al. 2016] / graphsage [hamilton et al. 2017] / monti et al [2017] / etc.. most of these approaches are more restricted version of this work / hartford et al. so we wouldn't expect them to perform any differently from the hartford et al. baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work.	0
maybe that's ok, you have enough data to learn this, but i would at least like to see plots showing that indeed your model is capturing these important features.	0
you also claim your model can handle the anomaly on 4 november, but in preprocessing your data you excluded 15 days that were anomalous.	0
cons and questions 1. the theory in section 4 suggests that the degeneration problem originates from underfitting; i.e., there's not enough data to fit the embeddings of the infrequent words, when epsilon is small.	2
this involves initializing the student weights to have the same singular vectors as the training data inputoutput covariance but with all singular values equal to some amount epsilon.	0
but there are some weak points in this proposal: 1. it's well known that spectral methods are frequently sensitive to perturbations of the datasets.	0
overall, to use the underlying manifold structure to align data batches is an interesting and straightforward proposal, but i hope the authors can address these question carefully and make the argument stronger.	0
pros: 1) a simple and reasonable formulation 2) visually good reconstruction of samples and convincing interpolation between samples on the celebahq dataset.	2
 “method more greatly resembles the original data than other ganbased methods” method does not resemble data  “due to their exact latentvariable inference, these architectures may also provide a useful direction for developing generative models to explore latentspaces of data for generating datasets for psychophysical experiments.” this is mentioned a few times, but never supported  acknowledgements should not be in the review version (can violate anonymity) 5) minor: why is a gaussian around the midpoint used for interpolations?	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
# lack of further analysis of the dataset data collection part itself seems to be the biggest contribution to this work.	0
 this work is concerned with the problem of batch contextual bandits, in which a target contextual bandit policy is optimized on the data generated by a different logging policy.	0
pros: using vaes for modeling class conditional distributions for data is an exhaustive approach.	2
2) using vaes to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet?	0
however, i have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below).	0
the second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.	0
https://arxiv.org/abs/1810.01392 pros:  interesting observation of density modelling shortcoming  clear presentation cons:  lack of a strong explanation for the results or a solution to the problem  lack of an extensive exploration of datasets	2
 the last paragraph of the related works section mentioned some related work with shortcomings as working only on lowdimensional data and features of specific types, yet the experiments are also mostly done on lowdimensional datasets.	0
"it is doubtful whether the importance of activation of neurons based on ""current data"" is sufficiently verified in sequential learning (in the experimental section, avg performance for importance weight sometimes appears to come with performance improvements but not always)."	0
then the experiments show such a definition is actually not correct, but rather a datadependent one.	0
 this paper is about training a neural network (nn) to perform regression given a dataset (x, y) 'and' a black box function which we know correctly maps from some intermediate representation to y. instead of learning a nn directly from x to y, we want to make use of this black box function and learn a mapping from x to the intermediate representation.	0
the authors propose to train in three different ways: (1) offline training: train an auxiliary nn that approximates the black box function based on data generated by sampling the input uniformly (or similar); then train both the auxiliary nn and the argument extractor nn together endtoend using (x, y) data, (2) online training: train the auxiliary nn and the argument extractor nn together, based on (x, y) data; data for training the auxiliary nn comes from the argument extractor nn during the main training, and (3) hybrid training: pretrain the auxiliary nn as in (1) and then train both nns as in (2).	0
experimental results show:  this approach leads to better performance than regressing directly from x to y in the small data regime,  this approach leads to better generalization (being able to add more image numbers during test),  this approach learns faster than an actorcritic based rl agent,  this approach can be useful even if the functionality of the blackbox function inherently cannot be estimated by a differentiable function (lookup table)  the resulting argument extractor nn is useful when used with the nondifferentiable black box function,  hybrid training is the best; offline training is the worst,  penalizing low output entropy helps.	0
this inductive bias comes with a strong benefit when the assumption is true  as demonstrated in the toy dataset experiment  but when it is not true, the visualization would strongly distort the underlying structure of the model.	0
i suspect that the while drpr might be good at visualizing relationships between class labels  especially which class can be easily confused with another  but would be worse at faithfully representing each data point, especially the ambiguity of class labels on individual ones.	0
pros: ' extensive theoretical and empirical analysis ' simple idea that generalizes to multiple usecases, which implies robustness of the approach as a methodology cons: ' unimodal assumption is likely not realistic, which would result in misleading visualization of data ' visualization analysis focuses on how classrelationships are preserved rather than faithful representation of each data point, which is a wrong target ' synthetic experiment is conducted on a single, too simplistic one; more examples are needed to understand the capabilities of the model in more detail ' the bias of knowledge distillation is not controlled	2
the cost of the sparsification is essentially the application of the trained neural network to a small number of data points in order to compute the sensitivity scores pros:  the method works empirically, in that their empirical evaluations on mnist, cifar, and fashionmnist classification problems show that the drop in accuracy is lower when the neural net is sparsified using their corenet algorithm and variations than when it is randomly sparsified or the neural network size is reduced by using svd.	2
it seems surprisingly spot on 0.5 as in f&s for synth vs. synth, but looking at the data in fig. 12 (rightmost panel), i find the value 0.5 highly surprising given that all the blue points lie more or less on a straight line and the point at a scaling factor of 0.5 is clearly above chance level.	0
the evaluation focuses on simulated robots, however gathering nonexpert data on real robots would be very expensive, so the approach would not make a lot of sense here (even if we replace trpo a more sample efficient rl method).	0
re: the synthetic dataset 1. it's nice that you've tested out different single synthetic edit patterns, but what happens in a dataset with multiple edit patterns?	0
pros:  this paper proposed two different data representation for the tokens in text, implicit and explicit.	2
questions: 1. the content of text supposed to be programming language, but neither the model design nor synthetic dataset generation specify the type of text.	0
on the positive side, the paper is wellwritten and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets.	2
(this is similar to the mode collapse problem that often occurs with gan training in practice, but at least a gan generator is required to exactly match the full data distribution to achieve the global optimum of that objective.)	0
2.4 (on ama) it’s claimed that “one would need large minibatches for generating a good estimate of the mean features...this can easily result in memory issues”, but this is not true: one could trivially compute the full exact dataset mean of these fixed features by accumulating a sum over the dataset (e.g., one image a time, with minibatch size 1) and then dividing the result by the number of images in the dataset.	0
on (2), i did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset.	0
(and yes, i saw the celeba results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old dcgan results, i can't tell.)	0
my first concern is the training data.	0
the authors claim to justify (2) by saying that the extra g_t  h_t term is o(.sqrt{t}), but the whole appeal of adaptive algorithms is that the .sqrt{t} terms are datadependent.	0
taking the action sequence as an input (which is one of the main novelties in the paper) is likely to require a lot of data, and maybe this is fine on relatively simple mujoco tasks but i see it as a potential issue when trying to expand this to more realistic problems.	0
the greedy attack is more successful than the delete1 attack, however it is a straightforward application of greedy optimization on discrete data and is not very novel or interesting.	0
it's true they do, but they are being updated on the basis of the unilateral opinion of the selected component about the likelihood of the data.	0
the point is not (at least yet) that these methods are competitors to causal inference methods that do “require explicit knowledge of formal principles of causal inference,” but rather that we have a proofofconcept that some elementary causal understanding may emerge from typical rl tasks when agents are faced with the right kinds of tasks and given access to the right kinds of data.	0
2. the experiment lacks of real data.	0
general concerns: any method for improving the performance of an optimization process via additional preprocessing must show that the additional overhead incurred from preprocessing the data (in this case, organizing the minibatches) does not negate the achieved improvement in convergence time.	0
"""implicit bias"" hypothesis appears in many places, for instance in work of nati srebro and colleagues (""the implicit bias of gradient descent on separable data"" (and followups), ""exploring generalization in deep learning"" (and followups), and others); it can also be found in variety of recent generalization papers, for instance again the work of srebro et al, but also bartlett et al, arora et al. e.g., arora et al do detailed analysis of favorable biases in order to obtain refined generalization bound."	0
for experiments, i would really like to see experiment showing fourier coefficients at various stages of training of standard network on standard data and standard data but with randomized labels (or different complexity in some other way).	0
the authors claim “this work is the first to apply modern object detection deep learning approaches to document data” but there are previously published works.	0
3. lack of assessment of the dataset characteristics and how they relate to algorithm performance.	0
these 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a blackbox).	0
"i suggest that without evidence for this loss you soften the claim to ""this is intended to help encourage the model to learn a heirarchical tree structure"" figure 4: it appears that each row indicates a different video in the dataset, but then in 4f you still have two rows but they appear to correspond to different algorithms... a vertical separator here might help show that the rows in 4f do not correspond to the rows in 4ae."	0
cons: the idea is only tested on relatively simple dataset.	0
" (a.6) can you comment on the pros & cons of ""label regression"" for classification and how does it compare with approximate inference when softmax is put on top of a gp (perhaps illustrating by a simple experiment on a toy dataset)?"	2
pros:  this work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))a very interesting configuration of weak supervision, an increasingly popular and important area  the treatment is thorough, proceeding from establishing the minimum number of u datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments cons:  this is a crowded area (as covered in their related work section).	2
thus this work connects to many areas both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multisource weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).	0
 other prior work here has handled k classes with k u sets; could have extended to cover this setting too, since seems natural overall take: this learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.	0
pros:  create a labeled dataset for binary code vulnerability detection and attempts to solve the difficult but practical task of vulnerability detection.	2
cons:  the operation that creates dataset may introduce bias or variance.	0
pros: presentation of new application of representation learning models construction of a new dataset to the community for binary software vulnerability detection the proposed model shows a good performance cons: the presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.	2
from the last remark, it is unclear for me if the dataset is representative of binary code vulnerability problem the proposed architecture is reasonable and maybe new, but i find it natural with respect to existing work in the literature.	0
however the notion of vulnerability is not defined and it is difficult for me to evaluate the interest of the dataset.	0
the architecture is general enough to work on other problems/tasks  which is good  but the authors focus on the binary vulnerability code dataset in the experiments.	0
i understand that some technical assumptions are needed in a theoretical work, but i would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible.	0
the results depend on the property of data matrix, but not the output values.	0
 my main concern is that the pretraining of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data.	0
so this paper achieves comparable results to gail but uses much less data amount.	0
"an example application is the ""wikispeedia"" dataset, in which nodes are connected in a graph, but a datapoint (a wikispeedia ""game"") consists of a sequence of nodes that are visited."	0
a comparison of methods on an important dataset is no doubt useful, but i don't think iclr is the right venue.	0
pros and cons () interesting idea () diverse experimental results on six datasets including benchmark and realworld datasets () lack of related work on recent catastrophic forgetting () limited comparing results () limited analysis of feature regularizers detailed comments  i am curious how we can assure that svm's decision boundary is similar or same to nn's boundary  supportnet is a method to use some of the previous data.	2
cons  the idea of storing a small subset of a original dataset for each task has been already explored in [nguyen et al. 18], and thus is not novel.	2
the paper doesn't introduce any novel concept or technique but it simply compares two established techniques on mnist dataset.	0
my overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the sasnet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.	0
forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.	0
however, the synthetic regression task is a nice proofofconcept, but thorough regression evaluation could perhaps include the boston housing prices dataset or some uci datasets.	0
it is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected).	0
pros and cons () sota performance on the dstc2 dataset.	2
"note: the authors cite a paper in the introduction ""hierarchical attention transfer network for crossdomain sentiment classification"" (li et al 2018) which also achieves state of the art results on the amazon reviews dataset, but do not compare against it."	0
[pros & cons] () this paper tries to extend dual learning from word level to hidden state level; () multiple languages are involved in this framework; () experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used wmt datasets; () the paper is not easy to follow.	0
the result in table 5 shows that odd can outperform erm for real world datasets, but the improvement seems to be marginal.	0
concerning the experiments: in my opinion the picture of the dataset ant and cheeta is irrelevant and could be removed for more explainations of the method.	0
cons experiments are only limited to small scaletraditional graph datasets.	2
here are some specific concerns: 1) i could not find a link to an opensourced version of the dataset(s).	0
i tried hard to discover a 'detailed accuracy number on a benchmark dataset with unchanged setting' but found only case 4. by ‘unchanged’ i mean it is not a subpart of the whole dataset, or using a rarely seen nn architecture.	0
in fact, i wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar).	0
(2) they also perform human evaluation of personalitycaptions and found that humans tend to prefer these captions over the neutral captions pros:  the paper introduces a first largescale dataset for personality captions that has 215 traits and ~200,000 images.	2
 the analysis of models is done not only on the personalitycaptions dataset but these models are also evaluated on the traditional coco dataset.	0
cons:  the paper doesn’t analyze and compare the benchmark with existing datasets.	0
'this dataset was not available at the time of the submission, but for the revision it would make sense to also evaluate on the new xnli dataset of conneau et al. (emnlp 2018) for multilingual nli experiments.	0
 the work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely elmo in this paper) on many downstream tasks, including glue tasks, but also alternative nlp tasks.	0
my only concern is about the evaluation on metaqa, which seems a not widely used dataset in our community.	0
cons: although the classification signal is counted as the advantage of this system, it is not clear how it will adopt to multiclass scenarios which is one of the major applications of segmentation (such as sun dataset).	0
for lowshot learning on omniglot, the proposed method is outperformed by [antoniou et al, 2017] at all values of k. more importantly, i’m concerned that the comparison between the two methods is unfair due to the use of different dataset splits, as demonstrated by the drastically different baseline accuracies.	0
3. the lack of analysis on domainclass dependency of each dataset makes the analysis of experimental results weak.	0
the result on the subset of atis helps, but it is less solid than the evaluation on wikisql because the dataset is too small (only 126 examples in the test set) and there are no external baselines.	0
however this has been done by a number of authors:  vlachos and clark (2014): http://www.aclweb.org/anthology/q141042  berant and liang (2015): https://nlp.stanford.edu/pubs/berantliangtacl2015.pdf while the idea of using such oracles for structured prediction tasks in nlp was first proposed by daume iii et al. 2009: https://arxiv.org/abs/0907.0786 furthermore, it has been applied for rnn decoding in nlp, see: https://arxiv.org/abs/1511.06732 apart from this, some further comments:  the subset of sql tackled in this paper is less expressive than what has been done in previous work on atis and geoquery datasets.	0
it lacks a bit in structure (e.g., i would introduce the problem and dataset before introducing the model) and sometimes fails to explain what insights the authors draw from certain results, or why certain results are reported.	0
or, if you're concerned about some of the overlyspecific language in more domainspecific wikipedia articles, then you could restrict the dataset to be the 100k most frequentlyvisited wikipedia articles or something like that.	0
experiments are carried out on the ucf101 dataset, in the whitebox and blackbox settings, and show that the method is able to effectively attack the system.	0
however, i have two concerns: one is that the test dataset is very small.	0
based on the fact that your method has a high correlation with dive  delta s (chang et al. 2017), i guess that cmd does not work very well when the dataset contains random negative samples, but work well when all the negative samples are similar to the target words.	0
this might be true for most real world dataset, but i believe the degree of clusteredness may vary by dataset.	0
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
in general, the paper is not showing stateoftheart results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.	0
weaknesses: 4. related work: 4.1. the paper mentions the zeroshot vqa work from teney & hengel, however, given that is one of the most related works, i expect a more thorough discussion of the similarity in dataset and method.	0
the author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fmri?	0
what would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet?	0
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
cons:  all of the experiments were done on toy datasets.	0
it is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be ok, but it seems to be a fairly straightforward extension of existing models.	0
weaknesses: the main complaint i have is that the authors only use the mnist dataset.	0
i understand that what i ask for is very difficult to answer, but experiments with more datasets and different types of queries (such as satisfiability) might have made me happier.	0
experiment: the authors conducted experiment mostly on youtube8m dataset, but did not specify which version they used.	0
the hierarchical rnn model may be effective on the youtube8m dataset, but other models such as the twostream model from simonyan and zisserman and i3d from carreira and zisserman are popular on other datasets such as kinetics.	0
the authors make a secondary claim that they are able to improve upon irgans via their proposed approach but then they do not substantiate these on all the datasets which seems like a notable oversight.	0
i am more concerned about the situation where the capacity of the model is challenged by the size of the dataset.	0
the authors report that this architecture produces better results than previous methods on one of the qm9 dataset properties (u0) but performs worse than many of the other reported methods on other quantities.	0
for example: 1) the authors claim that, compared to neural models, their model has the advantage of the interpretability (for small datasets), but they also have neural components in their model.	0
 pros: this paper  proposes a method for producing visual explanations for deep neural network outputs,  improves quality of the guided backprop approach for strided layers by converting stride 2 layers to stride 1 and resampling inputs (improving on a longstanding difficulty with such approaches),  shows fairly rigorous experimentation demonstrating the applicability and properties of the proposed approach, and  releases a new synthetic dataset and benchmark for visual explanation methods.	2
existing video codecs such as h.265 and software like ffmpeg are optimized for longer, highresolution videos, but even the most realistic dataset used here (kinetics600) only contains short (10 frames) lowresolution videos.	0
of huge concern to me is the fact that very good results are presented on the sprites dataset.	0
second, this paper studies memory network in particular, but memory network is not used for multihop reasoning in a real dataset.	0
it also lacks generality as only one (proprietary) dataset is used.	0
however for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones.	0
for example, the method outperforms random sampling for small datasets, based on number of samples, but what happens when you look at execution time?	0
it's concerning that in fig. 1 convergence on a toy dataset takes more than 2000 iterations.	0
i’ve seen work using a reducedmnist dataset, which is probably created by random subsampling, but still more relevant than many of the aspects of embeddings cited in this section (the paragraph about arithmetic operations for instance).	0
this however is conceptually not nice since there is no uniformity across the dataset.	0
the paper unfairly dismisses prior work by making factually incorrect claims, e.g. section 2 claims “indeed, papers like (hernandezlobato & adams, 2015; gal & ghahramani, 2016; lakshminarayanan et al., 2017; kendall & gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of rmse.	0
pros: 1. the interpolation performance on the middlebury dataset is good.	2
review: pros: the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.	2
"cons:  details are lacking about the ""anon"" dataset introduced in this paper (where do the photos come from and the labels, visualization of a few examples...)  there are not many technical issues discussed in the paper and that is fine as the main idea is relatively simple and its effectiveness is mainly demonstrated empirically, but i feel the paper is missing a discussion about the importance of the initial classifier trained to estimate the target prior probabilities for the source labels and whether it is crucial that it has a certain level of accuracy etc.  the approach in the paper implies a practitioner should have access to a very large target dataset and the computational and time resources to appropriately pretrain a complex network for each new target task encountered."	0
[cons & details] (1) as stated in the abstract, “…, but that the improvement diminishes as the size of data grows, indicating that powerful neural mt systems are capable of implicitly modeling roleword interaction by themselves…” (1) the main concern is that, considering ril does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline.	2
in the digit dataset, could you give some explanation on (1) why improved 3 achieves 2.8% improvements compared with improved 2 on the first task, but achieve marginal improvement (0.9% for the second task, and 0.3% for the third task) and even worse result (0.5% for the last task) on other tasks?	0
overall rating this paper suggest interesting observations and useful dataset, but provides relatively less analysis on these observations.	0
the results on synthetic data seems promising, but there is insufficient evaluation being performed on real and larger dataset where the mode collapse problems are more likely to happen.	0
you have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.	0
pros: ' this work set a strong baseline for the retrieving target paragraph for question answering on the squad dataset. '	2
the same experimental setup has been extended to use cifar10 as an additional, more realistic dataset, the use of potentially more powerful lstms as well as grus, and several runs to have more statistically significant results  which addresses my main concerns with this paper originally (i would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying).	0
so although they wrote expectation wrt p(x) in (3), the p(x) cannot be a large dataset like the celeba dataset as they intended, but p(x) is limited to a small dataset like scutfbp.	0
authors could have reported such a comparison on xray dataset too but they did not.	0
unfortunately the paper falls short in two main areas:  novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on velickovic et al 2017)  impact: the results achieved in the experiments are very small improvements compared to the baseline of rgcn (~ 0.01 in two experiments and ~ 0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyperparams, ...) however, on a positive note, the paper has been written very well and i really liked the frank discussion on page 8 about results on mutag dataset.	0
your method may generalize, but how do we know that if you've only tested it on one small dataset?	0
 as identified by the authors themself, lacking of supporting experiments on largescale dataset and realworld models.	0
in figure 3, editing in performed on the celeba dataset, but again, subjective comparison among the gan's is precluded by the fact that different examples are chosen.	0
weaknesses:  the improvement over the existing method is incremental;  the regularization on routing decision may not really be necessary as, in dndf, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the dndf paper;  the datasets tested are standard image datasets, not even captured from vehicles or video surveillance.	0
the increments presented are reasonable and justified, but the experimental results, specifically on the larger datasets, warrant further investigation.	0
[overall] it’s great that ntp was scaled up to handle larger datasets, however further analysis is needed.	0
they synthesize their mnist dataset but corrupting a subset of mnist digits with noise and treating actions as rotations.	0
the authors evaluate their method by examining reconstructions of the mnist digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding).	0
overall thoughts: using datasetspecific features for evaluation metrics makes a lot of sense, but i don't feel totally satisfied by this paper's investigation of the specific proposal of a vae, and am particularly worried about whether the metric just ends up preferring models similar to that vae.	0
i found the paper difficult to understand, but as far as i understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.	0
strengths:  i appreciated the quantitative comparison on the lowd toy datasets (2d and 3d mogs), reporting modes captured, and jsd.	2
results on mnist are good, but of course this is a very simple dataset.	0
my only concern is the experiments: 1) some of the benchmark datasets for the proposed task as well as some wellknown methods (see battaglia et al’18 and references in there) are missing.	0
pros:  provides insights on why adversarial training is less effective on some datasets.	2
"in summary: pros  claims sota results on two good benchmarks for zeroshot learning  approach is original cons  paper lacks a lot of methodological and experimental details some minor details:  ""we found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset."""	2
the experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.	0
while the paper does present a new dataset built in minecraft which is suitable for demonstrating the strengths of the proposed method, the reviewer does not find this significant.	2
i think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes.	0
little is known about the newly collected datasets, it is not clear how to interpret the relative improvements in validation loss compared to the original metrics of allamanis et al 2016, and the paper lacks necessary comparisons to othef pretrained embeddings, so though the overall claim that pretrained embeddings should be used for this task seems to hold up, it is not clear that this is a complete argument for the method chosen by the authors.	0
the model outperforms other multitask seq2seq models on the snips (goo et al., 2018) dataset, but is still behind the traditional slotfilling models (goo et al., 2018).	0
my main concern is that it is unclear to me how cds (crossdomain schema) can be generalized to the other semantic parsing datasets, e.g., the overnight dataset (wang et al., 2015), which also contains multiple domains.	0
there is several interesting ideas and a new dataset introduced, but i would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review.	0
and it seems the simplest model is working best because, for the size of the dataset (small, unlike standard cv data), the nonsimple models are overparameterized and more nonlinear (for the lack of a better phrase) to train?	0
" please explain this line ""lpnnobif degrades with large t, and even .tau slightly above 1 makes a difference""  finally, please explain the trend in the results in table 1. for example, why is the performance of the proposed method poor on the flickr dataset, but better on the dblp dataset?"	0
experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.	0
so i have concerns whether this approach is able to generalize over different datasets.	0
overall, i’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.	0
" perhaps tailoring the mal lists to each specific dataset would make sense (i understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield ""better"" results)."	0
"for example, it is unclear to me what ""weight space traversal"" means, ""training size"" is mixed with ""dataset size"", and ""we will show that convergence ... to final weights"" seems to be a trivial comment (unless there is some special meaning of ""convergence rate""), etc. it also lacks some clarity and organization in the results  some more summarizing comments and sections (and in particular, a separate and clearer conclusion section), as well as less repetitions of the qualitative comments, should largely improve the readability of the paper."	0
(hope i haven't missed anything) weaknesses: recent datasets for action recognition, e.g., moments in time, charades, youtube8m etc, are missing.	0
i understand that there are no ground truth cts factors for mnist/fashionmnist, but this makes me think that a dataset such as dsprites (aka 2d shapes) where the factors are known and has a mix of discrete and continuous factors would have been more suitable.	0
pros:  tests on several datasets  seems fairly generally applicable.	2
strengths  strong results  testing on both baglevel / singlelevel relation extraction  insights via multiple ablations — variation of the number of layers, exploring densely connected data subsections with cycles to identify examples for multihop inference  evaluation on new, humanannotated test set issues  evaluation only on one task, and one dataset (although, with more detail).	2
the lack of experiments outside of one video action classification & captioning dataset (and one additional one for a transfer learning study) limit the empirical generality of the findings.	0
they make it sound like the authors introduce a new task and dataset for evidence scoring, but instead, they merely train on squad with existing annotations.	0
originality: pros: the suggested model outperforms others on three qa datasets.	2
weak points: ' the evaluation is limited to only mnist datasets. '	0
originality: pros: the suggested model outperforms others on two datasets.	2
 the authors mention two types of language models (word and character level), and also 4 text datasets to train the lms on, but do not provide results for all combinations.	0
and finally, the authors claim that most modern datasets can fit to the accelerator memory if reduced 10x, but in my experience the network and it’s activations (which are stored during training) occupies most of the gpu memory even on high end accelerators, not leaving enough space to store a large dataset even after data reduction.	0
first, the experiments are only done on a small scale dataset (cifar10), which is ok in general, but questionable when the proposed method explicitly targets big data regime and making the training faster.	0
i’m also very surprised not to see the moment of training mode transition on the plot (i.e. the moment when the model switched from restricted dataset to the full one), the lack of it can indicate an implementation error.	0
the collected dataset, which is a contribution of the paper is also poorly explained.	0
2) experimental results the experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of stateoftheart competitors among the very many related methods published recently (e.g. [28], just to mention a small subset).	0
on the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the fig. is very confusing.	0
in summary, my evaluation is as follow: 'pros'  pretty original problem formulation  generally well written paper 'cons'  lack of comparison with simple baselines in basic dataset distillation setting  use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated  possibly a mistake in the theoretical analysis of the linear case indeed, i found the paper to be generally quite clear and enjoyed reading it.	0
cons i feel, the authors should have extended the experiments to imagenet which is a much larger dataset and validate the findings still hold, i feel the discussion section and comparison to other methods needs to be worked to be more thorough and to tease out the benefit of each of the various terms added to the loss functions as currently all we have is final numbers without much explanation and details.	2
i don’t think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.	0
cons  some of their gains are due to choice of dataset for training or removing the first principal component  advantages that other comparable models may or may not have.	2
is there any reasoning why this method performed slightly poorly on this particular dataset.	0
the relation to dataset shift was very interesting, but it’s unclear what’s the exact connection between the proposed algorithm and the dataset shift.	0
"(3) lacking solid experiments: in section experiment, the authors claim ""finally, we show the tradeoff for pruning resnet50 on the ilsvrc dataset."	0
 it would be beneficial to provide some more insight into the statistical tests casually reported (i.e., where did the p values come from)  the dataset appears to be available online but will the code for the gala module also be published?	0
significance:  clustering is an important problem and the authors show some results, but on only three datasets: mnist, newsgroups, reuters.	0
main concerns:  what is the stateoftheart on mnist and their other datasets?	0
 derived similarity metric doesn't require knowledge of the entire dataset (in comparison to sif  pca) cons  performance seems to be slightly better than sif, wmd, and averaging word embeddings, but below that of sif  pca  unclear motivation for the model, was it derive an online similarity metric that outperforms sif(without pca)?	2
suggestions: (1) to show that current salient random variables do not make the dataset theoretically uniform but are still approximate enough, why not construct some distinct heldout salient variables (such as memory/grid/marker query times, executing time) from existing ones, construct narrower test sets accordingly, and hopefully show uniform models still perform significantly better than baseline?	0
in the case of face images a reasonable argument can be made, but in a dataset such as cifar10 how should we linearly interpolate between a horse and a car?	0
"the paper notes ""novel stylizedimagenet dataset"" and shows that models can learn shape/texture features both but there is not much detail/explanation on why ""stylized"" is the novel approach and also the methodology of constructing data by replacing with painting from adain style transfer (huang & belongie, 2017) is not discussed/explored."	0
i believe otherwise the results are incomplete since fig. 4 shows the biases of these models on in dataset but doesn't show if these biases are removed by training on sin.	0
i agree that this is a useful area to study and direction for the community to go, but as the introduction of this paper states, this is the most interesting when the parties have control over logically separate components of the modeling pipeline and also when joint training of the components is being done, potentially on disjoint and private datasets.	0
for the more realistic kinetics dataset, the proposed method is competitive with h264 and h265, but only in a very limited range of bit rates.	0
supportnet is compared to five methods (all data: network is retrained with new and old data, upper bound for performance, icarl: stateoftheart method for incremental learning, ewc: only ewc regularizer added, fine tune: only new data, random guessing: random guess to assign labels) on six datasets (mnist, cifar10, cifar100, enzyme function prediction, hela subcellular structure classification, breast tumor classification).	0
(2) the authors claim that icarl suffers from overfitting on real training data (section 4.1) however table 2 shows icarl only on the enzyme function prediction which is also the dataset where the difference in performance between icarl and supportnet is the largest.	0
to ensure reproducability of your results i ask you to provide the respective codes e.g. on github (can be done anonymous)  repeating myself from the last review: there is a lot of work addressing that making a kernel psd may not be good idea  you provide experiments for a small number of data where your kernel is now psd but what is with the other data (where e.g. in pekalska and followers it was shown that making them psd is bad ... )  is your approach solving this  or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?	0
the results on the taxibj dataset (not tatxtbj, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.	0
regarding the methodology for training the classifier, i am not familiar with these datasets but using just a 1/10 of the data to train classifier seems a bit extreme ?	0
 cons  the experiments are conducted thoroughly in the imagenet, but the selection of the dataset is not appropriate.	2
the proposed approach is interesting and promising, but the selection of the methods and datasets to be compared is not appropriate.	0
i also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience i lump the 2 nondata ones together): (1) dataset: the dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation.	0
the authors justify this dataset by pointing out that existing realworld datasets underrepresent rare emotions (e.g., afraid), but that’s just a reflection of how these emotions are distributed in the real world.	0
overall, the paper doesn’t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but i think there are problems with both.	0
therefore, i have concerns that the proposed framework may only be significant on the arc dataset.	0
experiments over kitti dataset also shows a rather incremental improvement over godard's method.	0
pros: the work contributed a dataset where the task has relatively objective criteria for success.	2
however if this is considered as a dataset description paper and the right expectation is set in the openings, it may still be acceptable.	0
the authors demonstrate on the cifar10 dataset that the ensemble has improved robustness against a wide variety of whitebox and transferbased blackbox attacks compared to other adversarial training techniques.	0
indeed, it is difficult in general to quantify the results of generative models, but most other gan papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset.	0
the final contribution is a new method to learn the depth of residual networks during training, but this is insufficiently explored (only tested on the toy dataset) so its practical significance can not be evaluated at this stage.	0
the authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue.	0
however i have the follow suggestions/minor comments: 1) it will be nice to have also some plots showing the performance of the proposed method on the imagenet dataset.	0
pros: 1. releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly; 2. the authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores; 3. the authors implemented an extended list of representation learning algorithms and compared them on the dataset; cons: 1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework.	2
lastly, (3) could be a helpful and interesting observation of wikihop dataset / memory network, but pointing out the flaw of a dataset or the model alone does not seem to have enough contribution for iclr.	0
the paper performs some sensitivity analysis with resepct to variance selection, but the study is again on one dataset.	0
perhaps this would be less of an issue if the authors had worked with e.g. imagenet, but for these smaller datasets it would definitely be worth to be on the safe side.	0
table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that vgg16 was also used for cifar and caltech, however other statements seem to say otherwise).	0
c. table 2 compares the nonpersona hredgan with phredgan on udc, but the authors do not provide a comparison between these two on the tv dataset in table 1. d. the comparison between phredgan_a and phredgan_d is inconsistent for the two datasets (table 1 and table 2).	0
novelty/impact  creation of a new dataset on a new and interesting problem  useful comparison of modern networks on the task  gapnet  lacking technical novelty, insight, and performance is unconvincing  demonstrates that endtoend learning outperforms cell centric approach  was this really surprising or even new information?	0
it is used to comment that overfitting is not an issue by showing training and testing performance on omniglot dataset but maybe would be more useful if learning to compare was also shown in those plots to compare the level of overfitting between the two models.	0
at least in the appendix… my major concern is that both datasets are not dealing with real background noise.	0
i’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.	0
"''detailed comments'' _paper strengths_  the idea to use a negative video example for unsupervised detection learning seems novel  the proposed method is simple and the needed data can be collected with widely available equipments  the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example  the authors collected realworld data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)  the authors compare to a number of nonlearning approaches from opensource implementations (the reviewer cannot judge whether any relevant technique is missing)  the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes _paper weaknesses_  the authors clearly reduced the horizontal margins of the standard iclr style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard iclr style template upon update of their manuscript  the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. makerbased approaches could be employed for detecting the object  the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multiobject examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is selfoccluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)  other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging nontraining scenes with heavier clutter, nonseen lighting changes and occlusions to support their robustness claim  the proposed method cannot operate inthewild (e.g. youtube data) as it makes very strong assumptions about the required input data  the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene  the authors make no comparison to other unsupervised detection approaches (e.g. to the selfcited jonschkowski et al. (2017)) to prove shortcomings of other methods on the newly generated dataset  as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for egomotion estimation) makes applications to nonvideo data impossible, also none of the experiments exploit the nontemporal property of the approach to show single frame detection on a more varied set of scenes  the experiment showcasing the proposed application to ""learning from demonstration"" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task  the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)  the reviewer cannot follow the references to objectcentric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an objectcentric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion  the ""random search optimization"" discussed in section 3.3 is not a valid method as it ""solves"" this problem of instable training by picking the best of n runs with varying random seeds  figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach _reproducibility_  given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work."	2
pros:  to confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the clevr dataset for visual reasoning diagnosis.	2
3. in b.1 in the appendix, the street view house numbers dataset is mentioned, but no results appear in the main text, why not?	0
codeslam (best paper at cvpr'18) is referenced but there is no comparison with it, while a comparison on the euroc dataset should be possible.	0
this training procedure shows competitive performance (after training) on the permutationinvariant mnist dataset versus other more standard network architectures, but is more robust to both adversarial attacks and random noise.	0
highlevel comments:  using only a single dataset, and one on which the classification problem is rather easy, is cause for concern.	0
pros:  this investigation gives a significant amount of insights on gan stability and performance at large scales, which should be useful for anyone working with gans on complex datasets (and that have access to great computational resources).	2
strengths:  first (i think) work on generic whitebox attacks against object detectors  interesting (and aesthetically pleasing) textures resulting from the attacks weaknesses:  evaluation was limited to one dataset and two object detectors.	2
these should be optimized on an evaluation set, but the authors only mentioned that they split the dataset into training and holdout (test) set (section 4.1).	0
the experimental validation of the fix is carried out similarly to godard et al 2017, with identical losses, on the same kitti dataset, using the same splits, but also on the virtual kitti dataset.	0
this can help us not only better understand the models, but also the dataset (vqa) and the task in general.	0
considering all the model and dataset complexities, the improvements over blackbox models are mostly marginal.	0
we can optimize eqn3 very well on any dataset collected from our policy; but i don't expect deploying the vae to production with c(r') as the desired user response will give us anything meaningful.	0
 descriptions of training details are reasonable, and the experimental results across several datasets are extensive cons  the network structure may not be novel, though the performance is very nice.	2
however, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.	0
my primary concerns with this specific paper are twofold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what i would expect to work on a new dataset (2) the tradeoffs aren’t well calibrated in the sense that i don’t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesn’t seem trivial to set tradeoffs).	0
accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
in conclusion, the authors observe that quantizing weight/gradients systematically lead to a slight decrease in performance but provides promising improvement in term of training speed review  the paper is well written, documented and wellsectioned, with well written theoretical guarantees and thorough experiments, including one on a large dataset.	0
they also perform a test with limited compute and show that the results correlate well with a bigger dataset, but show some bias.	0
the authors also compared with non gan methods and experimented with small datasets, both are not necessarily within scope but a welcome addition.	0
the experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that relu variants do not perform much differently on smaller datasets, and only make a difference in very deep networks.	0
the tinyimagenet numbers seem a little off; i don't have personal recent experience with this dataset but this blog post claims 56.4 with normal relus in a vggish architecture.	0
from my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.	0
another clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why?	0
 because all questions are generated via templates in particular (and plots also follow a certain generation process, albeit noisy), i am concerned that it would ultimately be easy to invert the process to crack the dataset.	0
for example, adp and adpt runtimes were very close on wikitext103 dataset but very different on billion word corpus (table 3 and 4).	0
it always feels like the multimodal community is divided between vqa and clevr datasets, but there should be a lot in common between them.	0
some minor concerns:  the use of 360x640 as resolution  the use of fcn8 instead of something based on resnet or densenet i would like some more details on what is happening with vistas dataset.	0
this dataset seems to be best suited to evaluating intent classification and slot filling (intentslot), but the current work fails to improve over what goo et al. 2018 report on this data.	0
negative: we are not sure how significant these results are for the following reasons:  ms coco image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on longterm dependencies.	0
the positive contributions include a new task definition of controlling multiple attributes, an augmented dataset that is more appropriate for the new task, and a simple but effective model which produces improved results.	0
my concern with the paper in it’s current form is that the different dropout structures/schedules are priors and it is not clear from the current analysis exactly what prior is being specified and how to match that to a particular dataset.	0
experimental evaluations on a standard large benchmark dataset of uspto show improved prediction performance compared to previous methods of jin et al, 2017 and schwaller et al, 2018. comment:  given that a chemical reaction can be regarded as a multistep chain of bond breaking and forming, thus the method part seems a quite natural extension of the past effort but also sounds rather incremental even though the performance gain exists.	0
closely related previous work (neyshabur 2017) scaled to 128x128 resolution on a much more difficult dataset  imagenet dogs but the authors did not compare in this case.	0
" this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
it would be improved by more thorough testing, but that is less important than the dataset, metrics and basic benchmarking provided.	0
mnisttomnistm has better baselines (pixelda performed better on this task for example), office is not suitable for domain adaptation experiments anymore unless one wants to be in a fewdatasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for nnbased domain adaptation); the results on cell were not convincing, i don't know the dataset but it seems that baseline nn does better than da most of the times. '	0
maybe you could have the same noisy dataset but with a small portion of random points having the wrong label.	0
 the paper builds upon deep image prior (dip)  work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc. the paper proposes a new architecture for the dip method which has much less parameters, but works on par with dip.	0
 summary: the manuscript proposes a multidomain adversarial learning (mdl) method called mulann, to leverage multiple datasets with overlapping but distinct class sets, in a semisupervised setting.	0
section3 starts with the approach but then mentions datasets and tasks (3.1).	0
of course this now involves training the entire cnn on the augmented dataset, rather than just the last layer, but how relevant is the two stage training approach that the authors propose?	0
# concerns although they have results to back up their claim on their proposed dataset and problem.	0
paper weaknesses:  it would be more interesting if the dataset was created using multiple types of probe objects.	0
 the authors provide a nice summary of all the models analyzed in section 3.1 and section 3.2. cons:  while the results on sqoop dataset are interesting, it would have been very exciting to see results on other synthetic datasets.	0
strengths  thorough analysis with a good set of questions weaknesses  some peculiar evaluation and presentation decisions  introduces 'yet another' synthetic visual reasoning dataset rather than reusing existing ones i think this paper would have been stronger if it investigated a slightly broader notion of generalization and had some additional modeling comparisons.	2
 strengths:  the paper identifies a valid limitation of the maml algorithm: with a limited number of gradient descent steps from a single initialization, there is a limit to the ability of a fixedsize neural network to adapt to tasks sampled from a diverse dataset.	2
experiments show some improvement compared to (trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets.	0
not allowing for deletion of node, and especially edges, is a potential drawback of the proposed method, but more importantly, in many graph datasets the type of nodes and edges is very important (e.g. a knowledge base graph without edges loses most relevant information) so not considering different types is a big limitation.	0
i can see how a structural change in the network can be observed but what exactly constitutes a communication event for the datasets presented?	0
the method is shown to be effective on spherical mnist, modelnet, stanford 2d3ds and a climate prediction dataset, reaching competitive/stateoftheart numbers with much less parameters.. less parameters is nice, but the argument could be strengthened if the authors could also show impressive results in terms of runtime.	0
the author reported 1000 points for modelnet which is ok for that dataset but definitely too small for indoor scenes.	0
" the breiman nonnegative garotte (https://www.jstor.org/stable/1269730) is a similar wellknown technique in statistics finally, i liked the paper and wanted to give it a higher score, but reduced it because of the occurrence of many broad claims made in the paper, such as: 1) method works on mnist => abstract claims it generally works on vision datasets 2) paper states ""typically used is fixed variance init"", but the popular toolkits (pytorch, keras) actually use the variance scaling one by default 3) the badly explained distinction between connection and weight and the relation that it implies to prior work."	0
pros: 1. wellexecuted paper, with convincing empirical results on the newly collected dataset.	2
with respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using).	0
# little or ambiguous impact of the result the accuracy difference in table 1 seems to be very minor and difficult to understand it due to lack of additional information about (1) how much bias each target dataset has, (2) a statistical test, and (3) real examples where the adversarial model can only answer.	0
the dataset is then used to evaluate a number of recurrent models (lstm, lstmattention, transformer); these are very powerful models for general sequencesequence tasks, but they are not explicitly tailored to math problems.	0
strengths: i am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models.	2
weaknesses: the dataset created here is entirely synthetic, and the paper only includes one single small realworld case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).	0
4. this general idea is somewhat similar to a june 2018 arxiv paper (taskdriven convolutional recurrent models of the visual system) https://arxiv.org/abs/1807.00053 but this is a novel contribution as it is uses the brainscore dataset.	0
2  those more familiar of the graph convolution literature will be more familiar with gcn [kipf et al. 2016] / graphsage [hamilton et al. 2017] / monti et al [2017] / etc.. most of these approaches are more restricted version of this work / hartford et al. so we wouldn't expect them to perform any differently from the hartford et al. baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work.	0
eight models are tested against nine datasets with 10 classes each but a varying number of samples.	0
the accuracies of each model for each task and dataset are reported, but there is little insight into what causes cf. for instance, do some choices of hyperparameters consistently cause a higher/lower degree of cf across models?	0
kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the cf even clearer.	0
but there are some weak points in this proposal: 1. it's well known that spectral methods are frequently sensitive to perturbations of the datasets.	0
pros: 1) a simple and reasonable formulation 2) visually good reconstruction of samples and convincing interpolation between samples on the celebahq dataset.	2
 “method more greatly resembles the original data than other ganbased methods” method does not resemble data  “due to their exact latentvariable inference, these architectures may also provide a useful direction for developing generative models to explore latentspaces of data for generating datasets for psychophysical experiments.” this is mentioned a few times, but never supported  acknowledgements should not be in the review version (can violate anonymity) 5) minor: why is a gaussian around the midpoint used for interpolations?	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
# lack of further analysis of the dataset data collection part itself seems to be the biggest contribution to this work.	0
the second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.	0
https://arxiv.org/abs/1810.01392 pros:  interesting observation of density modelling shortcoming  clear presentation cons:  lack of a strong explanation for the results or a solution to the problem  lack of an extensive exploration of datasets	2
the arguments of the paper help clarifying why the networks are so sensitive to small shifts of the objects (poor subsampling) but generalize well (there is a bias in the location of the objects in the dataset).	0
 the last paragraph of the related works section mentioned some related work with shortcomings as working only on lowdimensional data and features of specific types, yet the experiments are also mostly done on lowdimensional datasets.	0
i'm not very familiar with the area itself but i was surprised to see in section 7 that the results are not compared to full bayesian methods (possibly on a dataset that lends itself well to that).	0
this might be true, but it is empirically proven only by single dataset and single run.	0
it is surely good to present the toy example with the mnist dataset but the ethnicity domain is less difficult than what the authors claim.	0
 this paper is about training a neural network (nn) to perform regression given a dataset (x, y) 'and' a black box function which we know correctly maps from some intermediate representation to y. instead of learning a nn directly from x to y, we want to make use of this black box function and learn a mapping from x to the intermediate representation.	0
this inductive bias comes with a strong benefit when the assumption is true  as demonstrated in the toy dataset experiment  but when it is not true, the visualization would strongly distort the underlying structure of the model.	0
an eperiment over a nontoy dataset (eg imagenet), and on non computervision dataset (eg from nlp) would be a minimum, besides the overfitting concern described above.	0
re: the synthetic dataset 1. it's nice that you've tested out different single synthetic edit patterns, but what happens in a dataset with multiple edit patterns?	0
questions: 1. the content of text supposed to be programming language, but neither the model design nor synthetic dataset generation specify the type of text.	0
on the positive side, the paper is wellwritten and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets.	2
2.4 (on ama) it’s claimed that “one would need large minibatches for generating a good estimate of the mean features...this can easily result in memory issues”, but this is not true: one could trivially compute the full exact dataset mean of these fixed features by accumulating a sum over the dataset (e.g., one image a time, with minibatch size 1) and then dividing the result by the number of images in the dataset.	0
on (2), i did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset.	0
(and yes, i saw the celeba results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old dcgan results, i can't tell.)	0
3. lack of assessment of the dataset characteristics and how they relate to algorithm performance.	0
"i suggest that without evidence for this loss you soften the claim to ""this is intended to help encourage the model to learn a heirarchical tree structure"" figure 4: it appears that each row indicates a different video in the dataset, but then in 4f you still have two rows but they appear to correspond to different algorithms... a vertical separator here might help show that the rows in 4f do not correspond to the rows in 4ae."	0
cons: the idea is only tested on relatively simple dataset.	0
" (a.6) can you comment on the pros & cons of ""label regression"" for classification and how does it compare with approximate inference when softmax is put on top of a gp (perhaps illustrating by a simple experiment on a toy dataset)?"	2
i agree that this is a contribution, but since this dataset is built upon an existing dataset with source code, and the dataset construction techniques themselves are not novel, especially for machine learning community, i do not see a significant contribution in this part.	0
i think this model design is not specific to the binary vulnerability detection, but should also be applicable to other vulnerability detection settings, e.g., the original ndss18 dataset.	0
pros:  create a labeled dataset for binary code vulnerability detection and attempts to solve the difficult but practical task of vulnerability detection.	2
cons:  the operation that creates dataset may introduce bias or variance.	0
pros: presentation of new application of representation learning models construction of a new dataset to the community for binary software vulnerability detection the proposed model shows a good performance cons: the presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.	2
from the last remark, it is unclear for me if the dataset is representative of binary code vulnerability problem the proposed architecture is reasonable and maybe new, but i find it natural with respect to existing work in the literature.	0
however the notion of vulnerability is not defined and it is difficult for me to evaluate the interest of the dataset.	0
the architecture is general enough to work on other problems/tasks  which is good  but the authors focus on the binary vulnerability code dataset in the experiments.	0
i understand that some technical assumptions are needed in a theoretical work, but i would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible.	0
b) i have to admit that i am not extremely familiar with common experimental evaluations used for derivativefree methods but the datasets used in the paper seem to be rather small.	0
for example in abstract there is a simple claim that is presented too strong: we also demonstrate that training such an augmented cnn with representative outdistribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen outdistribution samples and blackbox adversarial examples without training it on any adversaries.	0
pros and cons () interesting idea () diverse experimental results on six datasets including benchmark and realworld datasets () lack of related work on recent catastrophic forgetting () limited comparing results () limited analysis of feature regularizers detailed comments  i am curious how we can assure that svm's decision boundary is similar or same to nn's boundary  supportnet is a method to use some of the previous data.	2
forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.	0
however, the synthetic regression task is a nice proofofconcept, but thorough regression evaluation could perhaps include the boston housing prices dataset or some uci datasets.	0
the study is also interesting although the lack of publicly available datasets limits the extent of it and the strength of the results.	0
"pros:  a simple idea  encouraging experimental results cons:  confusing read  no clear intuition is given  restricted to lowdimensional datasets  strong baselines needed  the plots are too small to see (impossible to see when printed) other comments:  the authors are using the term ""feature vector"" to refer to a data point."	2
 the empirical results could make this paper compelling, but the comparisons are rather shallow, in that they compare on only a few datasets and don't compare extensively to many of the state of the art methods.	0
strengths:  interesting new objectives for representation learning based on increasing the js divergence between joint and product distributions  good set of ablation experiments looking at local vs global approach and layerdependence of classification accuracy  large set of experiments on image datasets with different evaluation metrics for comparing representations weaknesses:  no comparison to autoencoding approaches that explicitly maximize information in the latent variable, e.g. infovae, betavae with small beta, an autoencoeder with no regularization, invertible models like real nvp that throws out no information.	2
[pros & cons] () this paper tries to extend dual learning from word level to hidden state level; () multiple languages are involved in this framework; () experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used wmt datasets; () the paper is not easy to follow.	0
the result in table 5 shows that odd can outperform erm for real world datasets, but the improvement seems to be marginal.	0
cons experiments are only limited to small scaletraditional graph datasets.	2
experiments: the experiments are presented in a subset of 5 classes from cifar10 (also used by weinshall et al.), but also in the full cifar10 and cifar100 datasets.	0
in fact, i wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar).	0
cons:  the paper doesn’t analyze and compare the benchmark with existing datasets.	0
pros 1. positive signals have been shown on multiple datasets.	0
4. another only concern is that whether the proposed method can be well applied on large scale datasets such as imagenet.	0
while the results seem promising, there are several issues that may potentially weaken the queryefficient claims made in this paper, especially due to the lack of sufficient attack comparisons (on smaller datasets) and inconsistent threat models when compared to existing works.	0
datasets like mnist and cifar can be used to show that the proposed idea is working as a proof of concept, but it might be difficult to draw a generalized conclusion from the results in the paper like the use of unsupervised loss during training helps.	0
however this has been done by a number of authors:  vlachos and clark (2014): http://www.aclweb.org/anthology/q141042  berant and liang (2015): https://nlp.stanford.edu/pubs/berantliangtacl2015.pdf while the idea of using such oracles for structured prediction tasks in nlp was first proposed by daume iii et al. 2009: https://arxiv.org/abs/0907.0786 furthermore, it has been applied for rnn decoding in nlp, see: https://arxiv.org/abs/1511.06732 apart from this, some further comments:  the subset of sql tackled in this paper is less expressive than what has been done in previous work on atis and geoquery datasets.	0
comments)  it is something strange why the authors used shallower resnet on imagenet and deeper ones on cifar datasets, maybe it was due to the training time, but the authors should clarify it.	0
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
in general, the paper is not showing stateoftheart results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.	0
the result section given in the paper is its weakness and requires a more indepth analysis:  the results given for omniglot are impressive  the experiments analyzing the impact of diversity and routing depth are interesting and offer interesting insight into the architecture  the results do not show learning behavior over epochs; this is not necessary, but would give an additional insight into the learning behavior of the architecture  the experimental settings are confusing: why are the different experiments performed with different datasets?	0
this makes it seem as if the authors cherrypicked the best results for the different experiments (this might not be the case, but the results on omniglot alone are good enough that negative results and a detailed discussion of them would not have hurt the paper, but enriched the discourse)  additional experiments that offer a transition from larger datasets to smaller ones would be interesting; seeing how the performance of the architecture behaves e.g. on cifar10 for 1k, 5k, 10k, 25k and 50k would have illustrated how well the architecture is able to generalize from different numbers of samples in summary, i think the paper analyzes a very important problem and has a lot of potential.	0
the current experiments show that the method works better on lowdimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.	0
cons:  in figure 6 no comparison with modelfree (policygradient type) of approaches  there is not a lot of detail on the value of the generative adversarial network for the user behavior dynamics, thus this prevents the reader from fully understanding the contribution  only 2 datasets are used  only 100 users for test users seems few  why only 1000 active users were sampled from movielens?	0
cons:  all of the experiments were done on toy datasets.	0
it is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be ok, but it seems to be a fairly straightforward extension of existing models.	0
i understand that what i ask for is very difficult to answer, but experiments with more datasets and different types of queries (such as satisfiability) might have made me happier.	0
the hierarchical rnn model may be effective on the youtube8m dataset, but other models such as the twostream model from simonyan and zisserman and i3d from carreira and zisserman are popular on other datasets such as kinetics.	0
the authors make a secondary claim that they are able to improve upon irgans via their proposed approach but then they do not substantiate these on all the datasets which seems like a notable oversight.	0
for example: 1) the authors claim that, compared to neural models, their model has the advantage of the interpretability (for small datasets), but they also have neural components in their model.	0
overall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models.	2
for example, the authors claim that mixfeat can reduce overfitting even with datasets with small sample size, but did not provide any training cost or errors in figure6 to support that claim.	0
for example, the method outperforms random sampling for small datasets, based on number of samples, but what happens when you look at execution time?	0
the paper unfairly dismisses prior work by making factually incorrect claims, e.g. section 2 claims “indeed, papers like (hernandezlobato & adams, 2015; gal & ghahramani, 2016; lakshminarayanan et al., 2017; kendall & gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of rmse.	0
however, there was one work presented last year at nips which concerns itself with the same problem, which is getting rid of learning rates: “training deep networks without learning rates through coin betting” by francesco orabona and tatiana tommasi, nips, 2017. they don’t compare on the same methods and the same datasets, but i think the authors should be aware of this work and perhaps compare themselves with it.	0
[cons & details] (1) as stated in the abstract, “…, but that the improvement diminishes as the size of data grows, indicating that powerful neural mt systems are capable of implicitly modeling roleword interaction by themselves…” (1) the main concern is that, considering ril does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline.	2
you have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.	0
the results presented are also impressive: i don't think the iwslt deen results are in fact state of the art (e.g., edunov et al. (naacl 2018) and deng et al. (nips 2018) outperform these numbers, though both papers use bpe, whereas i assume the current paper does not), but the results on the other two datasets appear to be.	0
it however needs more thorough analysis or experiments that validate the ideas as also experiments on harder, largescale datasets.	0
weaknesses:  the improvement over the existing method is incremental;  the regularization on routing decision may not really be necessary as, in dndf, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the dndf paper;  the datasets tested are standard image datasets, not even captured from vehicles or video surveillance.	0
 because of all the different datasets, etc. it was a little hard to call the grammar induction results, but they at least look competently strong.	0
because the authors aren’t evaluating on particularly difficult language datasets, they should provide statistics about the datasets w.r.t to vocabulary size, average length, etc. consequently, a lot of information that is necessary for evaluating the strengths and weaknesses of this paper are missing from the writeup.	2
the increments presented are reasonable and justified, but the experimental results, specifically on the larger datasets, warrant further investigation.	0
[overall] it’s great that ntp was scaled up to handle larger datasets, however further analysis is needed.	0
the experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but i am not entirely sure whether the given architecture is necessary, see comments below.	0
overall thoughts: using datasetspecific features for evaluation metrics makes a lot of sense, but i don't feel totally satisfied by this paper's investigation of the specific proposal of a vae, and am particularly worried about whether the metric just ends up preferring models similar to that vae.	0
strengths:  i appreciated the quantitative comparison on the lowd toy datasets (2d and 3d mogs), reporting modes captured, and jsd.	2
my only concern is the experiments: 1) some of the benchmark datasets for the proposed task as well as some wellknown methods (see battaglia et al’18 and references in there) are missing.	0
there are e.g. datasets (protein data  see tino) get very bad classification models if the negativ contributions are removed  accordingly they are not just noise but contain valuable information to the problem  it would be good to add a few sentences at the beginning of the paper to (super brief) review the core idea/concept of siamese networks  the english is sometimes a bit bulky and it would be good to check it by a native speaker e.g ' proposal do not require'  proposal does not require ' unlike the our'  follow by a spellchecker 'embeddng', 'interms', 'simalry'  'followed by removing it by flipping'  with flipping the contribution is not 'removed' but mapped to the positive part of the spectrum  how does your approach compare to a classical embedding of the indefinite kernel matrix into a pseudo euclidean space (see e.g. tino or pekalska)?	0
pros:  provides insights on why adversarial training is less effective on some datasets.	2
but there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc. the authors also rely mostly on the fid metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.) the writing is understandable for the most part, but the paper seems to lack focus  there is no clear take home message.	0
with lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.	0
strengths  i applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the gp adapter framework (gpgru) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)  it likewise outperforms both commonly used preprocessing (gruf) [2][3] and the much more complicated neural net architecture (gruhd) from [6] (across two datasets and tasks)  the simplicity of this approach probably lends itself to additional customization and innovation  the literature review seems quite thorough and does an especially nice job of covering recent work on rnns for multivariate time series and irregular sampling or missing values  the experiments are thorough and welldesigned overall.	2
here are my broad concerns: 1. even though the datasets used are small (mnist/fashion mnist), the experimental validation of adversarial attacks is only performed on 100 test examples.	0
little is known about the newly collected datasets, it is not clear how to interpret the relative improvements in validation loss compared to the original metrics of allamanis et al 2016, and the paper lacks necessary comparisons to othef pretrained embeddings, so though the overall claim that pretrained embeddings should be used for this task seems to hold up, it is not clear that this is a complete argument for the method chosen by the authors.	0
my main concern is that it is unclear to me how cds (crossdomain schema) can be generalized to the other semantic parsing datasets, e.g., the overnight dataset (wang et al., 2015), which also contains multiple domains.	0
 imvlstm outperforms many baselines including popular interpretable models on three different datasets, and the interpretation part is not super rigorous, but convincing enough.	0
pros: clearly written; experiments on the datasets chosen do seem to suggest that the proposed methods have potential.	2
experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.	0
so i have concerns whether this approach is able to generalize over different datasets.	0
of course, it would be nice if additional datasets could be included as well, but this of course depends on the computational resources the authors have available.	0
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html therefore, though the claimed accuracy of the proposed method is 89%, it is still not the stateoftheart result and it seems to be lack of tuning for the bp approach to perform similar level of accuracy.	0
to learn the weightings on the fly in your adaptive approach will address the complexity issue but may in parts degenerate performance  now it is a question what the user is more interested on  you mainly ignore the point how the landmarks have to be selected  random is (often) not a very reasonable choice  see respective work around this in your datasets the problem may not show up because you have images only which have very strong characteristics and are intrinsically very low dimensional.	0
my concern is, given this is an empirical work, the number of datasets used in evolution is a bit small.	0
3) the authors show the effetiveness and improvement of the approach to various attack methods as compared to existing defense techniques 4) the authors evaluate their technique on mnist and svhn datasets cons or shortcomings/things that need more explanation : 1) it would have been really good to the kind of adversarial examples generated by this technique look like as compared to the examples generated by the other strategies.	2
" perhaps tailoring the mal lists to each specific dataset would make sense (i understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield ""better"" results)."	0
(hope i haven't missed anything) weaknesses: recent datasets for action recognition, e.g., moments in time, charades, youtube8m etc, are missing.	0
pros:  tests on several datasets  seems fairly generally applicable.	2
lastly, even though the claims sound theoretical, they are not derived from any set of first principles but come from observations on a few datasets.	0
the second concern is that the set of chosen datasets for indistribution and outofdistribution are far from challenging: the datasets are so different that a linear classifier based on the local image statistics can separate them.	0
originality: pros: the suggested model outperforms others on three qa datasets.	2
weak points: ' the evaluation is limited to only mnist datasets. '	0
originality: pros: the suggested model outperforms others on two datasets.	2
 the authors mention two types of language models (word and character level), and also 4 text datasets to train the lms on, but do not provide results for all combinations.	0
the experiments involve synthetic regression and classification datasets but there are no novel insights that advance what is already known about the hyperparameter optimization (e.g., see [3]).	0
and finally, the authors claim that most modern datasets can fit to the accelerator memory if reduced 10x, but in my experience the network and it’s activations (which are stored during training) occupies most of the gpu memory even on high end accelerators, not leaving enough space to store a large dataset even after data reduction.	0
2) experimental results the experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of stateoftheart competitors among the very many related methods published recently (e.g. [28], just to mention a small subset).	0
i don’t think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.	0
iii) after seeing the results in table 6, i have quite concerned about the practical performance of set transformer on relatively large datasets (like 1000 points each class in the settings.)	0
 unburdens the needs for extensive datasets for irl based approach to be effective  to a large extent, circumvents the need of having to manually engineered features for learning irl reward functions cons:  although the current formulation is novel, there is a close resemblance to other similar approaches  mainly, imitation learning.	0
"it is briefly mentioned in 4.1 under ""datasets"" but this seems insufficient."	0
significance:  clustering is an important problem and the authors show some results, but on only three datasets: mnist, newsgroups, reuters.	0
main concerns:  what is the stateoftheart on mnist and their other datasets?	0
i agree that this is a useful area to study and direction for the community to go, but as the introduction of this paper states, this is the most interesting when the parties have control over logically separate components of the modeling pipeline and also when joint training of the components is being done, potentially on disjoint and private datasets.	0
a list of additional datasets is provided in table 5, but only the performance metric is listed, which is meaningless if it is not accompanied with figures for size, latency and speedup.	0
the only takeway about the additional datasets is that the proposed lbpnet can match or outperform a weak cnn baseline, but we don't know if the latter achieves stateoftheart performance (previous figures of the baseline cnn suggest it doesn't) and we don't know if there's significant gain in speed or size.	0
pros: ' findings provide us useful direction for future research (that dataparallelism centered distributed training is going to hit the limit soon) ' extensive experiments across 5 datasets and 6 neural network architectures cons: ' experiments are a bit too much focused on image classification ' error bars in figures could've provided greater confidence in robustness of findings	2
supportnet is compared to five methods (all data: network is retrained with new and old data, upper bound for performance, icarl: stateoftheart method for incremental learning, ewc: only ewc regularizer added, fine tune: only new data, random guessing: random guess to assign labels) on six datasets (mnist, cifar10, cifar100, enzyme function prediction, hela subcellular structure classification, breast tumor classification).	0
to ensure reproducability of your results i ask you to provide the respective codes e.g. on github (can be done anonymous)  repeating myself from the last review: there is a lot of work addressing that making a kernel psd may not be good idea  you provide experiments for a small number of data where your kernel is now psd but what is with the other data (where e.g. in pekalska and followers it was shown that making them psd is bad ... )  is your approach solving this  or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?	0
regarding the methodology for training the classifier, i am not familiar with these datasets but using just a 1/10 of the data to train classifier seems a bit extreme ?	0
the proposed approach is interesting and promising, but the selection of the methods and datasets to be compared is not appropriate.	0
strengths:  clearly written and well presented  learning to generate clarification questions is an important topic  interesting combination of seqgans, mixer and selfcritical baseline for policy gradient updates  a range of good baselines for this novel task setup weaknesses:  minor: automatic evaluations are kind of useless here and the datasets are rather artificial for this task  major: generating clarification questions cannot be the end goal in and of itself (see above explanation) other comments:  section pretraining, paragraph question generator: i do not understand the reference to answer generator in this paragraph.	2
the authors justify this dataset by pointing out that existing realworld datasets underrepresent rare emotions (e.g., afraid), but that’s just a reflection of how these emotions are distributed in the real world.	0
pros: ' establishing a connection to other topic of research often facilitates productive collaboration between two fields ' provides a new perspective to understand prior work ' provides new useful algorithms cons: ' experiments were conducted on small models and small datasets ' unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear walltime would actually be reduced with these algorithms.	2
lastly, the experiments are done on some standard but small benchmarks, and my personal experience with these datasets are that it is very easy to overfit, and most of the effort will be put in to prevent overfitting.	0
authors discussed the results in figure 4 for six realworld datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.	0
moreover, the authors also show interesting properties of the holographic loss, along with some interesting properties of the minima etc. cons: my major criticism of this work is that, while this seems like an interesting idea, the authors do not really provide extensive results on real world datasets.	0
perhaps this would be less of an issue if the authors had worked with e.g. imagenet, but for these smaller datasets it would definitely be worth to be on the safe side.	0
table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that vgg16 was also used for cifar and caltech, however other statements seem to say otherwise).	0
c. table 2 compares the nonpersona hredgan with phredgan on udc, but the authors do not provide a comparison between these two on the tv dataset in table 1. d. the comparison between phredgan_a and phredgan_d is inconsistent for the two datasets (table 1 and table 2).	0
results on three canonical datasets using blackbox and whitebox adversarial attacks suggest that exl can be helpful in defending against bb attacks, and is easily combined with other adversial training approaches such as pgd to further improve robustness.	0
empirical studies show that structuraljumplstm is (slightly) better than stateoftheart methods in terms of both accuracy and speed over most but few datasets.	0
the article motivates the work as combining raw multimodal sensor datasets, but no real tasks are shown.	0
at least in the appendix… my major concern is that both datasets are not dealing with real background noise.	0
i’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.	0
pros:  this investigation gives a significant amount of insights on gan stability and performance at large scales, which should be useful for anyone working with gans on complex datasets (and that have access to great computational resources).	2
till now, there have been no good density modeling results on large images when taken into account large datasets like imagenet (there have been encouraging results like with glow, but on 5bit color intensities and simpler datasets like celeba).	0
 experiments are ok, but on pretty small datasets, and for single hidden layer nns.	0
the novel gan framework is evaluated on several datasets such as mnist, document layout comparison and clipart abstract scene generation pros:  the paper is trying to solve an interesting problem of layout generation.	2
the authors do not implement methods based on the adversarial polytope due to their present unscalability, but that argument would be better supported if the authors were addressing larger models on harder datasets (vs mnist and cifar10).	0
however, i am concerned with its scalability beyond the toy datasets.	0
my concerns are as follows:  having only results on two flawed datasets (considering the inverse relations in them) makes it hard to evaluate the quality of the method.	0
 descriptions of training details are reasonable, and the experimental results across several datasets are extensive cons  the network structure may not be novel, though the performance is very nice.	2
appendix b gave some information about models but specified the targeted datasets.	0
the extension of gnns is simple, but effective across all datasets in comparison to external baselines for cnn/dailymail, internal baselines for c#, and a combination of both for java.	0
accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
the authors also compared with non gan methods and experimented with small datasets, both are not necessarily within scope but a welcome addition.	0
the experimental sections, tables, and plot of learned threshold values are good, but impression from the swish paper and others was that relu variants do not perform much differently on smaller datasets, and only make a difference in very deep networks.	0
from my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.	0
another clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why?	0
it always feels like the multimodal community is divided between vqa and clevr datasets, but there should be a lot in common between them.	0
2. it is nice to see many experiments, but without preexisting knowledge about the datasets and their tasks, i can only make relative judgements based on the provided comparisons against other methods.	0
" this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
yes, the authors conduct some experiments to show that their algorithms achieve good performance in some benchmark datasets, but a careful discussion (if possible, theoretical) of when such an assumption is viable and when it is an oversimplification is necessary (analogous assumptions are used in naive bayes or variational bayes for simplifying the likelihood, but those are much more flexible, and we know when they are useful and when not).	0
 the discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; i think it could benefit from tightening up the wording but mostly i just had to read it a couple times.	0
 summary: the manuscript proposes a multidomain adversarial learning (mdl) method called mulann, to leverage multiple datasets with overlapping but distinct class sets, in a semisupervised setting.	0
they bounds are loose, but not vacuous, and follow the same order of difficulty on a handful of datasets as the true generalization error.	0
cons:  the datasets used in the paper are quite simplistic.	0
cons or unclear points: 1) why the paper does not include all biological datasets (6 datasets in total, only 4 used in this papaer) presented in (verma & zhang, 2018) in the experiment section.	2
section3 starts with the approach but then mentions datasets and tasks (3.1).	0
 the authors provide a nice summary of all the models analyzed in section 3.1 and section 3.2. cons:  while the results on sqoop dataset are interesting, it would have been very exciting to see results on other synthetic datasets.	0
experiments show some improvement compared to (trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets.	0
not allowing for deletion of node, and especially edges, is a potential drawback of the proposed method, but more importantly, in many graph datasets the type of nodes and edges is very important (e.g. a knowledge base graph without edges loses most relevant information) so not considering different types is a big limitation.	0
i can see how a structural change in the network can be observed but what exactly constitutes a communication event for the datasets presented?	0
" the breiman nonnegative garotte (https://www.jstor.org/stable/1269730) is a similar wellknown technique in statistics finally, i liked the paper and wanted to give it a higher score, but reduced it because of the occurrence of many broad claims made in the paper, such as: 1) method works on mnist => abstract claims it generally works on vision datasets 2) paper states ""typically used is fixed variance init"", but the popular toolkits (pytorch, keras) actually use the variance scaling one by default 3) the badly explained distinction between connection and weight and the relation that it implies to prior work."	0
the multitask experiments used 4 different datasets to encourage diversity, but k=2 showed the best results.	0
pros:  the reported compression results with a ganbased framework for large images are impressive  comprehensive set of results with kodak, raise1k and cityscapes datasets  the paper is well written with the core results and idea being well articulated cons:  primary concern: the quality metrics are unclear esp.	2
for mnist, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for cifar10 the epsilon parameter is identical to wong et al. does that indicate that the results presented in this paper do not scale beyond simple datasets like mnist?	0
eight models are tested against nine datasets with 10 classes each but a varying number of samples.	0
kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the cf even clearer.	0
strengths:  substantial number of experiments (6 datasets), different domains  surprisingly simple methodological fix  substantial literature review  it has been argued that charlevel / pixellevel rnns present somewhat artificial tasks — even better that the authors test for a more realistic rnn application (reading comprehension) with an actually previously published model.	2
but there are some weak points in this proposal: 1. it's well known that spectral methods are frequently sensitive to perturbations of the datasets.	0
 strengths:  good coverage of related work  clear presentation of the methods  evaluation using established semeval datasets weaknesses: 1. it is not entirely clear what is the connection between fuzzy bag of words and dynamax.	2
 “method more greatly resembles the original data than other ganbased methods” method does not resemble data  “due to their exact latentvariable inference, these architectures may also provide a useful direction for developing generative models to explore latentspaces of data for generating datasets for psychophysical experiments.” this is mentioned a few times, but never supported  acknowledgements should not be in the review version (can violate anonymity) 5) minor: why is a gaussian around the midpoint used for interpolations?	0
the improvements gained with wasserstein and gradient penalties are interesting but results on more datasets and longer samples are needed, especially section 4.2 would benefit from quantitative and human evaluation.	0
2) using vaes to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet?	0
the second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.	0
https://arxiv.org/abs/1810.01392 pros:  interesting observation of density modelling shortcoming  clear presentation cons:  lack of a strong explanation for the results or a solution to the problem  lack of an extensive exploration of datasets	2
 the last paragraph of the related works section mentioned some related work with shortcomings as working only on lowdimensional data and features of specific types, yet the experiments are also mostly done on lowdimensional datasets.	0
on the positive side, the paper is wellwritten and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets.	2
pros: i think the authors have compared to the number of baselines on three medical imaging datasets and show that their method via various metrics clearly outperforms others on this specific medical imaging application.	2
a similar paper reporting alp to improve robustness in smaller datasets (cifar10) was submitted to iclr (https://openreview.net/forum?id=bylj6oc5k7) but was withdrawn after the authors performed additional experiments.	0
it is unclear the extent to which better classification performance on the clean input generalizes to datasets such as imagenet overall, i think the results are promising, but i'm not fully convinced that similar results cannot be achieved using standard crossentropy losses with 1hot labels.	0
regarding results, effort has clearly gone to keep the comparisons as fair as possible, but with these large datasets it is difficult to disentangle the many factors that might effect performance (as acknowledged on p9).	0
pros:  this work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))a very interesting configuration of weak supervision, an increasingly popular and important area  the treatment is thorough, proceeding from establishing the minimum number of u datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments cons:  this is a crowded area (as covered in their related work section).	2
thus this work connects to many areas both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multisource weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).	0
 other prior work here has handled k classes with k u sets; could have extended to cover this setting too, since seems natural overall take: this learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.	0
interestingly, the paper shows that performance of multiple gan variants can be improved with their proposed method on several image datasets strengths:  the proposed method is very interesting and is based on sound theory  connection to optimal transport theory is also interesting  in practice, the method is very simple to implement and seems to produce good results weaknesses:  readability of the paper can be generally improved.	2
strengths:  a new auxiliary learning algorithm  positive results on cifar data set weaknesses:  novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space  there is no attempt to provide a theoretical insight into the performance of the algorithm  the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance  experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario  the paper is poorly written and sentences are generally very hard to parse.	2
they do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set.	2
pros importance of the issue exposition and relation to previous work experimental results (although these were for smaller data sets) appendices really helped aid the understanding cons real world usefulness clarity	2
however, i feel the paper is lacking a convincing evidence (from what i could find the authors base all the conclusions on one set of similar experiments performed with one generator architecture on one data set) in order to be viewed as a significant contribution to the generative modeling field.	0
1) only deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting.	0
the collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.	0
conclusion: my two biggest concerns are: 1) the algorithm is not tested on large data seta 2) the algorithm is not tested with the models of limited capacity.	0
you present a number for c2 in section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1).	0
due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter .lambda=0.00011 on the cifar data set.	0
strengths  i applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the gp adapter framework (gpgru) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)  it likewise outperforms both commonly used preprocessing (gruf) [2][3] and the much more complicated neural net architecture (gruhd) from [6] (across two datasets and tasks)  the simplicity of this approach probably lends itself to additional customization and innovation  the literature review seems quite thorough and does an especially nice job of covering recent work on rnns for multivariate time series and irregular sampling or missing values  the experiments are thorough and welldesigned overall.	2
more data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments).	0
pros:  it provides insights on why adversarial training may not improve robustness on the test data set.	2
authors claim the proposed strategy is efficient, but they didn’t provide the results on the imagenet1k data set.	0
it would have been great to have seen a data set on which the method performs poorly  that would give additional insight into its strengths and weaknesses.	2
" the technical contribution of this paper is reduced to adding a ""personality embedding"" to approach the image captioning problem (authors also propose a new task and data set, but motivation is not convincing)."	0
these were good data sets a few years ago and still are good data sets to test the code and sanity of the idea, but concluding anything strong based on the results obtained with them is not a good idea.	0
in addition, the paper is build upon o the 51 model as in figure 2 and the graphical comparison between the empirical esd and the expected esd of the five models in table 1, and they lack any mathematical/rigorous definitionsee table 2. the simulations are performs over a particular data set and a particular setting, and i wonder if the observations would be different for a different data set and a different setting.	0
the english data set was used but only to exploit it for the transfer learning.	0
pros: ' available source code ' good experimental results ' easy to read ' interesting idea of encoding how active the various possible operations are with special weights cons ' tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train darts models, which they then show to transfer to two other data sets, respectively ' shared with most nas papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures ' theoretically not very strong, the derivation of the bilevel optimization is interesting, but i believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested	2
 data augmentation is a useful technique, but can lead to undesirably large data sets.	0
the authors claim that their approach, dymon is adapted to the many challenges of biological highthroughput data sets: noise, sparsity and lack of temporal resolution.	0
cons in general,  in miniimgenet data set, although they are beating maml, but they are not able to beat other competitors in 5shot classification.	2
###highlights 'well written introduction and a good overview of generative modeling 'very good visualizations and explanation of the detection methods 'thorough results on zeroknowledge gradient based attack detection rates 'interesting direction on adversarial examples ### areas for improvement 'more time should be spent on discussing the off manifold conjecture; the paper is based on the correctness of this conjecture though recent work has called its validity into question 'only gradient based attacks are discussed though the paper title suggests robustness to all attacks 'perfectknowledge attacks should be explored in much more detail ' the paper should streamline the results and findings, the large number of tables and results will confuse the reader and do not add to the argument 'recent work on generative models for adversarial robustness should be discussed (https://arxiv.org/abs/1805.09190, https://arxiv.org/abs/1811.06969) 'the two class cifar10 results are not particularly convincing as this is a substantial simplification of the cifar10 problem ' the results on the full cifar10 data set make use a the extracted features of a pretrained discriminative cifar10 network, this introduces the problem at layer activation adversarial attacks, but this is not mentioned in the paper (https://arxiv.org/pdf/1511.05122.pdf) overall i think the work shows promise but is not yet ready for acceptance	0
pros importance of the issue exposition and relation to previous work experimental results (although these were for smaller data sets) appendices really helped aid the understanding cons real world usefulness clarity	2
  using a generative model as the surrogate distribution for kernel twosample test is novel  an important and new application of deep generative models  strong experiments on synthetic and realworld time series data sets  very clear writing and explanation of the idea  reply sample segments from both directions (past and future) while in the practical setting, cpd is usually sequential and in one directional  lack theoretical understanding of the limit of the neuralgenerator in the kernel twosample test	0
the first one is wellexplained  easy to read cons:  ideas are not very surprising; and just tested on a few data sets; things could be more robust.	0
more data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments).	0
 top pros:  well motivated approach with good examples from clinical setting  sound proof on why information theoretical approach is better than mle based approaches  experiments on diversified data sets to show their approach's performance, with good implementation details.	2
these were good data sets a few years ago and still are good data sets to test the code and sanity of the idea, but concluding anything strong based on the results obtained with them is not a good idea.	0
the proposed algorithm is evaluated on several benchmark data sets using several benchmark neural network architectures and the results indicate a reduction in communication costs are compared to a nonsparsified variant of federated learning strengths: • the algorithm jointly leads to compression of a neural network and reduction in communication cost of federated learning.	2
the authors state that iterations of the fixed point procedure converge rapidly in practice, but it seems like it's only been evaluated on these synthetic data sets.	0
neg: figure descriptions are not very clear when it comes to comparing the results, they do use a prepossessing step for their algorithm which they do not incorporate into the results pros: clear outline of the data sets used for benchmarks.	2
pros: ' available source code ' good experimental results ' easy to read ' interesting idea of encoding how active the various possible operations are with special weights cons ' tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train darts models, which they then show to transfer to two other data sets, respectively ' shared with most nas papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures ' theoretically not very strong, the derivation of the bilevel optimization is interesting, but i believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested	2
 data augmentation is a useful technique, but can lead to undesirably large data sets.	0
the authors claim that their approach, dymon is adapted to the many challenges of biological highthroughput data sets: noise, sparsity and lack of temporal resolution.	0
my concern for using such data sets(the resolution of images is low and the distribution is simple) is that: although the noise seems to corrupt most of the image, the distribution of the image is not complex, so the generative model can recover it easily.	0
pros: the proposed method allows to accurately impose an energy constraint (in terms of the proposed model), in contrast to previous methods, and also yields a higher accuracy than these on some data sets.	2
the main concern is the experiments do not quite show the stateoftheart result at all.	0
cons: 1) experiments are not very exhaustive and at times a bit confusing.	0
main concerns: the experimental section should make experiments on standard datasets (i.e., uscd, trancos, mall, pklot, shangai, penguins) using standard evaluation protocols (mae, game).	0
experiments on tiny graphs show better results than learning based baselines, but worse results than mcs solver.	0
concerns #2: somewhat unsurprising experimental results  this paper shows various experimental results, but some experiments seem to be trivial.	0
both experiments are poorly described, and even after reading the appendix several times, some serious detective work was necessary to piece together what happened in the experiments.	0
this is to some extent both 'boon and bane': on the one hand, the paper contains a lot of information and concepts that need to be understood; on the other hand, the experiments go 'wide' but not 'deep'.	0
3. experiments and analysis: a. the paper compares against neuralmap, and reports improvements, but doesn't give a reason as to why this happens.	0
the paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria.	0
weakness: 1. the experiments lack the recent important baseline 'symmetric cross entropy for robust learning with noisy labels, iccv2019', which are the current sota.	0
what are the dots in equation  the experiments are very thorough and the results are very good, but i have few clarifying questions: ' the procedure for choosing beta/gamma is not clear, and i see that for every experiment those values change. '	0
1. my most worrying concerns are about the experiments.	0
the experiments are lacking in that evidence except for four rooms domain, which is much simpler a domain.	0
 page 1: it is mentioned that ‘the number of parameters can be the same as the original network’ but the experiments do not include such architecture.	0
i think i will be way more convinced if the experiments are done on a singlescale feature map (anchor also works with singlescale but i am not sure without defining different anchors it can work just on c4 for example).	0
 lack of experimental or theoretical depth: the proposed method is presented asis; no theoretical analysis of its behaviour is performed; while this is not necessarily a problem, as there are several empirical experiments, the experimental section is not sufficiently detailed: for example, no limitations of the method are being discussed and the presented results are not stateoftheart accuracies.	0
it is currently hard to assess the empirical results due to rather small improvements and the lack of repeated runs of experiments.	0
i believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger.	0
the authors claim they have extended to 4 sources, but all experiments in the paper seem to only involve two sources. '	0
i think it could be clearer and contain more experiments but it is otherwise rather convincing proof that dnns learn lowfrequency patterns first.	0
the paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work.	0
weaknesses  while the work is wellmotivated and the experiments provided show a proofofconcept of the approach with thorough analysis, the paper could be significantly strengthened by considering more domains for inferring physical parameters  simply inferring whether balls will pass above, pass below, or bounce off obstacles is a good first step, but does not provide enough evidence to evaluate the generality of the proposed method to intuitive physics tasks.	0
i would have liked to see experiments on more recent architectures (the focus of the paper is on a dated architecture (alexnet)); there is analysis on units in googlenet and vgg16 but it would also be interesting to see results for more modern architectures (e.g. densenet and resnet).	0
i have major concerns about the novelty, and experiments in this work.	0
my second concern is that the value of n (number of sample points used in nystrom approximation) has not been specified for the presented experiments (unless i missed it).	0
the author conducts extensive experiments to show falcon based method not only surpass other depthwise separable convolution models but also give up to 8 times efficiency while giving similar accuracy.	0
 the experiments are quite lacking.	0
i appreciate that there might not be much consistency in terms of which methods outperform others, but large scale experiments would at least present evidence of this point.	0
however, experiments that would support/falsify the following points could be good:  rws and iwaestl don't suffer from nonunified objectives because iwaestl has nonunified objectives but doesn't diverge,  [targeting direct divergence] is more useful than (or as useful as) [lower variance gradient estimators].	0
this paper should be rejected because (1) the paper lacks important latest references on domain adaptation, (2) the paper misuses the notations that makes the paper is not easy to follow, (3) the algorithm is not well justified either by theory or experiments, and (4) the presentation should be further polished.	0
recommendation in its current state,i believe that the paper is lacking the technical precision and experiments to be useful to other researchers,and hence recommend rejecting it.	0
all the experiments in this submission can only show you are better than the baselines, but can't convince me your approach actually works.	0
overall, i found the formulation quite similar with dladmm (wang et al. (2019)), the contribution is incremental, the analysis did not show why multiconvex formulation problem 2 is good (the rate is the same as standard results), and the experiments are questionable and also did not show advantages of the formulation and proposed method.	0
the paper is well motivated and experiments are correct, but the quality improvements overall are a little underwhelming.	0
my concern, however, is the lack of factorization machines (fm) in the experiments.	0
and the authors have conducted experiments to show that such selfexplaining mechanism based on attention model can achieve comparable classification accuracy with original blackbox models.	0
reasons to accept:  strong empirical results on an interesting application  wellwritten paper  thorough experiments reasons to reject:  incremental methodological contribution  likely difficult to reproduce	0
i have some concerns regarding the novelty, analysis and also the experiments.	0
comments: [1] a bunch of experiments are conducted [2] chinese ner is a hard problem, but it would be great to see the proposed approach generalizable to other tasks.	0
3. for the experiments, the authors introduced resnet architecture which is based on resnet architecture but is novel in its design with the use of selu and removal of batchnorm and with different initialization schemes.	0
experiments are appropriate but lack baseline comparison with generative models and a thorough description of the unsupervised validation procedure.	0
in sections 4.4 and 4.5, experiments are discussed, but results are not presented. '	0
their experiments show that this is a promising approach, but probably requires further research to achieve state of the art results.	0
i am critical about the paper because 1) the experiments show that all more recently published exploration methods are worse than weak baselines, for which it lacks enough justification and convincing explanations.	0
it is good that these experiments are repeated ten times however the authors should show the standard deviations too.	0
from an experimental point of view, i find (3) particularly concerning because it just means that in their learning rate for the popnorm experiments are 1/kappa times those for the batchnorm baseline.	0
overall, i think there are significant concerns with the paper, both in terms of writing and the soundness/novelty of the technical results and experiments.	0
the experiments provide results to demonstrate the effectiveness of 1), but not clear about 2).	0
it would be nice if authors add more experiments, but i know that this is always a complicated request during a rebuttal period.	0
from the technical point of view this is limited novelty, but still an interesting improvement of the model; however the experiments and results do not support the claim that using such model improves image captioning result in a significant way.	0
so it would be in order to not just dismiss the alternative in a halfsentence, but to give it proper consideration  especially since in terms of dissipation, it actually performs better than the hard constraint in the experiments.	0
the problem is interesting and wellmotivated, but i have some concerns with the proposed approach and experiments.	0
the planning experiments are interesting, but difficult to follow at least from the main text.	0
the second major concern is on the experiments.	0
while i like the overall approach and believe it could work, the experiments seem to have some weaknesses: 1. it is not clear to me why table 1 contains only upg and apg with pruning half the edges, without natural pruning baselines like uniformly subsampling the edges by a factor of half, or constructing the graph with half as many edges to begin with.	0
overall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other stateoftheart methods.	0
larger scale experiments would of course be nice and strengthen the paper but more importantly it would be great to see some form of toy example or demonstration of the principle improving robustness as well over just results.	0
it seems that the proposed work is a good attempt that applies universal learning to adversarial training, but more experiments are required to support its usefulness and effectiveness, especially for the weak attack like fgsm.	0
it would be nice if experiments could be performed on a more popular benchmark such as atari, but overall i think this is an interesting paper with a reasonable contribution.	0
minor details: there are some good contents in this work, but for it to be a strong 'empirical' contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show stateoftheart results.	0
the problem setting of this paper is interesting and the theoretical contribution is nice, but the empirical studies could be improved: 1. it is prefer to append some experiments on realworld applications.	0
 the experiments use the same setup as in cohen et al, but the results are not compared with those in cohen et al. it is not clear how to judge the significance of these results without comparison to any other method of evaluating robustness.	0
perhaps an entropyregularized setup is a useful comparison to show that it provides marginal benefit over the setup studied, and this might resolve the lack of clarity around the implications of the claims made from the first set of experiments.	0
but if we are being honest here, the experiments are very lacking to support such a bold claim.	0
i think there is value in the approach, but it is hard to see clearly at the moment given that the exposition is difficult to follow and the experiments aren't very compelling.	0
 in the second to the last line on page 4, 'level of noise (sigma^2) can be set for each class separately', but the authors did not explain how to set them, and in the experiments sigma^2 is set as the same scalar.	0
i would have been interested in 'false detection' experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.	0
as far as experiments are concerned, section 4.1 presents results on mnist, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705].	0
however, i have some concerns about the proposed method and experiments:  1. in general, i think the arguments in the paper are still a bit vague to be demonstrated using only experimental results.	0
to summarize, i think the paper could easily be accepted to a future conference, but i think it is important to:  make connection between the insights and exploration clear, specifically designing the introduction, lit review, and experiments around this connection.	0
• below eq. 16 it is said « notice that c has been omitted », but it is unclear whether it was not included to alleviate notations or because it disappears naturally in the mathematical derivation of eq. 16 • the motivation for introducing zeta below eq. 17 is unclear, especially since it seems to play an important role considering that zeta = 0.005 << 1 is used in the experiments (with no explanation as to how this specific value was selected) • hat{l}(z_i, theta, phi) (between eq. 17 and 18) does not seem to be defined • what is the motivation for using the gaussian u(z) as described in section 5?	0
one weakness of this work is the lack of largescale experiments, for example, pruning a mobilenet on imagenet.	0
however i still don't think the experiments are convincing enough.	0
another idea for experiments is doing crossdataset evaluations where different datasets may have different leaf categories but shared high level ones.	0
my main concern is the limited evaluation as all the experiments are performed on the cifar10 and synthetic data.	0
secondary experiments should be in the appendix but i feel this is primary.	0
i'm sorry if i did not get the main message of the experiments, but even after reading the paper 3 times, i did not understand what the authors wanted the reader to conclude with these experiments.	0
 code was included but no description or readme file was provided for how to run the experiments.	0
futhermore, as the authors point out in the introduction, distillation alla hinton 2015 usually assumes both teacher and student models are soft predictors (e.g., neural nets), but the models used in the uci experiments trees and svms, so i wonder how exactly they did this. '	0
the method is interesting, novel and seemingly efficient; but it is insufficiently defined, the method is not motivated and experiments are quite weak with little comparisons and no experiments with practical value.	0
2) all experiments were conducted in a whitebox settings, did the authors try to explore gray/black box settings as well?	0
the experiments validate this; the difference in rouge score are less than a point, which is the kind of fluctuation one expects due to random seed differences, etc.  i find it odd that rouge score is dismissed as a loss to train against (referred to as a heuristic in the end of section 4.1), but then 1 rouge point difference is considered a 'significant' improvement (making such claims without statistical significance testing is misleading).	0
this is probably due to the design of experiments which only model pairwise interactions, but then this questions the relevance of the proposed empirical evaluation with respect to the claim.	0
 in the experiments, h=100 replications are performed, which is appreciated, but unfortunately the corresponding confidence intervals or standard deviations are not shown, only the mean performance.	0
other remaining concerns are more generally the lack of justification and clarity of the proposed framework and implemented variants, and the inadequacy of the experiments with respect to the claim (using m=2 basically means only the first depth of the tree is useful).	0
the claim that 'this paper is the first in demonstrating that halfprecision can be used for a very large portion of dnns training and still reach stateoftheart accuracy' may not correct, in fact, nvidia's apex has already supported using mixedprecision or entirely halfprecision to train dnns, and there is no clear evidence that the proposed method is better than theirs due to the lack of experiments on more tasks and datasets.	0
the experiments indicate that there is a small, but consistent, improvement over the baselines.	0
similarly, discussions of how the proposed method can solve the issues listed may benefit from some rewriting, or even simple toy experiments to demo that those are actually the concerns that are not addressed by prior work.	0
despite the achievement indicated by the experiments, i still have some concerns about this paper.	0
strengths:  extensive finetuning/intermediatefinetuning experiments on a range of nlp tasks.	2
weaknesses:  this paper presents a lot of experiments.	0
overall, i find that the idea of unifying qa, text classification and regression is interesting by itself, but the experiments cannot justify their claims well mainly due to the mixed results.	0
my main concern with this work is actually with something that would otherwise be a strength  the very large number of experiments.	0
i think a yet another baseline is missing in the experiments, which is a strategy of clicking points in the periphery of the black box while avoiding (or minimizing) overlaps of the regions deblurred by previous clicks.	0
 summary:  key problem or question: assessing / improving the robustness of object detectors to image corruptions (simulated fog, frost, snow, dragonfire...);  contributions: 1) a benchmark (obtained by adding imagelevel corruptions to pascal, coco, and cityscapes) and an experimental protocol to measure detection robustness , 2) extensive experiments quantifying the severe lack of robustness of multiple stateoftheart models, 3) experiments showing that data augmentation via style transfer (geirhos et al, iclr'19) improves robustness at little cost (at most 2% performance degradation on clean coco images).	0
the usage of “for instance here”, gives the readers a feeling that tucker is just one possible way of doing this decomposition, but not necessarily the actual decomposition for the reported experiments.	0
the idea of making experiments on pseudoindustrial instances is interesting, but the pdp algorithm trained on those instances (i.e. pdpmodular) is rapidly degrading as the ratio increases.	0
the work seems rather incremental and the experiments have some methodological flaws.	0
this paper should be rejected because (1) this method only combines existing techs, such as stochastic generative hashing (eq.1 and eq. 6), and lacks novelty; (2) lack of introduction to related work and baselines, (3) the experiments results can not support the claim, i.e. the effectiveness of cgh in marketing area, and (4) paper writing is awful and very hard to follow.	0
strengths  the paper is wellmotivated and relevant to the ml community  lowlevel speed optimizations are needed but overlooked in the community  reasonable choice of experimental conditions (focus on unbatched cpu evaluation, testing on a selection of 4 different tasks)  proposed techniques are sensible weaknesses (roughly in order of decreasing significance)  gains over tensorflow performance is advertised in the intro, but only mentioned anecdotally in the experiments.	2
 the experiments put much focus on speed performance over different batch sizes, but this is (1) not the focus of the paper (unbatched cpu operation is the focus), and (2) is little informative because the introduced methods (which do not benefit much from batching) are not compared against tensorflow (which does benefit from batching).	0
but the authors do a thorough job evaluating the strengths and weaknesses of their method through ablation studies on latent variable capacity and comparing to existing baselines in their experiments.	2
pros: the work presents a simple idea that is presented neatly with sufficient experiments.	2
the idea is quite simple however for example i find the pseudo code in algorithm 2 is quite hard to grasp maybe due to some typos questions to authors:  we only see performance comparison in dialogue response generation experiments.	0
additional experiments which targeted a specific design choice at a time made me much more convinced that the techniques proposed in this paper are useful not only for this particular context but also more broadly applicable.	0
the experiments are rather convincing (however, the fid metric is subjective, but it's impossible to calculate the loglikelihood scores).	0
weaknesses: though extensive experiments have been done by revealing what leads the vulnerability and the effectiveness of proposed method.	0
the conclusion claims that the proposed shallow architecture exhibits 'lower training difficulty' but i did not see any support for this in the experiments.	0
it works hard to achieve a good experimental result, through many tricks listed in the “additional implementation details” in page 4, and through the data augmentation used in page 5. however, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in oliver et al. (2018).	0
i suspect if the model was insensitive, you could've fixed them for many of these experiments, but seems that is not the case.	0
the topic is interesting, but i haven't been convinced through the limited scope of the experiments, or the arguments made what the real point is.	0
for the experimental part, the comparison experiments in subsection 4.1(toy examples) and subsection 4.3 (color image superresolution) lack comparisons with the latest methods.	0
the experiments conducted on vgg and resnets display the comparable accuracies between the bon and the standard big models, but with much more compact representations and balanced computational cost.	0
it should provide not just an algorithm and some numbers in experiments, but also why we need the algorithm, what's the key insight of designing this algorithm, what the algorithm really did by the algorithm mechanism itself instead of just empirical numbers.	0
(3) experiments: my biggest concern about the experiments is that human evaluation should be conducted, given the subjective nature of the task.	0
the experiments show that grf achieves similar performance to graphnvp (a previous flowbased model for molecule generation), but has about 100 times fewer parameters.	0
pros:  extensive experiments were performed to test the methods, and the results seem promising.	2
i will vote 'weak accept' for this paper, as i think it is incremental and the experiments are too limited.	0
i do understand that the authors cite oh et al. 2018 who apply the same technique of sparsification, but oh et al. also conduct additional experiments in ale.	0
also the brn baseline is included in table 3 for imagenet but is missing from the coco experiments.	0
i will keep my score, but do believe that further experiments and small adjustments to the writing will see a future version of this accepted.	0
the conducted experiments show that using ast paths from root and leaves are good for ast node generation, but whether those inputs are robust and sufficient should be further explored.	0
the paper is mostly clear, some claims are not backed up by experiments and the experiments are lacking.	0
however i am not entirely sure that the impact of these results will attract the interest of a broad audience since the experiments presented are on “small datasets” and with some rather shallow neural nets.	0
mixae features prominently in your related works, but is not compared to in your experiments.	0
it looks from the experiments that this is true but some theoretical insights about why it is the case can yield a strong paper.	0
its justification is plausible, but it would be more convincing if other dynamic approaches were considered in the experiments.	0
this is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper.	0
cons: the paper is not wellwritten and the experiments and discussions are not support the ideas, more specifically i can mention several concerns: 1 motivation: the flaw of the previously proposed calibration metric is not explained clearly.	0
the general trend for all experiments is that for lower annotator workloads, communitympa is better, but performance for both models generally increase with higher annotator workloads.	0
i was expecting to see a similar comparison in the experiments section but svrpg and hapg do not appear in the experiments although their convergence rates are comparable to vrmpo.	0
cons:  the experiments are not sufficient to validate the arguments.	0
decision i vote to reject this paper because the formulation of the penalties lacks clarity and because the experiments are incomplete and likely provide misleading results.	0
2) comparisons of zero initializations the experiments on the zero initialization scheme is an interesting investigation, but it is incomplete as there is no direct comparison with training without sepup.	0
while the topic is interesting and the paper is clearly written, there is a lack of control/comparison experiments, such that the paper’s conclusions are not backed up by the analyses.	0
 most experiments in the paper are conducted on a small data set and this is a big downside of the paper.	0
this paper is fairly intuitive, but i am not sure about the fairness of the comparisons in the paper, and the level of rigor of the experiments.	0
however, i think there are several shortcomings with the experiments: the experiment contains only one (fairly old) attack method.	0
thoughts after author feedback: i really appreciate the response of the authors to my review, which included some interesting new experiments and explanations addressing concerns i raised.	0
they perform experiments evaluating the whitebox and blackbox robustness of their detection scheme.	0
the paper can be strengthened by the following: better framing of the problem improve the descriptions on the experiments carried out including a quantitative discussion on the results and the corresponding uncertainties systematic studies on the behavior of the proposed method components including a mathematical description in the probability space more detailed comments: better framing of the problem: the framing of the problem and the task lack at least a qualitative discussion on the issues arising in using visual based vs lidar/radar.	0
i do appreciate that the authors conduction various experiments using different dataset and architecture but i do not understand why the authors want to separate the experiments into parts: adaptive methods and adaptive momentum methods.	0
 aside from sgd with momentum, the experiments part also lacks several important baselines.	0
 overall i am positive about the paper and have no major asks but if time and resources permit perhaps trying out the same experiments on imagenet to see if the trend holds.	0
the problem is interesting and potentially important, but many experiments are too simplistic and lack strong baselines.	0
major comments i have a number of serious concerns about the paper's motivation, logic, and experiments:  first, the paper motivates the proposed regularization as a way to encourage the network to have line attractor dynamics.	0
concerns:  although saving checkpoints is “cheap” and shows empirical good performance, the models are somehow dependent on each other, particularly in experiments where consecutive checkpoints are saved.	0
i am glad that there were thorough experiments to address this concern and demonstrate that it works.	0
1. my biggest concern is about the experiments.	0
even though the experiments show effectiveness partially on vggnets, but the overall improvements are not sufficient for me to claim the general effectiveness of the method unless the paper could provide additional results on broader range of architectures and threat models.	0
)transform and ctransform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.	0
however, i have two concerns about their method and experiments.	0
the authors mention the initial learning rate used for the experiments, but it is not clear why those values are used?	0
i am not convinced that we can draw valid conclusions from the experimental results for the following reasons:  the experiments are lacking important details.	0
the evaluation section lacks experiments that evaluate the computational savings.	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
the experiments on omniglot and miniimagenet are coherent with the theory, but i am not completely sure of their impact.	0
my main concerns are about the experiments:  for the translation experiments, the transformer numbers are lower than those reported by vaswani et al. (2017) across the board, for both the 'base' and the 'big' settings.	0
the paper proposes to combine the implicit function idea with output mode estimation, however the problem definition is vague, competing bayesian methods and density estimation methods are ignored, the experiments are insufficient with little stateoftheart comparisons, few datasets and clear problems in the learning.	0
 i may have missed this point, but how many seeds did you use to run your experiments?	0
however, i found the presentation of material very confusing and poorly organized, some claims misleading, and some experiments missing.	0
it's mentioned in the last paragraph of the intro that you do two experimental protocols, but these are not referred to (at least not by the same name) in all of the experiments.	0
the experiments are described from a technical perspective but i did not understand what they are actually supposed to show.	0
 could you also compute the sum of the exponential of logits for the synthetic data, since it is the only metric that is evaluated in the real data experiments but not in the synthetic data?	0
in summary: i believe this interesting work, but requires more experiments in different environments and additional ablation studies to show the feasibility of the proposed method.	0
nonstandard experiments it is not clear why the authors did not use the standard nchain task but rather used the modified version.	0
overall i find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data.	0
while the ideas (particularly the smooth rendering system) are interesting, the limited nature of the experiments and the lack of comparisons with baselines or prior work makes it difficult for me to support acceptance.	0
the proposed method is well motivated and quite justified by the experiments abliet lacking comparison with previous published results.	0
my main concern is with regards to the experiments, which are clearly not enough detailled.	0
another downside is that, although sim2real is used as a motivation for this paper (since this is the most typical scenario where states are available during training but not during testing), the experiments have not been performed using physical agents.	0
the novelty is somewhat limited: they're plugging together two existing approaches (hart and sab) to allow for interaction, but i am recommending acceptance because i found the experiments surprising (as a negative result)  they show that independent hart models are a strong baseline for tracking in settings that involve interactions, but that are nevertheless solvable without knowledge about the state of other objects.	0
pros: the methods seem practically useful as verified in the experiments.	2
the authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when farkas' layer is not used.	0
the work is predicated upon the assumption that the ratio of said costs , but the authors state that an unspecified subset of their experiments violate this assumption.	0
i see why the contribution can be viewed as minor, but it is wellmotivated and looks like a nice set of experiments.	0
 the experiments provide interesting insights, especially figure 2 and the graph coloring results in table 3. cons:  showing improvements in the number of steps compared to vsids is not interesting because vsids as implemented in minisat and stateoftheart solvers like glucose has been tuned to minimize running time rather than number of steps.	0
pros:  in general, the numerical experiments are at the level of the state of the art.	2
the experiments are also simple, but straightforward and easy to reimplement.	0
empirical experiments on couple of molecular graph data suggets that graphnvp approach performs as well as prior approach but albeit without any rule checker.	0
my third concern is about cub200 experiments.	0
pros: 1. nice application of bert to grounded instruction following tasks 2. good empirical results cons: 1. not much technical novelty 2. empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise) other comments: 1. are the bert weights frozen or finetuned along with the rest of the model?	2
i especially appreciate the additional experiments, but i’m still confused why the authors do not perform a clear ablation study to support their claims.	0
pros  well designed experiments with sensible baselines.	0
my main concern is the motivation of the paper and the experiments.	0
my main concern is about the experiments.	0
while there are parts of this paper i like very much and i think it is technically sound, i feel the experiments section is lacking somewhat (see comments below).	0
 the experiments are lacking a little.	0
the experiments are somewhat rudimentary, but they are perfectly adequate for a paper whose contribution is primarily methodological and theoretical.	0
weaknesses of the papers: 1. no effective baselines are used for comparisons in the experiments.	0
there are some details lacking, for example how specifically are the metatest mdps chosen for the mujoco experiments?	0
for example, the technique can be applied to any architecture, but the experiments in the paper are limited to a single architecture; and additional experiments with architectures and tasks that commonly use ensembling would make the experiments more compelling, ideally with comparisons to external results.	0
 as the authors reproduce the experiments, it would have been useful to add the original results in the table whenever it is possible  the paper gives a feeling that it lacks some rigor (missing ref, detected bug, mathematical formalism is light); it makes me a bit skeptic regarding some experimental conclusion.	0
however, the overall level is slightly below the iclr level for the following reason: incomplete bibliography, lack of complementary experiments, lack of discussion.	0
on gnnfilm's training stability and regularization: i am interested in the negative result described in the following sentence: 'preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by shchur et al. (2018))' do the authors have any intuition about why the results are highly dependent on the random seed?	0
the two main reasons are: (i) i am missing concrete examples of where application of lattice embeddings is more beneficial than simple continuous (or binary) valued embeddings; (ii) if the main application of this algorithm is supposed to be compression, then i am missing sufficient background section on alternatives to the presented algorithm and comparison to these within the experiments (i am admittedly no expert on the use of ml algorithms for compression, but a simple search reveals, e.g., [1] as a relevant baseline).	0
the experiments are a little insufficient, as the authors used very small datasets, and there is no comparison with other carefully tuned optimization algorithms, which, circle back to my previous concern, is that whether such an algorithm could actually outperform a carefully tuned sgd, adam.	0
2) the claim is that ep learns robust features for deep learning methods, but both the analyses and experiments are not enough to show that.	0
my main concern remains the experiments.	0
weaknesses more experiments can be conducted to demonstrate the effectiveness of llql.	0
then the second contribution claims that the calibration term that is added to the loss function improves the ood detection as well as calibration in the network, but the experiments are not designed to show the impact of each regularizer term separately in improving the ood detection rate.	0
given my major concerns about the theoretical motivation and comparisons to past work, i do not find the experiments comprehensive enough to prove the value of the proposed approach to the community.	0
 the authors introduce a fid score for video which is similar to fvd, but fid is only used to report results in one experiment, while fvd is used for the rest of the experiments.	0
2. the paper mentions the attack problem and proposes an attack algorithm (algorithm 2), however i am not able to find any experiments on the attack.	0
while the proposed approaches are theoretically sound, i have several concerns, mostly on the experiments.	0
however, i am not convinced by the presentation of the paper and by the lack of multitask learning baselines in the experiments.	0
also, it discusses some open opportunities on how the embeddings can be used as a base knowledge source and some challenges in them pros: 1. the examples and analysis of embeddings involved in the paper is detailed 2. the authors have run lots of experiments to position their idea correctly.	2
however, i have concerns about the experiments.	0
i liked to see experiments performed on different tasks and datasets but overall that section could be significantly improved.	0
multiple experiments that are less thorough would also benefit the paper, but less so.	0
asis, i give this paper a weak reject, but would be willing to increase if some of these experiments were tried.	0
the approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.	0
the paper reject: the paper lacks novelty and have very weak experiments.	0
the experiments lack strong baselines, and comparisons with previous ml solutions, how important is the design of the cnn for this task.	0
but i have serious concerns about the experiments.	0
this itself does not make the paper bad but combined with the unconvincing experiments it's a serious weakness.	0
the experiments seem sensible but i have some suggestions for improvement.	0
4) the experiments seem interesting but i am not sure how to interpret the results.	0
this would be fine if one could improve the stateoftheart method's training time by a lot, but according to the experiments, the performance of adversarial robustness is far from the adversarial training, for example in cifar10 epsilon=8/255, there is a 12% drop, which is a huge gap.	0
overall, although the paper does well in motivating the problem, the lack of rigorous experiments and poorly structured writing advocate for a weak rejection.	0
 the paper presents a, to my knowledge, novel approach, to avoid the leakage of metadata in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)  the paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below) weaknesses: 1. experimental evaluation: 1.1. the paper only evaluates on the training set.	0
i agree to other reviewers, that the limitation to linear debiasing is a concern for the paper, but the authors have clarified it now in the abstract an other locations; the additional experiments with and rbf kernel have shown that indeed the formulation only does mainly do linear decorrelation.	0
 the evaluation of the idea is not complete while it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other blackbox attack methods in the limited query budget regime, i would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.	0
2. overall assessment: while this paper is quite fun to read, it is not innovative enough and it lacks some critical experiments and error analysis to be accepted this time.	0
i have some concerns on the writing and experiments of this paper.	0
however, i find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, detailed under: experimental concerns: — evaluating on 200 examples seems very small.	0
weaknesses  lack of baselines / experiments.	0
i trust the results because i performed exactly the same experiments for cifar10 with longer nonregularization periods and found that there is no effect (this is also that the authors show in the paper) but i didn't test on other datasets and obviously didn't think about potential benefits for compression.	0
the toy experiments in section 6.1 were illustrative but seem to only consider adaptive probabilities without the adaptive moments?	0
i don't doubt the experiments results, but if one discovers something like that, it needs to be explained.	0
another downside of this work is that the convergence analysis is done on amsgrad instead of adam which the experimental results are based on, why were experiments not done with altqamsgrad?	0
the traditional answer often revolves around limited computational resources, but for this set of experiments it is particularly concerning.	0
2. as a paper focusing on experiments, the results are not enough, in the following senses, a) there are other existing work of mcts in continuous action spaces, but they are not mentioned, like 'monte carlo tree search in continuous action spaces with execution uncertainty', yee et al., 2016.	0
the experiments also lack various important details like the model architectures and other hyperparameters, and overall are mostly 'proof of concept' experiments rather than exhaustively testing out the proposed method.	0
the results all appear sensible and expected, but the experiments are somewhat weak.	0
5. claims are made about how many layers a certain dataset needs for sufficient classification through heuristic experiments; however they are not thorough enough in terms of ablation to fully make this claim.	0
6. extensiveness of experiments  i do like the toy dataset as an example, but to show effectiveness of this framework, a larger breadth of datasets could have been used.	0
i have two concerns as follows: 1. the authors conduct experiments on multiple small kg datasets such as fb15k and wk15k.	0
final evaluation  quality: experiments could have been cleaner, but they basically demonstrate the patterns the paper intended to show.	0
clarity: i could understand most experiments at a high level, but i found it hard to understand the motivation and the rest of the experiments.	0
all the information in the experiments are somewhat 'indirect' (success rate, test accuracy, etc.) to answer this question, but there is no direct evidence.	0
both seem like quite interesting directions but seem like preliminary experiments that don’t work convincingly yet.	0
the gan experiments shows the gan loss makes gtn realistic (as expected) but there are no quantitative results on mode collapse.	0
pros: the idea itself is interesting, the related works are discussed well, and mnist experiments are very interesting.	2
i'm not a fan, but adding results on a sota imagenet paper always helps with making the experiments section crisper. '	0
the experiments look interesting, but there are some minor concerns.	0
the experiments shows that it can work in some cases, but i do not see an explanation (the 'residual learning' paragraph is high level and i do not get an insight from that.	0
it's ok to have some contrived experiments, but it seems like all experiments on which you were able to provide evidence that your technique was helpful are contrived.	0
furthermore, the experiments seem to lack the details on the number and type of gpus used for obtaining the reported results.	0
however, the traditional rl experiments are lacking.	0
the experiments are illustrative but more comparisons with the other methods will be appreciated to make it more convincing.	0
 the experiments are illustrative, but might be too toyish under the federated learning setting.	0
(major concern) in most experiments, there is a huge gap in the performance between fawd and farm.	0
the authors spent a lot of time discussing ondevice computing, but all their experiments are just simulations on standard benchmarks.	0
for such a paper concerning training on edge devices, i would expect to see some experiments on real edge devices.	0
though my concern on writing has been resolved to some extent, i'm still unsatisfied with the empirical experiments.	0
experiments seem to show the proposed benefits but are done with artificial models/simulations.	0
i also thank you for the additional experiments concerning (33).	0
all that said, i'm comfortable marking the paper as a 'weak accept' since i think there is a real scientific contribution here, but i encourage the authors to make a serious effort to improve their presentation and tightenup their experiments.	0
the topic of the paper is interesting and the approach seems to be solid, however the experiments are not so convincing.	0
the paper is nicely written and easy to follow, but with relatively thin contributions besides the large amount of experiments.	0
 the abstract and intro summary of contributions didn't do a very good job of conveying what the methods do, although they are clearly explained elsewhere (see suggestions below)  a lot of time is spent on detail of experiments and long, clear explanations (this is great), but makes it read a bit like a lab report.	0
the specific numbers used in the experiments are reported, but it is not clear how these were chosen, and how one would choose them for a new dataset or model.	0
the experiments show that with only the first step, the proposed method is worse than the original batch learning, but by adding the second label smoothing step, there is improvement over the original batch learning setup.	0
in the experiments, it seems that the proposed method has consistency but no other baselines have consistency.	0
most of these concerns had to do with the limited scope of the paper's initial experiments.	0
given the harsh discretization of the scoring system (1, 3, 6, 8) i cannot in good faith increase my score from a 3 to a 6, but i would like to make it very clear that these changes to the experiments do improve my view of the paper  moving from a weak reject to neutral, though that is a not an official option.	0
i agree with the authors that both schemes would have been within the 2^m possibilities i encourage them to consider, however i do not fully agree with some of the conclusions the authors draw for these experiments.	0
the experiments on the scan dataset concern a standard rnn model learned from data.	0
 the experimental validation is largely lacking, as the authors only perform experiments on imagenet and do not compare against recent stateoftheart bayesian sparsification methods (sbp, vib, l0regularization).	0
overall i think this is an interesting paper but experiments could be improved.	0
this idea could be a good tradeoff between full rnn (slow) and no rnn (lack of context), but the following is missing: (1) ablations on speed vs results by locality window, (2) experiments on more widely reported and larger datasets and models, at least including some language modeling task (wiki or lm1b) and some translation task (like ende).	0
iii) lack of more challenging, or largescale experiments.	0
in sum, the paper provides a novel insight on how pruning affects the performance at the example level, but does not provide a solution, and the current set of experiments is insufficient to validate that the empirical findings that the authors report generalize to other types of pruning approaches, such as inputdependent pruning.	0
however, while the overall approach is intuitive and seems to yield desirable results, i have concerns regarding the experiments, comparisons to prior work, and the exact contributions of this work.	0
3) while the experiments in sec 4.2 clearly demonstrate the benefits of the approach, the ones in sec 4.1 and 4.3 are less convincing: 3a) sec 4.1 shows that the slot based transition model generalizes better, but this is only in comparison to a naive fullyconnected baseline.	0
i also like the triviaqa experiments but the numbers are way behind the standard reading comprehension results.	0
weaknesses:  i think it is a valuable contribution, but my major concern is that the authors only conduct experiments for the classification task, whereas the original bayesian dark knowledge approach also deals with the regression task and shows some interesting results (see sect.	0
some experiments are run with cifar and others with 'random' data, but random in which sense?	0
the paper proposes random policies or initial frames, but as i understand experiments are only reported for the latter design choice.	0
lots of relevant experiments are reported but they don't support clear conclusions and i'm not sure how well the models were tuned.	0
my major concern on this study is the experiments.	0
### updated after author response ### the authors have tried to address my concern by rerunning the experiments, which i greatly appreciate.	0
i understand that it is tough to develop a new technique and 'compete' against so many already existing ones, but nevertheless i would suggest to at least extend one of the experiments to a largerscale comparison.	0
4. experiments  the experiments are lacking a brief explanation: what are they supposed to show or analyse?	0
i tend to reject the paper in its current form because (1) the idea of using mixture of trees to do policy extraction is somewhat incremental; and (2) experiments do not consistently show significant performance gain over the existing approach.	0
the proposed algorithm looks very simple, but it appears that it could be effective through some experiments on two data sets.	0
however, i have several concerns about the experiments, motivation, and algorithmic decisions which make me hesitant to recommend the paper for acceptance.	0
2. the paper contains some simple experiments, but i do not believe they are an adequate enough evaluation of the proposed approaches.	0
however putting everything together seems to work as demonstrated by the experiments.	0
authors mentioned this in the intro, but perhaps show some convincing experiments to make sure risi2019 will definitely fail vizdoom?	0
experiments highlight that the nll and reward can be very poorly correlated, that improvements in nll initially improve reward but can later degrade it, and that models with similar nlls can lead to very different rewards.	0
when i read the abstract i had the impression that the paper actually had a theoretical analysis showing the correlation problem, but there is no such thing, only experiments.	0
one concern i have about the experiments of fig 3 is that nll can be really bad, thus distorting rho, which is not a robust measure.	0
another concern about experiments is that i am not convinced that they were performed with sota mbrl methods and hyperparameters (as demonstrated by sota performance on known benchmarks).	0
overall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete. '''''''''	0
in most of the experiments the student has a (much) lower bitwidth than the teacher, but they also apply the method with a student network with less parameters.	0
they perform experiments on different architectures (alexnet and resnet18) and different kd losses, and the conclusions are always consistent with the hypothesis, but it’s not clear whether the improvements are statistically significant or not.	0
the experiments are well done, but i am not sure what we learn.	0
in annex d.3 and specifically fig. 5, network depth is addressed, but virtually no further details or performance measures are given for these experiments.	0
i am willing to revisit my score if the above concerns are appropriately addressed and requested experiments are provided.	0
(2) the experiments give tons of number but it lack of detailed analysis, like specific win/loss case of this model.	0
concerns the learning rate schedule on the cifar experiments is unconventional.	0
in the current revision, the paper lacks coherencythe proposals in each section do not seem connected to one anotherand it is difficult to assess from the experiments whether the proposed algorithms address a known problem in existing methods.	0
 while the proposed approach and evaluation metrics are novel, and the results generally support the claims of the paper  toy experiments result in recovery of the groundtruth concepts, concepts identified for text and image classification offer feasible takeaways  there is still a lack of proper humaninterpretability aspect of the discovered concepts.	0
however, i have some concerns related to the novelty of the algorithm and some details in the experiments.	0
the main concern of this paper is the reproducibility of the experiments.	0
overall, this work makes good contributions to the community, but the experiments need improvements.	0
cons: 1. it is pity that the trained model is only evaluated on one test set and experiments are conducted on one language pair.	0
in section 5.1 the paper mentions 'the singlemodel achieves a bleu score of 29.7, already outperforming the current best system', but in tables 1 and 2 it seems that the best score with single model is 28.7, not 29.7. i feel that the results are a bit disappointing given the scale of the experiments.	0
figure 2 seems to be dynamic based on the main text, but i cannot tell if it's the case for other experiments.	0
reviewer has several concerns regarding the experiments.	0
my two main issues with the manuscript is that the theoretical part is written very imprecisely and that the experiments are not convincing due to the lack of good baselines and of statistical power.	0
 the outofdistribution experiments seems lack of thorough study.	0
while the idea has some merit, it is an interesting premise to train a recurrent generator to create images in multiple domains, i feel as though the experiments are quite lacking.	0
however, the comparison to other approaches does not show a clear advantage, and the related work (and experiments) lacks recent techniques.	0
 as said above, the experiments are interesting, but somewhat artificial.	0
the paper is interesting but more details could be added to both theories and experiments.	0
 experiments: the proposed methods show a lot of potentials, but the description in this section is sometimes unclear.	0
pros:  the experiments are of good quality, providing a lot of ablation studied and hyperparameter specifications.	2
(1) in the experiments, the practical benefit of adding the auxiliary detection task is demonstrated, but the final scores of the proposed model are clearly below those of current state of the art in terms of both reliability and accuracy.	0
strengths:  analysis how perturbation target and search image influence tracking performance  demonstrates that including detection objective during training improves performance weaknesses:  makes claims about siamese trackers in general but only experiments with authors’ architecture  analyses in figs.	2
pros: faster speed of adversarial training could substantially democratize ability to work on adversarial ml neural net attack could overcome gradient masking in some situations cons: some of the main arguments seem incorrect, although the paper may still be strong if these incorrect arguments are removed and the experiments are further substantiated.	2
3. another problem of the experiments is that lee et al. [2018] proposed a stochastic model for video prediction, but the authors only compared the lhstm with its deterministic version.	0
in conclusion, the paper presents a set of nice experiments, but doesn't really shed too much additional light on the scientific nature of the lottery ticket hypothesis.	0
the experiments seem promising, but i have the following questions before being able to assess the results:  how many lstm units are in the model in each experiment?	0
still, i have several minor concerns regarding the algorithm and experiments.	0
however, the related work is not always well described, the experiments lack important comparisons, and the practical effects of pcgrad should be explained in more details instead of focusing on a proof for a convex case.	0
the experiments were also noiseless, and they had gaussian or the fourier sampling (which is not random, but rather heavily biased toward dc).	0
the experiments show enough promising results to make this interesting, but they have not explored the details of the extensions.	0
the authors show with their experiments that multigrid memory networks outperform dnc and other models that lack its multigrid inference property.	0
while the experiments show the proposed networks’ superior performance over baselines, my main concern about this paper is the lack of comparison with more recent memoryaugmented models [1,2].	0
however, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak.	0
6. the final accuracy of all the experiments are shown using the max of top5, however appendix d shows quite a significant variance for the methods.	0
b. missing experimental depth: the experiments shown in the paper are interesting, but only scratch the surface.	0
======================== update after rebuttal thank you to the authors for addressing a number of my concerns, adding additional text and experiments.	0
my current vote is borderline reject but can be improved by further highlighting why quadcopter dynamics are only partially obsevable and further experiments.	0
experiments: my main concern with the paper is the experiments of more complex systems.	0
since the main contribution of this paper is the empirical results on the three tasks, my concerns regarding the experiments need to be addressed before i can vote for accepting this paper.	0
however i do believe this paper is relatively weak in contribution, especially the experiments can be done more thoroughly.	0
pros  the writing is great and easy to follow  the method is theoretically motivated  experiments prove effective cons  the proposed method may not work well for complicated environments (1) in the discussion after def.4, given an alignment task set d_{x,y}, how do we know whether a common (w.r.t.	2
 there is tts experiments on the demo website, but i didn't find any details.	0
review summary: despite the clear computational advantages of the proposed implementation, this paper should be rejected because (1) the authors claim multisample dropout to be a “new regularization technique”, but the conceptual differences between standard dropout and their proposed method are not clearly stated, (2) one of the main claims regarding “better generalization” is not well supported by their experiments, and (3) the experiments do not consider the stochasticity of the methods they are evaluating.	0
5) it is not professional to report only training loss and validation error for some experiments, but report training loss/error and validation error for some others.	0
i appreciate that the authors report some performance in terms of ece in the supplement, but i think it would be very informative to report performance in terms of ece for all domainshift experiments: the brier score conflates accuracy with calibration (see eg the 2 component decomposition), whereas ece directly quantifies calibration and is hence easier to interpret and arguably the more meaningful measure when quantifying calibration.	0
negs:  while there are plenty of experiments, there is a lack of detailed descriptions.	0
 experiments:  i'm not sure i'm convinced by the statistical tests used on the lstm results; they demonstrate that your approach, with a few specific sets of hyperparameter settings, does better than the baseline, but not that this represents a valid claim about your activation function's effect on this lstm model in general.	0
in the experiments they show that having the prototype part improves robustness and accuracy, but the authors never show what interesting prototypes they learn.	0
the lack of discussion on some of the common problems for vaes and the experiments are the weakness of the paper.	0
comparing wyner vae and vccaprivate, i can see why it is hard to justify the difference of the two based on the formulations, but i also find it unconvincing from the experiments to conclude that one is better than the other, especially when there are many hyperparameters to tune.	0
the experiments in the paper are somewhat lacking.	0
however, i have some concerns: 1. the experiments on the celeba dataset are mainly subjective.	0
convincing quantitative experiments on disjunction and negation lacks.	0
al. the authors have carefully analyzed weaknesses in previous work and i think their experiments do suggest that they have improved on them.	0
the paper provides experiments on cifar10 and cifar100 where the improvements of using such regularization is visible but not sufficiently significant.	0
pros:  intuitive results with a well designed workloads and experiments.	2
second, for any person working in blackbox optimization it is clear that 16 experiments is next to nothing.	0
this paper is leaning toward rejection because (1) the proposed method lacks novelty, (2) it contains technically imprecise parts and (3) the effectiveness is not fully validated in the experiments.	0
more detailed questions:  what are the standard deviations for the experimental results (as you reported in table 4 but not in other experiments)?	0
5. can the model be generalized well for data collected in different experiments (i.e., different tissues) but from the same machine?	0
these synthetic setting can be used for sanity check, but cannot be the main part of the experiments.	0
extensive experiments show a small but consistent improvement over a baseline method.	0
suggestions for improvement on the experiments my main concern with the experiments is that a similar small improvement over the baseline could be achieved by tuning the hyperparameters in an alternative simpler regularization method.	0
overall, the proposed method is evaluated under elaborate and detailed experiments and enjoys promising results, but lacks novelty and theoretical contribution.	0
summary: my main concern about this paper is its novelty, as the method essentially uses the method of narang et al., 2017, albeit with a different threshold, with the sparsity patterns of mao et al., 2017. the experiments demonstrate that the method is effective at pruning, but do not provide any timings to evaluate the resulting speedups.	0
strengths:  the paper's experiments show an improvement in the model's performance relative to past work, utilizing a large number of comparison models.	2
not saying the current trend is correct or incorrect in doing so, but lack of experiments and details leave this claim unsupported.	0
the ideas of the sampling or interpolation experiments go in the right direction, but in their present version, they are not entirely convincing.	0
(d) hyper parameter tuning: last but not least about the experiments is the hyper parameter tuning which is not addressed.	0
in the experiments, the authors compared proposed?noise with proposed, random, bald, coreset, and entropy, but i think the only fair comparison here is between proposednoise and proposed.	0
however, my concerns about the types of experiments and comparisons that were done (see below for more details) are big enough that i cannot approve this paper in its current form.	0
e.g., brookes et al., design by adaptive sampling, arxiv:1810.03714 the paper is clearly written but for such a simple method one would need really convincing results and experiments.	0
from these experiments, the authors observe that pruning large score weights generates instable but high test accuracy and smaller generalization gap compared to pruning small score weights.	0
however, as far as i can tell, the ranker model is treated as a black box, so it could easily be some random forest model, etc. if there are some implicit assumptions that the ranker model is a neural network, this should be made explicit; if not, the discussion should be revised (and, of course, nonneural models should be used in the experiments).	0
as far as regularization based on the local curvature is concerned, i would like to see some more experiments that compare the proposed technique to adagrad/adam and other techniques that purport to condition the gradient based on local curvature.	0
the main contribution lies in 1, and the derivation for the additive coupling block (volume preserving) is neat (although fairly straightforward), but the derivation for the affine coupling layer (nvp) is not useful nor insightful; they are local lipschitz bounds (so require bounds on all intermediate activations, which is difficult as pointed out by the authors), and the numerical value of this bound was not used at all in relation to the numerical experiments  i imagine the bound is loose.	0
main arguments: my main concern is about the experiments: why were cohen et al.’s models used instead of salman et al.’s?	0
even though it has many theoretical analysis and experiments, the paper itself is poorly written.	0
the authors also provide experiments that includes the usual loss v. epoch, but also loss v. wallclock time (which is nice when proposing secondorderlike methods where extra computations are necessary), and a test error v. epoch (which is again nice for secondorder methods as explained below).	0
(3) also due to (2), the experiments shown in the paper are on toy data and hence lack of strong empirical support.	0
i believe the lack of multioutput regression experiments makes the paper a bit weak, therefore i changed my score to 3 and vote for rejection.	0
i looked at the provided code hoping it would help to clarify some of the implementation details, but the code is not at all a complete collection of routines that could recreate the experiments.	0
2) for the uci experiments the comparison is only made against deepensembels or other vi methods, however to the best of my knowledge mcmc methods are superior in this setting given the small dataset size?	0
last but not least, authors have carried out experiments in a larger scale to get more stable results.	0
these sorts of controlled experiments are useful, but the actual reported performance for both models are somewhat off of the stateoftheart and it's not clear that the relatively small benefit the authors show over their baselines are maintained for higher performing architectural configurations.	0
overall, i have concerns with some of the contributions, experiments, and presentation, which leaves me at a weak reject.	0
finally, the authors claim that the method alleviates the need to postclassification smoothing, but this cannot be straightforwardly concluded from the conducted experiments.	0
independent of any other concerns, i would be hesitant to accept the paper with the current writing given the very general nature of assertions made despite experiments in far more specific settings.	0
“both objectives in … are however difficult to optimize for anything but small scale experiments.”.	0
i tend to vote rejection for this paper, given that the proposed algorithms seem incremental compared to the existing algorithms, and the experiments seem not sufficient enough to support the core claim proposed in the paper.	0
as this term is already present in eq. 6 it would be interesting to repeat the experiments, dropping the second term (minimality) but instead sweeping over the strength of the entropy regularization term .	0
in the paper the authors keep mentioning that the method can be applied to gps but i don't see experiments or algorithms for it? '	0
for experiments, the results presented in table 1 are good but there are no official baselines (e.g. from some prior works) to make the comparison more reliable.	0
the experiments should be rigorous cause it lacks reliable baselines for comparison.	0
 comments on rebuttal i don’t think that the authors made a valid argument to address my concerns about theoretical justification and experiments.	0
in addition to this, the paper only presents results on mnist in a toyish setting, this makes me feel the paper may be more suited for publication in a workshop (idea is interesting, small scale experiments to illustrate the insights, but not complete enough to be published at a conference).	0
i think the idea is still interesting and worth pursuing, but given some of the new observations and experiments run the paper needs to make more changes than i would find reasonable for acceptance.	0
conclusion: the paper introduces large claims and empirical results that correlate with, however the provided experiments are not done with enough control to attribute gains to the design choices provided in the paper.	0
the paper tries to solve a problem in the task unsegmented setting, but why there is no such setting in experiments that the metatraining set cannot be segmented to different tasks?	0
moreover, there are no experiments comparing the proposed approach to the stochastic atari environments investigated in pathak et al 2019. i understand this would require dealing with discrete action spaces, but i don't see why this would be infeasible.	0
while the authors motivate the use of the proposed intrinsic reward for learning to solve tasks in sparse reward environments, the experiments do not include moreover, some of the tasks used for evaluation do not have very sparse reward (e.g. acrobot but potentially others too).	0
it is concerned with the examination of pruning experiments for a lenet on the mnist dataset.	0
pros:  well written cons:  experiments are weak  unclear narrative; what's the one key message?	2
for example, questions like ‘how does the distribution of tasks affect intrinsic rewards’ or ‘does intrinsic reward generalise’ are not really answered by providing metrics of performance or generalisation in controlled experiments, but by providing some example cases.	0
pros: the proposed method, despite being simple, seems to empirically work well in terms of the mce criterion evaluated in the experiments.	2
cons  there doesn't seem to be a proper baseline in any of the experiments conducted.	2
 there's lack of analysis / interpretation of results for section 4.1, e.g. what is the motivation of the experiments and what is the conclusion.	0
the experiments on cifar10 is lacked.	0
## strengths  the paper addresses a relevant problem with an intuitive approach  the paper is well written and easy to read  the analytic toy experiments at the end of sec 4.2 and in 6.2 help understand the properties of the learned representation  the proposed method, when applied to the modelbased algorithm planet, shows improved robustness in settings with artificially introduced visual distractors in simulated dm control suite tasks ## weaknesses (1) unclear whether artificial distractors are indicative of behavior with real distractors: the fact that distractors do not follow coherent motions but instead randomly change position between two consecutive frames makes it hard to estimate how this would perform on more natural distractors.	2
to tie my points #1 and #2 together, i feel the authors did experiments on a variety of different tasks, but these style transfer and subject verb agreement tasks are not particularly interesting or realistic  instead this space should be devoted to discussions of the advantages of their method and analysis on its performance, which is quite lightly covered.	0
sce is supposed to be more effective in case of having more number of classes however authors do not shed light into this properly by taking advantage of comparing their cifar100 and imagenet 1k experiments.	0
i assume it might be because they intended to compare to (guo et al, 2017) but there are more experiments there for comparison.	0
this is partially answered with figure 2, but it would make this easier to see if the experiments included stronger baselines, e.g., by adding the same regularization tricks to consistency regularization methods, perhaps in table 3. finally, since future work on pseudo labels will follow this paper’s setup, hyperparameters such as lamba_a, lambda_h, and alpha should be chosen carefully instead of fixing them.	0
auxiliary experiments on cifar100 or something with more than one class would go a long way towards allaying my concerns on this front.	0
given that no hardware requirements are presented it is difficult to judge if expecting multiple runs is unreasonable but unless each run requires weeks of the authors’ full hardware capacity, there is no reason for the authors not to include error bars or expected variances on the numbers for as many of their experiments as possible.	0
2. there are two stronger assumptions in the paper, but they are never verified in the experiments.	0
the authors claim that the context image must only contain the layout in any format which makes it possible to extract information about navigational space from it, but in the experiments the context image corresponds to the full map – and it is probably equivalent to the observed images, but we can’t be sure as we haven’t been told.	0
the experiments show that the proposed approach leads to fewer evaluations but larger mean errors.	0
i still think that the ideas are interesting but that the experiments do not demonstrate enough of the proposed method.	0
4. there are a number of issues with the experiments that i think could use clarification or improvement: (a) the authors compare to a 'baseline' but i don't believe this baseline is defined anywhere.	0
(d) the number of steps required was provided for the harmonic oscillator experiments, but not the van der pol ones.	0
i have some concerns about the analysis and the experiments of the paper: most of the analysis is tailored for a very simple linear discriminator case which for the wgan means just matching the first moments.	0
the experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures.	0
i have some concerns about this paper: (1) the analysis lacks theoretical insights and does not seem to be very useful in practice; (2) the proposed method for graph sparsification lacks novelty and the experiments are not thorough to validate its usefulness; (3) the writing of this paper is messy, missing many details.	0
the methodology is however not particularly well motivated and the experiments do not convince me that this proposed measure is superior to other reasonable choices.	0
decision this paper presents promising empirical results, however the experiments are limited, making it difficult to place in the broader work.	0
negative points: (1) lack of experiments on benchmark and large environments.	0
however this is not shown in the experiments.	0
i argue for a weak reject of this paper (in its current form) because (1) the presented method seems to be too incremental, and (2) i am missing experiments that compare to other stateoftheart methods (e.g. on the task of 3d reconstruction from single images).	0
in experiments, there is no evidence showing the benefit from the pixel cooccurrence there is a lack of much details.	0
as an advice, i think it may be better to not squeeze in all the results into one paper, but rather focus on some aspects and analyze them really well, with clear explanation, motivation, and preferably with demonstrating practical gains that result from the analysis  not just hypothetical, but supported by experiments.	0
pros: 1) an interesting topic of trying to understand generalization and transfer in deep learning 2) multiple types of experiments, including visualizations of loss landscapes at convregence and at initialization, plots of hessian eigenvalues, measuring the deviation of the weights from their initial values, measuring the variance of the gradients of the weights, measuring the transfer between different datasets, measuring the transfer performance depending on the durarion of pretraining.	2
they also mention conclusions from experiments using the food101 and places datasets, but don't show these results anywhere.	0
cvpr 2018. the paper contains mostly empirical evaluations however the provided experiments do not well support the claim that cnn works well with geometric structures in geolocalization.	0
however the experiments are quite limited, with only 2 training datasets.	0
pros:  extensive experiments have been done on several datasets, both synthetic and real.	2
to sum up, a very nice idea that shows promise but experiments on other problems is missing for a complete picture.	0
one of the main drawback of the present paper is however the lack of convincing experiments.	0
in conclusion, the idea proposed in the paper is interesting to me but the effectiveness of each module is not very wellsupported by the experiments.	0
simulations are constructed to verify the arguments in the paper but there is no experiments on real dataset.	0
i wasn't referring to any specific experiments, but the questions you are raising in the paper, for example in the conclusion section.	0
3. some of the experiments seem like interesting starting points but do not support general claims.	0
review:###review summary  overall this is almost above the bar for me to accept, but i think there's enough concerns about the method and experiments that i'm hesitant.	0
weaknesses include a vulnerability to outliers, experiments that don't seem to think about practical effects like dayofweek in forecasting, and experiments that leave out baselines to help directly assess the impact of neighbors.	0
the proposed algorithm looks robust and wellmotivated, but the text of the article and the experiments can be improved.	0
yet another concern is about the experiments.	0
one weakness of the paper is that the relative incremental nature of the study, but given the thoroughness and clarity of the experiments, and the nontriviality of the architecture search, i think this is a valuable contribution for the iclr community.	0
this is fine for me in principle, however haptic measurements would probably have large errors, while it seems that the experiments use ground truth values for these points.	0
my concerns are as follows: • considering the existing reverse relation issue in fb15k and wn18, i suggest conducting the experiments on the fb15k237 and wn18rr from [6] instead.	0
right now the outcome of the experiments are a bit vague based on lack of hypotheses.	0
the experiments show the technique works as advertised, but the importance of the result is low.	0
2) experiments: a major concern reported throughout the paper is the instability of training and the risk for overfitting.	0
with that said, i think that the methodological contribution is incremental (sampling from a conditional rather than marginal distribution), there is at least one major correctness issue that needs to be addressed, and the analysis of the experiments fails explain why the models perform differently.	0
3. in general, i thought that the experiments were well done, but stops short of explaining 'why' the methods perform differently.	0
my overall decision is borderline (learning towards accept), as the experiments were well done and serve as a good proofofconcept, but i am unsure if this approach will scale well outside of the particular tested domain.	0
i'm a little concerned about the fairness of the comparison experiments.	0
my decision is to reject the paper due to methodological issues with the experiments and lack of evidence wrt/ dataset variety.	0
batchsizes in the experiments vary, but no justification is provided for how these are selected.	0
experiments: i want to give the paper credit for performance on cifar100, but this is difficult without explicit points of comparison.	0
 the second concern i have is about the experiments.	0
my concerns with experiments and clarify remain unaddressed, and are amplified by reading the other reviews.	0
the major concern i have is the reasonability of the experiments.	0
shouldn't the experiments be showing these weaknesses in some sort of controlled setting?	0
some claims in the paper also need to be tempered:  the abstract suggests performance improves 'remarkably', but experiments do not support this.	0
weaknesses  the lack of experiments.	0
my main concern towards this paper is about the experiments part from several aspects.	0
====================== after the rebuttal i thank the authors for their response but i still feel that the assumption is not welljustified and there is still a lot to improve in terms of experiments.	0
this can be made concrete using many experiments in the paper, but let’s take the activation functions experiment of 3.2 as an example.	0
the specific technical details on some experiments were either difficult to find or were lacking.	0
although this applies more broadly to most of the experiments, we can use section 3 as an example again: the details on the experiment in 3.1 were found in the caption of figure 1, whereas i would have expected them either in the main body or clearly listed in their own table; the details on the experiment in 3.2 specify that everything is identical between the tests of the two activation functions, however it is never specified exactly what is being altered (and by how much) to vary the epsilon value.	0
although this learning framework seems like a potentially interesting future research direction, i tend to lean towards rejection for the reasons that: (1) the theoretical analysis is difficult to follow and there is sometimes a lack of clarity throughout the paper, (2) it isn’t very clear how easy this framework would be to implement aside from the theoretical guarantees and there aren’t any experiments or proofs of concept that would demonstrate the feasibility or practicality of the proposed framework in a real scenario, (3) the paper would benefit from more discussion of how their work differs from related techniques (like hierarchical rl, various forms of metalearning, etc.).	0
decision: weak reject 1. interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a lowlevel feature representation, but i believe the experiments could be improved.	0
2. i think that the claim that the use of neural networks with discrete inputs can approximately solve nphard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands i don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.	0
this is an interesting approach, however the experiments make me doubt the effectiveness of the proposed method.	0
i have a few concerns regarding the experiments however, which explains my score: 1. in figure 2, does marthe diverge for values of beta greater than 1e4?	0
the experiments only show that the proposed method got a good performance, but the analysis of the reason is not shown.	0
experiments: what does it mean to be application agnostic but restricted to particular datasets and losses?	0
also in figure 3b, the attack accuracy are no different between causal models and dnn on test 1. the last minor question is why only parents of y are included in causal models in the experiments, but not the markov blanket as stated earlier in figure 1.	0
in this work, the goal is not to develop any more complex policies, but to have different agents on the same task learn diverse policies (and since the experiments are in mujoco, the degree of diversity is limited).	0
====================== after the rebuttal i thank the authors for their response but i still feel that there is a lot more to improve for this paper in terms of intuition and experiments.	0
the other experiments on cityscapes and pascal voc are certainly interesting, but the method is compared only against hung et al. which a different family of methods and the subset baseline (which is useful but not enough).	0
weaknesses:  i am missing a more systematic comparisons to baseline defenses in the experiments.	0
the authors do cite [1], but unless i missed something, their main argument for uniqueness is basically 'in experiments, we prefer s to the jacobian, because in order to compute s it is enough to look at the network as a black box that given an input, generates an output, without requiring further knowledge of the model.'	0
the experiments show superior performance of the proposed methods, but the datasets are only two for each tasks.	0
therefore the fact that the authors evaluate their approach only on a single environment / task combination, both of which they introduce themselves, weakens any conclusions the authors draw from their experiments.	0
during the rebuttal discussion i highlighted concerns and proposed possible experiments.	0
overall this paper is well structured/written and well placed in the literature, but i think it is not yet ready for acceptance due to the following key reasons: (i) i think dpvae, the currently better performing method, is illposed for ssad since it makes the assumption that anomalies are generated from one common latent prior and thus must be similar; (ii) i think the worse performance of mmlvae, which i find theoretically sound for ssad, is mainly due to optimization issues that should be investigated; (iii) the experiments do not show for the bulk of experiments how much of the improvement is due to metaalgorithms (ensemble and hyperparameter selection on a validation set with some labels).	0
second, there is a lack of technical detail in the experiments necessary to replicate them.	0
lastly, the experiments show the performance as a function of queue length for different features and for cnns versus decision trees, but there is no comparison to existing methods and very simple models are used  this means that again, it's difficult to gauge the efficacy of the approach and place this in the context of prior art.	0
in fact, it could even be that the task is just simple, so that more restricted model with fewer parameters generally perform better  i do not claim this is the case, but the experiments do not rule it out and, hence, do not confirm that the clever gato recurrence makes the difference.	0
(35) where they are proposed, but the experiments applies to resnet.	0
 given the lack of theoretical / conceptual guarantees that the methodology will work, our faith in the proposed methodology rests entirely on the empirical experiments.	0
however, my biggest concern is the experiments were not designed carefully to analyze how much the hindsight modeling contributed in the increase of performance?	0
despite these concerns, i feel the experiments are for the most part quite well thought out and executed.	0
besides the lack of experiments on real data, i find the paper’s material to be a bit disjointed and ununified.	0
this lack of baselines and reference to related work makes the experiments inadequate.	0
in general, my main concern with the experiments is that the section is written as if this is the first work proposing filter learning.	0
also, there could be downstream computational savings, e.g., at prediction time, if the dimension was very large, but this is not the setting tested in the experiments.	0
also, in this equation, the gradient of the loss wrt/ z samples, average of gradients over z samples times..., does not seem to match what the gradient would be given the algorithmic description in the supplementary material, a gradient of the (sample) average z times... ' the abstract states the paper is proposing a method for highdimensional feature selection, but all of the experiments have datasets with max.	0
evaluation: while the general flavor of question the paper studies is undoubtedly interesting, i found the paper severely lacking both in terms of the quality of writing (in particular, i was at confused about the goal of various sections/experiments), as well as the significance of the results the authors observe (and how they are reported  i found them to be oversold).	0
 as a generalized poshoc explanation generators verification framework, the experiments are seriously lacking and are not well designed to illicit broad applicability.	0
review:### after the original rating of 'weak reject' still holds as the authors failed to provide proper justification for the raise concerns and support their claims through additional experiments.	0
i thought the paper was written well, and its experiments were done quite carefully, but it was lacking on the novelty front.	0
the experiments are effectively done on toy data, which is fine, but the paper stops at that point.	0
i realize there are some experiments with resnet, but the focus is mainly on the singlepath vgglike architecture.	0
the learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture.	0
6. in table 5 of the appendix, why experiments use adam optimizer, but not momentum sgd as in the main paper to compare the performance of resnet14 and resnet20?	0
however, there are some experiments i expect to see but not yet included in this paper: 3.1. the authors only studies nesterov momentum in this paper.	0
the experiments are accurately described and performed but authors have only considered sequence of 2 tasks which is far from being considered as a continual learning setting.	0
2. another concern is about the experiments.	0
however, i have a few concerns on the experiments.	0
i understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.	0
the controlled experiments conducted in the paper are still informative, but the overall message would be much stronger if the empirical analysis can be extended to common benchmarks such as cifar and/or imagenet.	0
i had also requested to see the source code for the experiments as this would perhaps have illuminated a lot of the details left out in the paper, but the authors have not provided it.	0
i understand that the authors are not required to provide their code, but this should have been a relatively straightforward request in this case given the simplicity of the experiments and as i mentioned in my initial review, it would have been very useful in evaluating the paper.	0
 the authors mention tat h(t) matrix is highly sparse but also lowdimensional in all their experiments.	0
 for the experiments, authors use a specific 10000 steps budget but this seems to be highly curated.	0
 the experiments are run on 3 different random splits, based on the mean(std) of the evaluation metric, the performance of the proposed method does not vary much compared to baselines, especially on the second dataset, eg., gnn 0.25 / 0.02 compared with genn 0.26. also, glenn < gnn seems to imply including energy is not the most important part for helping the task, but rather the semisupervised joint training truly improves the performance.	0
this is an interesting paper: it warns the abuse of likelihoods computed by flow models with several numerical experiments, which are simple but clearly designed to support the claims.	0
summary  good research question concerning the robustness of flow models  simple and understandable claims supported by simple experiments  easy to read  can be improved more to clarify the difference from the previous work that study the flow model’s likelihoods.	0
2. address the concerns raised over the experiments.	0
 as the authors themselves acknowledge, the proposed method is similar in spirit to schuster et al. (naacl'2019) which is also much simpler but they do not compare to it in their experiments.	0
personally, i find the experiments a little lacking and it is particularly puzzling to me why the authors restricted the scope of this work to only character level lm tasks.	0
2 incremental novelty over the prior work (frcl by titsias et al 2019): this baseline is the closest prior work to this work which according to the experiments shown in table 2 are slightly outperformed by the proposed method.	0
while i accept the response for the remaining questions from authors but i am still concerned about the weak experiments and an issue brought up by r3 regarding lack of enough comparisons with frcl on any other datasets besides split mnist and pmnist.	0
there are several areas where the explanation can be compressed, making room to add more informative details (which are in the appendix), or increasing the size of the figures (which are too small)  the paper seems lacking a bit in experiments.	0
without discussing existing work and offering detailed comparisons and experiments, this paper essentially just shows that rl can be applied to such problems, but the reader wouldn't know whether it is the best tool, or simply if rl is the hammer that is used to treat every problem like a nail.	0
main arguments apart from the interesting approach, this paper should be rejected due to several reasons: (1) the specific application field, considered in this paper, have to be generalized to more general cases, in order to be valuable for machine learning community, (2) from rl algorithms perspective, the novelty of proposed method is questionable due to lack of literature review and advanced approach to deep reinforcement learning, (3) requirement of explainable ai is essential for deployment and in spite of the quite sufficient explanations of the algorithm’s works, this paper does not well justify its superiority over the traditional methods either by theory or practice, due to experiments suffering from the lack of variability and missing the main point of improvement, which leads to generally insufficient contribution (4) the paper is unpolished and lacks specific formulations from the existing literature on related subjects.	0
additional experiments can on different environments structure (city grids, metro network) of complexity of the environments (the width of the cell in the grid) to measure the performance of the algorithm based on hyperparameters (3) realworld data brings more applicationbased matter into the subject, but requires more thorough investigation on the optimality of the solutions, proposed by rl method.	0
 the experiments are claimed to be zeroshot, but 35s of speech is required.	0
4) my only concern regarding the paper is w.r.t some arbitrary decisions made in the experiments e.g. how was the exact neural architecture for f in section 3.2 chosen?	0
the paper also lacks experiments about batchrenorms on ddpg and td3, which would be a fair comparison against crossrenorm.	0
in further experiments, it is shown that crossnorm stabilizes learning in most contexts, but does not guarantee to converge in all settings.	0
given that the empirical result is pretty much the only support of the claim in this paper, the lack of more diverse experiments would weaken the contribution.	0
moreover, i have some concerns on the experiments.	0
this might be a minor point, but equation 1 suggests it is in fact a state vector, while i thought quantum optical experiments work with density matrices.	0
to reiterate, the reason i am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion.	0
to reiterate, the reason i am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion.	0
i think it’s in general very helpful to release the codes for reproducibility and replicability of the experiments, especially in the case of an incremental study.	0
negative points:  the importance of the problem is motivated by future assessments by downstream tasks but do not address this aspect in the experiments.	0
experiences on tiny imagenet are better than cifar but still a little far from what i'd call real images but i understand it can be difficult to run experiments on imagenet.	0
the paper shows somewhat thorough experiments on many datasets justifying this observation, but the theoretical part is rather weak since it doesn't seem to address this issue with the focal loss.	0
however, my main concern is whether the experiments can be representative enough for large scale experiments (e.g. using nonsubsampled imagenet dataset with parallel training using ssd storage).	0
also, i have some questions on the experiments: 1. experiments are only tested under one kind of random perturbation with different strengths.	2
2. it is mentioned in the introduction that some methods were proposed to filter out the input noise, but they are not compared in the experiments.	0
 i have a number of concerns about the experiments.	0
these experiments can serve to show that the trained network is not behaving erratically, but we cannot conclude any improvements.	0
the proposed algorithm for generating features seems relevant and correct, but there are shortcomings in the presentation and the experiments are not entirely convincing.	0
the experiments provide some evidence but not convincing, especially for the part about image classification.	0
it would be useful to compare different methods with different parameter settings ' the authors mentioned “visual pendulum tasks” but didn’t include them in the paper reproducibility  it's unclear to me how reproducible the research conducted in this paper was, and it would be useful to open source the code used to conduct the experiments.	0
finally, the parameter 'beam width' used in the evaluation of the value function but is only set to 1 in all experiments.	0
here are my concerns:  since the environments are taken randomly in the experiments, it is not investigated how sensitive the method is with respect to the choices of environments.	0
the paper is wellwritten and the experiments are thorough, so i have no major concerns.	0
the discussion is useful but in my subjective opinion could be toned down; the experiments and discussion in the main paper are strong and less speculative.	0
my concerns lie with the novelty of the proposed model and the insufficiency of the experiments.	0
considering tensor factorization with randomization for network robustness makes a lot of sense but overall the experiments of this paper are not wellconducted towards comparative studies with other sotas, although ablation study shows the considerably improved robustness from the proposed method.	0
a minor comment: in the introduction, it is said that 'terminal reward rl settings' are considered (the reward is given when the goal is achieved)  which is an extreme case of sparsereward rl problems  but in the experiments nonterminal reward environments are studied, e.g. 'appledistractions' where each of apples yields 0.05 reward.	0
however, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way, decision: even though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution.	0
the paper is well written and organized, experiments carried are extensive but the reuse of known neural networks, many simplifications (shortcuts), a not clear enough methodology (see below), limited processing & recognition tasks used to support it, do not justify in our opinion the main (overarching) work’s claim:  in 3.2 optimizing recognition loss/last paragraph: “interestingly, we find that image processing models trained with the loss of one recognition model r1, can also boost the performance when evaluated using recognition model r2, even if model r2 has a different architecture, recognizes a different set of categories or even is trained for a different task.”.	0
strengths 1) the authors present extensive experiments on many datasets.	2
on the other hand, it can bring over estimation to the statevalues due to the smoothness enhanced to the fitted v. it would be better to see e.g. when these concerns do not matter (theoretically) or they are not real concerns in practice (by further inspecting the experiments).	0
concerns #2: insufficient experiments  to evaluate ood detection quality, id/ood datasets should be stated and various metrics (e.g., auroc) should be measured like other literature, e.g., table 2 in [2].	0
they provide experiments showing the effectiveness of semanticadv; analysis on attributes, attack transferability, blackbox attack, and robustness against defenses; as well as user study with subjective.	0
cons: 1. in terms of experiments, the proposed approach adds a few million parameters to normal transformer (table 1), but in terms of interpolation it only improves 3% (extrapolation improves 0.5%) at 700k steps.	0
2. in terms of experiments, this approach is only evaluated on the mathematics dataset, but the argument for relational encoding is pretty general.	0
 the experiments only evaluate the likelihood, but it is not clear whether this is on a training or testing set.	0
 for the experiments, to justify it is robust to attack i think it's important to try on various blackbox attacks, including zoo (chen et al., 2017), natural evolution strategy (ilyas et al., 2018), nattack (li et al, 2019).	0
first, the paper is poorly written; there are many claims the authors are making without providing experiments/proofs/citations.	0
strengths: a lot of nice experiments with clearly advantageous results are given.	2
from the experiments, it dumbs the metalearning algorithms and make it pass the 'unittest', but i am not sure what other practical benefits it can bring to improve real systems.	0
2. the experimental results concerning network training are very hard to interpret, as they lack error bars, confidence intervals, and many critical experimental details (e.g. how many networks were trained, the hyper parameters for training, etc.) for these results to be interpretable, the authors should include a detailed description of the environment under which the experiments were performed, and present confidence intervals which demonstrate the significance of the improvement attained by sapl.	0
these are some concerns on the paper: 1) the effectiveness of the approach is not necessarily significant in all experiments.	0
the results on imagenet under 4bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations.	0
3. degradation levels (dl) are mentioned early on but the experiments and figures were not shown in the main text (deferred till appendix).	0
furthermore, the experiments on beamrider show that this concern is not a theoretical one but quite practical. '	0
cons experiments:  experiments were conducted only using a few recent results as baselines (icm, forward dynamics, rnd).	2
summary a good paper overall, but the experiments were relatively weak (common for most iclr submissions) and the novelty was somewhat limited.	0
as an example, for the experiments in charadessta dataset, they include scores for different ious levels, but they do not repeat this for didemo dataset.	0
 all first quotation marks are backwards in the document  i think the experiments ran were a bit lacking in robustness and details.	0
overall, this is a very well presented work, although it lacks some novelty and a few more thorough experiments to fully understand the improvements they show.	0
however it is not clear if this dataset will be publicly released or is only for internal experiments.	0
the description of the model was somewhat confusing to me, but my understanding is that the model does the following:  the dataset contains states, and each state has a label that says whether it is safe or unsafe  however, we don’t know the implicit constraints that determine whether a state is safe or unsafe  we learn a latent representation of the states, where we want the safe states and unsafe states to be separated in the latent space  we use a wasserstein distance metric in the latent space to construct a safety cost function the paper gives pseudocode for the algorithm, and experiments in a traffic simulator of the task of driving through a twolane roundabout with four exits.	0
 the paper is well written with clear pointer to existing work on doubly robust estimators with standard ignorability assumptions and the work for the linear, low rank model case by kallus et al. 2018. cons: major issues  there is no reason to believe that even for the synthetic experiments, that the vae posteriors would asymptotically yield unbiased ate's which is provably the case in (kallus et al. 2018) (of course for their restricted linear model/low rank assumptions).	0
i have three major concerns, including a lack of novelty, unconvincing experiments, and poor presentation of the work.	0
on the other hand, the generalization experiments seem much more promising, but i would like to see more (at least the humanoid robot as well) to confirm that this result is not only a specific case of this domain.	0
cons: it's weak accept rather than accept from me because of how the empirical evaluation were conducted in the paper, and i think the experiments conducted in the paper were a little bit weak (common for most iclr submissions).	0
reinforcement learning experiments could have been noteworthy but the authors have solved quite small problems and did not compare their results extensively against contender approaches like augmented random search.	0
concerns  most of all, the information the experiments conveys is too small to convince the argument of the author.	0
however, i do have some concerns regarding the specific contributions of this work, and several reservations about the experiments reported, and would overall argue for rejection.	0
i think the paper is interesting but needs some polishing to make it easier to read and perhaps some improved experiments in real datasets.	0
the authors contrast the guarantee with what film (a competitor approach) can do, but in the experiments film is not taken as a competitor in multitask supervised learning, leaving a big gap between theory and practice.	0
in particular, i believe the authors have 'not' answered their first proposed question 'does our method enable effective multitask learning both in settings where there is substantial overlap in tasks and where there is little overlap' properlytheir best evidence may have been mt10 and mt50 experiments, but even in those experiments, i am not sure whether the authors want to take the results as suggesting there are 'substantial overlap' or 'little overlap.'	0
in experiments, there are edge deletions, but no edge addings.	0
about the experiments, i have several concerns.	0
the paper focuses on the understanding of rl when deep qlearning diverges, however, most of the conclusions in the paper are not based on the necessary theoretical proof, but the observations on the experiments.	0
for example, if we take 1/depth as a measure, it would correlate very well with generalization in all these experiments but 1/depth is definitely not the right complexity measure or generalization bound.	0
from the cons written above, this paper has too many unclear parts in the experiments and method section.	2
also, i’m curious why these experiments are only conducted on the fsdd set, but not on the gsc set.	0
in sum, i think though the paper makes contribution on exploring better flow models but the novelty is relatively weak, the discussion and comparison of related work is insufficient and the experiments are not convincing or have mistakes.	0
a lack of quantitative evaluation further exacerbates this issue: nearly all experiments, even those with associated plots, are characterized qualitatively and without reference the performance of related methods.	0
while the improvement in computation is there, what i find lacking is the experiments demonstrating clear evidence of overcoming catastrophic forgetting problem.	0
 the paper mentions the experiments are conducted on mujoco but the appendix mentions the classical openai gym experiments.	0
in experiments, only cifar10 results have been reported, but i wonder what is the error bar looks like?	0
 the experiments show promising results but only on cifar10 and only with the outdated bnn, also as a necessary baseline it would be important to show the effect of the bias initialization on relu networks.	0
experiments: as far as i can see, the difference between roots and gqn is that gqn is a little more blurry in its output (figure 2) and roots has a slightly better mse for lots of objects (table 1) but produces nlls similar to gqn.	0
the qualitative experiments are nice but would be substantially improved by showing that:(a) that gqn doesn't do any of these (b) that roots can train on 23 objects and test on 35 objects by simply changing the prior on k  this is one of the primary advantages of objectcentric representations of scenes (the ability to handle arbitrary numbers of objects).	0
i suggest to move the details of experiments in section 4 to the appendix, but it may depend on the authors’ writing style.	0
another sales pitch that the paper brings up a lot is the low space complexity, however this benefit has not been fully demonstrated, given the smallscale network/experiments.	0
1. my major concern is about the experiments.	0
the increase in accuracy isn't large enough across experiments to allay my concerns.	0
judging from very specific settings in different experiments, this could be a serious concern. '	0
this amounts to 1) quantitative experiments which seem to only test how well each method works in relation to the very metric which only your proposed method directly optimises  this is nice but not surprising.	0
cons: 1) the experiments are limited.	0
strengths: i liked how the authors distinguished between the underparameterized and overparameterized regimes in the analysis and experiments.	2
pros:  novel latent method for goal conditioned prediction (sequential and hierarchical)  really cool experiments on navigation using the predicted frames  outperforms used baselines weaknesses / comments:  missing baseline: the human 3.6m experiments are missing the baseline from wichers et al., 2018. i would be good to compare against them for better assessment of the predicted videos.	2
major comments:  authors need to include more related work and describe the main related paper they mention (peterson et al 2018) as well as describe how their work fits in with previous work  while the idea here is novel and impactful, the experiments used to explain the importance of superordinate labels do have not much compelling information and are not well described  4.2 plots for visualization are mentioned to be in the appendix, but are not there minor comments:  fig2 large subordinate group text would help  lots of typos throughout and grammar mistakes o typo ‘use vgg16’ and then ‘vgg16’ in same paragraph bottom of page 4 o typo top of page 2 “convolutional neural network(cnn)” o appendix list – ‘banna’ typo under fruit o page 1 intro ‘for both behavioral and computer vision’ doesn’t really make sense o page 3 top section ‘new one’ should be ‘new ones’ o bottom of page 3 ‘room from improvement’ o last line of conclusion – ‘classificacation’ consensus: this is a very interesting and potentially impactful idea, but the experiments used to defend and explain the importance of superordinate labels are relatively weak.	0
i also have some concern regarding the fairness of the comparison in experiments.	0
<conclusion> although this work is practically promising, my initial decision is ‘reject’ mainly due to lack of technical novelty and limited experiments.	0
in the experiments, the performance improves significant when using out of domain dictionary, but less significant for indomain dictionary.	0
i also have a concerns about the experiments.	0
the experiments describe some ablations, but does not really answer some questions: is regularization important if one uses kmeans initialization?	0
it would be good to clarify it somewhere since the paper only mentions 'loglikelihood' but report a loss that should be minimized  section 4.4 'differently from the other experiments, we choose a learning rate of _x000f_ = 0.02 since the original value does not lead to convergent learning.	0
the experiments are very poorly described, so it is difficult to gauge if they are valid: most importantly, the authors point out that the estimated error rates do not always match the actual error rates.	0
for example, they appear to only be valid with respect to an assumed distribution over z. the paper’s experiments however focus in large part on what happens when the model is evaluated on values of z that were outside the support of the distribution over training domains.	0
despite the intriguing nature of this paper, there are several weaknesses which make me less enthusiastic about the quality of the paper:  the experiments are done only on cifar10 and cifar100.	0
the results include 4 common cnn architectures and experiments on cifar10 and cifar100, which is acceptable, but only single results are shown, which makes it hard to judge the significance of the performance of the redistributed networks, especially given that there seems to be no particular trend in the performance of any of the templates.	0
overall, the idea is somehow interesting, but the experiments are weak.	0
(i acknowledge getting experiments working on imagenet is no small undertaking in terms of both engineering time and cost, but it would improve my confidence to see those results.)	0
however, i find it concerning that the authors study pruning on this specific model but no others, particularly because they focus on a different network (namely, resnet20) in all other experiments in the paper.	0
my main concern is that experiments are only performed in the twotask setting, which is highly restrictive.	0
overall the experiments seem lacking at the moment in rigorous comparisons.	0
furthermore, the authors should highlight all this in the experiment text, e.g. noting ewc does poorly but this is because we use a different protocol than this and this paper etc. regarding the experiments under this light they do look more reasonable.	0
but i do think it could use some further writing revisions to emphasize/clarify key points: a) the method is not new (it says e.g. in abstract “new model” which is misleading) but its application in cl is underexplored b) the experiments show poor performance on existing methods because most of those are not designed nor work well for the shared head “task agnostic” setting, while metric learning handles it gracefully.	0
3. the experiments are lack of comparison with other exploration methods.	0
it does not connect imgep and msee, but connects visit count and entropy; 3) the experiments choose one way to approximately reaching boundary of visited and nonvisited states, which is lack of comparison with other choices; 4) the experiments look promising, especially on supermariobros, but more experiments on other tasks and comparisons with other exploration methods are needed to evaluate the proposed method thoroughly.	0
the authors mentioned that such an objective function is asymmetric but didn't explore the implications of such an objective function in the empirical experiments. '	0
the paper contains easy to understand, concrete experiments and results, but seems altogether a little underdeveloped.	0
the methodology is sound, but the synthetic experiments around which much of the paper is based may not be sufficiently novel and give little indication of broader implications.	0
moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis.	0
namely, umn is claimed to work on samplewise label noise, but there are no experiments to support this (note that this is different from nonuniform classdependent label noise).	0
major points: the generative process defined in equations 2 and 3 presents a model for inputdependent label noise, but the corruptions in the experiments are conditionally independent of the input given the true label.	0
the authors demonstrate their results on svhn, cifar10, cifar100 cifar10.1 using a variety of network architectures including vgg, bagnet, wideresnet, alexnet, etc. major concerns: 1. as presented, the experiments are not convincing.	0
the random noise and realworld corruption experiments are definitely helpful to clear this doubt, but only partially.	0
weakness: 1) experiments: my major concern is the fairness of the experiments.	0
in terms of experiments, the assumption made is that we have access to a set of 'good' trajectories (which is easy in the proposed environments, but may be difficult in the reallife).	0
weaknesses : 1. combining xd with multilingual tuning is not effective in improving average results or even in case of target languages 2. final system is adhoc as experiments on a particular set of languages have been used to support claims.	0
one drawback with experiments is the lack of comparison with slalom for inference if goten is assumed to be a framework for both training and prediction in a privacypreserving way.	0
 strong experiments and benchmarks cons:  some sections are not explained well and unclear as mentioned earlier.	0
the authors argue 'interpretable' in their title and abstract, but their method and experiments do not show this point explicitly.	0
particularly, details regarding the task setup, how many demonstrations are needed per task/skill, architectural choices and hyperparameters are lacking and makes experiments hard to follow and understand.	0
finally, baseline experiments are limited to other weakly supervised learning segmentation approaches, but i think comparisons with unsupervised clustering methods would also be useful.	0
as a side note, for the experiments concerning degan vs the baseline, it would be nice to check what happens when the last classification layer is not initialized with lda parameters.	0
the experiments seemed limited in scope, given that the authors discuss reinforcement learning in general, but only provide results on the reconstruction error when doing imitation learning.	0
cons:  lacking more realistic experiments on larger datasets such as imagenet and models like resnets.	0
cons: 1) experiments are limited: 1a) the method is evaluated on custom and relatively toy datasets.	0
 the paper lacks practical experiments.	0
the experiments on mnist behave somehow similarly to the toy experiments, but the effects are less obvious in my point of view.	0
toy experiments are fine to illustrate the concept, but at some point, we need to move on to actual data.	0
likewise, the experiments part lacks comparison with appropriate baselines, which raises concerns on actual performance of the proposed approach.	0
similarly, i appreciate that you run some experiments using different initializations at finetuning, but we are limited in what we can conclude from these experiments too, since all share the same pretrained bert base and thus are very likely to share similar vulnerabilities. '	0
4, one additional thing that i concern about this paper is the reproducibility of the experiments.	0
the paper should contain the discussion toward this work (pros/cons against the proposed method) and also should compare in the experiments.	2
the authors claim that gans do not work well for their application, but it would be more convincing if there are citations or experiments tha t can back this up.	0
the other major concern of mine is that experiments.	0
 the second important shortcoming of the paper is lack of proper experiments to back up the claims in the paper.	0
as for the experiments, i'm no expert in this specific field, but i would be really surprised if this work is among the most competitive ones, and this paper does not compare to many recent efforts, such as [1], which i found by a quick google search.	0
> the statistics are shown in figure 11 fig 11 is in the appendix, but this seems like one of your main experiments.	0
2. the experiments show some promise, however the edit distance of perturbations is limited to one.	0
while i like the general idea, i feel that some experiments are currently lacking to fully support the authors' claims.	0
## feedback for improvement ' the authors should address the reviewer's concern about clarity and experiments.	0
 i find the description of the experiments confusing: you claim to use multitask training, transfer learning, as well as cotraining  but from what i can tell, the two methods are essentially the 'grover' method with extra text markers, and an autoregressive application of a seq2seq model (which is autoregressive itself in the case of the story model)  there is a concern about novelty: 'unlike these techniques, our task here is automating science journalism'  this in itself is probably not a contribution, however the 'ra' metric could be an important and novel contribution  i must admit i am not sure i understand the ra measure from your description.	0
 although this paper ambitiously proposes the automatic science journalism as a new challenging summarization task, what is actually done is nothing but primitive experiments about basic transfer learning.	0
the experiments are lack of details and therefore not clear enough to support the arguments: i. the author(s) did not mention how they chose or tuned hyperparameters, for both their method and baselinesthe comparison is not guaranteed to be completely fair; ii.	0
2 experiments: authors provide many experiments to support their claim which is great but they avoid the most direct way to evaluate the measure.	0
i have a number of concerns regarding the motivation, experiments and overall writing quality, so i am currently leaning toward rejecting this paper: 1. the justification for paths in section 2.1 is extremely handwavy.	0
experiments demonstrated that the proposed advspade is able to generate whitebox and transferbased blackbox attacks with reasonable performance (see table 1).	0
however, i have some serious concerns with the experiments: 1. the experiments lack fair comparisons to achieve a solid conclusion.	0
the authors mention that the ppo algorithms are in the appendix, but there are only three cherrypicked experiments, which may look suspicious.	0
expect to see experiments or necessary explanations for this concern.	0
more such experiments might have made the paper stronger, but in their current form i don't find them too illuminating or surprising.	0
weaknesses: (1) experiments: my biggest concern of this paper lies in its experimental design.	0
i understand that this paper is more like a theoretical paper, but proper experiments are still needed to justify your theory.	0
while i am very sympathetic to the aims of this paper, and i feel like it makes an important message (that there is no such thing as an imperceptible manipulation of text), i feel the execution of the paper is somewhat lacking, and the experiments need a lot of tightening up before the paper is ready to be accepted.	0
the presentation is clear in the sense that the reader can understand the design of the technique and of the experiments; but less clear in the sense that the term bias is overloaded: early on (introduction) the authors state that removing the gender dimension is equivalent to removing gender bias (this is attributed to bolukbasi et al. 2016 and prost et al. 2019).	0
there also exists an extensive literature of sentence retrieval, and some relevant works in rl such as (dulacarnold, 2015) algorithm part:  this paper focuses on rl, but i have a few concern about the soundness of the experiments.	0
i am particularly concerned with the rl experiments run with n=1.	0
note that this observation does not apply for n=3, and i have no concern regarding those experiments.	0
in the end, i recommend paper rejection for the following reason:  i feel that the paper is lacking sufficient analysis  the rl experiments with n=1 concern me  the novelty is extremely limited over the literature and (urbanek et al 2019).	0
 for pose estimation, why don’t perform experiments to report the results on the complete set of human joints, but choose headonly, even if you claim the method could be applied w/o any modification?	0
in this paper, i don’t find those experiments; (3) this paper compared the results with word2vec and glove, but not clear which version to compare.	0
this is not inherently a problem, but i would argue that it necessitates that experiments prove these assumptions to be useful.	0
it is possible that the results in the original paper are not reproducible or that the experiments differ, but this should be stated in the paper, since the natural assumption would be that the baseline was not adequately reproduced.	0
my major concern on this paper is that experiments are not designed to measure the impact of each design choice.	0
i have some following major concerns about the paper: (1) this paper conducts extensive experiments to demonstrate the performance of n^3 in table 2, but why not compare n^3 with the work published in 2018 or 2019?	0
to start, there should be additional detail on what these classifiers are (see note about general lack of clarity in experiments section below).	0
i find the experiments inconclusive due to lack of statistical significance testing, insufficient data, and inadequate hyperparameter tuning.	0
to make the empirical evaluation more rigorous and convincing, i think the authors should: (1) repeat the adversarial robustness experiments on more commonlyused datasets such as imagenet and cifar, and also evaluate robustness of their models more thoroughly, using techniques like blackbox attacks.	0
as of now, i am recommending rejection, but i would be willing to reconsider my score if the authors performed the abovementioned experiments.	0
i am not comfortable enough with this field to carefully assess this paper but i don't think this paper is above the acceptance bar due to (i) relatively weak novelty of the proposed approach and no discussion on its shortcomings and limitations; (ii) experiments with architectures well below the state of the art.	0
2) about the experiments: 2.1) every result in this paper is solely on mnist (apart from table 1 which reports some results on celeba but doesn't compare to other models).	0
however, as stated before, the results fail to provide significant insights into pointbased neural architectures due to (1) lack of specific applications the authors are trying to solve and (2) arbitrariness of the chosen experiments lead to equally arbitrary conclusions.	0
in all these experiments, the anticollapse regularizer indeed seems to have some role, but its true significance is a bit unclear to me.	0
 lack of correlation between vulnerability and robustness in the experiments.	0
experiments on the cifar10 an pascal voc 2012 considering a good set of representative explanation methods show the strenghts and weaknesses of the proposed method.	0
>the experiments compared with many baselines cons: >figure 2 should compare the dgru (che 2018) cell which is the baseline as well.	0
cons:  experiments do not include an analysis of the performance of the model in the hard to differentiate actions (which is the main motivation for the proposal).	0
overall, this paper has some novelty in its core idea in the context of adversarial defenses, however, the exposition lacks conviction, and the experiments are lackluster.	0
 section 5.2: 'we run the same set of experiments as above, but only with innerloop exploration': why?	0
2) the experiments although extensive in number of environments (for training) are poorly evaluated.	0
the experiments are poorly evaluated in the following sense:  the authors provide in figure 3 the results from evaluation of seerl against the chosen baselines.	0
1. it is reasonable to learn a sparse binary gate to reduce the frequency of memory access, but the authors did not further demonstrate its benefit through experiments.	0
doing this means that empirical results should be strong to back up these decisions, but the experiments are only on mnist and, although they outperform ewc, do not outperform other continual learning works.	0
my main concerns are regarding the experiments.	0
this is not necessarily a weakness, but i'd have a higher standard on experiments if the experiments are among the main contributions.	0
the final recurrent unit looks very similar to gru but the authors do not compare it with gru in any of the experiments.	0
weaknesses of the paper:  although the results are performed on a wide variety of tasks, the compression rates obtained are not on a par with the quantity and abundance of experiments provided.	0
2. there're experiments on a variety of datasets, but there is no proper analysis of the obtained results  why it works in some cases but does not work in the other.	0
the paper is lacking experiments involving rl.	0
although the paper contains some interesting aspects, i find there are several critical limitations, especially (a) lack of motivation, (b) gap of theoretical results, and (c) insufficient experiments.	0
that said i had a few concerns which make me believe that the paper could do with more work and experiments before it can realize its potential impact.	0
(however i very much appreciated the experiments on pretraining bert in table 3, so thank you for this).	0
my main concerns are on experiments: q1.	0
this paper should be rejected primarily because (1) the algorithms are not well justified either by theory or practice; (2) the two approaches described are loosely connected and therefore making the paper lack of focus; and (3) the experiments lack comparison with many other existing stateoftheart neural compression methods.	0
this is a well written paper but, in its current form, it isn't above the acceptance bar: (i) the idea of using consistency terms as regularizer is not novel and related works on consistency losses have not been adequately cited; (ii) the experiments are in general well executed but are not quite comparable with the stateoftheart.	0
the experiments, although look fine, lack the robustness/generalizability often desired of such complex architectures by trying it on more complex sequence data.	0
2. i see the joint classifier in sda as the bulk of the novelty: the testtime augmentation experiments and teacher/student models to relieve the costs of these testtime augmentations are good to have but do not seem novel, and their applicability is not restricted to the sda approach. '	0
the opening of the abstract discusses applications to diverse architectures and data modalities, but the experiments are limited to very similar image classification models.	0
overall, the approach seems promising, but is missing some experiments and explanation that would validate its innovations.	0
regarding the experiments, there is a notable lack of baselines as mentioned above; also, for sample quality it is desirable if quantative measures (e.g. fid or inception score) are included.	0
this paper should be rejected because (1) the contribution is incremental, similar ideas has been explored in previous papers; (2) the experiments are too toy to demonstrate the practical use of the proposed method.	0
2) lack of comparisons with previous methods: the claim that the proposed method has a higher discrimination power than interpretable models is not supported or verified by experiments.	0
in my opinion, there are few interesting experiments in the paper, but the global plot that spatial information is not useful does not work.	0
the significance of experiments (especially in section 4.1) is questionable for several reasons:  the experiments on fixed initialization are nothing but a sanity check.	0
but do not provide any details of how these are selected # continual learning  experiments lack strong continual learning baselines, such as 'continual learning with deep generative replay' (neurips 2017), or subsequent 'generative replay with feedback connections as a general strategy for continual learning' (arxiv), or 'progress & compress: a scalable framework for continual learning' (icml 2018).	0
comments regarding experiments from sec 4.1 the results indeed show the performance is greatly improved over using random images from the dataset, however obtaining 45% on cifar10 would hardly be considered “distilling the dataset” as the full set of data yields performances of 90% on this task.	0
 addendum: after writing this review the authors have responded to my concerns regarding the cl experiments and have begun to recompute some of the experiments using a more rigorous comparison with promising results on permuted mnist.	0
weaknesses: missing experiments on larger datasets and other problems, missing comparisons to more competitive compression techniques.	0
for nonconvex deep learning based experiments  the authors claim that the autoencoders considered in the paper are benchmarks for deep learning optimization but that is unfortunately an outdated viewpoint.	0
strengths 1) their method achieves a significant improvement in performance over baseline selfsupervised video representation learning methods on 2 datasets: ucf101 and hmdb51 2) the paper presents though experiments with different architectures and training settings in order to be fair to the baselines.	2
conclusion  the author proposed a new pruning technique for dwc (mobilenet v1&v2) which would be novel, but the experiments should include more items to verify the method.	0
concerns:  all experiments are done with mobilenet (v1 and v2), i wonder if the algorithm works well on other architectures.	0
the experiments show that the method works reasonably well, but several important baselines are missing.	0
to summarize, the paper presents an incremental modification to vae to learn disentangled representation, and experiments lack comparison with strong baselines.	0
overall, while the proposed approach is an interesting one, i have some serious concerns regarding the presentation and experiments, as listed below.	0
concerns: 1. i am not convinced about the strength of the baseline models, especially in the language modelling experiments.	0
empirical evaluation on fairly small scale experiments demonstrates these techniques can productively be employed to develop new and powerful models concerns: there is, in my opinion, rather too much content for a conference paper of 8 pages.	0
we also need to consider other real advantages such as quality, latency, memory usage, etc.  my biggest concern is in the experiments.	0
on the positive side, there are certainly differences in the design between this paper and se, and the proposed block seems to be more lightweight than se (although the cost for employing se is tiny) .moreover, the authors present a large number of experiments by applying their block to several different problems and architectures.	2
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
on the other hand, i do have some potential concerns about the experiments, mainly that 1) the confidence intervals of tables 1 and 2 are suspiciously small for awgim (an explanation for why that is would be valuable) and 2) i'm wondering whether hyperparameters were tuned separately for each models in the ablation study (given that there are many variants compared, i doubt they were, though that would be a fairer ablation).	0
however, the paper lacks evidence to validate most of its claims, in particular it fails at 1) establishing that the problem actually exists, by theory or experiments (or carefully selected references), 2) establishing the existence of a causal relationship between conditioning and latent variable being independent and posterior collapse, again either by theory and / or targeted experiments 3) validating claims about the method's performance empirically.	0
in its current state it is not possible to judge the contributions due to a lack of clarity in the experiments section and because a highly relevant metric is omitted (longterm translation and rotation error).	0
with two sets of well designed experiments, train large network with small teacher or poorly trained teacher, the authors claim knowledge distillation has a strong regularization effect.	0
pros:  experiments in section 2 are novel and interesting, as far as i am aware  experiments are conducted across different datasets and networks, suggesting generalisation  paper is well written (apart from the title.	2
cons:  the use of different hyperparameters across experiments is concerning  tfkdself is just a rebranding of bornagainneural networks  it is not clear how/why the virtual teacher surpasses label smoothing i propose the paper should receive a weak accept.	0
1. experiments in section 5 lack the comparison with not only baseline, but also with one of the previous attempts to adapt training with mixed image sizes.	0
are there maybe experiments that could be run to exclude obvious shortcomings (e.g. maybe these tasks are too easy and overfit in their training)?	0
more detailed concerns / questions: ' i’m confused about the size of the external memory in the experiments.	0
this would be perfectly fine if the experiments demonstrated clear improvements but that is not the case.	0
strengths:  conditioning dilation on learnable weights is a sound idea, which intuitively makes sense as to why it could offer improvements  there is a strong variety of experiments performed  pad kernels are tried on several different architectures, which is good  reporting the negative results at the end of section 4.2 is a strong truthful admission that is rarely done, and we commend the authors for doing this.	2
experiments examine whitebox, graybox, and blackbox attack scenarios, where the graybox scenario corresponds to an adversary having knowledge of the network model but not the contents of the cache.	0
the sarnet is usually better in larger scale experiments but it is hard to say if it will generalize to other environments as all experiments are on multiagent particle environment.	0
therefore, due to lack of clear positive signal from few experiments and issues mentioned below, i provide an initial rating of weak reject which i would be happy to increase once following concerns are resolved.	0
moreover, i also have some concerns about the experiments, which will be detailed in the following.	0
strengths:  systematically layout of experiments on what kind of perturbations are most relevant for robustness  one of the first papers to take on the problem of robustness in fewshot learning.	2
experiments on simple synthetic data are performed to show the effectiveness, but these experiments are too simple, more thorough experiments on real data sets should be conducted.	0
the experiments are well described and extensive and some of them do give some more substantial performance improvements, but overall i think the paper does not explain the motivation for the algorithm sufficiently for iclr publication.	0
pros: 1. the writing is clear 2. the literature review seems thorough 3. segmentation results provide ample evidence cons: 1. the global classification results are not convincing enough to claim 'significant improvements in terms of imagelevel classification', as stated in the paper, according to the experiments.	2
the authors do try to show that the final results are better (which isn't the same thing as saying that the method is fixing posterior fading); but i don't think these experiments are convincing.	0
secondly, the experiments show that the feature maps learned by pathvae and betavae are both significantly outperformed by a supervised training, but there is no commentary about that.	0
there are many more experiments in the appendix but in general it is very hard to see a simple and consistent story that is supported by experiments.	0
overall, proposed concept is quite reasonable but supplied experiments have methodological issues.	0
authors suggest their framework as regularizing technique to neural ode, but they conduct experiments without adjoint method.	0
overall, the paper is marginally below the acceptance threshold, but i am willing to increase my score if some more clarification are provided and the results of the experiments are revisited and extended (see comments below).	0
regarding the evaluation, my first concern is that the experiments are all on atari, in which the visual variation is extremely limited, compared to realworld observations.	0
thus, i would like to see additional experiments that evaluate how this attack performs in the blackbox setting.	0
this paper provides numerous experiments but this should not replace novelty or presenting as novel something already present several times in the literature.	0
minor question and comments: 1) i searched for the number of clusters you used for the experiments, but can't find it.	0
(3) the results of experiments demonstrate the effectiveness of noisy ssc and noisydr ssc, but this paper not only lacks the description of experimental data, but also lacks the largescale experimental data.	0
regarding the experiments, besides the lack of comparison to sota results, and comments on important questions raised by oliver et al., there are important details missing, especially in the semisupervised field.	0
 for the resnet56 experiments in table 1, the pp1 and pp2 methods of singh et al., 2019 seems to give similar accuracy but much higher compression rates as the proposed method.	0
the experiments are limited, but the results are promising, particularly figure 3. overall the paper feels like it has a lot of content without a lot of substance.	0
i could easily be swayed on this by seeing some realrobot experiments, but i understand how difficult and expensive they are to come by.	0
the experiments show promising results, but many of the claims made about the predictive model are not sufficiently supported.	0
2. since this is clearly a theory paper, lack of experiments can be tolerated.	0
so, due to concerns around novelty and limited experiments only on traffic junction with some critical details missing, i would give initial rating of reject.	0
during experiments the authors show that (a) their method is competitive on most benchmarks with simpler methods but can be beaten in some conditions and (b) rpr yields strong results for the googlenet architecture.	0
now, my problem with these results of table 2 is this quote from the paper: “although the perplexity is still improving, we stop the pretraining for practical reasons to control the duration of the experiments.” the difference between the baseline and the other systems shows that pretraining is very useful to all 3 downstream tasks, but this performance of the various models could be highly dependent of when the authors applied early stopping.	0
my main concerns are regarding the experiment results: 1. the experiments are conducted on a not very realistic setting of splitting cifar10 and mnist by class labels.	0
the paper mentioned that the multisource model may have advantage over information corruption and balance between encoder/decoder model sizes, but no experiments are shown to demonstrate that.	0
the experiments would be more convincing with stronger baselines, but the current paper may already generate useful discussions within the community.	0
the authors did not discuss this in the experiments, but this is very important in evaluating the applicability of the proposed algorithm especially on largescale problems.	0
at a high level, this paper is experimentally focused, but i am not convinced that the experiments are sufficient for acceptance. ''''''''''''''''''''''''''''	0
authors tried to analyze the constructed metaknowledge graph by some experiments, but these discussions are farfetched.	0
the paper says the main comparison point is nvil but different experiments either mention ali, vcd or pcd which is confusing.	0
cons:  network architecture used for experiments as well as the exact details of the datasets are nonstandard, making results very hard to compare with other papers, so one needs to rely on provided baselines only.	0
we agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period.	0
one weakness of the paper is the lack of experiments.	0
===== final decision ===== my current “weak reject” rating is primarily based on the unclarity of the main paper and secondary regarding the concerns for experiments.	0
weaknesses of the paper:  both the reparameterization of the clipping function and the weight normalization before quantization approaches seem not novel to me, see for instance: 'weight normalization based quantization for deep neural network compression', cai et al.  the experiments are lacking the widely used resnet50 baseline (and table 4 should be in the main paper instead of the appendix).	0
they answer to all the raised concerns with clarity and concision and performed additional experiments.	0
i only have some concerns about the experiments.	0
experiments:  i am not an expert of maml, but i would not consider this as different tasks, just as different environments for the same task.	0
additional feedback: some of the experiments could be slightly more convincing  particularly figure 5 which is lacking error bars.	0
it seems like not all training hyperparams are specified in the appendix  eg the settings specified in table 9 only apply to the few shot learning task, additional hyperparams are specified for babi, but i cannot see the training hyperparams for the experiments in sections 4.1  4.4. minor correction: 'as turing machine is finite state automata' > 'as turing machines are finite state automata'	0
moreover, i appreciate that experiments were conducted on real robot datasets, but this seems to be more of an exercise in latent space anthropomorphism than practical evidence of a feasible control policy.	0
the experiments seem to cover a lot of ground:  toy polygon dataset demonstrated to be hard for existing sota in set representations  sets of points from mnist images  graph classification (competitive with a recent graph convolution approach)  integrate with relation nets for deep set prediction the authors acknowledge (sec 7) the limitation imposed by requiring the input argsort (and size) at the decoder, but point out that even as a representational pretraining or regularization, this strategy can help to improve set prediction strategies not subject to the same constraint (like rn, as in 6.5).	0
the experiments are in general wellorganized even if they lack clarity at some point.	0
these concerns (in addition to the strengths highlighted above) mostly surrounding experiments form the basis of my rating and addressing these would definitely make the paper stronger.	2
============== update postrebuttal: indeed, my concern is that a simple t(s,a,s') estimator, using the same samples, could infer the same characteristics in each of the experiments (number of broken vases; effect of box pushing; terrain), and could then be used for modelbased planning.	0
the proposed method seems to have the potential to be a good at exploration but the paper does not show quantitative experiments for hard exploration tasks such as montezuma’s revenge, or other atari ale environments.	0
remarks: sec 3.1: it seems that    au is an important hyperparameter for dividing the noisy data into labeled and unlabeled sets, but in sec 4 i only see that    ao is set to be 0.5 or 0.6 for cifar, what about other experiments?	0
overall, i’m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) resnet and ii) other (lower entropy) input images.	0
overall, my main concerns are with the thoroughness of the experiments, and that the two main contributions (summarized above) may not be enough.	0
a minor concern is about the experiments.	0
one minor point is that averaging experiments over 4 seeds only is usually not optimal in these environments, but in light of the sheer amount of baselines and benchmarks excusable.	0
strengths  this paper presents some experiments on adversarial learning when the policy of the victim is fixed and blackbox with interesting findings, demonstrates that the adversary can successfully figure out the weakness of the victim.	2
with a lot of experiments conducted, the author fails to summarize them into an (or a few) interesting conclusion(s), but end up with a pagelong conclusion paragraph, which makes the paper less focused but like listing miscellaneous experiments.	0
 the problem is well formulated and the experiments are also sufficient to support the claim (could be better though, see cons below).	2
cons:  experiments limited in sumo cases.	0
the general idea is reasonable and the proposed model is technically correct, but i have the following concerns mainly about the originality and the experiments.	0
closely related to the above if i have understood correctly all the experiments including the gene networks are on synthetic data, it would be good however to see if synthetic data can help generalize to real data.	0
the experiments show that this leads to scattering networks that perform about equally well, but are much sparser.	0
the idea is simple but effective, the experiments are thorough and improvements over the bert baselines are significant.	0
theoretical results are limited to sgd with minibatch size 1 but the insights carry over to larger minibatches in the experiments.	0
however, my concern is about the experiments.	0
weaknesses: i think some comparisons with adagrad and related methods should be performed in experiments.	0
this is true for all cases (including all experiments in this paper) except for binary classification, where the cross entropy is less than 1 when the score on the correct class is around 0.5. it is not important but would be better if you could mention this point.	0
one downside is that the paper does make fairly aggressive claims (e.g. 'performs better than all existing provable l_2defenses'), but then only compares to two prior / baseline approaches in the experiments.	0
i have several concerns regarding the model and experiments: 1) since d^{'^{f}}_k subset d^f_k, would the model minimize w.r.t.	0
the experiments did compare with a number of previous algorithms, but the comparisons are all to algorithms that train a single agent at a time, and no comparisons are made to other distributed rl algorithms.	0
 in experiments section, the authors miss a few important blackbox attack baselines.	0
for starters, all the theoretical motivation describes a particular way in which mixout is supposed to aid in downstream tasks for the case of convex functions, but there are no experiments that match their assumptions and which demonstrate this is the actual behavior we see.	0
the authors not only conducted experiments on tasks with less examples (the main focus of this paper) but also on a task with sufficient training examples.	0
note a small typo in the displayed equation just above fig 4: the sum is over but it's written (and also using     ext makes it look nicer, )  original review  the paper proposes a neuron pruning technique that can compress an existing pretrained neural net (though the experiments actually do additional unspecified 'finetuning' training).	0
(2) experiments were very promising, but i'm not convinced about the baselines.	0
4. the model keeps track of a global state z^g, but it is not analyzed in experiments.	0
(ii) concerns regarding the experiments  as also found curious by the authors, it is concerning that the end_aux results are so low for ood detection.	0
the experiments are well done and encompass not just prediction and inference but also control.	0
on the negative side the paper does make a slightly rushed and unfinished impression by not showing any, even qualitative, experiments on real video.	0
in its current form, i do not think that this passes the iclr bar, however i do think its close and, if the experiments i prescribed continued the trend, i would suggest its inclusion.	0
the experiments lack a scalability test with related systems, the scalability test included in the paper only takes into account other definitions of the system.	0
<conclusion> although this work shows a new application of nas for object detection, my initial decision is ‘weak reject’ mainly due to lack of technical novelty, limited experiments and poor writing.	0
<postrebuttal comments> authors' responses partly resolved my concerns on the experiments.	0
at a style level, i would not describe this paper as specifically a visualizing weaknesses paper  instead it is more like a framework to learn to generate states that satisfy some predicate of the qfunction (as noted by experiments that try to identify critical states, especially positive states, etc.), and i would consider renaming the paper accordingly.	0
positive aspects of the paper: a) extensive experiments, which try to find the best parameter configuration for each of these models.	2
although the paper's experimental setup could be done in a more proper way  nested cv; lack of which degrades the conclusiveness of the comparisons, i still think the paper is better to be accepted than not, given that 1) as pointed out by the authors the computational cost of the current experiments was quite high already, 2) the issues of replicability and reproducibility are important and gnns are quite popular for various applications and 3) the results are interesting and important to be considered for future gnn works.	0
 in the experiments, the authors show the performance of sadam for neural network training and related to my previous remarks, i have the following concerns.	0
the paper is already very long, so there likely isn't any space for fleshing out this section, but it would be nice to have the experimental results included in the main paper, because most of the experiments are left to the appendix.	0
2. opinion and rationales i’m leaning towards “accept” for this paper since it presents two interesting contributions (albeit of incremental novelty) to the approximate inference area, it has clear execution and super clean presentation, and the experiments clearly demonstrate the values of the proposed approaches.	0
 beautiful drawing cons:  experiments are a little light.	0
4. table 1 provides a good overview of the results but it should include standard deviations for experiments that were averaged.	0
cons:  in the experiments, the authors should still compare one or two blackbox attacks which also use model outputs only, to demonstrate the effectiveness/efficiency of the proposed attack scheme.	0
i would be interested to have an answer to the following concerns:  it was not totally clear for me what is are the changes that the paper proposes in the block level and network level and if these modifications are present in their experiments.	0
the experiments demonstrate not only a strong results over baseline adam with warmup learning rate but the robustness of the optimizer.	0
 i also had some concerns with the scalability of the approach in large features spaces but am willing to give the authors the benefit of doubt on this one based on the 3 hour number they quoted from their experiments (which indicates something that is not prohibitively slow)	0
i do, however, have concerns about the experiments that are done to validate claims in the method section.	0
for the experiments on longtext generation using emnlp news 2017, it is not clear how the data is partitioned as training data, validation data and test data to get the results in figures 4(a), moreover for the results for leakgan, rankgan seqgan, maligan, it seems that they are copied from other papers, but again how the data set is partitioned is not clear for example in seqgan's paper, and most likely, the data is partitioned in a different way, so the results are not comparable.	0
for lack of space, one has to through quite few references to fully understand the experiments.	0
the additional experiments comparing the continuous relaxation and the unrelaxed regularizer were interesting, but i found figure 2b a little hard to understand.	0
overall, i don't thing experiments are thorough enough to demonstrate that the method is empirically better than competing approaches, but i believe that is not the main point of the paper.	0
the experiments currently are lacking to compare amo with existing techniques.	0
for example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from section 4. it would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.	0
b) maybe i am missing this, but i don’t see any reference on what alpha and beta from equation (6) were used in the experiments.	0
 figure 3 xaxis runs till different values, but the description says training steps are 1000. how long are the experiments run for?	0
for the experiments, there are some missing detail and concerns: 1. the finetuning using ulseq (eq 7) procedure is not well explained.	0
i do have a range of questions & requests for clarification (see below) but i believe the experiments as presented, plus some additions before camera ready, will make for a good paper of wide interest.	0
further experiments highlight the benefits of orthogonality in deep learning but the explanations feels lacking.	0
for experiments validating the efficiency of the proposed method this is clear but for the classification experiments this is less obvious.	0
i do see the empirical experiments demonstrating this but some more insight into this is perhaps important.	0
in the end, it is a good paper in terms of theory, although the experiments is lacking in crucial places (no comparison with many existing papers that do attempt to invert hessian) and lack of analysis of claimed efficiency, which i can only hope don’t turn out to be a big deal.	0
theoretically, i understand the advantages of the proposed algorithm, but still, it is more convincing if stronger experiments can be conducted.	0
2) please report the error bars of the accuracy computed over multiple runs of the experiments (according to appendix e multiple runs were performed, but only the mean values of the accuracy are reported).	0
however, due to the lack of novelty in any of the applied techniques, and the fact that the experiments could be expanded, i recommend a weak accept.	0
a lot of important stuff has been pushed into appendix and experiments/setup require more details but overall i believe paper has significant contributions and results.	0
cons: 1. experiments: the paper reports the number of correct neighbors obtained while searching on average and at 0.95 quantile.	0
 lack of experiments in larger contour detection datasets (like semantic border dataset or cityscapes).	0
 updated  rating:  weak accept the authors addressed the initial concerns i raised, providing detailed explanations and additional experiments.	0
if the authors can address my concerns, mostly motivating the importance of the experiments for weakness #1, and providing an explanation (possibly correcting me) for weakness #2, i will be happy to increase my rating.	0
 strengths:   careful initial experiments to motivate the study.	2
 extensive experiments across various settings, e.g. varying the teacher model size, varying the decoding strategies, investigating ban/moe/sequencelevel interpolation, etc.  weaknesses:   it would have been interesting to consider a synthetic data setting such that one has access to the true underlying data distribution, such that approximations are not necessary.	0
after rebuttal: the authors' new experiments and response answers most of my concerns.	0
concerning experiments that are not about reinforcement learning, the paper proposes different strategies to learn a resnet architecture for imagenet.	0
2. the paper presents a bound on the population risk, but the experiments do not include a comparison of the expected worstcase error rates with the empirical error rates achieved on the test set.	0
experiments show lcn models outperform existing methods for oblique decision trees, but ensembles are often matched or outperformed by random forests.	0
from what i can make out, not only should this paper cite neural logic machines but they should in fact be comparing against it via experiments.	0
(2) there are several modules introduced in the paper, but there isn't much analysis of them during the experiments.	0
as usual, more experiments are always welcome, and given the strengths of gan based generators for faces a text based facial image generator could have been a great addition.	2
their method is simple, but achieves very strong results and there are a ton of experiments in this paper (it is 41 pages).	0
cons:  in their experiments, they use the number of classes as an approximation to the number of connected components (appendix e) and train classconditional generative models.	0
i found this super instructive, but it is only _one_ case on _one_ manifoldi would like to see more details here; maybe some of the experiments in the appendix could be moved to the front?	0
therefore, although the idea is quite incremental, the overall work is well written and experiments with ablation study on different model’s components bring the value into it.	0
to this end, i would encourage the authors to repeat their experiments for synthetic data, but generated under linear and generalized linear scms.	0
2. i might have missed it, but i could not find the info about how the train/validation split is formed for the experiments following section a.1.	0
parts of the discussion stay informal as the authors themselves admit, but i appreciate that rather than providing mathematical decoration the authors focus on welldesigned experiments that support their claims.	0
last but not least a recent paper in an icml workshop on automatic machine learning (https://sites.google.com/view/automl2019icml/acceptedpapers) had a paper on 'improving automated variational inference with normalizing flows' by webb et al. this method looks a lot like a baseline method for this paper before task adaptation and wgf is considered and would effectively subsume the first batch of experiments entirely.	0
i am not very familiar with the modern transformer architecture experiments, so cannot evaluate their quality, but they seem sensible from an outsider's perspective.	0
 interesting experiments showing tradeoff among lack of memory (sgd) and full memory (adagrad), applied on a realworld machine learning setting of largescale natural language modeling with transformers.	0
the work successfully constructs a negative identity hidden to hidden gradient matrix but i still have a few concerns about the theory and the experiments.	0
authors’ main motivation is to study small batch size experimental setup which is important in segmentation and detection, but they don’t include main competitor (brn) in these experiments.	0
i think that the magnitude of the contribution is medium>low, however (namely that they have an approach which is wallclock efficient, for single trajectory transfer), however the precise details of this contribution/claim have not been adequately tested: we do not see experiments which highlight the wallclock or inference cost competitiveness of the approach, and we do not see experiments against other, existing, approaches for latentvariable based rl (mostly these involve modelbased rl, however these would probably be easy to derive for many of the test cases).	0
2. in the experiments i can see that in some cases a longer probe performed more poorly than a shorter probe (for example fig3a for 2d navigation.	0
i agree that the authors extend a lot on this papers, especially in terms of dataset and completeness of experiments, but they are definitely closely related, and the fact that it is not cited is a serious flaw to the current version.	0
in addition, there is a lack of experiments it is always nice to see comparisons to baseline even though the theory implies you would do better.	0
the experiments empirically demonstrate that: 1. rotation exists in the training dynamics of practical gans; 2. gans often converge to an lssp than an lne, but still, achieve good results.	0
my major concern is related to the last part with experiments on the movielens data.	0
= strong/weak points  the graph representation of the problem is novel, and draws both on core ideas from hindleymilner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity)  the neural message passing architecture is adapted to the problem, handling features not present in the standard gnn literature (hyperedges, ...)  experiments compare with relevant baselines and consider interesting ablations, studying the effect of the gnn extensions in detail.	0
in experiment qta2 the authors (with a small issue, see weaknesses) show that their proposed modification to the loss function causes their outputs to be better matched to the speech conditioning as measured by the fixed embedding network trained in stage 1. finally in qt3 they measure how well their generated faces can be used with the original face embedding network to perform image retrieval (i am a bit unclear on the details of this experiment, see questions) strengths  ' the authors' proposed pipeline and modified loss function offers a generalized framework for jumpstarting conditional gan training ' their pipeline does not assume humanspecified labels, but instead access to paired data from different modalities, which is can easily be obtained for many conditional image generation tasks (inpainting, superresolution, colorization, etc.) ' their pipeline also doesn't seem particularly finetuned for the speechdriven image synthesis task, so it seems reasonable to believe it could be adapted to other tasks ' their results are qualitatively compelling, and they make a convincing efforts in experiments qta13 to quantitatively show that the conditioning information is affecting the output ' the paper is generally wellwritten and easy to follow weaknesses  ' without completing the loop, and showing that the second stage of this pipeline helps with identity matching for speakers, i'm not clear on what the motivation for this particular form of conditional image generation is (this is unfortunate, because their framework is quite general, and it seems feasible that the authors could have applied it directly to a task where the output of conditional image generation is directly useful) ' i interpret the main purpose of experiment qta 2 as validating the effectiveness of their proposed loss modification, it seems like a natural experiment to make this claim more convincing is to compare a generator trained with eq 4. against real images (hopefully getting a much lower matching probability than the 76.65% reported in the first experiment of this section) ' it seems like an important ablation study is testing the effect of jumpstarting the gan training with the pretrained networks of stage 1, and reporting what happens when one or both of these networks are initialized randomly (assuming that initializing randomly hurts performance significantly, this would be further evidence of their framework's strengths) ' the novelty of the proposed approach is limited so far as i can tell (slight architectural modifications, and adding a negative sampling term to the discriminator loss) initial rating  weak accept questions  ' in experiment qta3 i'm confused by how including multiple faces from the same video clip affects measuring retrieval accuracy.	2
however the stated task is certainly a nontrivial one, and the qualitative results and experiments give compelling evidence that the authors are proposing a powerful framework for conditional image generation.	0
revised rating after rebuttal: accept explanation of revised rating: the authors addressed my concrete concerns about missing experiments in the rebuttal.	0
pros: ' very well presented and clear ' thorough experiments with baselines and comparisons with competitors ' novel and efficient approach of redesigning the classifier as a postprocessing step after the representation training cons: ' i did not find any single value of the 'temperature' coefficient that you use for the different datasets!	2
while i like and acknowledge the fact that the paper examines these frameworks critically and has also some didactic value, i still have some concerns regarding the current experiments, and would like to see some additional analyses in the paper.	0
the methodological contributions do not seem very sophisticated, but the experiments show that the proposed method, despite being very simple, works very well in practice.	0
my primary concern is that they should show performance on cifar100 not just cifar10, and i certainly hope these experiments will be included during the rebuttal.	0
however, i have some concerns regarding experiments and comparison to previous work.	0
better test errors and within 57x slower than the conventional ibp methods with worse errors cons: > extensive experiments with more advanced networks/datasets would have been more convincing, esp.	0
i have several confusions or concerns about the synthetic experiments 1. in the synthetic experiments, is the evaluation task different from the training task?	0
 update: the authors have addressed my initial concerns carefully through extra experiments and details in the appendix and i have updated my rating accordingly.	0
throughout the paper, the experiments and results raise questions on the robustness and generalization of existing exploration methods across various atari games, but the paper puts absolutely zero effort into investigating if there is a quick fix to the questions it poses.	0
if it is saying (1st) then i find it contradictory that it is not ok to focus on mr but it is ok to focus on atari as a single domain; if it is saying the second then also it is contradictory because the paper only experiments with the atari suite.	0
comparing with the paper by yin et al. (2019), it seems to me that the technical contribution is rather incremental (the binary tree version is a variant of the hierarchical softmax, and asrk seems very straightforward), up to the point that the first set of experiments is actually only about vanilla arsm.	0
 the proposed black box estimator seems quite useful as demonstrated in figures 2 and 3. although the authors evaluate their approach of few simple domains  it would have been useful if there were more extensive experiments performed for ope problems.	0
novelties /contributions/good points: ' votes from the clustering models to create a graph ' using a graph to identify the most important samples for pseudo labelling ' modification of the ladder network to be used as clustering algorithm ' good amount of experiments and good results weaknesses: ' the whole experiment leading to table 1 in page 2 is unclear for me.	0
in the experiments, you use the features extracted from resnet50 but what about finetuning this network rather than adding something on top or even better starting from scratch.	0
generally the paper does a good job at demonstrating benefits in modeling ability and generalization, but the experiments applying this model to control are not very convincing.	0
i think there is a lot of promise in the approach, however some of the improvements over past work have not been adequately tested (integral approach and sensitivity to # of integration steps) and the control experiments are not very convincing although it seems like they could be.	0
i lean towards a weak reject for the paper as is, but if the authors flesh out the control experiments and do some ablation studies i will improve my score.	0
the experiments on the control suite of openai gym are simple (pendulum and cartpole only) but thorough, and compare the proposed method with a noninductive bias method (still relying on neural odes), a simpler naive and geometric baselines.	0
the experiments on mnist, and cifar10 show that proposed model can find sparse neural network models, but unfortunately with little performance loss.	0
i enjoyed the first version of the paper, but it was missing experiments on larger games and some questions on smaller games were unanswered.	0
the paper continues the line of work on crosslingual contextualised word embeddings, and it brings several minor contributions, but overall i do not see it as a very inspiring piece of work, and it leaves open several very important questions, in particular its relationship to prior work and some potentially stronger baselines than the ones reported in the paper, plus more experiments with more distant language pairs.	0
in introduction, the paper aims to 'better understand bert’s multilingualism', but i do not see how it contributes to our better understanding of bert's multilingualism besides a pretty straightforward claim that it shows less multilingual potential when doing experiments with greek and bulgarian that use different scripts.	0
my main concern relates to the experiments.	0
 experiments across multiple snr and generalization on noise patterns the authors mention at the beginning of section 4 that a random block is corrupted, but in the end the experiments are done on a constant corruption size on the cifar and imagenet images.	0
4. though the improvement over prior methods is good, the experiments lack an appletoapple comparison.	0
 the experiments show that the method works very well in uigeneration domain  the paper repeatedly claims general applicability to program synthesizers, but is only evaluated in the specific domain of uigenerating programs.	0
pros of the paper: 1) trajectory predictions on the two planar vehicle dynamics experiments was impressive.	0
cons of the paper: 1) the experiments did not display a proper comparison against the hybrid method mentioned in section 1. the experiments also did not compare against adjoint methods in the multiagent example, or in the lowdata regime for the singleagent example.	2
 the authors suppose fully observable markov games in the paper, but it makes me confused when i consider the experiments in the submission.	0
my only concern is the comparison experiments.	0
cons:  all experiments results are based on same backbone, which makes all discoveries much less reliable.	0
 when you first discuss hologan, you mention one of its main downsides being scalability and then proceed to not only explain that but also use a hologanlike architecture in one of your experiments.	0
i'd either remove the scalability argument or justify not just that but also how that's not relevant to your experiments.	0
### experiments  you mention k is fixed, but where does the initial k come from?	0
 review update: thank you for the detailed response, it raised my opinion of the paper as it reduced my concerns on the rigor of the experiments performed.	0
while i truly commend the effort undertaken to perform the experiments, i have several concerns which i explain below and would be happy to raise my score if they can all be addressed satisfactorily: 1) while the authors interpret the experiment results in sec 4.1 in a positive way, the results don't seem to necessarily indicate good systematic generalization.	0
in one experiment, these images should be chosen with random movements but the number of such images provided to the classifier should be increased in subexperiments to gauge if giving more varied views bridges the performance gap between the classifier and the rl agent's generalization performance.	0
i understand that there may not have been enough time to run many of the experiments i suggested, but i still think they are worth considering for the future.	0
here are my main concerns: 1) in the experiments in section 3, only a limited test set is used.	0
2) section 4.1: in these experiments, the training set size is increased, but the test set size is kept constant (and small), so the train/test size ratio also increases.	0
the experiments reveal that the first three factors, but not language, promote systematic generalization.	0
to be clear, i don't think it's necessary to add additional experiments to the paper, but the current results should not be overstated in their generality.	0
even within the reported experiments, the results suggest that systematicity is lacking in several places.	0
minor comments: typo: 'upto a constant' postrebuttal update: i increased my rating to weak accept because the authors addressed my main concerns by a) giving additional information on the update of the buffer, b) improving the presentation of the experiments, c) fixing the description of maxentirl and d) by showing empirically that both the lower bound increases / wassersteindistance decreases.	0
the proposed technique seems to be a fairly small extension to existing work, but together with its analysis and experiments it is sufficiently novel.	0
i would however appreciate it if the authors could clarify the way they sample the architectures and address my questions about their experiments.	0
the numerical experiments are well conducted, but i am not totally convinced by their results.	0
on the experiments: 1. why we use model 1.0, 0.95, 0.9 for figures 1c, 1d, 2c, 2d but 1.0, 0.9, 0.8 for figures 1a, 1b, 2a, 2b?	0
overall, the authors perform a significant amount of experiments, but they did a poor job in summarizing the results.	0
recommendation: for lack of necessary experiments and limit novelty, even if i like part of the approach design, this is a weak reject.	0
3. for the wikidata, the author reports the filtered mean reciprocal rank of the conducted experiments, where the author provides not only the overall score but also the temporal mrr.	0
overall, i think this paper is interesting as it provides an incremental extension of complex to the temporal case, and the experiments to support the formulation show improvements in mrr.	0
pros:  as far as i checked, the numerical experiments are strong.	2
weaknesses:  the design and experiments are largely empirical without theoretical derivation.	0
pros: ' the experiments are well designed and the results show the effectiveness of the approach. '	2
the results look good, but error bars,on the cifar experiments at the very least, would be appreciated.	0
strong points of this work include the writing and experiments.	2
the authors put huge amount of effort into the experiments but only describe the proposed technique in a very rough and abstract way, lacking necessary technical details to formulate the technique.	0
while this paper is more incremental and novelty may be slightly lacking, i think the breadth of experiments and competitive results warrants an acceptance.	0
(1) the analysis is nice in that it shows the methods work, but doesn't demonstrate benefit of their method over other methods (2) given that there are no results showing this method has better worstcase rates than other methods, we rely on experiments to see the actual benefit.	0
we understand numerical experiments are time consuming, but it's disappointing that you're not curious yourself whether your method outperforms bo.	0
overall, the paper contributes to the conversation around multitask learning but would benefit from comparing again external work on multitask learning (e.g. see under minor) and from bridging between theory and experiments (e.g. experiments with the models described in the theory section  linear/relu).	0
the last paragraph before the conclusions briefly mentions experiments and comparisons but without giving any details.	0
overall, i think the improvements are a bit incremental, but the experiments seem to support the claim that they are beneficial.	0
2. similarly, details about the experiments are also somewhat lacking: a. how many gpus were used to train the models?	0
the authors mention 8 v100 in a.3, but i am not sure if this was the setup for all experiments.	0
pros: i found the enfake experiments enlightening.	2
cons:  the architecture experiments were not that insightful and they authors did not reach concrete suggestions on a minimum number of parameters or a minimum depth.	0
second, the generalization to unseen scenarios are mentioned in the introduction but not really carefully studied or evaluated in the experiments.	0
the paper mentions in the 4.2 that it was impossible to run the experiments for genesis for more than 256 components  but the genesis paper has numbers more like k=9.	0
strengths: the experiments section is very thorough, and the visualizations of state counts and intrinsic reward returns are insightful.	2
my main concerns are about the experiments and the baselines.	0
i have just one concern and it is about the experiments for the knowledge graph completion task.	0
especially when the set of is large (high dimensional) ' possibly use a pretrained model and only tune the , mlps overall my decision is weak accept, the paper lacks of novelty and the experiments could be more extensive.	0
the paper has some nice contributions and i find this research direction to be very exciting, which is why i think it merits acceptance, however i find the experiments (section 4) could be greatly improved.	0
https://arxiv.org/abs/1709.07174 (i have my own views, but i'd like hear the authors' thoughts first) my biggest complaint is with regards to the experiments.	0
the imitation learning experiments on a small spring dynamical system are a necessary sanity check for further work, but many other more complex systems could be empirically studied and would have made this paper stronger.	0
overall, the approach presented in the paper is a bit incremental and the experiments are somewhat simplistic.	0
the experiments use simple domains/problems, but give good insights into how the model performs.	0
the paper would greatly benefit from scaled down experiments, which would allow them to compare their architecture search approach to recent approaches [r1, r2], blackbox optimization methods in the family of evolution strategies (es, nes, cmaes), thompson sampling [r3] or even bandits tasks for which bayesoptimal policies are tractable (gittins indices).	0
these vary in complexity, runtime, etc., but they deserve mention and either explicit comparison in the experiments or reasoning to justify the omission of such comparisons.	0
cons: 1) the experiments section is written very poorly.	0
there are some very specific conditions under which this is known to be true empirically (for example, see the experiments in burda’s iwae paper and hoffman’s dlgm paper), but in general, one should be careful with this claim.	0
2. authors claim that no information leakage between class and content representation in sec 1.1. however, the experiments only verify “no class information in content code”, but miss the inverse proposition (is there any content information is class code?)	0
the paper is already quite dense, especially with the appendices, however there are a few points that could still be detailed in my opinion: first of all, what were the observation models used for the reconstruction loss in the experiments?	0
 update postrebuttal: the authors have addressed the majority of my concerns, through both text changes and significant additional experiments.	0
the experiments seem convincing to me overall, however, i have the following concerns:  the performance of the model seems similar to ppo in the large statespaces (section 4.3), which somehow is disappointing.	0
 the performance of the model seems very sensitive to the choice of    au (figure 6 right), which is set to 0.5, but it is not mentioned how this parameter is chosen (or at least i couldn’t find it) and how much the performance of the model in the other experiments is affected by the choice of this parameter.	0
i feel the experiments lack some details: which is the decoder used for dimensionality reduction?	0
5. the experiments are good, but very focused on mnist tasks.	0
the experiments are functional and show good results, but i would appreciate more diversity in the tasks.	0
i would recommend to focus on fewer experiments, but present more thorough results.	0
this submission is lacking experiments comparing fedavg to the proposed method under these settings (which can be simulated using available datasets).	0
(as said before the replay is also a nice addition, but it seems an addon of the maintext, shrinking the space to analyze the main contributions of the paper in the experiments.)	0
the experiments are comprehensive, but on the other hand, it is also quite expected that the proposed model should work better than an mle baseline, since sequencelevel supervision is provided.	0
(2) experiments: i have some concerns regarding the experimental setup.	0
[pros]  clearly written  clear motivation  correct derivations  interesting algorithm [cons]  experiments are a little weak (and focus on a single domain)  would have liked to see an explicit algorithm for the optimization procedure  small lack of clarity in the presentation of section 4.1notation q_t is not introduced for example  more discussion about the evaluation metric  linking it more to prior work	0
my main concerns focus on the novelty (compared to existing methods that are similar but not discussed) and the experiments.	0
questions  see the concerns strongpoints  the author provides the simple and effective pruning method and verifies the performance with a sufficient amount of experiments.	0
the proposed model seems to compare well with different baselines, but the presentation of the experiments is not that clear.	0
experiments: overall, the experiments are very insightful but limited since you only show the training loss and the validation performance is not evaluated at all.	0
3) the cifar10 experiments in table 1 are encouraging and the toy experiment in 2d is illustrates nicely the deficiencies of the current mi estimators cons 1) the experimental section is lacking many details to fully understand how and what experiments were performed and how comparable they are to prior work 2) the paper would benefit greatly from a thorough editing to clarify the presentation  there are many missing concepts and definitions that makes it hard to follow without intimate knowledge of related literature.	2
additional remarks: sec 2.2, 2), “streamlining” is unclear circumstances 2 and 3 can be quite easily derived from circumstance 1; also they are not evaluated empirically; it would be nice to have experiments for them, and they can be moved to the appendix in case of lack of space eq (19) while nice, seem to bear no significance for the proposed method and the rest of the paper; consider removing it section 4.2 paragraph 2: “shrinking” for different layers wasn’t mentioned before, and is not immediately clear what it means; the reader needs to be intimately familiar with the dim paper to understand.	0
i did not gain much intuition into how alr might generalize to high dimensional settings and was concerned by the fact that the distribution used was heavily handengineered and did not match up with the ones used in later experiments.	0
c) finally, the authors employed a range of different hyperparameter settings through their experiments but gave little guidance on how to choose these settings in practice or how sensitive their proposed method is to changes in these hyperparameters.	0
cons: 1. the comparison of the experiments was not complete.	0
however, i have some concerns that might require clarification or additional experiments: 1. the novelty of the proposed method is somewhat limited in my opinion.	0
the synthetic data experiments agreed with theory, but the experiment on cifar10 did have some gap between theory and experiment, although the cifar10 with relu experiment agreed with theory.	0
given the lack of thorough experiments in the paper, i am giving the paper a borderline rating.	0
pros:  the problem that the paper solves is fairly relevant, and some experiments (such as cross morphology imitation learning) are promising in concept.	2
the toy example in the introduction was good but the experiments did not reflect the complexity of that example.	0
which doesn't make any sense since the policy prior is obtained using the vae, (d) the equation at the end of sec 4.3 has r(s, a) whereas reward is defined as r(s_t, s_{t1}) but they are not equivalent when dynamics are stochastic (9) experiments: (a) what is airl?	0
in zeroshot transfer experiments, the proposed method performs better than all four other baselines compared; but is worse than mo et al. (2019) in known categories.	0
the claim that the proposed method handles spurious information is well supported by the experiment in mountain hike, but not quite so by the atari experiments.	0
the results seem to be promising with comprehensive experiments including whitebox attack, blackbox attack by transferring, and attacks on defences.	0
 the authors provide some measure of standard deviation on some experiments but not on all of them.	0
cons:  the most critical downside of this paper is its insufficient experiments to support the whole idea, where we will detail in the next.	0
this in itself would not be much of an issue if the experiments highlighted advantages and limitations of the proposed method, but that is not the case.	0
in addition, as another issue with clarity, the algorithm has one main additional hyperparameter, r_max, but the description of the experiments does not appear to mention the value of this hyperparameter.	0
i am still unsatisfied about the experimental contribution, but i guess producing a paper full of theory and good experiments is a tall ask.	0
the method seems sound and i think this in itself is a great achievement., but the experiments lack focus.	0
the authors mention five question on page 6, but it is not very clear with examples/set of experiments address which question.	0
one question i have for the authors: in the experiments on the imdb dataset, the authors claim that the generalization error is worst at sigma_{epsilon} = 0, but it appears that the error is actually larger for sigma_{epsilon} = 0.4?	0
finally, and importantly, while the experiments show some interesting first results, they are limited, i am not able to judge the strengths and weaknesses of the method, and thus i cannot assess the usefulness of the proposed method.	2
the authors conduct experiments in three datasets, and the results show that their proposed meta attacker is more efficient than other blackbox attack methods.	0
experiments with different number of attention heads would be helpful, but in the current state are rather confusing (see the comments below).	0
the experiments for this gate are not included in the main text but got placed in the appendix.	0
so i have given weak reject because the paper has strong theory but weak experiments making it hard to trust the conclusions.	0
the existing experiments are already thorough so this would be going above and beyond, but that is why it might help raise my score from weak to strong accept.	0
 the experiments are all reasonably well done, but i could not find where the authors report which optimizer was used and how the hyperparameters were set.	0
i understand that the authors are guiding the presentation towards a version of sumandsample they can compare with in theory and in experiments, but in the process they're doing a disservice to the reader.	0
the authors also show how the method can be used for blackbox adversarial attacks without need for gradients or logits and model microstimulation experiments.	0
my main concerns are about the incremental novelty and experiments are heavily done on one search run, especially the search space is not the same as baseline darts.	0
two possible simple experiments to compare is, using the original darts space and training set, 1) do not update the _x0008_eta but use a fixed initialization that all eta is the same (to mimic original darts concatenation); 2) add eta to original darts as well and rerun 1).	0
in principle there is just one novel idea, namely using kwta activations to improve adversarial robustness, but this claim is investigated thoroughly, in theory, and demonstrated convincingly in experiments.	0
### possible improvements to the paper 1. the main weakness of the paper is that the authors repeatedly mention that encoding the position as a multiplicative factor which is multiplied to the frequency gives leads to a more decoupled/interpretable embedding but their experiments are solely focused on accuracy measurement.	0
 easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
[pros] 1. i enjoy reading this paper: probing cnns is not easy, but it designs experiments in an intuitive way and rigorously performs ablation studies and analysis.	0
q: i can understand why you removed the pooling layers, but did you try to run some of your experiments with these as well?	0
the submission is mostly clear, however there are some significant ambiguities in the experiments.	0
the experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness) disclaimer: i am completely out of this area but it is an easy read and an interesting angle.	0
strengths ' proposed method builds on a sound theoretical basis; although the linear dynamics model appear somewhat limited compared for the complex dynamics, thorough experiments are conducted to demonstrate the effectiveness of the method.	2
the fact that no actual difference in performance between acgnns and acrgnns was noticed in the only nonsynthetic dataset used in the experiment should prompt the author to run experiments with more real life datasets, in order to empirically verify the results, but this is a minor point.	0
experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.	0
the method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.	0
the experiments are not exhaustive and leave some open questions, but the first results are highly promising.	0
for example, you mention that you test different variants of nerd, operator variants, and mathqa, but then the results are not mentioned for these experiments until the next page.	0
 extensive experiments on evaluating 15k architectures over 3 datasets  detailed statistics on the search space  good baseline experiments comparison main concerns about this dataset:  comparing to nasbench101 in terms of 'algorithm agnostic', it is in a 'moreorless' game but not a 'yesorno' one, so that aanasbench does not seem appropriate.	0
experiments on translations into three morphologicallyrich languages, english{arabic, czech, turkish}, indicate fairly small but consistent and statistically significant gains in bleu.	0
2. experiments are done on translation into three morphologicallyrich languages with both concatenative and nonconcatenative morphology, and results in small but mostly consistent bleu improvements over the best subword, hierarchical, and character baselines for each language.	0
for the derivations and theory, i checked as much of the derivations as possible, although i'm not familiar with all the assumptions used throughout, but they seem relatively reasonable and supported by the experiments.	0
this reviewer moves for a weak accept on account that the paper is well written (with quite thorough experiments explaining improvements in sample efficiency and possible limits in final task performance) but specifically targets ale where execution is so cheap.	0
experiments on random labels and looking at different generalization measures are all nice but they are not sufficient for showing that this normalization technique is actually useful in practice.	0
i find the further experiments performed by the authors of very good quality overall, but i'm still not particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work.	0
however, my primary concern with this paper is that it’s not sufficiently distinct from the previous work of lee et al, 2019. after all, most of the experiments in that paper would have required the type of implementation that is described in greater detail in this paper.	0
second edit: i was also unable to respond to the final comment about the uci experiments in its own comment, but thank you for providing the estimated depths.	0
there are few small concerns regarding experiments which are discussed in detailed comments.	0
it seems that transformers would be an ideal setting for their approach, as transformers have rather high dimensional word embedding matrices, but the authors do not run experiments with the nowubiquitous transformer.	0
for example, in the summarization experiments, word2ketxs embeddings corresponding to 8000dimensional embeddings are compared to a standard model with embeddings of size 256. the lstm and layers for the word2ketxs model would become quite large but their size is not taken into account.	0
it would still be good to see more experiments, but i hope this paper gets accepted.	0
cons:  for the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbelsinkhorn.	0
this comparison appears in other experiments, but seems to be missing here for some reason.	0
general advice/feedback:  should provide an explanation of the row in table 2 showing that a simple linear transformation is able to achieve accuracy and convergence times comparable to those of the nau  should provide an explanation of the universal 0% success rate on the u[1.1,1.2] sampling interval in figure 3  inconsistent captioning in figure 2c, missing 'nac• with'  should clarify in section 4.1 that the 'arithmetic dataset' task involves summing only 'contiguous' vector entries; this is implied by the summation notation, and made explicit in appendix section c, but not specified in section 4.1  it is unclear what experiments you performed to obtain figure 3, and the additional explanation in appendix section c.4 regarding interpolation/extrapolation intervals only adds to the confusion; please clarify the explanation of figure 3, or else move it to the appendix  the ordering of some of the sections/figures is confusing and nonstandard: section 1.1 presents results before explaining what exactly is being measured, figure 1 shows an illustration of an nmu 2 pages before it is defined, section 3 could be merged with the introduction grammatical/typesetting errors:  'an theoretical' : bottom of pg 2  'also found empirically in (see trask et al. (2018)' : top of pg 4  'seamlessly randomly' : middle of pg 5  'we choice' : middle of pg 6  inconsistent typesetting of 'nac' : bottom of pg 6  'hindre' : middle of pg 8  'to backpropergation' : bottom of pg 8  '=?'	0
even though this modification ends up relating to a previously published algorithm, i judge the submission to be worthwhile publishing for the following contributions:  clarity/didacticism of the exposition, the minimal problem, the positioning,  the theorem,  the (hopefully to be completed) experiments the experiments are my main criticism of the paper, in particular the bsuite ones that was absolutely impenetrable for me: not only the experiments but also the results.	0
i understand that the authors leave it to future work but then the experiments feel out of place.	0
q1.4) i highly value enlightening small scale experiments and do understand that computational resources are not available everywhere however i think it would benefit the paper greatly if the proposed method is demonstrated on some reasonably sized dataset (at the very least one of full mnist, fashion mnist, freyfaces).	0
while i find the experiments to be somewhat lacking, this is sufficiently offset by the theoretical contributions of the paper.	0
concerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models.	0
to summarize, the unification under the infonce principle is interesting, but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.	0
in my opinion, i think the focus of the proposed method lies in the hierarchical representation through progressive learning, but the experiments are involved more with disentanglement.	0
regarding the metric used in the experiments, the authors mention that the proposed disentanglement metric migsup is what they first developed for onetoone property, but it seems that it was already proposed in [2].	0
the authors' comments and further experiments address most of my concerns.	0
pros  the idea is fresh, well explained and experiments are sufficiently thorough.	0
the authors don't have to do all of these, but just a few suggestions: 1) the experiments, to my understanding, are all feed forward networks.	0
strengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines.	2
my main concern with the proposed method is how it would scale when the statespace becomes substantially larger (than the 2 dimensions x and y used in the experiments).	0
if my concerns regarding the experiments are positively addressed, i'm willing to improve the score.	0
the extensive experiments here add value to the conversation about the lottery ticket hypothesis, but are not otherwise groundbreaking.	0
the writing and experiments could use some improvement, but i believe that the majority of the iclr audience would enjoy seeing this result (even though it would have no impact on most people's research) = detailed comments  page 4, sect.	0
while the analysis of complexity is sound and convincing, and the fact of being able to train larger reformers is very interesting, i have some questions and concerns about the approach and experiments.	0
the results should not be shown just for the first repetition of the experiment but for all independent runs of the experiments, e.g averaged over 30 independent cppn evolutions, for pgl, ogl, random, and hgs!	0
the experiments along the xaxis vary according to initialization, but also according to the nature of the goal space and other features.	0
the experiments are logically ordered with the initial set covering the standard fewshot learning benchmarks with appropriate baselines (though the results for fewshot tieredimagenet are lacking in this respect), with most essential details given in the main text and full details, including those related to the datasets in question and hyperparameter selection, documented in appendix h. metalearning does seem uniquely wellpositioned for tackling the task of continual learning and it's heartening to see this being explored here with a degree of success  it would be interested to see how its performance compares with standard continual learning methods (such as ewc) on the same task.	0
 the models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application.	0
since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, ondevice ml) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).	0
the strengths here are the extensive experiments and the easytoimplement method.	2
thus, the contribution of the paper is not lost, but it is better to compare sccg with the method in [a.devraj & j.chen (2019)], empirically and theoretically.	0
however i have a few questions, clarification of which would be very helpful in my mind:  (i) a major issue that seems to be raised in the earlier sections is that there is a hyperparameter deltathe threshold for the likelihood ratio testthat everything depends on, and must be chosen empirically.	0
in particular, this paper explores methods to provide robustness certificates to lipschitz bounded classifiers but does not compute and validate these certificates empirically.	0
4) originality the use of a localized (in the knn sense) set of inducing inputs to improve gp inference but the impact needs to be better quantified empirically.	0
overall i thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is.	0
attention has been empirically shown to bring improvements in many visual tasks but certain methods (such as selfattention) can be quite expensive in computations and memory.	0
one is the idea of dropping neurons as an adversarial attack, which i think has been studied empirically but not theoretically (to my knowledge).	0
then, the authors empirically compare several methods on 1) adversarial attack on imagenet with deep networks, and 2) blackbox poisoning attack on logistic regression.	0
most settings and criteria are given empirically and this paper seriously lacks theoretical analysis.	0
the paper motivates this by showing empirically that the kparity problem with some bias to the labels can be learned by a neural network, but that this does not work when there is no bias in the labels, implying that this bias is necessary for successful learning.	0
the potential scalability of evolution strategy optimization to more complex, nonconvex reward representations such as deep networks is mentioned, but never empirically demonstrated.	0
 was discarding all but the strength subclass from the persuasion dataset and empirically motivated decision or just something you did apriori?	0
it does not have to outperform ddqn, but even a comparative study empirically would be an insightful and comprehensive contribution.	0
review:###this paper proposes, analyzes, and empirically evaluates powersgd (and a version with momentum), a simple adjustment to standard sgd algorithms that alleviates issues caused by poorly scaled gradients in sgd.	0
constraining lower layers might empirically help  in particular speed up learning  but conceptually there is no reason to reduce the lower layers' capacity to somehow turn different inputs into the same output.	0
for another example, the paper claims on page 3: “these observations are specifically important because they connect dropout, an algorithmic heuristic in deep learning, to strong complexity measures that are empirically effective as well as theoretically well understood.” i agree that the connections are important, but similar connections have already been established by (at least) cavazza et al. [aistats 2018], wang & manning [icml 2013], and wager et al. [neurips 2013].	0
it would also be important to empirically evaluate the related approaches to better understand the pros/cons of wbn compared to these approaches.	2
however, my main concern remains: if ratedistortion in an individual model is a useful method for evaluating generative models, you should compare it empirically with other metrics that have been proposed for this purpose (e.g. precisionrecall).	0
this seems technically viable but unclear empirically.	0
<review summary> the proposed method seems simple but empirically performs well.	0
my concern is that the observed fim (or the ntk) is known to have fast decaying spectrum; (karakida et al) has shown empirically that the decay can be exponential.	0
 the authors claim that the twobuffer estimation is better and lead to better gradient estimation, but it is not demonstrated empirically or theoretically.	0
the limitations of the three local curvatures are shown empirically and theoretically cons: experiments seem inconclusive, with no discussion of the results proposed global curvature characterizes the optimal embedding parameters but not a different, efficiently calculable discrete curvature to approximate them analysis of particular graph families doesn’t necessarily inform what to expect from embedding large graph data overall, i lean towards rejecting this paper.	0
the 4th paragraph claims, 'we empirically find a correlation between weight stability and performance', but this is not at all obvious from figures 67. i'm not sure what a more stable evolution looks like.	0
pros: the proposed method, despite being simple, seems to empirically work well in terms of the mce criterion evaluated in the experiments.	2
the modifications are shown to work empirically but don’t result in a significantly better model.	0
 'hence empirically, our xregularization has lower standard error but higher robust error than rsl.'	0
however, the paper in the current state cannot be accepted for the following reasons: (1) the novelty is low, this very type of decomposition is already widely studied (2) the paper is not clear as to what the contributions are, and why they are justified, theoretically or empirically, (3) the review and comparison with the stateoftheart is lacking and (4) the experimental setup is simplistic and not convicing.	0
empirically, your investigations are nice, but it would be good to consider some other shiftinvariant kernels as well: inverse multiquadric, matérn, or spline rbf kernels would be prominent options.	0
that this is also empirically observed is, of course, reassuring, but these observations are better suited for the appendix, in particular given that the paper went over the recommended page limit.	0
on page 8, it is noted that 'the value of alpha_i is empirically tuned for each environment' but for which algorithm was it optimized?	0
my biggest concern is the validity of the proposed exponentiated gradient update, at least empirically.	0
i believe that the large improvements in some of the domains are significant, but it would be best to have this confirmed empirically.	0
this paper does catch a problem and uses a straightforward solution but empirically effective, but i still have two main concerns.	0
i have a couple concerns about this choice: 1.1) in order to show that the evt indeed helps empirically in the way that an adversarial classifier enforces the infnorm of vectors follow the generalised extreme value (gev) distribution, at least three plausible distributions from each form of the gev distribution needs to be checked.	0
i see this somewhat has been empirically shown in fig 2 with accuracy on the original data, but what is the mechanism not to forget what have being learned so far?	0
however, i think there are some weaknesses of the paper that would put the paper slightly below the acceptance threshold: empirically the environments are not diverse enough, and they have an implicit structure assumed and favorable for the proposed method  there remains a question whether the method is general and not taskspecific.	0
the use of relative goal specification my help improve generalization but that can only be shown empirically.	0
1) ablations/comparisons to gqn: this may just be a cultural thing, but i'm puzzled by the claims that certain things (direct comparisons to gqn on feature representation, and ablations) don't need to be empirically done.	0
this paper shows that da induces rugosity (theorem 1), but rugosity does not improve generalization (empirically).	0
the lack of significant technical novelty is acceptable if the proposed method is empirically shown to perform very well.	0
adhoc method to address the problem is introduced and works empirically, but it is not justified clearly.	0
all in all, i can see some value to this work  certainly a relatively small modification that has some practical benefits, but i'd like to see this better positioned  empirically, especially  and better analyzed (cf. my comments about l2).	0
however, the paper lacks evidence to validate most of its claims, in particular it fails at 1) establishing that the problem actually exists, by theory or experiments (or carefully selected references), 2) establishing the existence of a causal relationship between conditioning and latent variable being independent and posterior collapse, again either by theory and / or targeted experiments 3) validating claims about the method's performance empirically.	0
the main concern that i have with the paper is the experiment depth: in specific, the paper does span a number of related works, but it does not empirically expose the systems comparison in sufficient detail.	0
the process is certainly interesting, but seems neither empirically nor theoretically studied well enough in the current form.	0
although the forgetting problem pointed out by this paper is interesting and worth studying, the proposed method (1) lacks novelty and (2) does not perform well empirically.	0
the author empirically find that because these kinds of topologies converge faster during training and inevitably every nas algorithm during search don't train upto convergence but only up to a bit and make decisions based on partially converged statistics there is a bias in selection towards these topologies.	0
and 'once incremental learning has been defined and characterized for the toy model, we generalize our results theoretically and empirically for larger models'.	0
the idea of dks seems relevant, but both conceptually and empirically it seems very close to deformable convolutions.	0
overall, i don't thing experiments are thorough enough to demonstrate that the method is empirically better than competing approaches, but i believe that is not the main point of the paper.	0
the authors also consider (empirically) applications of these algorithms to 'input obfuscation': namely generating samples which are effectively noise, but the network classifies them as 'structure' (e.g. digits on mnist).	0
main comments to authors: pros: interesting combination of techniques (iaf flows and wgf/stein inference) to do metasampling empirically appealing results as pertaining to raw performance metrics like accuracy weaknesses:  the evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains.	2
the experiments empirically demonstrate that: 1. rotation exists in the training dynamics of practical gans; 2. gans often converge to an lssp than an lne, but still, achieve good results.	0
although it is not empirically but the expected version, it gives a good insight about the advantage of directly minimizing 01 loss under label noise.	0
minor comments: typo: 'upto a constant' postrebuttal update: i increased my rating to weak accept because the authors addressed my main concerns by a) giving additional information on the update of the buffer, b) improving the presentation of the experiments, c) fixing the description of maxentirl and d) by showing empirically that both the lower bound increases / wassersteindistance decreases.	0
due to the above weaknesses in the evaluation, i am not fully convinced that the claimed contributions are substantiated empirically.	0
the imitation learning experiments on a small spring dynamical system are a necessary sanity check for further work, but many other more complex systems could be empirically studied and would have made this paper stronger.	0
there are some very specific conditions under which this is known to be true empirically (for example, see the experiments in burda’s iwae paper and hoffman’s dlgm paper), but in general, one should be careful with this claim.	0
additional remarks: sec 2.2, 2), “streamlining” is unclear circumstances 2 and 3 can be quite easily derived from circumstance 1; also they are not evaluated empirically; it would be nice to have experiments for them, and they can be moved to the appendix in case of lack of space eq (19) while nice, seem to bear no significance for the proposed method and the rest of the paper; consider removing it section 4.2 paragraph 2: “shrinking” for different layers wasn’t mentioned before, and is not immediately clear what it means; the reader needs to be intimately familiar with the dim paper to understand.	0
generally, i think the insights into the maximum eigenvalues are useful and important on their own, but perhaps some additional discussion up front clarifying which results were derived theoretically and which were observed empirically could be useful.	0
 'in fact, empirically, networks with finite but large width trained with initially large learning rates often outperform ntk predictions at infinite width.'	0
the fact that no actual difference in performance between acgnns and acrgnns was noticed in the only nonsynthetic dataset used in the experiment should prompt the author to run experiments with more real life datasets, in order to empirically verify the results, but this is a minor point.	0
the problem considered is interesting, and the authors provide a straightforward but empirically effective method.	0
general advice/feedback:  should provide an explanation of the row in table 2 showing that a simple linear transformation is able to achieve accuracy and convergence times comparable to those of the nau  should provide an explanation of the universal 0% success rate on the u[1.1,1.2] sampling interval in figure 3  inconsistent captioning in figure 2c, missing 'nac• with'  should clarify in section 4.1 that the 'arithmetic dataset' task involves summing only 'contiguous' vector entries; this is implied by the summation notation, and made explicit in appendix section c, but not specified in section 4.1  it is unclear what experiments you performed to obtain figure 3, and the additional explanation in appendix section c.4 regarding interpolation/extrapolation intervals only adds to the confusion; please clarify the explanation of figure 3, or else move it to the appendix  the ordering of some of the sections/figures is confusing and nonstandard: section 1.1 presents results before explaining what exactly is being measured, figure 1 shows an illustration of an nmu 2 pages before it is defined, section 3 could be merged with the introduction grammatical/typesetting errors:  'an theoretical' : bottom of pg 2  'also found empirically in (see trask et al. (2018)' : top of pg 4  'seamlessly randomly' : middle of pg 5  'we choice' : middle of pg 6  inconsistent typesetting of 'nac' : bottom of pg 6  'hindre' : middle of pg 8  'to backpropergation' : bottom of pg 8  '=?'	0
this choice prevents the model from learning divisions, which the authors argue made learning unstable for the nalu case, but allows for an a priori better initialization and dispenses with the gating which is empirically hard to learn.	0
''the authors make the following contributions:'' 1. they show that qlearning trained on multiple tasks with a context variable as an input (an rnn state summarizing previous transitions) is competitive to related work when evaluated on a test task even though no adaptation is performed 2. based on these observations, they introduce a new method for offpolicy rl that does not directly optimize for adaptation but instead uses a fixed adaptation scheme 3. the new method leverages data during metatesting that was collected during metatraining using importance weights for increased sample efficiency ''overall, we believe the contributions are significant and sufficiently empirically justified.''	0
it would be interesting to compare both works empirically, but perhaps also theoretically/conceptually.	0
the paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach.	0
because of the empirical results, i would rate this paper as the border line (around 5), but due to the slightly annoying rating system the rate appears as 6. comments: p2: 'efficient natural evolutionary strategies (xnes) (sun et al., 2009) has been shown to reach state oftheart performances on a large es benchmark.'	0
but there are major concerns with the empirical setup and methods presented in the section where they propose a method to choose samples for ssl from an unlabeled pool of images.	0
thus, the contribution of the paper is not lost, but it is better to compare sccg with the method in [a.devraj & j.chen (2019)], empirically and theoretically.	0
incremental conceptual contributions would be fine if (1) they result in a surprising theoretical result, or if (2) they result in a surprising improvement in empirical performance.	0
overall, i think the empirical results appear very strong, but think this paper is below the acceptance threshold due to three factors, in ranked order:  (1) the theoretical guarantee, which is positioned as a core contribution of the paper (and in fact claims it as 'the first to correct labels with theoretical guarantees', which is not true), is based on assumptions that seem overly strong; these are somewhat relaxed in a 'remark', but this seems unproven and is a confusing presentation regardless.	0
regarding (3): overall, i think the empirical performance reported in table 2, and overall thoroughness of the ablation in section 4, are major strong points for the paper the performance is very impressive!	2
however i have a few questions, clarification of which would be very helpful in my mind:  (i) a major issue that seems to be raised in the earlier sections is that there is a hyperparameter deltathe threshold for the likelihood ratio testthat everything depends on, and must be chosen empirically.	0
pros: 1. theoretical analysis and empirical improvement of the straightthrough estimator is an important avenue of work.	2
i'm also a bit concerned about the similar empirical performance but longer search time when comparing with other darts variants in table 1 (using search space s1).	0
pros:  the writing is good  satisfactory empirical results cons:  the proposed method is very similar to certain methods in the literature detail comments: (1) the proposed loss function eq.(8) is very similar to the contrastive loss proposed by hadsell et al. (2006, eq.(4)), which is used in siamese gan variants (juefeixu et al. 2018, hsu et al. 2019).	2
i basically subscribe to john schulman's comment below, both about empirical results and about citing selfimitation learning, but i have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature.	0
 lack of experimental or theoretical depth: the proposed method is presented asis; no theoretical analysis of its behaviour is performed; while this is not necessarily a problem, as there are several empirical experiments, the experimental section is not sufficiently detailed: for example, no limitations of the method are being discussed and the presented results are not stateoftheart accuracies.	0
it is currently hard to assess the empirical results due to rather small improvements and the lack of repeated runs of experiments.	0
my main original concern was that the empirical evaluation only studies a single type of situation of inferring physical parameters.	0
the main limitation of the paper is that the empirical evaluation only studies a single type of situation of inferring physical parameters (whether balls should pass above, pass below, or bounce off obstacles) but does not consider other situations for inferring physical parameters for prediction in a new domain.	0
the motivation for this modified version of adamw are unclear, but the empirical results are convincing and rigorous.	0
the empirical results demonstrate the capability of the framework but would benefit from some clarification and additional ablations.	0
this assumption is not supported in an empirical or formal way, but rather by appeal to some vague argument in section 3 that since the matrix is likely to be full rank at initialization, it is likely to remain so for large enough networks.	0
reasons to accept:  strong empirical results on an interesting application  wellwritten paper  thorough experiments reasons to reject:  incremental methodological contribution  likely difficult to reproduce	0
finally, one key property of the loss expressed in the paper is its differentiability and use with autograd, but this does not hold after adding the posterior regularization term which is based on the empirical p0quantile, correct?	0
4) originality the use of a localized (in the knn sense) set of inducing inputs to improve gp inference but the impact needs to be better quantified empirically.	0
the authors prove that (1) sgd with their procedure will find a stationary point under the robbinsmonro conditions for a fixed l and (2) sgd with their procedure will converge for convex problems as l is decreased to 0. decision and reasoning this paper should be rejected because (1) the proposed algorithm attempts to address the original issue of high dimensional nonlinear optimization of neural networks but violates the algorithm's assumption in practice, (2) the empirical evaluations are lacking  having only evaluated their method on a toy problem with up to only 6 dimensions and a relatively simple image classification task, and (3) the assumption of fixing the homotopy parameter in the theorem on the nonconvex case directly violates the intention of the algorithm.	0
however only 4 epochs were run, so i believe the xaxis should be 'iterations'' besides improving the quality of writing in the paper, i would strongly suggest that the authors improve their empirical evaluation.	0
i feel these assumptions are key to the proof, but they make the impact of the work lowers on practical situations where we have no knowledge and guarantee on the true hessian and its relationship with the empirical hessian.	0
_____________ overall, i think the problem studied in this paper is quite interesting, but i did not find the theoretical or empirical contributions to be sufficiently novel, deep, or informative to merit publication in their present form.	0
still, i have some concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented, and i am concerned that the analytical results are used to imply something stronger than they actually show.	0
 questions/concerns: 1. one of the two major concerns i have is the novelty of this paper in terms of its methodology and empirical value to the community.	0
2. the second major concern i have is the connection the authors established between its theoretical findings and the empirical findings.	0
in addition, the authors did not add new empirical evidence regarding my questions but mainly reiterated the applicability of the proposed method, so i will remain my review rating. '''	0
attention has been empirically shown to bring improvements in many visual tasks but certain methods (such as selfattention) can be quite expensive in computations and memory.	0
pros: the authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification.	2
minor details: there are some good contents in this work, but for it to be a strong 'empirical' contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show stateoftheart results.	0
the problem setting of this paper is interesting and the theoretical contribution is nice, but the empirical studies could be improved: 1. it is prefer to append some experiments on realworld applications.	0
 the third concern comes from the empirical results.	0
with empirical studies, they show that ar is achieved by regularizing nns towards less confident solutions and making feature space changes smoother uniformly in all directions, which prevents sudden change wrt perturbations but leads to performance degradation.	0
i have some concerns with the paper's claimed novelties, its empirical evaluation, and its overall presentation, and thus am initially recommending a weak reject.	0
this makes the concern in 2) more prevalent if the method is not generally applicable, and are likely to help on a problemspecific basis, it would be more informative to characterize 'when' one might expect the method to perform better, and support this with empirical results.	0
meanwhile, the empirical evaluation is somewhat lacking.	0
empirically, i find the cub experiment compelling but would welcome some ablations.	0
finally, i might miss something, but the empirical results showed do not seem to show better gains than the adam algorithm.	0
a lack of any compelling motivation could be overlooked if the empirical results where compelling.	0
strong and weak points this is a very interesting empirical study, especially since  it includes a comparison with modelfree algorithms,  it considers computational aspects and indicates what algorithms can be run in realtime,  the authors use opensource software (pybullet) as simulators, which makes the study more reproducible (although the code has not been shared yet) but,  4 seeds averaged across is clearly low, given the wellknown variance of rl algorithms.	0
this is probably due to the design of experiments which only model pairwise interactions, but then this questions the relevance of the proposed empirical evaluation with respect to the claim.	0
that said, there is some empirical evidence that batch norm achieves similar effects, but with a significantly reduced cost.	0
overall, the authors demonstrated a variant of rnn updates, but neither the theoretical nor empirical evidence was strong enough to 1. convince practitioners to switch from lstm/gru to eins, 2. provide insight/understanding to the workings of rnns.	0
i was hoping to find such an example in the empirical application however this section does not make the necessary connections with the theoretical development.	0
the paper motivates this by showing empirically that the kparity problem with some bias to the labels can be learned by a neural network, but that this does not work when there is no bias in the labels, implying that this bias is necessary for successful learning.	0
similarly, the additional hyperparameter details in appendix a are helpful to enable reproducability but without explaining how these values were chosen it is impossible to assess the rigor of the empirical evaluation.	0
the idea, although a bit incremental, is potentially useful to practitioners, as argued in the introduction, and some empirical result tend to suggest that the method can be useful.	0
my primary concern is that the paper is entirely empirical with little if any justification of the results.	0
the main reason i tend to reject this paper is that the motivation of their proposed algorithm is very unclear, lack of theoretical justification and the empirical justification is restricted on ppo  one policy gradient method.	0
it should provide not just an algorithm and some numbers in experiments, but also why we need the algorithm, what's the key insight of designing this algorithm, what the algorithm really did by the algorithm mechanism itself instead of just empirical numbers.	0
it seems that section 3 provides some empirical evidence supporting this strategy, but the description of the method is hidden in the experimental details.	0
update: i read the authors' rebuttal, which addressed many of my concerns and presented additional empirical results.	0
although the authors explain the difficulty of obtaining these numbers, a lack of them does not complete the empirical evaluation.	0
the empirical results are fairly limited and lack explanation in places and appropriate comparisons to existing work.	0
in this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis.	0
overall evaluation my main concerns about the paper is that the exact objective of the setting and the representation learning is not that clear and the empirical validation is limited to a qualitative assessment of the representation learning with no downstream task.	0
this aspect is never really evaluated in the empirical section, but it would be one of the most interesting uses of the learned representation.	0
pros: significant empirical advance?	2
i am not necessarily interested in an actual empirical evaluation, but including this in the related work section would likely be interesting for the reader.	0
(should be e) edit: i still believe this paper lacks empirical evidence that the proposed loss would help train deep models without harming performance on tasks where depth matters.	0
the idea of the design is reasonable, but the theoretical and empirical results are not strong.	0
in light of the relatively low novelty and the lack of compelling empirical performance for the proposed combined mt system, i do not feel that this paper is appropriate for iclr at this time.	0
concerns:  although saving checkpoints is “cheap” and shows empirical good performance, the models are somehow dependent on each other, particularly in experiments where consecutive checkpoints are saved.	0
i would suggest the authors move some paragraphs in appendix to the paper, for instance, 5.1.1, 5.2.2, and 5.2.3. overall, the paper is presented with extensive empirical evaluations, but less theoretical justification.	0
4. in general this paper lacks empirical analysis on why distillation helps improve the transferability.	0
as a result, i think the paper is exploring an interesting direction, but that the empirical work might be too preliminary for publication.	0
cons:  some of the simplifications for proving are empirical, so that the proof itself is not that rigorous.	0
the lack of comparison to such baselines makes this paper quite incomplete, especially as it is an empirical paper itself.	0
besides among the variants, one clear pattern is that uniformly random picking of indices is worse than other schemes (lefttoright, leasttomost, easyfirst) which is not unexpected but no conclusive empirical evidence can be found for relative differences between the performances of other 3 schemes.	0
it is fine that the authors focus on empirical results of translation quality but then i would like to see more strong and extensive evidence that supports the use of such approximation.	0
it does not have to outperform ddqn, but even a comparative study empirically would be an insightful and comprehensive contribution.	0
review:###this paper proposes, analyzes, and empirically evaluates powersgd (and a version with momentum), a simple adjustment to standard sgd algorithms that alleviates issues caused by poorly scaled gradients in sgd.	0
i am mainly concerned with the significance of the content in the paper and positioning with respect to past theoretical/empirical work.	0
in its current form, the paper has two main weaknesses:  it is poorly written & organized  it was a fairly weak empirical evaluation in order to address the first issue:  the authors should significantly improve the quality of the prose, which can be confusing & difficult to undersrand  the introduction needs to be significantly crisper: in its current form, it is far too general and does not describe the rest of the paper; the authors should explain ... 1) what is the problem they are working on (currently present, but far too long) 2) what is the proposed approach & why is it novel (missing) 3) what are the main results & their significance (also missing) in order to address the second issue:  3.1 needs more details; it is this reviewer's understanding that the current corpus consists of 1065 documents (which is extremely small in size); how many sentences are there in these documents?	0
pros: 1. nice application of bert to grounded instruction following tasks 2. good empirical results cons: 1. not much technical novelty 2. empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise) other comments: 1. are the bert weights frozen or finetuned along with the rest of the model?	2
however, the empirical results are not convincing/rigorous enough to provide the reader information on 1) whether cmsa is a useful method (since it is used only with bert and does not seem to affect results on its own compared to mp, sa, tn, etc.) and 2) when should one use/not use bert and cmsa (bertcmsa actually does quite poorly acc.	0
while there is some interesting and potentially useful novelty in the approach, i have some concerns about the empirical evidence and modeling to truly determine the mmlp's utility.	0
the authors can find this conclusion in many places, for example: https://courses.cs.washington.edu/courses/cse521/16sp/521lecture6.pdf (theorem 6.3) https://www.cs.princeton.edu/courses/archive/fall14/cos521/lecnotes/lec11.pdf (corollary 2) i noticed the author claimed in the paper “based on empirical observations presented later on we argue that multitask networks not only benefit when the cosine is nonnegative but more so when task gradients are close to orthogonal.” (page 3).	0
i understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis.	0
the paper fails to provide any theoretical insights but a thorough empirical evaluation.	0
however all empirical results are only from mnist.	0
# quality  the proposed idea is very simple but seems very effective in practice as shown by the empirical results.	0
on page 1, the authors claim that empirical evidence suggests the lack of correct causal modeling is an important factor for lack of generalization, generation of unrealistic captions, and difficulties in transfer learning.	0
the empirical results in table 1 and 2 seem solid, but i'm not familiar enough with past results in this area to evaluate their significance.	0
besides the empirical results, the 'theory' developed in this paper also has several fundamental weakness (will discuss in detail below) and are lack of solid connections to robustness and the proposed 'bounds'; the proposed 'bounds' are questionable and are not sound bounds, and they can hardly be justified theoretically; so it is not surprising that they cannot outperform ibp, which is based on rigorous minimax robust optimization and sound overapproximations of neural networks.	0
in some papers on empirical defense, linearity sometimes can help to reduce pgd error; however in the certified setting, a direct surrogate to certifiable robustness like ibp usually produces the best results.	0
the new empirical results still do not address any of my concerns  ibp still significantly outperforms the proposed method.	0
the empirical results are promising but in my opinion can be further improved through minor changes to experimental procedure.	0
overall i felt that the empirical results were fairly promising but some questions remained for me.	0
the theoretical result does make it an interesting contribution, but the empirical section weakens the paper quite a lot.	0
the authors didn't demonstrate any intention of changing this name, but if they decide to submit this paper to a next conference, with stronger empirical evidence to back up the paper claims, i recommend them to drop the altq learning name as well.	0
the authors claim “empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum“ but clearly from the formula itself, one needs to differentiate between minima with low and high function values.	0
the empirical evaluations of their algorithm seem to outperform/be competitive compared to other nas methods on all benchmarks used in the paper, however only darts is evaluated on their search space and the other results are taken from the corresponding papers.	0
i appreciate that the authors made an effort to not just show empirical results but also motivate their findings by theory, although the argumentation stays a bit superficial.	0
my concerns and suggestions are mainly focused on the empirical part, which i think authors need to improve.	0
though my concern on writing has been resolved to some extent, i'm still unsatisfied with the empirical experiments.	0
unfortunately, there are many concerns which are still not convincingly answered: (i) considering the focus of this work (as admitted by the authors in the rebuttal) is empirical and with limited technical novelty, more comprehensive results on many datasets are required.	0
this reduces the novelty but increases clarity, and may still make the work a useful empirical reference for these benchmarks.	0
clarifications re: baselines and empirical evaluations:  no comparison to ibrahim, et al., 2019  you can cluster in unsupervised setting but you can also aggregate within class in supervised setting.	0
in sum, the paper provides a novel insight on how pruning affects the performance at the example level, but does not provide a solution, and the current set of experiments is insufficient to validate that the empirical findings that the authors report generalize to other types of pruning approaches, such as inputdependent pruning.	0
one of my primary concerns, however, is that i am still not convinced whether the empirical findings presented here is indeed significant, as some reader might feel those results are not that surprising.	0
i fear however that one vs two is not the correct empirical generalization about attention that one should try to account for.	0
i think it's ok that some of the empirical results are inconclusive, but it should lead to more quantitative error analysis.	0
overall, i found the paper to be clear and well written, but algorithmic and empirical contributions are i think rather small (small or no empirical gains, incremental ideas).	0
however i would have been more convinced by some empirical evidence for this claim.	0
i think the empirical contribution is above the bar, but i do not think the authors gave enough credit to (serra & karatzoglou, 2017).	0
2. the paper provides theoretical results in terms of population distribution, the true but known distribution, however in practice, empirical distribution is used instead, for example the cross entropy in section 3 used in training, are these theoretical results still valid for empirical distribution?	0
some of the other authors have expressed a number of concerns about this paper's theoretical and empirical contributions, and i am not raising my score.	0
after the rebuttal phase, i now have an additional new concern about the clarity of the writing and the completeness of the empirical details.	0
overall i think this work is an interesting direction for designing static sparse neural network weight topologies, but it’s lacking in empirical evidence of their claims and could do better to tie themselves to existing literature in training sparse neural networks.	0
while the application is interesting, there is not much novelty in the method and factoring that, the empirical study is lacking as well.	0
and thorough empirical studies are done by comparing to not only baselines but also different loss parts of the proposal it owes.	0
the authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses me reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of me bias on task performance for a toy example.	0
on the negative side and besides empirical/experimental evidence, the paper would be much more convinving with some more insight into the model itself and some more qualitative evidence.	0
since the main contribution of this paper is the empirical results on the three tasks, my concerns regarding the experiments need to be addressed before i can vote for accepting this paper.	0
there are several concerns regarding empirical evaluation and baselines: (1) the improvements in validation error shown in tab.1 seem to be insignificant, and confidence intervals are needed for justification.	0
to summarize, the paper addresses an important problem of calibration under domain shift but it needs some more empirical work to show the real advantage and limitations of the proposed method in a practical setting.	0
i tend to vote rejection for this paper, mostly due to my feeling that the empirical contribution is interesting, but not novel enough for this conference.	0
3. the most serious concern is that although empirical results are promising, i have concern about the correctness that eq.(6) realizes disjunciton, and eq.(8) realizes negation.	0
it’s harder for me to precisely assess the significance of the proposed approach, but at a high level it looks reasonable and is backed by convincing empirical evidence.	0
considering the explicit instruction to judge papers exceeding 8 pages with a higher standard, i believe that the lack of a greater amount of empirical evidence is a significant deficiency of your otherwise very interesting work.	0
the motivation and the theoretical arguments are interesting, but the paper lacks in presentation and sufficient empirical evidence is also lacking to get fully convinced by the claims.	0
overall, i think this is a promising approach (the empirical results certainly bare that outo but to my eyes it lacks sufficient detail and specificity.	0
in particular, the proposed theory focuses in two phenomena (word similarity and word analogy) as stated in the abstract itself, but the empirical evaluation is limited to the word similarity task.	0
my concern is that the observed fim (or the ntk) is known to have fast decaying spectrum; (karakida et al) has shown empirically that the decay can be exponential.	0
 the authors claim that the twobuffer estimation is better and lead to better gradient estimation, but it is not demonstrated empirically or theoretically.	0
(3) also due to (2), the experiments shown in the paper are on toy data and hence lack of strong empirical support.	0
my current rating is weak reject based on the weakness of the writing and the lack of strong empirical evidence in support of the effectiveness of the proposed contributions.	0
overall, the content of the paper seems sound and the theoretical and empirical justifications seem well founded but i also can't claim to be an expert in this area.	0
my main concerns with the work stem from the empirical study and choices made there.	0
i agree this is probably the case, but this would still be a useful empirical result to show the degree of data required for these alternative.	0
the empirical gains in transfer learning can be simply attributed to:  more params it seems adding an lstm over bert embeddings already does some improvement, i would have loved to see this more exploited but it wasn't.	0
conclusion: the paper introduces large claims and empirical results that correlate with, however the provided experiments are not done with enough control to attribute gains to the design choices provided in the paper.	0
the empirical evaluation provides a number of proofofconcept ideas, but no in depth investigation of the properties of the approach.	0
however, i have a litany of concerns with the paper itself, concerning its high similarity with a paper published in may, its motivation, its presentation, its empirical evaluation, and the analysis presented within.	0
reconstruction could perhaps be motivated from the point of view of compression, but this paper makes no attempt to examine compression: ratedistortion tradeoffs are not considered, nor are any empirical metrics of compression ratio or likelihood such as bits/dim presented.	0
strengths: ' stateoftheart results on several downstream visionlanguage tasks ' empirical work to investigate different ways to perform conditioned masking weaknesses: ' some parts of the method needs clarification (see point 2 below) to better understand the details and practical advantages of the method. '	2
the authors propose an empirical analysis of different ways to mask the visual input, however this might not be a substantial extension of previous work.	0
i think the clarity of the paper should be quite improved (see detailed comment), hence why i think the paper is borderline, but i am leaning towards an accept given the significant theoretical improvements over the past literature (and positive empirical results), even though the algorithmic suggestion is somewhat incremental.	0
pros the paper involves the language modeling tasks in empirical results besides traditional image classification tasks, which helps to explain the convergence performance of optimization methods in a wider range of applications.	0
decision this paper presents promising empirical results, however the experiments are limited, making it difficult to place in the broader work.	0
unfortunately, the impressive results on dmlab are not sufficient for both the lack of deeper empirical study and better theoretical motivation for the architecture modifications.	0
the reason i am still against acceptance is the lack of stronger empirical evaluations.	0
structural metadata of wikipedia, such as crosslingual document alignments, is deliberately not exploited (some discussion is provided but i would have preferred an empirical comparison of local vs global extraction).	0
cvpr 2018. the paper contains mostly empirical evaluations however the provided experiments do not well support the claim that cnn works well with geometric structures in geolocalization.	0
i think the empirical results can be interesting for people working on geolocalization, but probably not for the audience of iclr.	0
i am not an expert in the field and i have a hard time judging whether the empirical improvement are exceptional but i would advocate against accepting the paper based on clarity alone at this point.	0
whether or not this is beneficial is an empirical question, but i wouldn’t say that the proposed method does something special to “control” how much adaptation is performed.	0
overall, this paper presents a simple heuristic to steer the policy towards riskseeking / riskavoiding directions, but could benefit from either more theoretical analysis or more empirical comparison with other methods.	0
concerns: 1) the main concern about this paper is the inaccuracy and very general statements without theoretical or empirical justification.	0
the authors state “we believe this finding to have farreaching consequences as it directly contradicts the popular hypothesis about copying caes.” the paper definitely shows some empirical evidence that supports the claim that copying does not occur, but these findings are all done with a single seed and by considering small subsets of datasets (celeba and stl10).	0
due to some experimental details (particularly concerning the environments), the empirical results may be invalid and it is difficult to assess them.	0
i understand we need that in the proof, but is there any reason this is also the case in empirical evaluation?	0
one of my major concerns is that it has a large overlap with fischer (2018) in terms of methodology, experiment settings, and empirical observations, which limits the general contribution of the paper.	0
 'hence empirically, our xregularization has lower standard error but higher robust error than rsl.'	0
however, the paper in the current state cannot be accepted for the following reasons: (1) the novelty is low, this very type of decomposition is already widely studied (2) the paper is not clear as to what the contributions are, and why they are justified, theoretically or empirically, (3) the review and comparison with the stateoftheart is lacking and (4) the experimental setup is simplistic and not convicing.	0
2. my main concern of this paper is the novelty, however, the empirical results are strong.	0
the empirical validation is relatively simple but in my opinion it provides enough insights about the advantages of the proposed method compared to vaes and gans.	0
i recommend rejecting this paper for the following reasons: (i) the algorithm developed here is extremely heuristic, no insight, theoretical or empirical, is provided as to why this could be a general algorithm, (ii) a major claim in the paper is that the automatic learning rate tuning does not have any hyperparameters but the actual algorithm does have parameters such as patience and successive doubling of the learning rate although they are tuned adaptively using adhoc heuristics, (iii) the convergence analysis is not at all rigorous, in particular the optimal oracle for sgd may not exist, and (iv) the baseline algorithms are not tuned and the minor improvements of the proposed algorithm over them is therefore not significant.	0
hence, i believe the empirical study is still significantly lacking.	0
main reason is as follows; i believe the idea is interesting but it needs a significant empirical work to be published.	0
however the empirical evidence of whether this approach will significantly degrade the model performance at the end of training is not provided.	0
my biggest concern has to do with the empirical results.	0
i don't think that a paper necessarily has to achieve empirical results beating all previous ones in order to merit acceptance, but the way that the comparison is currently set up doesn't seem to facilitate a clear comparison of the pros and cons of this method versus other ones in the literature.	2
guarantees implies strict results, but the authors use a weak attack (fgsm) to approximate empirical local robustness.	0
i find the observations interesting, but the contribution is empirical and not entirely new.	0
although the authors tackle a challenging problem, their empirical results are lacking to provably demonstrate that their approach outperforms existing baselines.	0
that this is also empirically observed is, of course, reassuring, but these observations are better suited for the appendix, in particular given that the paper went over the recommended page limit.	0
that weakens the empirical use of this algorithm.	0
on page 8, it is noted that 'the value of alpha_i is empirically tuned for each environment' but for which algorithm was it optimized?	0
 given the lack of theoretical / conceptual guarantees that the methodology will work, our faith in the proposed methodology rests entirely on the empirical experiments.	0
cons: even though i appreciate the paper's theoretical contribution, there are no empirical results other than the motivating example.	0
the current results simply show some examples, but provide no empirical way of judging which approach actually leads to more imperceptible changes.	0
overall, though the results are perceptually encouraging, i have slight concerns the empirical results reported.	0
the proposed approach does have empirical backing, but i would argue that it is a very straightforward application of the straightthrough gumbel estimator to gans, which is itself similar to existing work on applying the gumbelsoftmax estimator to gans (kusner & hernándezlobato, 2016).	0
the controlled experiments conducted in the paper are still informative, but the overall message would be much stronger if the empirical analysis can be extended to common benchmarks such as cifar and/or imagenet.	0
given the apparent lack of any technical contribution to machine learning theory or practice, the inconclusive empirical results, and the generally unpolished writing (e.g., long runon sentence in the conclusion, vague problem definition), i do not believe this paper is suitable for publication.	0
i would like to see this paper eventually published as i find the proposed technique original and quite relevant to current rl research, however i feel like its empirical evaluation is too weak at this time, which is why i am recommending rejection.	0
the main limitations of the current empirical evaluation are: • only 7 atari games are used (vs 49 in the ppo paper the proposed technique is compared to), without justification for how they were chosen, and it seems like only 1 run is performed on each game (while rl algorithms are well known to exhibit high variance) • on obstacle tower there seems to be also only one run of each algorithm (more runs could be done with different training & testing seeds in order to get an idea of the variance) • there is no comparison to ppornn on obstacle tower • i think a natural and important baseline to compare to is using the same architecture as in fig. 2 but where the mapping phi(o_t) is learned through regular backprop (using the same loss as when learning the mapping i(t)).	0
— overall, there is a general overstatement of contributions and results: this is certainly not the first ssal or usal and the statement relative to deep learning is subtle; some of the empirical results are interesting, but i am not sure about ‘spectacular gains’ (and these gains aren’t seemingly due to the contribution of the paper).	0
pros: the empirical improvement seems meaningful.	2
furthermore, empirical results are not only shown, but also analysed.	0
this paper does catch a problem and uses a straightforward solution but empirically effective, but i still have two main concerns.	0
given that the empirical result is pretty much the only support of the claim in this paper, the lack of more diverse experiments would weaken the contribution.	0
however, i will raise my score if the revision addresses my concerns or provide additional empirical evidence.	0
the empirical aspect of the work is incremental; by combining two prior works.	0
i might have missed something, but in my opinion the theoretical contribution of the work is rather tangential to the empirical analysis and results presented in the paper.	0
i have a couple concerns about this choice: 1.1) in order to show that the evt indeed helps empirically in the way that an adversarial classifier enforces the infnorm of vectors follow the generalised extreme value (gev) distribution, at least three plausible distributions from each form of the gev distribution needs to be checked.	0
while i don’t think novelty is a strict requirement, if it is absent, then it should be compensated with strong empirical results, but the paper lacks that as well.	0
main comments: while this paper proposes some interesting ideas, i am concerned about the soundness of the method, some of the precise implementation details and i believe the empirical evaluation could be greatly improved.	0
in particular, no empirical results are given concerning the sensitivity of the reported successes to choosing the correct value for k. we are only told that “k is usually a small number such as 5 or 10” (sec 3).	0
if we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?	0
overall i think this work attempts to solve an interesting problem with an incremental but reasonable approach, and more empirical evaluation is needed to make the paper meet the bar for acceptance.	0
6. as this is primarily a theoretical paper, focusing on simple classifiers is probably okay, but some sort of empirical comparison with other certified defense strategies is necessary.	0
lack of strong empirical performance along with the limited novelty detailed comments and questions:  the approach naturally extends to few shot classification problems once the mse loss in equation 5 is replaced with an appropriate cross entropy loss.	0
on the other hand, the proposed empirical study significantly lacks in many aspects.	0
'comments' the empirical advantage of dcem vs. unrolled gd is clear, but it's not clear to me what the intuition behind this is.	0
 lack of evaluations with variety of datasets (cifar10/mnist)/configurations (other bitwidth)  lack of the citation and comparison to many most recent works on binarized networks (except xnornet) comments: i consider this a wellwritten paper with great clarity and good empirical performance.	0
the empirical results seem sufficient to me, but i am not familiar with relevant baselines (see below).	0
the proposed change to the mpnn network architecture is rather simple and hardly physics inspired, but the empirical improvement seems substantial, so this too is a nice contribution.	0
the use of relative goal specification my help improve generalization but that can only be shown empirically.	0
in the rebuttal the authors addressed my concerned about the insufficient empirical evaluation and included other relevant baselines.	0
cons: it's weak accept rather than accept from me because of how the empirical evaluation were conducted in the paper, and i think the experiments conducted in the paper were a little bit weak (common for most iclr submissions).	0
concerns: 1) the central empirical result stated is that using this approach allows one to reduce amount of labelled data by 1020 %.	0
 i personally remained unconvinced about the response to the question on number of adaptation steps, as well as on the lack of deeper models in the empirical studies.	0
however the proposed approach is too incremental and the empirical evaluation could also be improved.	0
8) in section 7, the authors put out a hypothesis ' however, there is still generalization between samples: the agents are only trained on the limited' but the provided empirical study might not fully be considered to be designed to test this hypothesis.	0
(2) the empirical evaluations are lacking.	0
the proposed method sounds interesting and promising, but the empirical evaluation is unclear.	0
empirically, the proposed approach with momentum vector performs better than ewc but worse than ritter et al. (2018).	0
 this is an empirical study, but if there was theory to support the empirical observations, then the paper was more convincing.	0
my major concern is that more empirical investigation is necessary since the formulation provides a minor novelty.	0
third, while the caption of table 1 mentions that 20 trials were used, it is not clear if this was some sort of kfold cross validation, monte carlo, cross validation, the same splits but with different random seeds, etc. additionally, the variance across the different trials should be given; otherwise, it is not possible to tell if any of the empirical results are significant.	0
those approaches are quite different (and lack any sort of theoretical guarantees, for the most part), though, so empirical comparisons may not be so meaningful.	0
i find the proposed idea to be promising and quite intriguing, but i think that the paper has some room for improvement and provided empirical evidence might be insufficient (including for understanding the importance of individual model components), which in turn makes the claims of potential practical attractiveness less justified.	0
combined with the lack of multiple runs to provide more solid evidence for the empirical findings, i would reject this work.	0
it would not only be helpful to define it but also would be helpful to use a different notation for distance and the d used on page 3, presumably for 'empirical state distribution'.	0
i think that the paper displays some appealing empirical and methodological contributions, but it is not sufficiently theoretically grounded.	0
i would advise the authors to rephrase their work as a primarily empirical contribution, in order to emphasize the merits of their method over a lacking theoretical analysis.	0
the authors mentioned that such an objective function is asymmetric but didn't explore the implications of such an objective function in the empirical experiments. '	0
review:###summary: in this empirical study, the authors identify that batch normalization  a common technique for accelerating training  leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks.	0
the authors evaluate their method on blackbox attacks, whitebox attacks, and graybox attacks on mnist and fashionmnist, and show decent empirical results when compared to baselines.	0
the paper lacks complete discussion on their empirical results.	0
unfortunately, i have several concerns over the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which i detail below.	0
overall conclusion: this paper is largely empirical and lacks technical depth.	0
there are significant enough weaknesses in the empirical results that make me question the authors’ own implementations.	0
 the authors are recommended to compare with previous work on hierarchical generative modelings with objects and parts, such as (xu et al, iclr 2019), and other related work, such as spiral (ganin et al.) due to the limited contribution, lack of comparison with related work and limited empirical evaluation, i recommend rejection of the submission.	0
after reading the rebuttal, my concerns remain that: (1) lack of empirical evaluation on more complex real / synthetic datasets; (2) it is still unclear to me how such hierarchy is inferred from single images.	0
this naturally limits the empirical reach of the paper and is the source of my 2nd and larger concern.	0
the submission:  briefly describes the neural semantic encoder (nse) model (munkhdalai and yu, 2017)  claims that the dotproduct attention mechanism in nse is too simplistic for the task of summarization because dotproduct attention does not capture wordsentence and sentencesentence correlations, the latter two being important to summarize a document, and suggests utilizing the additive attention mechanism along with the pointergenerator mechanism instead  introduces the “hierarchical nse” model and a selfcritical sequence training scheme to optimize for rouge  provides empirical results for these models and for other relevant baseline models on the cnn/daily mail dataset at a highlevel, this submission is unpolished, imprecise, lacks technical (both experimental and theoretical) rigor in its description and explanation of its methods, and generally does not have a significant contribution, which is why i believe it should be rejected.	0
the paper directly responds to zhang et al. 2016, however many of the empirical results are in no way comparable making it difficult to understand how this paper builds upon or further extends the work by zhang et al. 2016. for instance, using the loss function as a surrogate for classification error while discussing the generalization abilities of the classifiers makes it difficult to understand how well each classifier is actually performing in the empirical results.	0
the biggest issue with the paper is the lack of empirical results, we are given no numbers to demonstrate performance making the results impossible to interpret.	0
while the problem/research direction is interesting, i have several concerns about the setup, proposed method, empirical evaluations, and theoretical contributions: ' the basic setup is not wellposed in my opinion.	0
however, the paper is nevertheless poorly written, and the empirical evaluation is weak.	0
overall, despite the encouraging empirical findings and depth of analysis, i have several major concerns regarding the work, as listed below.	0
the lack of significant technical novelty is acceptable if the proposed method is empirically shown to perform very well.	0
however, i have concerns about the empirical evaluation of the proposed method.	0
verdict: the paper brings a fresh perspective to the problem of representation learning in rlthe empirical study is on the thinner side, but i find the initial insights and results to be of sufficient interest to the community to lean toward acceptance.	0
the empirical study is interesting, but i suspect the paper would be stronger if at least some of the ppo results (beyond fig. 7) from the appendix could make their way into the main paper.	0
however, the empirical evaluation is lacking in terms of text generation quality and classification quality.	0
although i greatly appreciate this type of principled empirical evaluation, which i also think is lacking currently in our field (especially in deep learning), for this particular contribution, i have the following concerns for it to be published at iclr: 1).	0
in sections 3 and 4, why not use empirical distribution in stead of true but unknown population distribution?	0
the authors motivate their work by saying that soft attention is not interpretable, however i do not see any empirical evidence or analysis of their proposed hard attention mechanism as being more interpretable.	0
due to the lack of empirical evidence to justify the effectiveness and significance of the method, i vote for rejecting the paper.	0
this setting / motivation is a very relevant one, and the method seems like a sensible start, but neither the proposed approach nor the experimental treatment are substantial or novel enough to warrant acceptance; however hopefully something on this topic and approach with a more thorough empirical and/or theoretical treatment will appear later on from these authors!	0
the empirical results are okay but not remarkable.	0
it is a very incremental contribution in terms of methodology, but the empirical results are strong.	0
reasons to accept:  strong empirical results  thorough treatment of baselines and related work reasons to reject:  incremental contribution (especially compared to tsai et al., 2019)  writing lacks sufficient technical detail	0
to make the empirical evaluation more rigorous and convincing, i think the authors should: (1) repeat the adversarial robustness experiments on more commonlyused datasets such as imagenet and cifar, and also evaluate robustness of their models more thoroughly, using techniques like blackbox attacks.	0
given that my assessment of the methodology is correct (please correct me if i am wrong) leading to a lack of novelty and the lack of strong empirical results, i vote to reject this paper.	0
the main reason is that this work seems to be a quiet incremental work on top of offpolicy actorcritic [degris et al 2012] and acer [wang et al 2016], at the same time, lack of proper empirical and theoretical justification of the algorithm.	0
generally, i believe the idea has conceptual merit, but the empirical evaluation is not sufficient to finally judge its practical value.	0
my concerns of this submission are on the execution of the ideas, and the empirical analysis.	0
doing this means that empirical results should be strong to back up these decisions, but the experiments are only on mnist and, although they outperform ewc, do not outperform other continual learning works.	0
the theoretical analysis is not at all new, but does provide a nice way to understand the empirical results, and in its presentation here is more understandable to nonstatistical physicists than in the original formulation.	0
given the large empirical improvement over s4l, i don’t think the lack of differences is a huge issue.	0
specifically, this paper lacks theoretical and/or empirical justification on why individual neuron’s contribution during the learning process can be characterized equivalently as a coalitional game.	0
however, in my personal opinion, it is not motivated enough why multidecoder is necessary (except for an empirical evidences in table 2) when many people use a rnn model as a good but approximate function approximator.	0
 while empirical study could be interesting, but not much insight was not provided.	0
to sum up, the paper provides a few interesting empirical results but it is not clear what benefits are gained from these results.	0
2. the regularizer for maml seems interesting but only tested in omniglot dataset, i would like to see more empirical results.	0
robustness claims are backed up by empirical validation, however the experimental setup falls short to provide sufficient evidence.	0
the empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup.	0
overall, the submission proposes an interesting and stateoftheart method for selfsupervised video representation learning, but its technical contribution and empirical evaluation are limited.	0
empirical evaluation on fairly small scale experiments demonstrates these techniques can productively be employed to develop new and powerful models concerns: there is, in my opinion, rather too much content for a conference paper of 8 pages.	0
however i find it a bit hard to advocate acceptance just yet, mainly because of two concerns that a) this paper is largely about stitching multiple existing techniques (e.g. neural processes, influence functions, monte carlo dropout, etc.) together and may hence be a little bit lacking in its own originality, and b) both the presentation quality and the empirical studies still leave quite a bit to be desired (details to follow).	0
more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2stage vae (dai & wipf).	2
however, due to the fact that this work ambitiously introduces a wide range of architectural changes, it suffers from a general lack of thoroughness in empirical support for all of the proposed ideas.	0
in general, it feels like this work scratches the surface of some interesting empirical phenomena, but falls short of either providing conclusive evidence for them or of providing theoretical insight into them.	0
overall, i appreciate the empirical approach of the paper but i think the definitions, arguments and experimental design could be improved significantly.	0
the ideas in this paper are definitely interesting, but i'm more inclined to reject this paper for the following reasons: the theoretical contributions are hard to follow and, while the empirical results are encouraging, the technical contribution is small when compared to the existing literature (tzenand raginsky, 2019), (jia and benson, 2019), (rackauckas et.	0
this is actually an advantage of the paper, as it makes it a useful trick for a variety of systems that perform this task, but also increases the burden of empirical evidence for its usefulness.	0
however, my major concern is in the empirical evaluation, especially i'm observing that the comparison with (yang et al., 2016) is not presented.	0
============= following are the main concerns: ' in absence of any baseline, the empirical results are meaningless.	0
overall, i think there is indeed some novel contribution in this paper, but i feel it's quite incremental in nature, in terms of both theoretical analysis and empirical results.	0
i think this is a good paper which shows strong empirical results based on simple but effective idea.	0
7. one of the major concerns i have is that the empirical results don't seem that impressive.	0
detailed comments: the paper is interesting but largely empirical.	0
the proposed idea makes sense at a high level, and the empirical results look somewhat compelling, but the writeup is not particularly clear (see clarityrelated comments).	0
the author empirically find that because these kinds of topologies converge faster during training and inevitably every nas algorithm during search don't train upto convergence but only up to a bit and make decisions based on partially converged statistics there is a bias in selection towards these topologies.	0
the empirical support for some of the claims (e.g., superiority to (eastwood & williams, 2018)) is a bit weak, but other strengths mentioned above largely make up for that.	2
the research program of incremental learning for deep neural networks would show something like 'incremental learning exists when minimizing empirical loss', 'incremental learning and early stopping imply certain properties (like low capacity) on the resulting neural network' and 'these properties imply low generalization error'.	0
that is ok, in the sense that one could aim for these results to be modified and applied with empirical losses, and then a separate line of research could study how incremental learning bounds generalization error.	0
and 'once incremental learning has been defined and characterized for the toy model, we generalize our results theoretically and empirically for larger models'.	0
i think that this paper merits acceptance because (a) the motivation of taking a necessity in practice (somehow selecting models / injecting inductive biases) and making it more explicit in the approach is a good one, and because the thorough empirical survey (and simple, but novel, contribution of a new semisupervised representation learning objective) are likely valuable contributions to this community.	0
another concern is that even though the empirical results look quite promising, it would be good to stresstest the proposed score on more common ood detection benchmarks against stateoftheart methods to see if likelihood generative models with the proposed criterion are competitive there.	0
2. novel objectives for a multiagent training setup cons: 1. empirical results do not contain any baselines or prior work comparisons (only ablations of the proposed model) —— updates: thanks to the authors for their response.	0
 overall, this feels like a good paper, but i'm not too familiar with prior empirical results for populationbased rl methods.	0
the idea of dks seems relevant, but both conceptually and empirically it seems very close to deformable convolutions.	0
pros  the extensive empirical evaluation shows that the proposed method is effective in preventing performance degradation in an asynchronous setup across tasks and models.	0
the topic of empirical comparisons of nas algorithms is already very difficult to tackle in a fair way, but it gives thoughtprovoking strategies to evaluate the target nas algorithms.	0
however my main concern is on the empirical studies, as they don't seem particularly convincing to justify the claimed success of the rae over vae.	0
for the reasons above, i think the paper is borderline, but i am currently voting for acceptance based on the strong empirical performance.	0
overall, this seems to be a solid contribution (even if the empirical results are a bit incremental) and i recommend acceptance.	0
i do see the empirical experiments demonstrating this but some more insight into this is perhaps important.	0
it seems implausible on the face of it, but perhaps there is a theoretical argument, or empirical evidence, that makes it plausible.	0
review:###update: my recommendation has been borderline because the discussion of the paper about the nature of locality and compositionality seems to be less indepth than i would have expected, but if the authors will revise the submission to shift the focus of the paper to more focused on analysis and evaluation and weaken the claims that their model exhibits locality and compositionality, i would lean towards an accept as the empirical evaluation is extensive.	0
i do have some concerns of the work  the paper is empirical and the techniques are combinations of previous methods.	0
as the authors point out, the recent work by akrout et al. [3] is very closely related but the paper is lacking a significant discussion let alone an empirical comparison.	0
experimental results on generating adversarial examples from blackbox machine learning models as well as a binary classification problem demonstrate improved performance over several existing baselines, such as better query efficiency or faster empirical convergence in the loss function.	0
however, the major question i had as i read the paper was the efficacy on gpu, which the paper discusses, but does not implement, nor show any empirical results for, which weakens the paper.	0
10 the empirical validation is relatively simple but it illustrates quite well the theory.	0
it lacks baselines on more complex networks and could benefit from more empirical analysis of the theoretical benefits and properties of this approach.	0
cons: the empirical results could be clearer.	0
main comments to authors: pros: interesting combination of techniques (iaf flows and wgf/stein inference) to do metasampling empirically appealing results as pertaining to raw performance metrics like accuracy weaknesses:  the evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains.	2
on the other hand, i wish the paper provided a bit more intuition on when the rankone tensor approximation structure will give good results (e.g. section 5.3 gives an empirical measure; but what about some simple theoretical examples which give low values, to provide more intuitions?).	0
i am positive with respect to accepting this work, but find that there are a few unclear points in the evaluation that should be clarified to strengthen the empirical results.	0
the empirical comparison with tbptt is substantial, but the waters are muddied a bit by imprecise presentation of baselines.	0
pros: 1. the empirical results are strong.	2
the authors propose a particular type of network where all weights are constraint to positive values except the first layers, a monotonically increasing activation function, and where a single output neuron exists (i.e., for binary classification  empirical evidence for more output neurons is presented but not theoretically supported).	0
however, it lacks proper empirical evaluation and makes an impression of a work in progress.	0
the word 'empirical' is also confusing to me in 'b is approximated by empirical data' because b is not an expectation, but an 'integral' which has no 'empirical' opposite of it.	0
i think the empirical contribution of this work is clear to be accepted, but i give weak accept due to the following comments:  i think there’s a similarity between theorem 6 in magail paper and proposition 1 in the submission.	0
i could care a bit less on how good it is, but one can still make an empirical test (e.g. certified defense accuracy on 5x5 patches, but empirical test using reallife sized patches 40x40 or 80x80) and see how the results would be.	0
the idea is not to show new theoretical bounds for generalization gaps but stress the results of an empirical study comparing the already existing measures.	0
the paper is solely empirical but i believe that the empirical section is a bit weak, or at least some important points remain unclear.	0
you have, however, been clearly strongly influenced by the prior work and attempted to fix what you saw as their empirical shortcomings.	0
weaknesses:  the design and experiments are largely empirical without theoretical derivation.	0
i vote weakaccept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.	0
this paper presents an empirical step in the direction of showing that such attacks are possible, but in the context of the other adversarial attacks that are possible, this is not surprising alone and would be much stronger with other contributions.	0
my only concern is the lack of empirical support for the energy saving of the proposal.	0
'review': while i think the method has potential interest to the community, i found the empirical results lacking (particularly in missing competitors).	0
 postrebuttal: my only major concern was the lack of sufficient empirical evidence to support the idea.	0
there are some very specific conditions under which this is known to be true empirically (for example, see the experiments in burda’s iwae paper and hoffman’s dlgm paper), but in general, one should be careful with this claim.	0
empirical evaluations seems fair as far as i can tell, but i am not familiar with benchmarks for vaes.	0
furthermore, whether the online likelihood is large will depend not only on generalisation, but also on the training convergence, since not empirical risk minimization is used, but sgd with a fixed number of steps.	0
i do like this kind of simple but insightful result with enough empirical observations and discussions.	0
i maintain some concerns around the empirical evaluation in the paper (collating results from multiple sources with different experimental procedures).	0
 post rebuttal edit: following updates to the paper manuscript, which address my concerns around the correctness of the empirical results, i have updated my review score from 1 to 3.	0
the empirical evidence seemed promising in some direction though lacking in others.	0
due to a concern over correctness of some empirical results and issues with the presented derivations i have opted to reject this paper.	0
strengths:  lots of interesting empirical results here!	2
overall, there are several interesting empirical findings in this paper (modulo concerns about baselines indicated above).	0
 'in fact, empirically, networks with finite but large width trained with initially large learning rates often outperform ntk predictions at infinite width.'	0
second limitation is lack of empirical check.	0
update after author response the authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with singleagent exploration).	0
but i think given the reliance of the field on these datasets it makes sense, plus i think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method.	0
review:###update after rebuttal: i found the rebuttal convincing and i liked the fact that concerns regarding empirical justification were addressed.	0
negative aspects of the paper that the authors may address are:  the empirical validation is rather weak at the moment.	0
a pic with an architecture would be really helpful probably more datasets are required to have a more convincing empirical story section 3.4: even though you can't directly optimize for ber, there are ways that can work, instead of just replacing it with ce, for example this https://arxiv.org/pdf/1608.04802.pdf one critique is based on 3.4 i don't understand how can this be extended to multiple axis of intersecting groups e.g. not just mutually exclusive race values, but also gender for example.	0
i think you can even use the 19 point cdf output layer and evaluate the data likelihood under this, but i 'don't' think the 19 point l2 distance to the empirical probability distributions is the correct thing to do.	0
concerns & questions: there's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase.	0
though, i still think the contribution is incremental, since backpropagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to dreamer).	0
overall, i think the paper is interesting but the empirical results are not sufficient to support the main claim of the paper (improving generalization).	0
cons & questions: 1. what is the empirical convergence property of the algorithm?	2
it is slightly surprising however that their empirical results seem to indicate that these theoretical conditions do not seem to be necessary, which should be investigated further (and might help illustrate the dichotomy in some claims and results obtained in the disentangling literature recently).	0
given the complexity of many existing metarl methods this seems fine but could obviously be improved upon either with more empirical work or with some guiding theory.	0
i enjoyed reading this work and found the ideas interesting but the empirical comparisons are confusing and not convincing.	0
some empirical results are provided highlighting the cases where soft qlearning (levine, 2018) fails but thompson sampling and klearning do not.	0
using rotamer recovery accuracy as the benchmark measure the empirical results are close to performance as the rosetta energy model however always slightly worse.	0
concerning the section on empirical analysis, it might be of interest to investigate whether with a proper number of layers a gcn would emulate a geomgcn.	0
reasons to accept:  original and wellmotivated idea  clearly written paper reasons to reject:  problematic empirical evaluation (e.g., lacking recent baselines)  several performance numbers appear to be below random gcn baseline performance  general applicability of the approach (e.g., to nonhomophilous networks) is not clear	0
they show empirical evidence of the lowrank structure in few classical control tasks (mountain car, inverted pendulum, cart pole), and provide an iterative procedure  structure valuebased planning (svp), that is similar to value iteration but is able to exploit the lowrank structure to reduce the computational time.	0
''the authors make the following contributions:'' 1. they show that qlearning trained on multiple tasks with a context variable as an input (an rnn state summarizing previous transitions) is competitive to related work when evaluated on a test task even though no adaptation is performed 2. based on these observations, they introduce a new method for offpolicy rl that does not directly optimize for adaptation but instead uses a fixed adaptation scheme 3. the new method leverages data during metatesting that was collected during metatraining using importance weights for increased sample efficiency ''overall, we believe the contributions are significant and sufficiently empirically justified.''	0
the empirical results suggest that ppo is not sufficient for maintaining a valid trustregion, however the 'codelevel optimizations' that differ between the trpo implementation the ppo implementation are sufficient.	0
my primary concern with the empirical study is the use of only three random seeds.	0
my only real concern stems from the empirical results compared to some of the claims made early in the paper.	0
the authors make sure to put these results into context, but given the clarity of the results in the toy domains i would have expected clearer takeaways from the empirical results as well.	0
what's the wong et al. accuracy on just the first 1000 test exampes overall, i am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above).	0
i main concerns still lie in the novelty and experimental results.	0
while the problem being studied is important and the experimental results seem positive, there are a few concerns.	0
while the extensions are interesting, i find that the paper contributions are incremental and do not offer sufficient experimental results.	0
main concerns: the experimental section should make experiments on standard datasets (i.e., uscd, trancos, mall, pklot, shangai, penguins) using standard evaluation protocols (mae, game).	0
concerns #2: somewhat unsurprising experimental results  this paper shows various experimental results, but some experiments seem to be trivial.	0
however, the contribution of the methodology is limited and some experimental results seem to incremental.	0
weaknesses: 1) the main weakness of the paper is section 4.2's experimental setup.	0
 section 5 has (in contrast to the other sections) a lot of details containing the experimental setup, but it is missing a description of the competitor methods.	0
overall, i personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (ii) is missing from the current manuscript.	0
(iii) experimental section: i’m fairly happy with the imdb experiment, and the tcr to epitope binding is a nice nonstandard application but i find the quality and significance of the results a bit hard to judge.	0
but i also have some minor concerns: [1] the theoretical analysis and the experimental results are both well organized in the paper.	0
weaknesses: 1. the paper contains several confusing and contradicting statements or claims which are not supported by the experimental results: for example: 1.1.	0
 lack of experimental or theoretical depth: the proposed method is presented asis; no theoretical analysis of its behaviour is performed; while this is not necessarily a problem, as there are several empirical experiments, the experimental section is not sufficiently detailed: for example, no limitations of the method are being discussed and the presented results are not stateoftheart accuracies.	0
q2: the title still hasn’t changed on the current draft q4: to be more precise: ‘a novel databased approach for analyzing the stability of the closedloop system is proposed by constructing a lyapunov function parameterized by deep neural network’  this alone is not novel, you would need to specify how your method of doing this is new ‘a practical learning algorithm is designed to search the stability guaranteed controller’  this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the lagrangian approach, if this is novel) ‘ the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent’  this is not a contribution, but an experimental result.	0
ideally, an experimental setup with previously published results (e.g. control suite for diayn, seaquest for discern) would be considered, but i can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper.	0
cons  experimental validation seems highly inadequate due to lack of baselines.	2
therefore i temporarily give this paper a weak reject, but may change the rating with more experimental results provided in the rebuttal.	0
some of the experimental results are also overexaggerated (caption saying that things are superset while the experimental results show something different), and the relation and contextualization with regards to existing literature is lacking ( both against other works with similar aims but different methods, and with works with similar method but a slightly different aim) typos: page 7, 'are are much' page 8,'when kappa=0', no need for uppercase the references should ideally be cleaned and put in a consistent format.	0
due to the issues mentioned above, especially extremely simple experimental setup and lack of clarity, i chose 'weak reject'.	0
paper experimentally validates moniqua and show that it converges faster than the baselines on early optimization stage, but omit comparison of final results.	0
major concerns: 1. superior experimental results are shown only for (i) the constant stepsize schedule and (ii) only in the beginning of the optimization.	0
the experimental results suggest that novograd performs slightly better than sgd with momentum but it has more hyperparameters and we don't know whether the results are due to different computation efforts in hyperparameter tuning.	0
'in table 1, their experimental results show a slight improvement by using their method, but it's not significant. '	0
this paper lacks of research motivation and solid experimental validation.	0
however taking that as a given, they wellmotive the proposed architecture and achieve impressive experimental results.	0
however, the paper is poorly written and the experimental results are not as convincing.	0
(i) i think the experimental evaluation is the weakest part of the paper at the moment which i find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details.	0
i do not want to discount that making this connection and loss derivation may lead to significant results (which is left to be demonstrated experimentally), but i find the current theoretical contribution on its own not sufficient.	0
strengths ' this paper provides a novel postprocessing method that can relieve isotropy condition and shows experimental support that solving isotropy condition on word embedding vectors can improve its performance. '	2
while the paper reflects thorough and substantial work  both in the theoretical framework and in the experimental part, i have serious concerns about its clarity and about the experimental results and also some concerns about comparison with previous work.	0
the idea is interesting, but there are some problems on both the method's and the experimental sides: 1. nao [1] also embeds neural architectures to a continuous space.	0
overall, it lacks of innovation and experimental results are not strong enough.	0
i also have concerns on the experimental datasets.	0
the paper is overall well written but includes some vagueness regarding the proposed method as well as the experimental setup.	0
from an experimental point of view, i find (3) particularly concerning because it just means that in their learning rate for the popnorm experiments are 1/kappa times those for the batchnorm baseline.	0
i would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments.	0
i find the experimental evaluation to be useful enough for considering accepting this paper, but i will not strongly advocate for acceptance (unless the authors can better explain why we need yet another benchmark.)	0
strong/weak points simple but useful extension of the existing supernode idea experiments on a number of datasets and baseline gnn architectures,providing ample experimental evidence of the usefulness of the method.	0
however, the current draft has fundamental experimental flaws in its evaluation/presentation and lacks comparison against relevant cheap channelwise attention mechanisms (such as squeezeandexcitation).	0
although i found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below: 1) the idea of using attention score is simple yet seems effective.	0
as far as organization is concerned, a lot of real estate is spent on background material, and few experimental results are presented to support claims made in the introduction.	0
i think it contains many nice ideas, but the experimental section is somewhat lacking in terms of comparisons or insights which i can gain, and the theory has some missing elements too (which i shall discuss below).	0
in summary, it is good that a theoretical bound can be derived from the paper, but this paper's quality may need more enhancement particularly on its writing and experimental parts.	0
however, i have some concerns about the proposed method and experiments:  1. in general, i think the arguments in the paper are still a bit vague to be demonstrated using only experimental results.	0
quality: my main concerns are related to the experimental section.	0
the idea of discrete codes for fewshot classification is interesting and sufficiently novel, i am likely to increase my score if my concerns are addressed and the experimental section is strengthened.	0
the work then experimentally shows that their method, although does not outperform a nonselfinterpretabile baseline but has better performance and interpretation compared to rival methods.	0
i'm leaning toward rejection partly due to the limited novelty in the approach but mainly due to the experimental evaluation section which requires improvements.	0
the main issues are insufficient experimental comparisons and a lack of theoretical support for the method.	0
3. the paper proposes a general framework, but experimental results are presented for only one specific problem, the electrical impedance tomography.	0
in summary, i think this is interesting work, but a clearer explanation of the relationship between hrl and marl, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.	0
the experimental setup and results are sound but some of the tasks seem contrived to show the improved performance of tocca methods.	0
however no comparison is offered, be it theoretical or experimental.	0
moreover, the authors also do not demonstrate that rotationout actually decreases coadaptation in a meaningful waythey propose a specific definition for coadaptation but do not look at this quantity experimentally.	0
for instance, the authors mention that zerocentered dropouta might be more compatible with batchnorm, but provide no experimental evidence to support this claim.	0
strengths: (1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (one potential improvement could be to try noise distributions other than gaussian) (2) simple objective and consistent improvements.	2
my second major concern is with the experimental set up.	0
shortcomings: 1. while the paper proposes a sensible experimental protocol, certain questions remain: a) are the set of test time perturbations exhaustive and representative of the perturbations in the real world?	0
 summary:  key problem or question: assessing / improving the robustness of object detectors to image corruptions (simulated fog, frost, snow, dragonfire...);  contributions: 1) a benchmark (obtained by adding imagelevel corruptions to pascal, coco, and cityscapes) and an experimental protocol to measure detection robustness , 2) extensive experiments quantifying the severe lack of robustness of multiple stateoftheart models, 3) experiments showing that data augmentation via style transfer (geirhos et al, iclr'19) improves robustness at little cost (at most 2% performance degradation on clean coco images).	0
experimental results itself are fine but not complete.	0
strengths  the paper is wellmotivated and relevant to the ml community  lowlevel speed optimizations are needed but overlooked in the community  reasonable choice of experimental conditions (focus on unbatched cpu evaluation, testing on a selection of 4 different tasks)  proposed techniques are sensible weaknesses (roughly in order of decreasing significance)  gains over tensorflow performance is advertised in the intro, but only mentioned anecdotally in the experiments.	2
i think the paper is of limited novelty but includes interesting experimental results that helps to better understand the potential and limitations of tensor decompositions in deep learning architectures.	0
weaknesses experimental results were not easily comparable to prior work so it is hard to say if claims are wellsupported experimental results questions did authors try other values of lambda	0
the experimental part is the paper's main strength, as the method itself is quite incremental (replacing a vae with an ae).	0
p9: as shown in fig 7(a) > as shown in fig 7(b) p10: rout constrain > route constraint, gravity constrain > gravity constraint in sum, i think the paper is at the borderline but it could be improved and better by having more through experimental validation and more detailed presentation.	0
it works hard to achieve a good experimental result, through many tricks listed in the “additional implementation details” in page 4, and through the data augmentation used in page 5. however, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in oliver et al. (2018).	0
however, the current version of the manuscript has shortcomings regarding (i) lack of clarity in the exposition of the method’s relation to prior work, lowlevel implementation details and experimental setup and (ii) insufficient experimental results to back up some of the authors’ claims.	0
weaknesses:  experimental results are indeed very promising, however, gan implementation details and hyperparameters used for training, such as optimizer and learning rate, do not seem to be mentioned in the text.	0
for the experimental part, the comparison experiments in subsection 4.1(toy examples) and subsection 4.3 (color image superresolution) lack comparisons with the latest methods.	0
i understand the experimental setting is transductive, but even that cannot explain everything.	0
the experimental section contains a questionable selection on the train/test split (which should have been done blindly before looking at the results) and lacks any baseline comparison with previous approaches.	0
in addition to the above, other minor weaknesses of the paper includes typos, poor experimental practices, missing citations and related work, and other wording issues.	0
overall, this is a wellwritten paper on an exciting research topic but lacks sufficient analysis and experimental results to support the significance of the intended contributions.	0
the basic idea seems interesting, but unfortunately in the present form he paper is very difficult to appreciate, as it lacks of important details concerning methodology, experimental results, and comparison with respect to the stateoftheart.	0
it seems that section 3 provides some empirical evidence supporting this strategy, but the description of the method is hidden in the experimental details.	0
major comments:  overall, i find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to vibnet.	0
'lack of experimental results on the role of f_1 and f_2.	0
the proposed method demonstrates good performance, but the manuscript does not provide some experimental results on the source of performance gain.	0
the method seems reasonable, and the paper is wellwritten, but the results are only marginally better than other methods, and there are several weaknesses with the proposed architecture and experimental setup.	0
the experimental task is interesting (i'm okay with synthetic tasks of this form for unusual new architectures like this), but i'm not sure what it tests that isn't tested in the clevr and sortof clevr datasets which rely on similar relational reasoning to solve.	0
however, i have several concerns regarding the theoretical derivation and experimental evaluation.	0
additionally, it includes some of the ideas presented in this work, but in a modelfree setting, including a distributional treatment and multistep rollouts, so it is easy to imagine that some of the experimental gains presented here would be erased when using it as a baseline.	0
strengths:  the paper appears well formulated, and well motivated  the experimental results appear quite strong.	2
those extensions seem a bit incremental and are not well supported by the experimental results in the paper.	0
overall, the technical and conceptual contribution of this paper is insufficient for publication at iclr, even ignoring the concerns about its experimental evaluation.	0
the theoretical novelty is small; the experimental evaluation is lacking and missing crucial baselines, such as normalizing flows.	0
6. the experimental protocol is lacking.	0
cons: i have two primary critiques of the paper: (i) the experimental hypothesis is unclear, (ii) no engagement with the results of mathieu et al. [icml 2019], who also study studentt priors for reconstruction (and disentanglement).	0
the study has several shortcomings in consistently stating the problem and the result, completeness, reproducibility, quantification of results and the experimental methodology lacks a systematic approach to isolate the effects of the different components of the proposed method.	0
however, from the material shown, especially the lack of the experimental description and its systematic shortcomings, i cannot judge if the components of the method can contribute to improved vehicle detection and tracking.	0
the paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing.	0
overall, the work in this paper has the potential to be a contribution to iclr but lacks experimental completeness and clarity.	0
my main concerns are with the experimental evaluation:  i would encourage evaluation on a second environment.	0
the main weaknesses: (1) although the authors provide experimental examples showing images associated with various paths through the architecture, is not clear how interpretations can be associated with these paths.	0
although the authors do discuss an approach by which the number of buckets can be incrementally increased (thereby allowing for variation in the number of explanations generated), the experimental evidence is insufficient.	0
overall, this paper is a lack of insights and looks more like an experimental report.	0
i believe the convergence results could be a significant contribution, but the quality of the paper is hampered by its experimental design.	0
i am not convinced that we can draw valid conclusions from the experimental results for the following reasons:  the experiments are lacking important details.	0
however, i think the paper would need either (1) more thorough experimental results (see comments above, points 2 and 3 of weaknesses) or (2) more justifications for its existence (see comments above, point 1 of weaknesses).	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
the experimental section of the paper is lacking in some aspects:  one of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers.	0
i have only one serious reservation about this paper  and it is an extremely serious concern about the experimental setup, and i would ask that the authors clarify this point for me in a response.	0
the paper compares with a lot of missing value imputation baselines but the experimental setup is actually for extrapolation.	0
last point... but it does not undermine the soundness of the experimental protocol!	0
it's mentioned in the last paragraph of the intro that you do two experimental protocols, but these are not referred to (at least not by the same name) in all of the experiments.	0
the idea might have some merit, but the paper in its current state fails to convey it clearly and is too vague on the experimental settings for the results to be considered in any way.	0
i found the paper hard to follow (presentation needs improvement) and the experimental results are lacking.	0
the experimental results are not very convincing in improving classification over ood examples due to the lack of comparison with stateoftheart related works.	0
3. experimental setup:  one somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the bayesoptimal classifier has nonzero test error in expectation, and moreover the data variance is fullrank in the embedded space.	0
overall, this paper is a very promising step in studying adversarial robustness, but concerns about discussion of prior work, discussion of experimental setup, and conclusions drawn, currently bar me from recommending acceptance.	0
despite the lack of experimental demonstrations, the paper is one page over the suggested 8 page limit.	0
tsd 2016. lecture notes in computer science, vol 9924. springer, cham 2. experimental evaluation 2.1. missing ablations 2.1.1. the paper claims “amharic presents sophisticated languagespecific issues” but does not evaluate how the proposed approach handles those specific challenges.	0
as clarity and experimental evaluation are also major concerns of the other reviewers and it is unclear if and how they will be addressed in a revision i do not recommend accepting the paper.	0
while the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of kl divergence achieving the best performance and taking minimum over q functions outperforming using a mix of maximum and minimum over the q functions, i believe the paper would be greatly improved if the authors can provide a new offline rl method based on the brac that can achieve better performance than current approaches and is less incremental than simply combining vp and kl divergence.	0
the authors qualitatively describe those methods and their shortcomings, but the experimental section does not support those claims due to the lack of comparison.	0
the algorithmic ideas feel a bit too incremental and the experimental evaluation could be strongeri'd recommend trying the method on more complicated environments and including ablation studies.	0
the intuition that more complex heads will yield more diverse models is clear, but it would be great to see experimental evidence that this complexity helps.	0
 as the authors reproduce the experiments, it would have been useful to add the original results in the table whenever it is possible  the paper gives a feeling that it lacks some rigor (missing ref, detected bug, mathematical formalism is light); it makes me a bit skeptic regarding some experimental conclusion.	0
taking in into account the issues with the method (or its presentation) and the experimental weaknesses i recommend reject for now.	0
in the experimental section, the authors compare to defensegan, which was shown to be broken in 'the robust manifold defense: adversarial training using generative models' by jalal et al. the authors mention that they evaluate robustness using an untargeted whitebox pgd attack, but they do not specify which objective their attack is optimizing.	0
finally the experimental results look promising, but it would be great to also compare with other stateoftheart results such as lagrangian method, and conservative policy improvement (such as cpo).	0
maybe the paper studies hrl methods in different experimental settings, not considerered in existing hrl or options based frameworks before, but i would imagine similar conclusions can be drawn if a pure hrl system is studied?	0
the paper does not do enough experimental abltation studies to strengthen the claim, mainly lacking fundamental hrl task setups.	0
i would be willing to overlook 1 or 2 if the authors did a more thorough experimental evaluation which showed the method working well when compared to alternatives, but that is not the case right now.	0
unfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking.	0
while this is definitely a contribution, the unclear presentation and lacking experimental evaluation combine to decrease the value of the paper.	0
however, i am mostly concerned about the experimental setting: there are no comparisons with any other mtl algorithm.	0
while i think this work has the potential to be a significant contribution, i rate this a weak reject because the theoretical motivation and analysis of the experimental results are lacking the depth of evidence i would expect for an iclr paper.	0
you show that this works for a rescaling from 2pi/7 to 2pi/14, but it would be nice if there was experimental confirmation of this property with frequency > 1. '	0
pros of the paper:  the experimental results seem to be strong and encouraging.	0
moreover, in the experimental section, there is a lack of comparison with existing methods such as eql[1,2] and i consider it a major omission.	0
interpretation was emphasized in the paper, but it was not clear to me what induce interpretability in the model and how to interpret experimental results.	0
overall, i am concerned that the experimental part of this paper does not warrant the conclusions of superiority.	0
in summary, the paper lacks solid experimental results to make its conclusion convincing and its model generalizable.	0
the hamartia also makes their experimental findings and takeaways about 'real noise' questionable, as it is not clear they are testing real noise but consequences of properties of convnet embeddings.	0
4. from the experimental results, the proposed model improves gcn and gat, but the improvements on other baseline methods are a little bit subtle.	0
overall, the paper is well written but the contribution is very limited and i find some of the experimental comparisons unfair.	0
the experimental results look reasonable and thorough, however the methods are sold on the idea of better representations for data missing from the training set, whereas the results are focused on anomaly detection.	0
in addition, the experimental results in this paper show that two commercial systems can be easily compromised in a blackbox manner.	0
the description of experimental results is however missing important details required to evaluate the results presented: ' what are the details of the dataset used to evaluate the approach?	0
the most critical part is its lack of experimental validation.	0
overall, i agree with the main point of the paper but vote for the rejection for now, and the paper needs a major revision on its experimental validation and presentation.	0
my main concern with this paper is that there is zero experimental comparison against previous neural program induction approaches.	0
my main source of questions is the experimental results section, which i currently view as somewhat weak and a little confusing  i would be more than happy to increase my score if my concerns are sufficiently addressed.	0
the proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.	0
however, i find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, detailed under: experimental concerns: — evaluating on 200 examples seems very small.	0
in light of the experimental concerns and generally weak motivation/description of the proposed regularizer, i vote for rejecting the current version.	0
the paper addresses an important problem, however i do not find the highlevel motivation behind the proposed approach or the experimental results sufficiently convincing.	0
experimentally, my chief concerns are: 1. when the authors evaluate the proposed secondorder representations, they use networks with additional layers which do not seem to be present in the original baseline.	0
overall, my main reservations are: a) the lack of a conceptual justification for the proposed approach, and b) issues with the experimental evaluation, particularly in the reported baselines and how they seem to contradict prior work.	0
some experimental justification is given, but it is also important to provide some theoretical insight if possible.	0
 of mixed strategy on continuous game other:  the paper alternates between his and it when referring to players overall, i think this work is an outline for a nice paper but would like to see 1) clarification regarding its relationship to existing work 2) experimental baselines in which sga and stable opponent shaping are parameterized to produce nondeterministic strategies 3) cleaner writing	0
the empirical results are promising but in my opinion can be further improved through minor changes to experimental procedure.	0
section 6.3 was also an interesting experimental setup which is well suited to dasgrad but i felt that the baseline comparison was unfair.	0
another downside of this work is that the convergence analysis is done on amsgrad instead of adam which the experimental results are based on, why were experiments not done with altqamsgrad?	0
given the moderate novelty (which is good but not good enough as a standalone reason to accept this paper) and the notstrongenough experimental results, i feel more work is needed for the paper to be readily publishable.	0
however, my main concern is that the experimental results (page 8 table 2) does not support the merits of the proposed approach.	0
in my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.	0
hence, given the pros and cons, i am not confident recommending acceptance, and i think improved experimental results (error bars & wallclock, see above) would make the results more significant.	0
the paper includes an explanation about the dataset and the task but does not include the experimental environment (used machines and the time to conduct the experiment).	0
it is preferable if the authors can present any experimental validations concerning this issue.	0
my main concerns are in the experimental section.	0
i think that technically this idea is new, but there is important related work [1] that (imo)  does essentially the same thing  better experimentally validates the thing they do  was published in last iclr.	0
however, in my opinion there are a few weaknesses in the paper in its current state: (1) the clarity of sections 3.13.2 could be significantly improved as currently the proposed probabilistic model framework is confusing and not welldefined; (2) the experimental results are provided only for embedding spaces of dimensionality 23 which significantly hinders performance of prototypical networks compared to having a much higher dimensional embedding space, so it would help to see comparisons of spe and pn using highdimensional embeddings; (3) the central idea and proposed model seem to be very close to those of [2] which is mentioned in the related work, so, please, list differences with this prior work in the updated version.	0
another point concerning the experimental evaluation.	0
moreover, there is a large body of work in the drug discovery literature that uses sparse experimental data on the interactions of multiple target proteins and multiple ligands to build models that predict the outcome of biological assays for held out protein targets, where this problem is known as drugtarget interaction prediction, but these papers are not referenced in this work (e.g. reviewed in chen et al. molecules 23(9):115, 2018, ezzat et al. 2017, 2018, 2019).	0
resnets ensemble via the feynmankac formalism to improve natural and robust accuracies, arxiv:1811.10745, neurips, 2019 overall, this paper studies an interesting and important certified adversarial defense against labelflipping attack problems with a focus on certification on each test data, but more experimental verification is needed.	0
this is not really an issue if the paper includes extensive experimental validation and indepth analysis, but this is not the case.	0
 the experimental validation is largely lacking, as the authors only perform experiments on imagenet and do not compare against recent stateoftheart bayesian sparsification methods (sbp, vib, l0regularization).	0
in sum, although i believe that the paper proposes a very practical method that is easy to implement and is promising, due to lack of experimental validation against a similar approach, stateoftheart sparsification methods, and results on more datasets, i temporarily provide the rating of weak reject.	0
the experimental results are reasonable but can be strengthened.	0
the experimental evaluations are however a little limited: it's restricted to games, where the visual appearance of objects is almost identical.	0
at the moment, i recommend a weak reject as the main weakness is the experimental evaluation, but i could be open to increasing my score if my concerns are addressed.	0
3) experiments: while the experimental results on cifar10 look impressive, i have some concerns about the way the pgd attacks are run.	0
3. section 3.1 mentions a finetuning procedure in the end but the experimental part does not specify how much of a gain it delivered.	0
from my point of view, the main problem of this paper is that it is too messy and it is very difficult to understand what authors want to show, as i) there is a very important lack of experimental details (e.g., main aspects of models and controllers should be clearly stated) and ii) analysis is to wordy, authors should emphasize the message in each part.	0
i think the highlevel idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.	0
(3) i have some more minor concerns (or questions) about the experimental results.	0
the downside of this paper is in the experimental results and also complexity.	0
however, the experimental results seem very incremental and i’m not convinced there is a genuine signal there.	0
i vote to reject the submission, with my main concerns being with the experimental results.	0
although the proposed approach is interesting, this paper has several weaknesses (i) the method is not sufficiently justified or analyzed; (ii) there are missing links with previous work (notably on domain adversarial training); (iii) experimental setting is rather weak.	0
overall, the experimental setup is impressive, but the improvements in terms of bleu are relatively small, and the technical contributions seem quite thin to me for a ml conference.	0
i also didn't really find the experimental results to be that extensive, but i don't have a real sense of what state of the art is for differential privacy.	0
additionally, in the experimental part, the ablation studies indeed reflect the effectiveness of the proposed algorithm, but it looks like the deep decoder plays a key role in all cases.	0
as such, while the experimental results are reasonable, i find this lack of any technical novelty a negative factor.	0
however, in the current form, this paper is below the bar of acceptance due to some weaknesses: (i) rather strong assumption for the main derivation; (ii) lack of clarity and computational complexity of the proposed algorithms; (iii) weak experimental results and missing hint to current models / applications / concurring models.	0
3.5) experimental analyses so much focuses on to justifying the performance improvements of ddl is independent to the base model selected (e.g. mainly table 3, but also table 4, 5 and 6 include).	0
these may affect the results significantly, but the experimental section does not discuss their sensitivity.	0
pros:  the experimental results are impressive, the proposed block can improve the accuracy of cnns while requires little additional computation cost.	2
the main text of the paper is written well, but the experimental result section seems to be rushed and needs to be polished slightly.	0
comments on the results of the paper: the authors appear to contextualize their work in deep neural networks, but all of their experimental results are on matrix reconstruction or linear models on mnist.	0
conclusion: the proposed method is novel and interesting, but the experimental section has many issues as discussed above.	0
b. missing experimental depth: the experiments shown in the paper are interesting, but only scratch the surface.	0
on the positive side, the authors present very good experimental results on popular networks like resnet, and improve on other method's compression accuracy, with little to no reduction in accuracy.	2
the experimental results demonstrated that the proposed approach outperforms the stateofart methods by large margins in both whitebox and blackbox attacks.	0
on the negative side and besides empirical/experimental evidence, the paper would be much more convinving with some more insight into the model itself and some more qualitative evidence.	0
additionally, you only evaluate models with deterministic decoders and many of the experimental conclusions are highly specific to this setting but not noted. '	0
hence the experimental results seem a bit incremental to me, as far as i can tell.	0
pros:  the authors correctly identify input resolution as one of the aspects of lightweight network design that is often overlooked  they propose a practically viable training scheme that can be used to train & select networks given resource constraints  the paper is well written and includes many insightful experimental findings con: the authors specify the mutual learning from width and resolution as their main contribution.	2
cons: my concerns are related to counterintuitive experimental results and lack of clarity in parts of the presentation.	0
these is investigated in the experimental section but i don’t see a clear conclusion.	0
review:###this paper presents experimental data supporting the claim that the under aggressive hyperparameter tuning different optimizers are essentially ranked by inclusion  if the hyperparameters of method a can simulate any setting of the hyperparameters of method b then under aggressive hyperparameter tuning a will dominate b. one way to achieve this rather trivially is to do a hyperparameter search for b and then set the hyperparameters of a so that a is simulating b. but the point here is that direct and feasible tuning of a with dominate b even in the case where a has more hyperparameters and where hyperparameter optimization of a would seem to be more difficult.	0
however, my two main concerns are (1) inconsistency in baselines which cast some doubt on the improvements offered by the proposed approach, and (2) the fact that both the algorithm and the experimental evaluation seem to be a subset of that in concurrent work (especially uesato et al.).	0
more detailed questions:  what are the standard deviations for the experimental results (as you reported in table 4 but not in other experiments)?	0
the experimental results are interesting but their significance is unclear.	0
although the experimental results were better than convlstm(2015), but not as good as predrnn(2018), especially in terms of the mse metrics.	0
pros:  interesting method for decomposing tensors operations in convolutional architectures  outperforms immediate baseline (convolutional lstm) weaknesses / comments:  weak experimental section the authors mainly compare against convolutional lstm.	2
conclusion: the proposed decomposition method is interesting, but the experimental section fails to convince me as to whether the methods performance validates the complicated formulations.	0
the experimental results are limited but show both visually and quantitatively superior results.	0
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough for the following reasons: (a) weak datasets: authors have chosen some standard benchmarks but they do not seem to be convincing as the datasets are too easy.	2
3. the limited experimental results cannot resolve my concerns.	0
the rumour classification dataset is relatively small, but even more importantly, the experimental results on that dataset are not thoroughly analysed, for instance through an ablation test.	0
second, and the main problem with the paper, is that the properties of the alpha parameter are intriguing but the experimental evaluation is underwhelming.	0
however, i have some concerns regarding the experimental evaluation.	0
although the experimental results look promising, i have many concerns with respect to this paper as follows.	0
however, the experimental results are lacking, and do not support much the proposed method.	0
however, i found the writing / notation imprecise at times and the experimental section too small (lacking an extensive set of baselines, and only on two datasets).	0
in the paper's defense, it seems that they were following the experimental setup of wang et al. (2019), but the paper should elaborate more on the choice of evaluation datasets.	0
not only authors do not provide any evidence but also both the experimental results of the paper itself (results in table 2 and fig 4 are disproving this assumption) and the existing literature disprove it.	0
the motivation and theoretical sections seems sound and the experimental results are encouraging, however maybe not completely supporting the claim of new ‘state of the art’ on uncertainty prediction.	0
however i do not find the experimental section completely comprehensive why i currently think the paper is borderline acceptance.	0
cons: the major experimental results of the paper are in table 4 of appendix d. this is not a good effort to save space by moving the most important results into appendix.	0
i was expecting to see imagenet results in the experimental section but they are not there (or maybe some correlation between the gap and performance  robustness).	0
pros   nice, novel method  good experimental results cons   paper is poorly written  very few details of the scaling factor variations  only one dataset considered although the paper is written badly and the narrative is muddled, the underlying idea is a nice one, which is executed well experimentally.	2
overall, i am recommending this paper for a weak accept as i have some concerns over the experimental results that i would like clarified.	0
______________________________________________ after author responses and closer examination of the paper i have some additional concerns about experimental details.	0
the results on selfsupervised learning task (including the layerwise pruning results) and a subset of training dataset are reasonable and kind of expected, but it is still good that this paper provide solid experimental results to verify this.	0
authors acknowledge the fact that their experimental setup is rather limited in appendix c.1, which i agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the adwords problem, however they leave this as a future work, which i find unfortunate.	0
the experimental evaluations are performed with distractors that randomly change positions / directions, which fits the requirements of the method but not necessarily the behavior of distractors in real environments.	0
## suggestions to improve the paper (for 1) add an experiment where the distractor has more natural dynamics so that there is mi between the distractor positions in consecutive frames (e.g. ball bouncing in the image frame in the background instead of randomly jumping to new positions) (for 2) add an experiment with a rewardprediction only baseline, i.e. only actionconditioned reward prediction so that taskirrelevant parts are ignored by default (i.e. also no reconstruction objective, but also no mi objective) > show how planet performance compares to the so far reported numbers when using this representation for planning (for 4) add details about the architecture, hyperparameters and training schedule, for both the method and all comparisons to the appendix (for 5) add references to related works that use cpcstyle mi objectives for representation learning in the context of rl/skill learning:  [1] nachum et al., iclr 2019  applies cpcstyle objective to hierarchical rl setting  [2] anand et al., neurips 2019  investigates mi objectives for representation learning on a wide range of atari games (don't apply to rl)  [3] gregor et al., neurips 2019  while the main proposed model is generative they compare to a contrastive version that uses cpc to learn predictive representations (don't use it for rl)  [4] guo et al., arxiv 2018  similar investigation to [3] of cpcstyle objective for representation learning in rl environments (don't use it for rl)  it should also be mentioned that the original cpc paper already showed that adding cpcstyle auxiliary loss to rl improves performance (even though they did not compare to other modelbased methods)  add qualitative rollouts for predictions from the planet predictive network both with and without distractor to the appendix ## minor edit suggestions  'learning latent dynamics from pixels', hafner et al. is cited twice in the reference section  it might help to add the reward prediction module to fig 3 or mention in the caption that it is omitted, it is only described later in the text and was confusing for me on first sight [novelty]: okay [technical novelty]: minor [experimental design]: okay [potential impact]: high ####################### [overall recommendation]: weakreject  i am inclined to accept this paper but am not fully convinced that the random distractors provide a good intuition about how the proposed method would behave with more natural distractors.	0
the experimental section lack a validation/ablation study to help the reader understand the interplay between the number of blocks and the number of latent dimensions.	0
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough to show the effectiveness of the proposed metrics due to the following reasons: (a) the first proposed extension (sce) is too incremental.	2
however it needs to be written in a more effective way and be supported by a more rigorous experimental setting.	0
the forward kl placed on the latent space of a vae only encourages it to resemble a particular distribution (typically isotropic gaussian) but the information content passed through the bottleneck can indeed grow with the size of the latent space, as one can see experimentally by ablating the latent dimensionality.	0
for all my experimental analysis questions, the authors promised some analyses for future versions, but i was hoping to see at least a minor preliminary analysis at this point, to see if indeed my concerns are valid or not.	0
the experimental results look great, but i believe the paper is missing some ablation studies to assess the importance of certain components (see details below).	0
the authors mention in the experimental section what values they use, but there is no indication on how one would choose this value.	0
however, my main concern of the paper is the experimental part.	0
anyway, the experimental evaluation shows good results on all three datasets for your models, but its hard to be sure since you only have one comparison, an old lda, and nothing recent.	0
so promising work, but related work and experimental work need to be improved.	0
also the experimental gains seem incremental which makes me worried about such generalization.	0
the paper seems novel and sensible and has some experimental results that are not trivial but the writing is so difficult to follow that it makes it impossible for me to assess the contributions and even check correctness.	0
some examples of the most serious clarity issues are as follows: (i) section 6.1.1 lists 'sentiment analysis' as one of the tasks, but figure 2 has no entry for 'sentiment analysis', yet instead features experimental results for 'semantic classifier' which was never mentioned or introduced before; and (ii) figure 2 shows '1st layer lstm', '2nd layer lstm', and '3rd layer lstm', while (based on my reading) what the paper means are '1layer lstm', '2layer lstm', and '3layer lstm', hence highly confusing for the reader.	0
the paper is interesting and the experimental results are positive, but in my opinion the exposition could use some substantial work.	0
in section 3.1.1, the claim that 'we can hypothesize to approximately encode the grammatical role of the token and its lexical semantics' is pretty tenuous, especially given the apparent symmetry between learned roles and fillers in the encoder and given the lack of experimental investigation of the meaning of the learned encoder roles and fillers.	0
the idea seems original and well executed but i think the experimental section could be improved.	0
for one, the presented experimental results look interesting but the suggested method for improvement through adding noise to the latents (during training) is too much of handwaving for such a fundamental problem.	0
moreover it’s not clear to what extent the experimental results should ameliorate these concerns.	0
this maps to a fairly specific, but realistic enough set of realworld problems; the authors give an image recognition as well as an advertising prediction related problem as experimental examples.	0
i'd rate the novelty as medium (smart combination of existing methods), but the exemplary experimental evaluation elevates it to more than a systems paper.	0
=== recommendation === the experimental setup is rigorous but the current draft lacks some metrics that should be reported (as training times, parameter counts, memory requirements at training/inference) since the focus is on making cnns more efficient.	0
the presented experimental results are satisfactory but the studied networks are not quite sota: they are much more competitive alternatives to resnet and mobilenetv2.	0
in addition to lack of novelty, the experimental methodology is very weak.	0
the overall approach is interesting and the experimental results look promising, but it is quite difficult to accept the paper in its current state due to the following reasons: (a) though the paper is comprehensible, it is littered with spelling mistakes (just take the first sentence: problem > problems).	0
due to some experimental details (particularly concerning the environments), the empirical results may be invalid and it is difficult to assess them.	0
experimental concerns  is there a good reason to not try to compare on publicly available datasets like those used in the pointcnn paper (focusing only on the nontemporal versions of the model)?	0
 experimentally, few tasks are proposed comparing skewfit with other baselines like her and autogoal gan  but the differences in all the results seem negligible (example : figure 5).	0
2. the experimental section is entirely composed of nonstandard datasets, and  in general  the manuscript lacks almost entirely in critical details regarding how the datasets are generated; e.g. what is the pilot policy?	0
in that case, it may be nice to also include an experiment on a smaller dataset, e.g., mnist, which i believe this has already been conducted but it was in the appendix, to the main body of the paper as well to strengthen the experimental results in the paper.	0
however, the paper in the current state cannot be accepted for the following reasons: (1) the novelty is low, this very type of decomposition is already widely studied (2) the paper is not clear as to what the contributions are, and why they are justified, theoretically or empirically, (3) the review and comparison with the stateoftheart is lacking and (4) the experimental setup is simplistic and not convicing.	0
however the experimental results seem a little thin and i would expect a more thorough study.	0
concerns and questions i am very concerned about the experimental results.	0
a second concern is that i'm confused about the experimental protocol.	0
weaknesses: my major concern with this paper lies in the experimental methodology.	0
however, given the lack of theoretical novelty, the experimental methodology needs to be improved in order to significantly strengthen the results (assuming they continue to hold).	0
this is indeed the case, but it does not seem like here the authors fully overcome this issue: they do have additional weaklysupervised data, but they still strongly rely on linear 3dmm supervision (p6, “pairwise shape loss”, “adversarial loss”), and do not seem to provide experimental evidence that the model will work without it. '	0
the fact that the theoretical results are incorrect does not mean that the algorithm, or the general approach do not have value, but it does highlight the fact that this approach may only be effective for a specific class of problems similar to the experimental domains.	0
strengths of the paper:  the paper presents strong experimental results on resnet, squeezenet vgg and mobilenet architectures and provides the code, which looks sensible.	2
my main concern would be about the inference time but i consider that the experimental results suggest strong evidence that cat performs well on a wide variety of architectures.	0
the experimental results look quite promising, i.e., revealing the vulnerability of the deep neural network against blackbox adversarial attacks.	0
the experimental results show that the proposed method achieves stateoftheart attack efficiency in blackbox setting.	0
1. the experimental setup is lacking significant information, baselines and baseline tuning (see below for more in depth comments).	0
however, i feel that merely experimental results with correct but not significant theoretical contributions does not meet the bar for acceptance.	0
overall, it provides some new angles for nas search space design, but the experimental results are very weak.	0
however, i think that the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the tradeoffs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface.	0
yet, i think that the work in its current form lacks experimental analysis of the proposed conditional probes.	0
i cannot recommend acceptance of the submission in its current form, but i encourage the authors to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.	0
third, there is a lack of discussion of the experimental results to place them in context for readers.	0
the paper claims that it is about continual learning, but it does not give any experimental results on the continual learning benchmarks.	0
my current concerns are mainly on the appropriateness of the experimental evaluation.	0
the research direction is interesting but the earlier publication of the derived algorithm (iqbal and sha, icml 2019) and the issues discussed above with the experimental results lead me to conclude that the contribution is not yet sufficient to warrant publication.	0
____ cons: the primary deficiency in the paper is experimental.	0
my main concern is regarding the related work and experimental validation being incomplete, as they don't mention a very recent and similar work published in icip19 https://ieeexplore.ieee.org/document/8803498: 'optimizing the bit allocation for compression of weights and activations of deep neural networks'.	0
to experimentally support their hypothesis, first they show that functions along a single training trajectory are similar, however trajectories starting from different initializations may significantly differ.	0
the paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. the writing is also quite unclear, to the point that i often didn't understand exactly what argument was being made.	0
the paper provides extensive but unclear experimental results.	0
weaknesses: some information is missing from the description of the experimental setting.	0
the experimental evaluation is quite thorough and shows that the method performs quite well, especially considering it is unsupervised but is compared to supervised representation methods.	0
the experimental results show that the algorithm performs slightly better than the considered baselines in most situations considered, but the important question of the impact of hyperparameter selection for the method (e.g. the clipping at which the weights of 'outlier' parameters are set to zero) and the competing methods (e.g. the clipping in the trimmed mean estimator) is not addressed the authors indicate that the method is robust to some choices and fixes them in the appendix.	0
despite these shortcomings of the experimental evaluation, i still believe that the paper has merit.	0
the experimental evaluation brings some interesting behavior but is somewhat limited.	0
the experimental evaluation presents some expected behavior in the context of the bound, but i miss a real study trying to make use of the proposed framework to do adaptation in practice with comparisons to other strategies.	0
detailed comments: 1) my main concern is that currently there is very little explanation provided for the observed experimental findings.	0
3 lack of measuring forgetting: this is the most important drawback in the experimental setting.	0
overall i am concerned about the experimental setup and some of the reported results and hence intend to keep my score.	0
weaknesses it needs more experimental comparisons between fromp and frcl, like adding comparison results of fromp and frcl for splitcifar.	0
experimental results: the comparison with rhodin et al. on h36m is characterized as “slightly” lower, but i would call 71% vs. 58% a significant difference.	0
experimental concerns:  has the standard miniimagenet split been used?	0
============= final decision ============= while the paper addresses an important problem and reports improvements, there are many concerns with it including the writing, method, and experimental setup.	0
my “weak reject” rating, however, is mainly based on the experimental concerns especially regarding the hyperparameters and the baselines.	0
hence, the experimental results could be more convincing if the paper include more 4. lack of justification for the model architecture: some design choices of the model are not wellmotivated/justified.	0
2) the experimental results are interesting, but i don't find them compelling enough to recommend acceptance based on the results alone.	0
4) the experimental settings are not described well, thus lack of reproducibility 5) it is unclear which auxtasks were applied in fig. 2. further to better understand and analyze the results, it is required to conduct more rigorous ablation studies.	0
i am still in favor of the idea  applying simple, oldschool method into a new problem, and i also agree with r1 and r2 that the paper is currently lack of details and experimental results.	0
however it is not always the best in several metrics (as indicated in experimental result tables).	0
however i stand by my opinion that the experimental results would need to be very strong to warrant an acceptance, since the conceptual contribution is relatively limited.	0
the experimental section is developed but i find the experimental setting not clearly described (e.g., what is the metric?	0
experimental demonstration that the proposed innovation actually remedies the identified deficiencies should be provided, but is not.	0
my biggest concern in the paper is the experimental section that could be improved in several ways:  the paper misses broader perfoemance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper.	0
cons  the experimental evaluation is incomplete and does not expose the properties of the method fully.	2
the paper only considers affine autoregressive flows, but there has been a lot of recent work on nonaffine autoregressive flows that are more expressive, for example: neural autoregressive flows, https://arxiv.org/abs/1804.00779 sumofsquares polynomial flow, https://arxiv.org/abs/1905.02325 neural spline flows, https://arxiv.org/abs/1906.04032 such flows could improve the experimental results of the paper.	0
specifically, the bulk of the paper is concerned with describing experimental results of a thorough (but standard) hyperparameter search  considering things like transformer context size, learning rate (schedule), number of layers and key, value, query dimensionality; and does not offer any new architectural modifications / insights.	0
taken together, the existing experimental setup potentially creates an unfair advantage for the neural networkbased methods  while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neuralnetwork based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of wikipedia).	0
given that there are no equations to support the reader and that the original equations are not adequate i find it hard to understand sections 2 and 3. the key experimental result in table 1 is only briefly commented on despite featuring multiple models with different strength and weaknesses, multiple types of inference.	0
overall, i think this could be an interesting paper, but more work is needed to prove the effectiveness of the method, and to analyze experimentally in more details some of the claims from this paper.	0
2. the experimental results concerning network training are very hard to interpret, as they lack error bars, confidence intervals, and many critical experimental details (e.g. how many networks were trained, the hyper parameters for training, etc.) for these results to be interpretable, the authors should include a detailed description of the environment under which the experiments were performed, and present confidence intervals which demonstrate the significance of the improvement attained by sapl.	0
concerns that should be addressed: 1. no theoretical and experimental comparison with the baselines (see above).	0
weaknesses:  methodological novelty is low  this is a straightforward application of gcn  the objective of qa is a bit suspect for the sole reason that the training and testing is performed using experimentally resolved protein structures.	0
strengths:  the experimental results seem good.	2
i do not think that requiring this information is egregious, but currently the experimental comparison is not clear in this regard.	0
 edit after author response: i appreciate the authors' efforts in providing extensive responses to all of the reviewers' concerns as well as a significant general response detailing what seems to be a large amount of additional experimental work.	0
summary: i think the paper is looking into an interesting problem and is going in the right direction, but the experimental section is at this point no good enough to suggest an acceptance.	0
because of these weak points in terms of experimental results, i lean to reject the paper.	0
6. overall i like the experimental setup, but the tracking experiment is kind of distracting.	0
all in all, although the paper's experimental and theoretical results are useful and interesting as the use case of 'a valuation method', but is technically incorrect as calling it an approximation for data shapley values makes it not publishable.	0
 oh.et.al (https://arxiv.org/abs/1711.01768) proposed a blackbox reverseengineering method and provided experimental settings as well.	0
i have two main concerns with the work: (1) it is most closely related to mtl factorization methods, but does not discuss this literature, or provide these experimental comparisons; (2) the interpretation of why mint works is not clear: it is not clear that the universality is what makes it work, and there are no experimental analyses of what mint learns.	0
right now, experimental results lack any comparisons to other methods or any other way to assess the effectiveness of estimating physical derivatives.	0
review:###overall the method proposed in this paper is simple but effective, and adequate experimental results are given to show its performance improvements.	0
############## after reading the author's feedback and the comments from other reviewers, i keep the current rating but tend to a borderline score and it is ok if it must be rejected because of the concerns of limited applicability and the experimental.	0
backwards transfer might be included into the experimental section, but i could not find a statement that addresses this explicitly.	0
the results are impressive compared to the deep learning baseline, but i think further experimental validation should exist for properly comparing to prior work.	0
## minor edit suggestions  fig 2 seems to define the blue square as the target, the text next to it describes the blue square as the agent, please make coherent  for fig 7: the numbers contained in the figure are not explained in the caption, especially the numbers below the images are cryptic, please explain or omit [novelty]: minor [technical novelty]: minor [experimental design]: okay [potential impact]: minor ################ [overall recommendation]: weakreject  the exposition of the problem and treatment of related work are not sufficient, the actual novelty of the proposed paper is low and the lack of comparison to strong baselines push this paper below the bar for acceptance.	0
i have several comments/concerns regarding the experimental results.	0
i'm still inclined to reject the paper on the grounds of experimental comparisons and the open question of whether but recognize that my concerns are ones which may not be shared by all communities (and that this is not my community).	0
my main concerns with the paper are: 1. the key idea of using uncertainty to guide sampling was also the main concept in zhang et al. 2019. this submitted paper highlights differences in the models but does not provide an experimental comparison.	0
this appears to me as more of an engineeering choice, and experimentally one that perhaps give good results  but the lack of justifications of why equation 10 is even the right objective to solve makes the paper rather less appealing.	0
experimental results are compared to few baselines  but it is not clear whether these are even the right baselines.	0
2. the proposed method is sensible 3, the experimental evaluation shows great practical gain cons: 1. the method itself is incremental.	0
it is experimentally shown that the overparameterization helps to obtain better optimization, but too much overparameterization gives performance deterioration.	0
the experimental results are certainly interesting, but it would be important to provide a more detailed analysis of the model to get insights into the causes for these improvements.	0
regarding the experimental results: all models are trained on the same grid of embedding dimensions, but the proposed method is the only deep model.	0
the interesting experimental results in figure6 seem to support the authors’ claim, but precise technical justification is needed.	0
the experimental results are not entirely convicing due to the lack of certain baselines.	0
i think connecting the current learning rule to the activity of dopamine neurons requires quantitative comparisons with experimental data, otherwise although i agree that the method is biologically inspired, but whether it is biologically plausible is not clear.	0
this analysis coincides with the experimental behavior applied to neural networks, where one observes when training underparameterized models that sgd significantly outperforms svrg initially, but svrg is able to attain a lower loss value asymptotically.	0
significant work on writing and experimental side should be complete, but because this is novel and important work for classification, with some serious revisions, i would suggest accepting this paper.	0
finally, i have strong concerns about the experimental evaluation.	0
the experimental setup is ok if the method is restricted to robotic tasks, but too thin for the general setting of efficient planning with sparse costs.	0
the paper is easy to understand, and covers an interesting topic, so while i don't think it meets the bar of iclr (due to lack of convincing and nontrivial contributions) i think it could perhaps be made into a workshop submission with some of the following changes: ' a wider set of experimental settings, and more replication such that any differences become statistically significant.	0
i believe the experimental results could justify an accept, but i would not claim i am an expert in semisupervised learning on graphs.	0
my concerns are centered around the experimental evaluation.	0
the paper achieves good success rate based on its experimental results but doesn't convince me that 2) is also guaranteed.	0
they are rather recurrent networks with shared parameters across iterations, but you should also compare against them both conceptually and experimentally: 'learning to optimize.'	0
the paper presents interesting, valuable experimental findings, but it is not extremely exciting theoretically.	0
(minor) first/2nd paragraph of 3.1 seems a bit redundant making it hard to follow overall i think the idea to consider metric learning and local adaptation for continual learning is interesting, however the current work is currently lacking in both experimental evidence (appropriate comparisons) and clear motivation/difference to existing work for its particular instantiation of this idea.	0
while the idea of forming such equivalence sets is very interesting, my concern with the paper is both in terms of impact and experimental methodology.	0
i’m not expert in sentence embedding literature, so a bit tricky to evaluate, but baselines seem strong and experimental results on semantic textual similarity task.	0
summary: a good idea, but not well described, with strong assumptions not discussed, and with lowquality experimental results.	0
authors should make clear there are fundamental differents between both frameworks 3)overall, the experimental validation section is weak and an extensive description of network architectures is lacking.	0
3. the paper lacks an experimental section.	0
overall a strong contribution with supporting experimental results, but the certain parts need further explanation or rewriting for higher rating.	0
strengths  the paper clearly mentions all the experimental details  the paper has a nice set of qualitative examples that probe what the proposed model is learning weaknesses ' it is not clear what problem the paper is trying to solve.	2
while the idea is well motivated, this paper should be rejected, primarily due to a lack of experimental results.	0
the idea of decomposing a probabilistic model hierarchically is potentially interesting, but this paper has drawbacks in terms of experimental quality, significance, and presentation.	0
it might be sufficient if the experimental results would be strong, but given that they are not, becomes somewhat concerning.	0
although the algorithm is novel, there are several weaknesses in both algorithm design and experimental evaluation.	0
experimental comparisons: 1. from the tables, the authors show improved l0 perturbation but with much worse l2 perturbation, which was the focus in both boundary and opt attack.	0
the submission:  briefly describes the neural semantic encoder (nse) model (munkhdalai and yu, 2017)  claims that the dotproduct attention mechanism in nse is too simplistic for the task of summarization because dotproduct attention does not capture wordsentence and sentencesentence correlations, the latter two being important to summarize a document, and suggests utilizing the additive attention mechanism along with the pointergenerator mechanism instead  introduces the “hierarchical nse” model and a selfcritical sequence training scheme to optimize for rouge  provides empirical results for these models and for other relevant baseline models on the cnn/daily mail dataset at a highlevel, this submission is unpolished, imprecise, lacks technical (both experimental and theoretical) rigor in its description and explanation of its methods, and generally does not have a significant contribution, which is why i believe it should be rejected.	0
overall, the paper represents a large amount of experimental work, but deserves a more in depth discussion of the usefulness of the different components.	0
the use of nnsc seems promising but there are various concerns about the experimental details.	0
the experimental section shows a qualitative and quantitative analysis of the method, but not comparison to any compression method proposed in the literature at all.	0
in the experimental section, the authors explore the proposal but they do not provide a single comparison to the numerous compression methods that exist in the literature.	0
this is a fine goal — using a computational model of the brain as a means to better understand its latent processes when analyzing experimental data — but it is worth bearing in mind that actual models of the brain do not have much in common with typical deep learning architectures, even recurrent networks.	0
experimental analysis are quite limited, and the paper is lacking a detailed analysis; including ablative studies, a study of individual design choices, or qualitative examples/analyses of the learned behavior, to name a few. '	0
the idea is simple, but the experimental results show that for the tasks (e.g. v2) that require collaborations from multiple agents, the proposed approach outperforms other baselines including nfsp on both firefighting and search & rescue domains.	0
i am not aware of this approach from previous works, but seems like an interesting idea for action selection and learing a policy conditioned on future visitation of states (eq : 7)  from the proposed set of experimental results, it is not clear whether the proposed method is indeed useful.	0
4. in the experimental session, you mention there are n_s = 35 inform slots in total, but in the previous section, the paper mentioned n_s = n_s(inform)  n_s(request).	0
there are some concerns on the paper: 1) the experimental results are valuable, but more analyses and discussions are needed to draw more convincing conclusions.	0
weaknesses: (1) experiments: my biggest concern of this paper lies in its experimental design.	0
the main weaknesses are (1) the novelty of the proposed methods is low (2) results are hard to compare with previous work and some experimental details are omitted.	0
(2) experimental results do not report any previous published baselines, but just baselines proposed by the authors.	0
another problem is the lack of several sota genderdebiasing baselines in the experimental evaluation.	0
however, the method proposed is incremental, missing extensive technical or intuitive exploration of its effect and has limited experimental evaluation.	0
the paper is wellwritten and organized, i have several concerns: ' progressive knowledge transfer problem: the progressive knowledge transfer process as shown in the left panel of fig.2 is one of the main contributions of this work, while i cannot find any experimental analysis on its importance in comparison with jointly training the losses from all the layers. '	0
======================================================== decision: although the idea is interesting, it is highly experimental but i found that it is not straightforward to use it for general metrics although the authors said that it is trivial to generalize this for many metrics.	0
2. experimental evaluations: a) lack of use of standard metrics.	0
this being said, i can imagine that the proposed method can work with rgb images alone (while such a baseline may not), but this should be experimentally demonstrated.	0
this setting / motivation is a very relevant one, and the method seems like a sensible start, but neither the proposed approach nor the experimental treatment are substantial or novel enough to warrant acceptance; however hopefully something on this topic and approach with a more thorough empirical and/or theoretical treatment will appear later on from these authors!	0
the paper claims the experimental setup is identical to the hvae paper, so it is concerning that the results do not match, especially since the results in the hvae paper appear to outperform the proposed method.	0
strengths: 1. the paper tackles an important problem in a good and realistic experimental setup.	2
the analysis is essentially experimental but seems to display that the technique does fulfil its promises.	0
pros: they have experimentally shown some improvements in the different mltc metrics with (bertsgm) and mixed methods, compared to the vanilla bert, specially, for data sets with hierarchically structured classes.	2
cons: their experimental improvement for their mixed method, is not as significant for public data sets, less than 1% on average.	0
overall, i have concerns about this paper on (a) methodological and (b) experimental grounds.	0
overall i really like the direction of constructing equivalence sets and thinking about model behavior counterfactually but i feel this particular work has significant mythological and experimental flaws.	0
detailed arguments: [major concerns] ' the experimental results currently do not convey the value of the method. '	0
improvements: in order to improve the paper the authors should be able to at least experimentally validate (in a significant manner, e.g. more seeds or few seeds but highly performant when compared to the baseline) that the proposed approach has some benefits.	0
the experimental results are ok, but i have to admit that i am not fully convinced that the novel regularizer makes a huge difference.	0
however, the paper suffers from lack of clarity and in presentation, in exposition and in the experimental section.	0
in effect i am currently unable to comment in detail on that contribution, its novelty etc. to me, this is the main area where the paper needs improvement, but it is so critical that unfortunately it significantly impacts my scores experimental results seem quite interesting.	0
especially the experimental section is hard to follow as i found the figure captions lacking required information.	0
i am largely concerned about the lack of variety in experimental domains.	0
the primary shortcoming of this study is the lack of experimental rigour while testing the main hypothesis in the paper that the sparse convolution layer results in improved accuracies.	0
a few other comments: ' i have a small but potentially important reservation about the experimental protocol  i don't see any mention of a validation set, it's not clear to me how the hyperparameters were actually chosen? '	0
there is a lack of experimental evaluation.	0
in conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.	0
though some work is cited, experimental comparison to competing methods in both of these areas is entirely lacking.	0
however, the experimental results lack comparison against a simple baseline and it is unclear how the proposed smal algorithm would perform against basic multiagent rl techniques.	0
experimental weaknesses: a) there are claims in the paper: 'report stateoftheart results on two datasets for longrange action recognition: charades and breakfast actions' however, there is no comparison to any other work, of which there are many.	0
robustness claims are backed up by empirical validation, however the experimental setup falls short to provide sufficient evidence.	0
cons: 1. the experimental section lacks in depth, and tests only a very restricted scenario of anomaly detection.	0
in its current form the paper is not fit for publication at iclr, but after addressing the weak points and a more thorough experimental evaluation this could become a good paper.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
 i think some of the experimental results make sense, but as i zoom in the pictures, most of them have very strong halo artifacts, and it doesn't seem to meet a very high standard in terms of the qualities.	0
the empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup.	0
i think the general idea of this paper and the questions it poses are very interesting and in fact a rather underexplored area however the experimental results are quite underwhelming.	0
strengths: very clearly written, very straightforward technique, impressive experimental results.	2
the paper while considers an important question about training neural networks with a low rank constraint, i find it sadly lacking in details and is hard to understand the exact experimental setting and the reason for the shown gains.	0
### 3) insufficiently compelling experimental results ''the experimental results do not clearly convey the benefits of the two proposed approaches, due to a combination of insufficient/inappropriate baselines, limited number of benchmark datasets and metrics, inconsistencies/lack of detail in the way results are reported and not accounting for random variability.''	0
unfortunately, the lack of detail in the description of the experimental setup makes it difficult to assess the correctness of this evaluation procedure.	0
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
seventh, the experimental results are interesting, but apart from section 6, i see no real advocacy to tweak dl to be more tabular  compared e.g. to tweaking dts to be more topological.	0
however, in the experimental results, the paper states that the proposed nll loss and the prior work lie algebra formulation are similar, but the 2 degree yaw error is quite a bit larger than the prior work.	0
however, the overall novelty is incremental and experimental results are not solid.	0
decision overall, i think the proposed metalearning formulation seems legit, but i found the formulation of metalearning is hard to parse and the experimental comparison is not persuasive.	0
either of these can ruin the experimental results, but it was not confirmed that any of these malfunctions did not happen.	0
the experimental part in this version of the paper is however not so convincing and many details are lacking so that it is difficult to appreciate the performance of the model.	0
there is no theoretical foundation for the proposed model, but the interest of the components are experimentally assessed by an ablation study.	0
while the method is specifically designed to image classification  which in a way reduces a bit the scope the possible applications but this is a minor remark  i find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets.	0
i may have missed, but i did not really understand in the experimental setup how many additional target instances are used to train the classifier because i guess that the random aspect of the generator can help to generate multiple additional data but in this context the quality of the model can dramatically decrease.	0
however, the reviewer has some concerns about the overall significance and some other concerns on the experimental evaluations, and hence recommends rejection.	0
existing lifelong learning methods do have some limitations in their experimental settings, and this work seeks to address this concern.	0
i don’t think this paper is strong enough for iclr as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written.	0
related to that, another question concerning the experimental evaluation arises.	0
overall a wellorganized work, with a very well written paper and a clear incremental methodology (stated goals, experimental settings, derived conclusions).	0
there are some other concerns about the writing clarity, technical material and inadequate discussions for the experimental results (see questions/ concerns below).	0
i see these kind of statements often appear on papers, i know poore's paper provide experimental evidence of the validness, but i don't see any mathematical proof.	0
incremental accuracy improvements: also, when looking closely to the final accuracies, they are ~3% larger than the best published results which contradicts one of the conclusions 'our experimental results outperform the stateoftheart approaches by a large margin on a variety of benchmarks” (ie.	0
my major concerns are the experimental settings.	0
but, i have serious concerns about the positioning of this paper and the experimental evaluation as follows.	0
while the paper presents an interesting construction on the vae loss, i see a few issues with it which in my opinion should block its acceptance, mostly concerning the experimental results.	0
the experimental part is promising but should include comparison with other similar approaches.	0
in order to be publishable, they would need be justified by significant experimental proof, which is currently lacking.	0
overall, i appreciate the empirical approach of the paper but i think the definitions, arguments and experimental design could be improved significantly.	0
as nonexhaustive examples: 1) in page 3, robustness (as measured by neuron dropouts) is asserted to aid in distinguishing 'redundant functional units' and 'semantically redundant units' from other units because of differences in how robustness of those units change with model size; but this is not backed up and as far as i can tell, not experimentally investigated.	0
the experimental findings are interesting and should be reported; and implications and conjectures should be discussed; but they should be carefully separated.	0
the experimental results seem compelling, but i am not very familiar with the area, so it is difficult for me to judge.	0
it seems like the experimental results are quite good, but i unfortunately can't recommend this paper for acceptance since i found it too difficult to read.	0
suitable experimental results support the architectual decisions and illustrate the benefits of the proposed method, but should be revisited with respect to the given remarks below.	0
(3) the results of experiments demonstrate the effectiveness of noisy ssc and noisydr ssc, but this paper not only lacks the description of experimental data, but also lacks the largescale experimental data.	0
my other major concerns lie in the experimental results.	0
i fully appreciate that in some benchmarks, 5% can be a major improvement, but i am unable to place the experimental results provided in this paper in comparison to other stateoftheart plan recognition methods; only a naive affinitymodelbased baseline (gm) is compared against.	0
due to the issues of clarity as to what is being done, and the unclear experimental significance of the work, i cannot recommend acceptance at this time, but i look forward to hearing the authors' responses in the rebuttal phase.	0
the authors show the results on latest network architectures on imagenet dataset  resnet18, resnet50 and googlenet and the results either compare to other quantization methodologies and in the case of googlenet they surpass the state of the art admm method by 1.7% top1 accuracy the current experimental section does not have any runtime or model size numbers, it would be great if authors could talk about that and maybe add that information to get a more apples to apples comparison on not just accuracy but other factors.	0
as it is, the paper currently is easy to read, seems like a good idea and has good results but i feel it is important to add more detail in both methodology and in experimental results especially have more ablations and understand what is truly happening to the model.	0
however, this paper is not suitable for publication because the experimental comparison of two different pooling schemes lacks details and does not support a significant contribution.	0
i would recommend a weak accept, though i have a few concerns on experimental results, and hope that the authors can clarify them during rebuttal.	0
examples of missing definitions are given by k an n on top of page 3. generelly, the paper is quite self consist thanks to minor changes in existing algorithms to show experimental results for the theoretical insights, but instead of requiring the reader to find other papers it would be better to define all terminology in the appendix (see equation 6).	0
the experimental section mostly covers reinforce without a baseline (except for validation in section 4.2, but no results are shown varying the baseline), as well as mrt in a restricted scenario.	0
one of my major concerns is the unfair experimental comparison.	0
weaknesses: 1. except for the experimental evaluation, what is the advantage of rank statistics over directly comparing feature vectors?	0
overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation.	0
this largely resolved my concerns on the experimental side.	0
the newly added experimental results address my concerns.	0
i am inclined to recommend acceptance due to novelty of order robustness analyses and competitive properties of the method, but i would like clarifications to my experimental questions.	0
the experimental results are very good, however you are using offpolicy replay data and therefore have the issue that using the same data many times can (often) improve performance for any offpolicy algorithm.	0
overall the paper is very interesting, but more convincing experimental results would have to be collected to prove that the method is actually useful.	0
i'd be happy if the authors could kindly elaborate on the following major weaknesses: (i) unclear scientific motivation of using the rightskewness bias; (ii) lack of clarity in the experimental results; (iii) lack of depth and perspective.	0
review: this paper was interesting, because it has nice experimental results and seems like a good idea, but i feel like the paper needs to be improved.	0
weaknesses: (1) my main concern with the paper, which makes me vote for “weak accept” instead of “accept”: the metrics of the paper are compared against previous metrics (eastwood & williams, 2018) on 'only two' types of disentangled representations, namely factorvae and betavae, and in one experimental condition.	0
(there is plenty of other experimental material in the appendix which also introduces aae, but none of it seems to compare against previous 'metrics').	0
 apart from the concerns (highlighted in weaknesses), the experimental results generally support the claims of the paper.	0
however, the paper suffers from lack of experimental details and comparisons (and other weaknesses highlighted above) and therefore i am inclined towards my current rating.	0
most existing label noise works focus on certain noise models, but this paper is more general and proposes an interesting method to learn with noisy labels in a semisupervised manner, and the experimental results are promising.	0
it is certainly hard to discuss all the thousands of experimental observations, but the paper can benefit from some more finegrained analysis.	0
“token identifiability” method m. identifying t from z(t,l): a method m is proposed for this, and experimental results presented showing that, roughly, m succeeds at lower levels l but can fail at higher levels.	0
which is above all of the bert based models reported in table 2. i may be missing something, but if these numbers are actually not comparable then this paper should contain an explanation of how the experimental setup differs.	0
main comments: this paper has a clear motivation and decent experimental results (though some concern on baseline models, see below).	0
review:###summary:  key problem: neural architecture search (nas) to improve both accuracy and runtime efficiency of deep nets for semantic segmentation;  contributions: 1) a novel nas search space leveraging multiresolution branches, efficient operators ('zoomed convolutions'), and parametrized expansion ratios, 2) a decomposition and normalization of the latency objective to avoid a bias towards very fast but weak architectures, 3) a natural extension of the optimization problem to simultaneously search for teacher and student architectures in one pass, 4) a novel stateoftheart efficient architecture (fasterseg) found by the aforementioned nas algorithm, 5) a detailed experimental evaluation on 3 datasets and an ablative analysis quantifying the benefits of the aforementioned contributions.	0
overall, the method looks incremental and experimental results are mixed on small datasets so i vote for rejection.	0
=== post rebuttal update === the authors have addressed many of my initial concerns and provided valuable additional experimental evaluations.	0
review:###this paper proposes a new blackbox adversarial attack method called tremba, in which the search for the “adversary” is done in a reduced space z. summary of its contributions:  a attack method that improves query efficiency of blackbox attack  produces perturbations that are effective across different networks  improves attack success over sota defended networks in general, the paper is very well written, with clear mostly clear exposition and sufficient experimental verification.	0
what follows are the itemized pros and cons (mostly just points that would be good to address): [pros]:  well written  a good overview of previous methods and how the tremba fits within them  sufficient experimental validation [points to address]  in blackbox attack method of related works, did you mean to say, “targeted attack is much harder than untargeted attack for transferbased method.”?	2
but for this paper, i have doubts in the following aspects: 1. in the experimental part, the author compares the asnbased algorithm with qmix and vdn in the starcraft ii environment, but the performance of the benchmark algorithm on the 8m and 2s3z maps is significantly lower than that of the original paper.	0
this especially matters since the theoretical aspect is a key contribution of the paper and the experimental section remains on the light side (it presents mostly examples and lacks more extensive results).	0
 another experimental weakness is lack of comparison with existing nas methods on object detections such as (chen et al 2019, wang et al 2019 and ghiasi et al 2019).	0
the number of papers focusing purely on rl agent visualization is fairly small, but many deep rl papers include visualization as part of their experimental results, and in fact the paper cites some of these directly (like greydanus et al, 2017).	0
although the paper's experimental setup could be done in a more proper way  nested cv; lack of which degrades the conclusiveness of the comparisons, i still think the paper is better to be accepted than not, given that 1) as pointed out by the authors the computational cost of the current experiments was quite high already, 2) the issues of replicability and reproducibility are important and gnns are quite popular for various applications and 3) the results are interesting and important to be considered for future gnn works.	0
the paper is already very long, so there likely isn't any space for fleshing out this section, but it would be nice to have the experimental results included in the main paper, because most of the experiments are left to the appendix.	0
overall, it is a sound idea for lowresource setting with generally positive experimental results, but limited in novelty in terms of the proposed architecture (similar to [1', 2']) and disentangling language and knowledge idea (similar to [3']), lacks comparison with baselines for lowresource setting, and lacks discussion/reference of a few closely related works.	0
however, there is a concern regarding the experimental setup.	0
others concerns:  as a heavy experimental paper, most experimental results are in supplementary material, while the authors spent a lot of time in the main text explaining the conclusions found in other papers.	0
therefore the objective function used in the experimental validation violates assumption 1. now, i can imagine that perhaps in practice this does not matter too much, but surely the evaluation is not correct according to the theoretical claim.	0
the main strengths of this paper are extensive experimental evaluation using both quantitative and qualitative analysis, and significant improvement in performance on reallife data over previous works.	2
however the experimental investigation on real data does not compare to a baseline, which seems like an essential part of investigating the proposed method.	0
overall i like the idea and the theoretical analysis in this paper, but the experimental results could be further improved.	0
2) the paper lacks experimental verifications.	0
weaknesses 1 – limited experimental results.	0
while some of these concerns still remain to some extent, i believe this paper explores an interesting direction connecting human visual models with computer vision, obtaining good experimental results.	0
 previously  rating:  weak reject overall it is a clear and wellstructured paper, with interesting biologically derived architectures (strength #1), but as it stands the experimental results either do not completely support the claims of the paper (weakness #2) or are limited in scope (weakness #1).	0
while this is an interesting analysis to present, showing that most of the generated queries are nonsensical to humans and there is low interannotator agreement, i have an issue with the experimental procedure here: the victim model is finetuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators).	0
i still have some concerns, mainly regarding the fairness of experimental comparison.	0
the experimental results of the proposed method but also the baseline are simply too far from the state of the art.	0
the experimental results look promising but lack extensive evaluations given such a technical contribution.	0
additionally, the experimental evidence is lacking.	0
however in imho, the paper as it stands is premature for publication, the primary reason being the lack of strong experimental evidence that where the strength of this approach is compared to other methods compared.	0
to be specific, the results in table 2 are very close between mtmsn and the bertbased model proposed and it's not clear if the difference is because (1) the model is generally better; (2) this is a subset of the dataset that this model performs better; (3) this is because of the additional supervision signals provided (e.g. the results of fig 2a without the auxsup is almost the same as mtmsn) and if we provided similar auxiliary supervision for other models they would equally do well; (4) due to lack of reporting variance and errorbars across runs we see a small increase which may not be significant; ... again, the paper is very interesting, but i don't think it's clear and thorough to experimentally prove that the overall approach is working better.	0
i note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including iou, f1 score and cd and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. decision: weak reject because the idea is quite interesting, but i believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.	0
weaknesses:  the experimental comparison only included old baselines and the authors should compare to some more recent work such as tagan (nips18), and objectgan (cvpr19).	0
the present experimental section is lacking this depth.	0
overall, i still would like to see a more detailed/indepth experimental setup, but i realise that this not directly possible within the timeframe allotted during the rebuttal period.	0
the additional experimental results are, in my opinion, also a good addition to the manuscript, further clarifying the strengths and weaknesses of notears, cam and grandag under different regimes and modelling assumptions.	2
in my opinion, perhaps the weakest aspect of the paper concerns the experimental analysis, which is somewhat limited in its scope.	0
3. parallel to the previous point, all experimental results in the manuscript so far concern somewhat lowdimensional data and relatively abundant data .	0
i agree with the concerns raised by reviewer 2 regarding the experimental part and whether or not such theoretical study (far from the practical aspect) are of interest to the iclr community.	0
evaluation: the problem of discovering primitives is approached by the authors in a novel and interesting way, however in my opinion the paper should be rejected because: (a) the experimental section is not convincing enough to support the claim that the method captures the shared motions across different skills.	0
the baseline louizos et al. (2017) was designed to optimize group sparsity/speed, but the experimental results here only examine the compression rate.	0
authors’ main motivation is to study small batch size experimental setup which is important in segmentation and detection, but they don’t include main competitor (brn) in these experiments.	0
my major concern is that, with added complexity, the experimental results suggest that vaespp is not significantly better than the existing vaecf on most tasks.	0
the experimental results are strong and i appreciate the plots of the gradient error in a simple task, shown in figure 3. but i want to see the comparison of the final performance of each algorithm in these environments, and i doubt that the baselines do not converge.	0
to be honest, i do not like the way the authors frame their work (e.g. the way the method is motivated in section 2.3 or calling it a 'unified framework'), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.	0
while one may quibble about some details (see section below) the experimental set up is reasonable to illustrate the need and some of the challenges modelers are likely to face in the real world ( evaluation & questions i'm really torn because i really enjoyed the paper very much overall but i have some strong concerns as well.	0
however, it lacks indepth discussion to several key problems, rigorous analysis or complete experimental study to support the main claims, for example: why can the simple method achieve a more compelling tradeoff between accuracy and efficiency/memory costs comparing to a large single model or a naive ensemble of small models?	0
— i have a major experimental concern: when comparing against acet, the baseline of performing adversarial style training on random noise inputs seems more appropriate since it’s closer to the evaluation metric (which picks random noise as out of domain and not 80m tiny images).	0
the authors provide a large experimental section, however the key problem with the paper is that it blurs the evaluation issue.	0
experimentally, the results seem sound, but the variance in the results is suprisingly low (see e.g. figure 7 dqn) — did you change the random seed between runs (both environment seed and the seed for initializing the agent weights)?	0
i am absolutely not suggesting that the authors should providing experimental comparisons between their models and gan or vae, but simply that they compare them to their style of generative modeling in terms of advantages, disadvantages, use cases, and potential for applications.	0
my current recommendation is very borderline (weak accept) because of a lack of some experimental rigour (which i would love clarifications on), and missing related work, which i mention below.	0
the experimental results are nice, however they currently crucially lack quantitative statements that back up the qualitative results (see improvements below).	0
cons: i have two main concerns over the paper’s experimental rigor… #1 lack of simulation studies: the paper makes claims about the representation properties of pcmcnet but fails to validate them with simulation studies.	0
discussion and comparison to very significant related work is missing and experimental measurement of any advantages of the proposed method vs. adversarial training is lacking.	0
in figure 4 (a), as the l_0 norm counts the number of altered pixels i don't understand why the certified accuracy varies e.g. for epsilon 'between' 0 and 1. the experiment on the librispeech model seems interesting, but the paper does not contain sufficient information to understand and assess the experimental setup or the results.	0
the overall structure, the figures, and the experimental results are very nice, but there are two major issues that are holding it back.	0
pros: 1. good experimental results for the taskfree setting, in which no information about task boundaries is given.	2
pros: simple, intuitive method draws from existing literature relating to dropoutlike methods little computational overhead solid experimental justification some theoretical support for the method cons: method is somewhat heuristic mitigates, rather than solves, the issue of oversmoothing limited novelty (straightforward extension of dropout to graphs edges) unclear why dropping edges is 'valid' augmentation followupquestions/areas for improving score: it would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well).	2
despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper.	0
overall: a nice formulation, but weak experimental investigations (no comparisons to sota semisupervised translation) make the significance of the paper unclear.	0
the experimental setup did not allay these concerns as there was no mention for holding outofdistribution tasks/environments aside and the variation in environments is pretty narrow.	0
most importantly, their experimental evaluations lack substantial comparison to such related methods.	0
while the contribution seems a bit incremental and the experimental setting is a bit unclear and limited to lowdimensional state space, the inference of taskspecific subtask graphs based on past experiences and the proposal of a ucbinspired reward shed some interesting insights on how to approach metahierarchical rl where longhorizon tasks and sparse rewards have been major challenges.	0
though this work is well presented and indicates promising results, i think the paper should not yet be accepted due to the following main reasons: (i) the experimental evaluation indicates promising, but not yet convincing results; (ii) the computational complexity of nap seems to be a major limitation of rapp which is not addressed in the text; (iii) the added value/insights from the theoretical section 4 (motivation of rapp) are not clear.	0
(i) i think the experimental section shows promising, but not yet convincing results.	0
this together with lack of some experimental details makes the section very dense and difficult to parse.	0
(2) experiments: i have some concerns regarding the experimental setup.	0
on page 9 it is noted some experimental results are in the appendix but as there is a page and a half of space remaining before the 10 page limit, i would encourage to include all results in the main body of the paper.	0
however, the experimental part of this paper is lacking.	0
my main concern is that i did not find the that i did not find the method and experimental section to be fully comprehensive and further lacking many details which makes it hard to compare the results with prior work.	0
3) the cifar10 experiments in table 1 are encouraging and the toy experiment in 2d is illustrates nicely the deficiencies of the current mi estimators cons 1) the experimental section is lacking many details to fully understand how and what experiments were performed and how comparable they are to prior work 2) the paper would benefit greatly from a thorough editing to clarify the presentation  there are many missing concepts and definitions that makes it hard to follow without intimate knowledge of related literature.	2
i maintain some concerns around the empirical evaluation in the paper (collating results from multiple sources with different experimental procedures).	0
overall, i think the paper proposes an interesting unification and generalization of existing stateoftheart approaches [6, 4], but i think the experimental evaluation needs to be more extensive and clarified to judge the potential significance of the results.	0
cons: the reported experimental results are held only for biggan architecture while not considering different networks to ensure the stability of the proposed regularization.	0
proposed method does not necessarily need to have stateoftheart adaptation results to be accepted, but not reporting what stateoftheart performance is makes the experimental results incomplete.	0
i am still unsatisfied about the experimental contribution, but i guess producing a paper full of theory and good experiments is a tall ask.	0
negatives : the experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : 1. it misses out a recent stateoftheart method (slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way.	0
the experimental results turn out that the whole model is good at predicting features using only location information but does not outperform the rbf kernel (on validation) in terms of using spatial context modeling.	0
my main concern about the work is that the experimental evaluation is limited.	0
thus, ' from a conceptual viewpoint, it would be interesting to provide some applications in which the rlc condition hold, but the relative smoothness condition does not; ' from an experimental viewpoint, in sec.	0
the authors say both lead to improved performance  but don’t evaluate the approaches in the experimental section.	0
however, i have some concerns regarding the experimental results: in figure 1, even though the fifth mass seems to follow the exact pattern of observations for singlestep ll hnet, the l2 error is increasing after each timestep suggesting that the fifth mass might not be the best one to consider for comparison between different algorithms here.	0
the method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.	0
 the author claim in the experimental section that their method perform betters on both datasets, however when looking at table 1, wgangp performs better on cifar10 and when looking at the standard deviation we can see that the improvment is not significative.	0
i do have a few concerns, however, about experimental performance per environment being omitted from both the main paper and the appendix.	0
one of my concerns with this work is that the resnet baseline in the experimental results seems like too much of a straw man for the tasks.	0
the main shortcomings i see are that there are no experimental results comparing this method against any existing results; the authors do compare against their own resnet18 implementation, but this is not ideal.	0
to mention a few experimental weaknesses, in section 6.2 the authors could have performed much more detailed ablation studies and stress in more details the impact of using the vae alone versus using the random pertubation controller alone, they could say more about the goals they show to the system, etc. there is some information in figure 7, but this information is not exploited in a detailed way.	0
3. the experimental results are compelling – while the proposed method loses the accuracy a bit, but shows very good individual fairness under their used metric.	0
the proposed methodology is clean but effective, with extensive experimental support.	0
on emnist a natural candidate is the digit labels (and this turns out to be is indeed the case in the experimental section) but it is not very clear what conditions need to be satisfied.	0
however i do not find the experimental section completely comprehensive and some of the results seem to achieve worse performance than what is reported in the litterature for both the proposed method and baselines (see detailed questions below).	0
concerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models.	0
however the experimental results are always slightly worse than the rosetta energy function and i (strongly) suggest that the authors rephrase those statements to reflect that.	0
3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported.	0
weaknesses:  while the experimental results are convincing on the computation front, i have few concerns on the performance front.	0
my major concern is that the authors shall present more detailed experimental results.	0
in addition, i did not understand why the authors change evaluation metrics in table 3, i.e., from ndcg/recall@10 to ndcg/recall@5. i found ssept without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer.	0
however, the evaluation seems lacking to me  the evaluation convinced me that sap works, but i am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zeroshot test environment.	0
main concerns: the experimental section should make experiments on standard datasets (i.e., uscd, trancos, mall, pklot, shangai, penguins) using standard evaluation protocols (mae, game).	0
the ideas are interesting, however, the paper as it stands is lacking in thorough evaluation.	0
 no human evaluation is not conducted; rouge indicates small differences, but it can't be trusted without confirmation by human evaluation  i don't agree that using positional information is bad for the models.	0
my main original concern was that the empirical evaluation only studies a single type of situation of inferring physical parameters.	0
the main limitation of the paper is that the empirical evaluation only studies a single type of situation of inferring physical parameters (whether balls should pass above, pass below, or bounce off obstacles) but does not consider other situations for inferring physical parameters for prediction in a new domain.	0
this paper has great potential, but the lack of evaluation on a wider variety of physical phenomena makes it difficult to evaluate the generality of the claims made by this paper.	0
however, as i stated in the review, my evaluation of the method proposed in the submission is that it does not concern representation learning ('the employed features in table 3 are handcrafted').	0
i believe this evaluation is defensible, but of course the final evaluation is up to the chairs.	0
# weaknesses i have some issues with the (a) evaluation, and (b) presentation of the work in its current state.	0
i like the idea and the overall direction, but the current paper looks a bit preliminary both in evaluation as well as presentation.	0
but i have several concerns regarding the choice of prototypes and the evaluation of the interpretation: 1) according to eq. (2), it seems that all training samples are used as the prototypes (but with different weights).	0
first, the mahalanobisbased uncertainty evaluation makes sense, but it is not clear what it adds beyond the standard average negative loglikelihood (nll) metric.	0
(i) i think the experimental evaluation is the weakest part of the paper at the moment which i find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details.	0
in contrast, i find anomaly detection to be an important application of this method, but an evaluation solely on mnist that also lacks recent deep competitors [6] is not sufficient to assess the significance of the presented results.	0
the authors prove that (1) sgd with their procedure will find a stationary point under the robbinsmonro conditions for a fixed l and (2) sgd with their procedure will converge for convex problems as l is decreased to 0. decision and reasoning this paper should be rejected because (1) the proposed algorithm attempts to address the original issue of high dimensional nonlinear optimization of neural networks but violates the algorithm's assumption in practice, (2) the empirical evaluations are lacking  having only evaluated their method on a toy problem with up to only 6 dimensions and a relatively simple image classification task, and (3) the assumption of fixing the homotopy parameter in the theorem on the nonconvex case directly violates the intention of the algorithm.	0
however only 4 epochs were run, so i believe the xaxis should be 'iterations'' besides improving the quality of writing in the paper, i would strongly suggest that the authors improve their empirical evaluation.	0
method itself might be good, but evaluation is still bad, and what is worse  authors haven`t even tried to improve it.	0
however it is also quite vague on important aspects such as the exact modelling of transfer and limited regarding evaluation.	0
while it seems to be a legit metric i have some concerns listed below: 1. need a proof why the cnnbased evaluation metric is a metric.	0
i find the experimental evaluation to be useful enough for considering accepting this paper, but i will not strongly advocate for acceptance (unless the authors can better explain why we need yet another benchmark.)	0
however, i'm also concerned that the benchmark might push people to (directly or indirectly) start training on data more representative of vtab evaluation tasks, and the performance gains in the benchmark will not translate to better algorithms.	0
specific comments:  in table 1, the part of 'caption evaluation' the proposed method is in bold, but it seems that 'updown' method outperforms the proposed method in b@1 and b@4.	0
the evaluation on a few computer vision tasks shows the presented method performs as well as the typical attention methods, but it runs much faster.	0
however, the current draft has fundamental experimental flaws in its evaluation/presentation and lacks comparison against relevant cheap channelwise attention mechanisms (such as squeezeandexcitation).	0
lacking details in evaluation.	0
the paper claims that the investigated method is sota, but it's not clear this is the case, even in restricted class of similar episodic memory based models, see [1] for an independent evaluation of many such approaches.	0
i have concerns that the evaluation doesn't focus enough on applicationrelevant scenarios (why exclude cells that are not in the top 50?	0
i did not have time to check all the maths in the appendix (i only went through the derivations in a.1 and a.2), but they seem to make sense overall (though it is unclear to me if the new algorithm proposed in a.5 is a practical one, so i am not taking it into account in this evaluation).	0
i have some concerns with the paper's claimed novelties, its empirical evaluation, and its overall presentation, and thus am initially recommending a weak reject.	0
my main concern is the limited evaluation as all the experiments are performed on the cifar10 and synthetic data.	0
conclusion: in conclusion, the paper seems to present a novel method and evaluation metrics but has many issues as stated above.	0
meanwhile, the empirical evaluation is somewhat lacking.	0
i'm leaning toward rejection partly due to the limited novelty in the approach but mainly due to the experimental evaluation section which requires improvements.	0
there's little by way of human evaluation (there's a bit in the appendix, but it's not really focused on whether or not the model is useful for the claimed purpose of 'explanation').	0
the paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing lowlevel controller) for the multi agent case.	0
this is probably due to the design of experiments which only model pairwise interactions, but then this questions the relevance of the proposed empirical evaluation with respect to the claim.	0
introducing a new objective is meaningful not only in light of noisy labels (always a problem), but in light of how the evaluation is carried out.	0
2. missing comparisons: proposed method is interesting, but i wonder if there were a more standard evaluation to test the efficiency of the method, perhaps something like testing if representations learned using such data augmentations were more robust to adversarial perturbations?	0
while i share the excitement of using crosslingual models to improve monolingual performance, i also feel like this paper lacks novelty and further evaluation to be accepted at iclr, and would be more suited in a more nlpfocused venue.	0
strengths  the paper is wellmotivated and relevant to the ml community  lowlevel speed optimizations are needed but overlooked in the community  reasonable choice of experimental conditions (focus on unbatched cpu evaluation, testing on a selection of 4 different tasks)  proposed techniques are sensible weaknesses (roughly in order of decreasing significance)  gains over tensorflow performance is advertised in the intro, but only mentioned anecdotally in the experiments.	2
the paper indeed does propose a new idea, but without a comprehensive comparison/evaluation, the inferences on its usefulness may not be conclusive without such studies.	0
the paper does not seem to provide source code and part of the data used in this paper also does not seem to be public data like libritts (i would be wrong but i could read that the multispeaker training data are not public), although i appreciate the authors' efforts to provide the implementation and evaluation details as much as possible in the appendix.	0
for example, a variant of policy iteration where policy evaluation is not solved exactly, but instead approximated by applying a small number of bellman iterations.	0
the maddpg paper claims good performance for multiparticle environments, which contradicts figure 4 and 5. for example, maddpg claims about 16 touches per episode with 30% faster prey, but figure 4 has it converging to about 5. this makes me wonder if the parameters (n, l, k) are different in the two evaluations.	0
the thorough evaluation over the curated datasets shows pretty well that the changes in the model prediction are indeed due to a lack of robustness of the models themselves rather than the difference occurring from one frame to the other (occlusion etc).	0
based on the results presented, it seems that the performance saturates by adding more parallel data, but the authors fail to fully understand their evaluation data in the first place.	0
similarly, the additional hyperparameter details in appendix a are helpful to enable reproducability but without explaining how these values were chosen it is impossible to assess the rigor of the empirical evaluation.	0
cons my main concern is that the evaluation of proposed method is quite limited in the sense that the method is evaluated with only single dataset. only basic model is used to compare to proposed model.	2
(3) experiments: my biggest concern about the experiments is that human evaluation should be conducted, given the subjective nature of the task.	0
major comments:  overall, i find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to vibnet.	0
it would be nice to have the simple baseline of a classifier with a sparsity constraint, i.e. ' i.e. ablate the reconstruction loss i’ve given a reject because 1) the explanation of the method is not very precise and could be greatly improved, 2) the quantitative evaluation is not sufficiently convincing, given the lack of technical novelty), and 3) the qualitative evaluation is handwavy.	0
although the authors explain the difficulty of obtaining these numbers, a lack of them does not complete the empirical evaluation.	0
however, i have several concerns regarding the theoretical derivation and experimental evaluation.	0
most importantly, i have major concerns regarding the lack of indepth evaluation.	0
i do have a few concerns / questions though: 1. i am not convinced by the recommendation to use performance during training for evaluation purpose.	0
the paper suggested a trick of limiting how long an agent can go without receiving a reward, but it's unclear (1) if needing this fix is worth the proposed change, and (2) if the fix introduces additional gamespecific nuances in evaluation; e.g., are there any situations where this can be detrimental to properly evaluating performance, or introduce biases based on a game's reward distribution?	0
overall evaluation my main concerns about the paper is that the exact objective of the setting and the representation learning is not that clear and the empirical validation is limited to a qualitative assessment of the representation learning with no downstream task.	0
on section 4.1, the experiment of type conversion and normalization, again this is mathematically clear that the order would change the results, let's call imgbyte=x, then by substituting given values for mean and standard evaluation, equation (b) is simplified to (b) = (x127.5)/((127.5)'(255)) however simplification of (c) results in (x/2550.5)/0.5 = (x0.5'255)/(0.5'255)=(x127.5)/(0.5'255) the dominator of (b) and (c) are not equal, therefore, this is trivial that the results of these two the expression would not be the same.	0
in section 3.1, the authors write 'the hardware details are not present in the manifest, but are userprovided options when performing the evaluation.'	0
i am not necessarily interested in an actual empirical evaluation, but including this in the related work section would likely be interesting for the reader.	0
there only have a quantitative evaluation of the proposed framework with other superresolution algorithms, while lacking the qualitative evaluation of the framework itself.	0
overall, the technical and conceptual contribution of this paper is insufficient for publication at iclr, even ignoring the concerns about its experimental evaluation.	0
the theoretical novelty is small; the experimental evaluation is lacking and missing crucial baselines, such as normalizing flows.	0
the paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing.	0
experimental concerns  ## e1: need to consider runtime in evaluation none of the figures/tables that i can see report elapsed runtimes for the different methods.	0
my main concerns are with the experimental evaluation:  i would encourage evaluation on a second environment.	0
their evaluation setup largely follows the style of liu et al., but they construct a different subset of ilsvrc validation set, and some of the model architectures in their ensemble are different from liu et al. their results show that by including the distilled logits when computing the gradient, the generated adversarial examples can transfer better among different models using both singlemodel and ensemblebased attacks.	0
the evaluation section lacks experiments that evaluate the computational savings.	0
overall assessment: while i enjoyed reading this paper, i am leaning towards rejection due to the shortcomings of the evaluation section.	0
they also use a nn trained with logits features, but this assumes we can peak at the test distribution, so i ignore this entirely in this evaluation.	0
the current paper does improve the evaluation protocol of kottur 2017 by reporting variance over runs, but does not explore the parameter space more systematically, hence i am concerned that it may suffer from similar fragility.	0
the terms 'single head' and 'multi head' evaluation are in common use in continual learning, but are not referred to here at all.	0
overall, the proposed idea is simple and easy to implement, which is the main advantage of the paper, but it is evident that the analysis and evaluation lacks rigour, hence the paper will need to undergo significant revision to be in a publishable state.	0
while the setting of policy evaluation is novel, i am concerned about how many new techniques are to be gained from a theoretical point of view here.	0
i think it lacks a more thorough evaluation and description of the dynamics observed during the genetic evolution, and the performance of the baikal loss on other datasets (my quick experients with it on imagenet diverged i did not have the time necessary to tune the hyperparameters).	0
strenghts:  application on activity translation and evaluation by human raters is interesting  the proposed work is a nice application of an encoderdecoder architecture in case of multivariate time series weaknesses:  there is no methodological novelty.	0
overall, i like the direction the paper is exploring, but i think it would greatly benefit from adding detail on the outlined aspects and extending the evaluation.	0
tsd 2016. lecture notes in computer science, vol 9924. springer, cham 2. experimental evaluation 2.1. missing ablations 2.1.1. the paper claims “amharic presents sophisticated languagespecific issues” but does not evaluate how the proposed approach handles those specific challenges.	0
as clarity and experimental evaluation are also major concerns of the other reviewers and it is unclear if and how they will be addressed in a revision i do not recommend accepting the paper.	0
in its current form, the paper has two main weaknesses:  it is poorly written & organized  it was a fairly weak empirical evaluation in order to address the first issue:  the authors should significantly improve the quality of the prose, which can be confusing & difficult to undersrand  the introduction needs to be significantly crisper: in its current form, it is far too general and does not describe the rest of the paper; the authors should explain ... 1) what is the problem they are working on (currently present, but far too long) 2) what is the proposed approach & why is it novel (missing) 3) what are the main results & their significance (also missing) in order to address the second issue:  3.1 needs more details; it is this reviewer's understanding that the current corpus consists of 1065 documents (which is extremely small in size); how many sentences are there in these documents?	0
however their method does see a drop in recall (automatic metric and human evaluation).	0
the human evaluation shows that the proposed method do better on story generation, but this one is essentially text to text.	0
3. the approach is simple and easytoimplement, but has been shown to be effective across a broad range of problems, multiple modalities, and various evaluation metric.	0
however, that brings me to a big concern i have with the evaluation protocol.	0
minor suggestions: in first paragraph of page 5 where the authors divide the existing literature into the three particular cases, i am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convexconcave etc.) i understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper.	0
the algorithmic ideas feel a bit too incremental and the experimental evaluation could be strongeri'd recommend trying the method on more complicated environments and including ablation studies.	0
on gnnfilm's training stability and regularization: i am interested in the negative result described in the following sentence: 'preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by shchur et al. (2018))' do the authors have any intuition about why the results are highly dependent on the random seed?	0
score: the weaknesses of the paper are novelty, clarity, and evaluation.	0
my main concern of this paper is about evaluation.	0
for the other settings where metad does not leverage any knowledge about the evaluation preference, it lacks stronger baselines such as bo.	0
the paper fails to provide any theoretical insights but a thorough empirical evaluation.	0
i would be willing to overlook 1 or 2 if the authors did a more thorough experimental evaluation which showed the method working well when compared to alternatives, but that is not the case right now.	0
on the other hand, if the paper is about showing that a bigger video model generates better video, the paper is lacking a more in depth evaluation.	0
unfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking.	0
while this is definitely a contribution, the unclear presentation and lacking experimental evaluation combine to decrease the value of the paper.	0
on the other hand, there are shortcomings of the paper, one being the model evaluation on dataset such as mnist, svhn, and emnist.	0
 the paper presents a, to my knowledge, novel approach, to avoid the leakage of metadata in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)  the paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below) weaknesses: 1. experimental evaluation: 1.1. the paper only evaluates on the training set.	0
cast performs better across several metrics (including human evaluation), but i am not sure that the gain is from using the context in the manner that the authors suggest (increasing the generated sentence's coherence with the context).	0
it appears that the authors used the default hyper parameters for other style transfer models (hybrid seq2seq and controlgen), but wouldn't it be better to choose hyper parameters for those models based on the evaluation criteria (as is done for cast)?	0
i understand it helps to not have unnecessary modules in the module set, but it seems to be another weakness of the approach, and there was no evaluation against using the same full set for all tasks.	0
 the evaluation of the idea is not complete while it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other blackbox attack methods in the limited query budget regime, i would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.	0
overall, my main reservations are: a) the lack of a conceptual justification for the proposed approach, and b) issues with the experimental evaluation, particularly in the reported baselines and how they seem to contradict prior work.	0
the node classification is a very standard task in the performance evaluation of network embedding, but you put the results into the appendix.	0
however, the evaluation is lacking w.r.t.	0
natural robustness, more detailed evaluation on the graybox, blackbox and whitebox attacks.	0
they claim that the white box attacks are semantically meaningful and so could fool a human, but no human evaluation is presented and the examples which are illustrated do not demonstrate this property.	0
i agree that the number of function evaluations you use (coming from the oneshot model) is larger and computationally less expensive to obtain, however this does not guarantee that these are a good surrogate of the true objective that nas aims to minimize, i.e. the validation/test accuracy of final (standalone) architectures .	0
the empirical evaluations of their algorithm seem to outperform/be competitive compared to other nas methods on all benchmarks used in the paper, however only darts is evaluated on their search space and the other results are taken from the corresponding papers.	0
although the contribution of introducing a new relaxed formulation of the normalized cut is interesting, i have the following concerns regarding with the clarity, significance, and evaluation of the proposed method.	0
the parts that a bit lacking with the current version of the paper in this are the evaluation tasks are few and a bit simple and i think there needs to be more discussion on the 'coverage' of the intrinsic reward types.	0
another point concerning the experimental evaluation.	0
my biggest concern is the evaluation of adversarial robustness.	0
moreover, the evaluation is completely limited to a single data set (which is good as a strawman but not sufficient to make significant claims).	0
clarifications re: baselines and empirical evaluations:  no comparison to ibrahim, et al., 2019  you can cluster in unsupervised setting but you can also aggregate within class in supervised setting.	0
cons:  the work is more alike a course project than a novel scientific contribution, with all the metrics mainly inherited from the evaluation of clustering and other existing literatures.	0
i have as few specific concerns/questions about the paper that i would like the authors to address: ' they consider js divergence as a metric for evaluation.	0
# decision the paper proposes an interesting approach but a more thorough evaluation is needed before it can be recommended for publication.	0
hendrycks et al. does assume access to ood data, but not ood data seen during evaluation; in this way, they do not assume data is 'known ahead of time,' which they reiterate throughout their paper.	0
paper, in its current form, lacks quantitative evaluation, comparison to other methods (heavily cited in the paper), and any well established applications that can be quantified.	0
at the moment, i recommend a weak reject as the main weakness is the experimental evaluation, but i could be open to increasing my score if my concerns are addressed.	0
first, the paper only studies the accuracy impact of biohash but completely ignores the evaluation of search time and memory, which are crucial dimensions for evaluating similarity search algorithms.	0
below i discuss some of the concerns in detail: without multistep evaluation, it is hard to gauge the extent to which selfplay for internal consistency help in generalization of the roles.	0
2. the paper contains some simple experiments, but i do not believe they are an adequate enough evaluation of the proposed approaches.	0
i understand bleu is problematic for dialog evaluation as it treats references (gold responses) as the only reasonable responses (liu et al., 2016), but that makes perplexity as an evaluation metric problematic for exactly the same reason.	0
the main weaknesses of this paper are in the evaluation (see below), and while i understand that evaluating generate models for creative applications is difficult and fraught territory, i don't think the efforts taken here are sufficiently convincing.	0
weaknesses: there are three components to the evaluation, and each of them are problematic:  the first evaluation (fig 2) compares the average frechet distance between phrases generated by different models, and within the original dataset.	0
strengths:  nicely motivates the approach of separating foreground and background  fewer landmarks are needed than in previous work  approach seems beneficial for video prediction  clear and well written  detailed description of architecture and training weaknesses:  the changes and improvements feel somewhat incremental  some uncertainty about the solidity of the evaluation/comparability with baselines results on celeba somewhat weak overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement.	2
 typo: 'they main takeaway' > 'the main takeaway' strengths:  good related work  somewhat complete evaluation weaknesses:  no analysis with so many hyparparameters (reg lambda, number of concepts), and thus not sure about the validity of the simulation  idea is interesting but straightforward  not very interpretable results	2
 while the proposed approach and evaluation metrics are novel, and the results generally support the claims of the paper  toy experiments result in recovery of the groundtruth concepts, concepts identified for text and image classification offer feasible takeaways  there is still a lack of proper humaninterpretability aspect of the discovered concepts.	0
the removal accuracy is a bit less convincing but appears to be a fair and honest evaluation.	0
i could see that this has its uses, but an evaluation requires more than one example.	0
but i am not convinced that this random projection can learn useful representations, which brings me to my second concern about the evaluation metric.	0
within your evaluation you should compare the different optimization approaches (i.e., naive vs iterative) using the same model representation as your proposed algorithm only proposes a new optimization scheme but is not fixed to a representation.	0
review:###observing shortcomings of bleu and rouge, the paper proposes, jaune, a set of criteria for a good evaluation metric.	0
i think it would be valuable to present new axes to decompose the evaluation problem, but more work is needed to clarify and develop the axes presented in this paper.	0
another drawback of the evaluation is the lack of a proper statistical analysis of the results, given the small data and model sizes.	0
there are several concerns regarding empirical evaluation and baselines: (1) the improvements in validation error shown in tab.1 seem to be insignificant, and confidence intervals are needed for justification.	0
in addition to doubts on practical applicability, my second major concern is regarding the depth of the evaluation.	0
however, my two main concerns are (1) inconsistency in baselines which cast some doubt on the improvements offered by the proposed approach, and (2) the fact that both the algorithm and the experimental evaluation seem to be a subset of that in concurrent work (especially uesato et al.).	0
second, and the main problem with the paper, is that the properties of the alpha parameter are intriguing but the experimental evaluation is underwhelming.	0
another point that raises concerns to me is that the optimal value of alpha is determined by the vocabulary of the evaluation task.	0
in particular, the proposed theory focuses in two phenomena (word similarity and word analogy) as stated in the abstract itself, but the empirical evaluation is limited to the word similarity task.	0
however, i have some concerns regarding the experimental evaluation.	0
in the paper's defense, it seems that they were following the experimental setup of wang et al. (2019), but the paper should elaborate more on the choice of evaluation datasets.	0
additionally, the paper introduces a new quantitative evaluation metric for explanations, free of human intervention: irof (incremental removal of features) incrementally grays out the segments deemed as relevant by an explanation method and observes how quickly the endtask performance is degraded (good explanations will cause fast degradation).	0
the only obvious downside of aggmean and aggvar is that one would have to implement and run all constituent evaluation methods, which is expensive.	0
i am, however, concerned with the evaluation of the method and its practicality, as reflected by the following issues: 1. the method has many hyper parameters.	0
pros: 1) they collected a new smallscale evaluation dataset for evaluating the quality of semantic representations of various existing embedding techniques for code identifiers.	2
although the topic of learning from sets is relevant and using the fuzzy set theory for the task is interesting, i have the following concerns regarding with the clarity, significance, and evaluation.	0
however, evaluation for this paper is severely lacking.	0
overall, the paper has interesting algorithmic ideas, but there are critical issues in the evaluation and resulting claims being made.	0
while the authors motivate the use of the proposed intrinsic reward for learning to solve tasks in sparse reward environments, the experiments do not include moreover, some of the tasks used for evaluation do not have very sparse reward (e.g. acrobot but potentially others too).	0
the empirical evaluation provides a number of proofofconcept ideas, but no in depth investigation of the properties of the approach.	0
this seems an important part of evaluation but not explained at all.	0
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough to show the effectiveness of the proposed metrics due to the following reasons: (a) the first proposed extension (sce) is too incremental.	2
however, i have a litany of concerns with the paper itself, concerning its high similarity with a paper published in may, its motivation, its presentation, its empirical evaluation, and the analysis presented within.	0
while it is interesting to note that in this exact setup (mostly dim_z=512) lia outperforms the baselines wrt the chosen metrics, a more thorough evaluation would, for instance, sweep the choice of dim_z, and ideally present nll results (which i think are possible to evaluate given that lia has a flow model even if it’s not trained using nll, but i’m not 100% sure on this front and am open to counterarguments on this front).	0
anyway, the experimental evaluation shows good results on all three datasets for your models, but its hard to be sure since you only have one comparison, an old lda, and nothing recent.	0
the paper is very short and lacks many important details, such as where the data is collected from, how it is processed and split into training and evaluation sets, and how the initial token classification is performed.	0
the evaluation is done on two datasets, one with examples from nearoptimal players produced by mohex 2.0, and the other from randomly sampled but perfectly labelled examples generated by benzene.	0
the reason i am still against acceptance is the lack of stronger empirical evaluations.	0
specially the evaluation section lacks details and clarity: a) in the beginning of this section (page 7), the authors talk about “generalized” and “traditional” settings without properly defining them.	0
based on the error analysis, the labels look very noisy and subjective, but this seems to be a common problem in the visual predicate prediction task (hence the recallbased evaluation).	0
i'd rate the novelty as medium (smart combination of existing methods), but the exemplary experimental evaluation elevates it to more than a systems paper.	0
several concerns are listed: 1. it will be better if the author can show a structure map for encoder, classification, class condition embedding, and mutual information evaluation networks, to clarify their relationships.	0
additional comments/corrections  there were numerous typos and grammatical errors that were present in the manuscript that did not directly impact this evaluation but should be fixed in the future.	0
 empirical evaluation lacks strong baseline methods.	0
first, none of the metrics used are good evaluation metrics for frame prediction (i know they are quite common but that doesn't make them good) as they do not give us an objective evaluation in the sense of the semantic quality of predicted frames, specially for long videos.	0
i understand we need that in the proof, but is there any reason this is also the case in empirical evaluation?	0
at the same time, however, i would want to see more thorough evaluation than the current, e.g. pgd with more n, random restart, gradientfree attacks, or blackbox attacks.	0
another major weakness is that the paper lacks a quantitative evaluation scheme for its success.	0
more solid and extensive evaluation is definitely needed in the future to have a more thorough comparison and a more careful assessment of the limits of the proposed method, but at this stage, i think the evaluation is sufficient.	0
5. i thought the idea of looking at feature importance just before clinician intervention was a very clever evaluation, but i wanted the qualitative evaluation to go one step further.	0
# minor but does affect the evaluation ' paragraph after lemma 1: it is unclear why those conditional distributions are required instead of conditional distributions in eq. 3. '	0
[p6] can providing [p7] to known how [p8] our work open >> our work will open  revision  although some issue seem to have been clarified, two of my main concerns, i.e. the proposed evaluation and the text quality, have not been resolved.	0
overall the idea is interesting but i have some concerns mainly about evaluations and baselines which i am including below.	0
2. according to section 5.1, the search is performed on cifar10, but there is no evaluation on cifar10 at all.	0
further there are some doubts on the setup of the contrastive objective and the paper lacks comprehensive evaluation on standard environments.	0
i cannot recommend acceptance of the submission in its current form, but i encourage the authors to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.	0
my current concerns are mainly on the appropriateness of the experimental evaluation.	0
it is hard to judge the quality of the proposed method due to the lack of evaluation/comparisons.	0
however it seems to me this success rates were not computed using human evaluation (since the authors claim once the classifier is finished training, the attack success rate can be larger than 99%).	0
the evaluation compares pdm to rce, but from figure 2 one could guess that variance reduction alone (2c) performs very similarly to pdm (2e).	0
the graybox and blackbox threat model evaluations are also not the most interesting here.	0
i would be happy to change my score if authors can address the above concerns about considering distinguishing multitask learning from continual learning and providing a realistic evaluation setup with more than 2 tasks and comparison with current state of the art methods.	0
the experimental evaluation is quite thorough and shows that the method performs quite well, especially considering it is unsupervised but is compared to supervised representation methods.	0
the evaluation is detailed, but the results are unsurprising.	0
# weak points  there are two real datasets considered for ood evaluation, but i consider there are some flaws in their utilization for ood detection.	0
despite these shortcomings of the experimental evaluation, i still believe that the paper has merit.	0
the experimental evaluation brings some interesting behavior but is somewhat limited.	0
the experimental evaluation presents some expected behavior in the context of the bound, but i miss a real study trying to make use of the proposed framework to do adaptation in practice with comparisons to other strategies.	0
i would like to see this paper eventually published as i find the proposed technique original and quite relevant to current rl research, however i feel like its empirical evaluation is too weak at this time, which is why i am recommending rejection.	0
the main limitations of the current empirical evaluation are: • only 7 atari games are used (vs 49 in the ppo paper the proposed technique is compared to), without justification for how they were chosen, and it seems like only 1 run is performed on each game (while rl algorithms are well known to exhibit high variance) • on obstacle tower there seems to be also only one run of each algorithm (more runs could be done with different training & testing seeds in order to get an idea of the variance) • there is no comparison to ppornn on obstacle tower • i think a natural and important baseline to compare to is using the same architecture as in fig. 2 but where the mapping phi(o_t) is learned through regular backprop (using the same loss as when learning the mapping i(t)).	0
 the experiments are run on 3 different random splits, based on the mean(std) of the evaluation metric, the performance of the proposed method does not vary much compared to baselines, especially on the second dataset, eg., gnn 0.25 / 0.02 compared with genn 0.26. also, glenn < gnn seems to imply including energy is not the most important part for helping the task, but rather the semisupervised joint training truly improves the performance.	0
the main extrinsic evaluation of the paper is on xnli but only employs an english and a chinese model.	0
the authors claimed that the proposed new evaluation metrics are novel contributions of the paper but there is no discussion on why they are good metrics for evaluating the quality of synthetic datasets nor which metric should be used in what scenarios.	0
review:###the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
as said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
the characteristic of being able to provided deeper insights on strength/weaknesses and relevant factors on the innerworkings of a given method is something very desirable for every evaluation.	0
the paper argues for learning a set union function however much of the evaluation focuses on quite small sets of 2 or 3 items.	0
2. lack of any quantitative evaluation (such as localization or pointing experiment) of their approach.	0
besides, in tab.2 there lacks of necessary comparisons with recent relationbased approaches e.g.[r3][r4], which makes the evaluation less sufficient.	0
the work is sensible and the approach is clear but i found the evaluation and motivation lacking in key areas that i mention above.	0
finally, the parameter 'beam width' used in the evaluation of the value function but is only set to 1 in all experiments.	0
my only concerns with the paper have to do with evaluation.	0
main comments: while this paper proposes some interesting ideas, i am concerned about the soundness of the method, some of the precise implementation details and i believe the empirical evaluation could be greatly improved.	0
cons  the experimental evaluation is incomplete and does not expose the properties of the method fully.	2
overall i think this work attempts to solve an interesting problem with an incremental but reasonable approach, and more empirical evaluation is needed to make the paper meet the bar for acceptance.	0
my major concern is about the evaluation: it didn't compare with any existing work.	0
3. the adversarial evaluation section should be substantially revised to address several important flaws: (a) the evaluation for blackbox attacks is done in very nonstandard threat models (e.g. labelonly 3pixel blackbox attacks) that do not seem to be relevant to the blackbox robustness of a system.	0
in the evaluation section, we have baseline models bm1 and bm2, but they have never been introduced.	0
finally, it is also great to have some benchmarks of the algorithms being implemented, but at least for atari, i am not aware of previous work using the exact same evaluation setting, so it is hard to tell how they compare to other implementations.	0
the authors argue that their benchmark results are a key contribution of the paper, but i do not find these results particularly insightful, especially the atari ones that are not comparable to previous results (due to using a different evaluation method) and the lack of stateoftheart algorithms like rainbow or iqn.	0
you mentioned that “we omit the usual first term in the mmd loss …” but if so, why do you introduce this term to evaluation metric?	0
it is commendable that the authors have introduced adaptations and improvements to their baselines for a stronger and fairer comparison but the evaluation remains very limited.	0
this work lacks an evaluation of the number of crashes to complement the rewards depicted in figure 6. the particulars of the method used for computing whether the safety constraint is violated are somewhat surprising.	0
pros: 1. the motivation of the work is clear and solves an important task 2. the approach is sensible cons: 1. experimental evaluation is very weak.	2
i have a few concerns about the architecture formulation and lack of clarification and intuition in what appears to be the main contribution of the paper (sec 3.2 and 3.3) which i will detail below: a. in the evaluation, i really want to see a decoupling between the 'time encoding step' and 'attention based aggregation' on the performance to figure out to isolate different sources of performance improvements.	0
in the rebuttal the authors addressed my concerned about the insufficient empirical evaluation and included other relevant baselines.	0
cons: it's weak accept rather than accept from me because of how the empirical evaluation were conducted in the paper, and i think the experiments conducted in the paper were a little bit weak (common for most iclr submissions).	0
the concern is about evaluation.	0
however the proposed approach is too incremental and the empirical evaluation could also be improved.	0
the proposed idea is interesting, but its presentation and evaluation could be significantly improved.	0
 the authors compare with riochet et al., 2018 in table 4, but not in the rest of the evaluations.	0
a lack of quantitative evaluation further exacerbates this issue: nearly all experiments, even those with associated plots, are characterized qualitatively and without reference the performance of related methods.	0
(4) lack of qualitative samples for generated plans: for both the rope and the streetlearn domain the authors do not provide thorough evaluation.	0
the proposed method sounds interesting and promising, but the empirical evaluation is unclear.	0
strengths:  the paper studies an interesting problem of adaptive mri reconstruction  the review of mri reconstruction techniques is well scoped weaknesses:  the evaluation is rather limited and performed on one, proprietary, relatively small sized dataset  some simple baselines might be missing i like the idea of adaptive sampling in mri.	2
2. the proposed method is sensible 3, the experimental evaluation shows great practical gain cons: 1. the method itself is incremental.	0
the term 'scalability' is used as an evaluation metric, but never clearly defined in the paper.	0
evaluation is almost always difficult when it comes to generative art, but i am slightly more concerned than that.	0
however, i vote to reject this paper, because the evaluation is insufficient, and the paper lacks clarity.	0
finally, i have strong concerns about the experimental evaluation.	0
i am mostly concerned about the evaluation.	0
my concerns are centered around the experimental evaluation.	0
i do have some deep concerns about the evaluation metrics used to report the results that i will discuss next; these are the main reason for my current score, but i am willing to adjust it if the authors address them convincingly.	0
if the authors clarify that the overall cost of cs (including all of the factors listed above) is lower than for imp, address technical concerns about the evaluation, and evaluate cs as a pruning technique in an updated version of the paper, i will update my score accordingly.	0
overall i think this work is interesting and i would encourage the authors to try and add as much quantitative evaluation as possible, but also try and include qualitative information regarding specific sequences after prodding the models.	0
in either case it would be helpful to have this information in the description of the setting  since use of attention is the main contribution of this work, but not the only part of the method, i would recommend adding to the evaluation a method which is equivalent to the proposed one, but doesn't involve attention (i.e. only uses d).	0
nevertheless, i am not convinced that there is enough novelty and substance on this, i have some concerns on the evaluation, and i think that the overall presentation should also be improved:  i am not convinced by the 'knowledge distillation' approach.	0
summary i think the area of application of this work is extremely interesting, however the training and evaluation methodologies have to be updated in order to realistically measure the way this system might perform in realworld test conditions.	0
i lean towards rejection of this draft, as it has several weaknesses:  the connection between the evaluation (which mostly focuses on the speed benefits) and the claimed contributions is tenuous at best.	0
although the paper presents an interesting application of ml, i vote for rejection since i) the paper is similar to previously published works and lacks methodological novelty, ii) the description about the data and methods is not written clearly enough, and iii) the evaluation needs to be strengthened by additional baseline models and evaluation metrics.	0
4. the authors used the weighted mse as evaluation metric, which was used as training loss of the bilstm, but it is unclear if the same loss was used for linear and gradient boosting regression.	0
 the authors are recommended to compare with previous work on hierarchical generative modelings with objects and parts, such as (xu et al, iclr 2019), and other related work, such as spiral (ganin et al.) due to the limited contribution, lack of comparison with related work and limited empirical evaluation, i recommend rejection of the submission.	0
after reading the rebuttal, my concerns remain that: (1) lack of empirical evaluation on more complex real / synthetic datasets; (2) it is still unclear to me how such hierarchy is inferred from single images.	0
although the algorithm is novel, there are several weaknesses in both algorithm design and experimental evaluation.	0
 the evaluation tasks for blackbox attacks should be performed on blackbox scenarios such as google cloud api and amazon api settings.	0
concerning the human evaluation, i do not understand the numbers in table 3right: it seems that the model performs better at generating among 21 styles (category) than it does with 3 styles?	0
my second concern is the evaluation of the method.	0
overall assessment: the paper presents an interesting idea, but it has shortcomings in technical writing and evaluation.	0
while the few figures presented in the paper look plausible, the paper is unfortunately lacking both in evaluation and in presentation of the method.	0
however, the paper is nevertheless poorly written, and the empirical evaluation is weak.	0
my primary concern is that the most important evaluation (evaluation of phraselevel importance scores, sec.	0
however, i have concerns about the empirical evaluation of the proposed method.	0
pros:  welldone evaluation and comparisons with prior art!	2
my biggest concern is that the method is only evaluated on a single system: a good evaluation metric (automatic or not) must have high correlation with human judgments for a variety of systems.	0
i also have more foundational concerns with using purely unreferenced evaluation metrics in dialogue (i.e. metrics that aren’t trained on a dataset of (context, response, score) triples).	0
we know that text generator evaluation is a challenging topic, but i expect some common criteria like 'fluency or readability' and 'accuracy or relevance' are measured somehow either by human judgement or automatic evaluation metrics.	0
i understand that evaluation of the machine generated sentences is a difficult task , but without any evaluation the quality and correctness of the generated text cannot be justified.	0
however, the empirical evaluation is lacking in terms of text generation quality and classification quality.	0
my major concern lies in the evaluation.	0
although i greatly appreciate this type of principled empirical evaluation, which i also think is lacking currently in our field (especially in deep learning), for this particular contribution, i have the following concerns for it to be published at iclr: 1).	0
without such tools, what prevents future researchers to come up with evaluation methods that have the same shortcomings that you try to address?	0
another problem is the lack of several sota genderdebiasing baselines in the experimental evaluation.	0
however, the method proposed is incremental, missing extensive technical or intuitive exploration of its effect and has limited experimental evaluation.	0
2. experimental evaluations: a) lack of use of standard metrics.	0
paper uses tasks in habitat for evaluation, but doesn't use the metrics that come along with it.	0
my main concern with the paper is that – like many other papers claiming adversarial robustness – it does not perform a solid evaluation using a range of strong attacks.	0
3. adversarial evaluation: the authors should report accuracy under blackbox attacks and also whitebox adversarial accuracy as a function of epsilon (e.g., for the fashion mnist model, plot adversarial accuracy as a function of the attacker eps from 0/256256/256).	0
to make the empirical evaluation more rigorous and convincing, i think the authors should: (1) repeat the adversarial robustness experiments on more commonlyused datasets such as imagenet and cifar, and also evaluate robustness of their models more thoroughly, using techniques like blackbox attacks.	0
i think this is a reasonable approximation, but would like to propose at least a smallscale human evaluation of sentiment in the generated texts.	0
2. only one classifier is shown for the celeba data so it is hard to compare different blackboxes in the evaluation.	0
however, my main concern is the evaluation.	0
generally, i believe the idea has conceptual merit, but the empirical evaluation is not sufficient to finally judge its practical value.	0
the experiments are poorly evaluated in the following sense:  the authors provide in figure 3 the results from evaluation of seerl against the chosen baselines.	0
'shortcomings of the evaluation section' the proposed architecture has 13 layers including the input.	0
there is a lack of experimental evaluation.	0
from the computational point of view, it is intuitive that learning matrices will increase the computation cost but there is no comparison or theoretical evaluation for this.	0
evaluation is ok but restricted to only one benchmark for classification, the evaluation can maybe be extended.	0
what would probably make more sense if one can extract some more meaningful discrete factors of variation such as e.g. gender / hair color / etc. ' (minor) i find it a bit concerning that the metrics are actually computed on the output of the classifier, which is trained on the real data whereas at evaluation time it is fed with artificially generated images.	0
even though this paper focuses on the construction of macro actions, leaving the drl algorithm as kind of a black box, i still think how the integration and evaluation are done plays a huge role in this kind of problem.	0
evaluation of macro actions consists of looking at the average reward of a policy learnt with the macro actions added to the action set action set augmentation just adds macroactions to the existing action set (just as is done in most temporal abstraction methods) the macro action construction method seems to be a standard black, box ga using appendtosequence and changeelementinsequence as search operators.	0
of these two, the top performance is only around 26. the graph also has the yaxis labeled as % accuracy, but the standard evaluation metric on charades is meanaverage precision as it is a multilabel dataset.	0
in its current form the paper is not fit for publication at iclr, but after addressing the weak points and a more thorough experimental evaluation this could become a good paper.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
my main concern of this paper is the lack of quantitative evaluation.	0
i understand there is no readily available dataset for such evaluation, however the ball is in the camp of the authors to solve this problem. '	0
the empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup.	0
additionally, i think the paper suffers from a lack of clarity in some places and issues with the evaluation.	0
overall, the submission proposes an interesting and stateoftheart method for selfsupervised video representation learning, but its technical contribution and empirical evaluation are limited.	0
the paper did include cc_x0008_etavae and jointvae, but only for qualitative evaluation in unsupervised learning.	0
unfortunately, the lack of detail in the description of the experimental setup makes it difficult to assess the correctness of this evaluation procedure.	0
7. the ablation studies in section 5 should not be limited to latent space traversals, but include a quantitative evaluation as well.	0
(1) there is a serious lack of quantitative evaluation and comparison.	0
empirical evaluation on fairly small scale experiments demonstrates these techniques can productively be employed to develop new and powerful models concerns: there is, in my opinion, rather too much content for a conference paper of 8 pages.	0
cons: however, the paper also needs some improvement in its evaluation (see detailed comments below).	0
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
overall, the suggested method seems reasonable, but the evaluation seems weak. '	0
looking at fig.1, we see that training on fixed 160x160 size results not only in a slightly better evaluation accuracy at 224x224 (about 1%, if i had to guess solely with the figure) but also in a much lower computational burden.	0
i have one concern with the new dataset used for evaluation, that i didn’t see addressed in the paper.	0
i found however the distinction between domain and style not so convincing, and in the qualitative evaluations i cannot see what distinguishes them.	0
overall i found the model an interesting contribution, but the evaluation raises too many questions.	0
while the method is specifically designed to image classification  which in a way reduces a bit the scope the possible applications but this is a minor remark  i find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets.	0
however, the reviewer has some concerns about the overall significance and some other concerns on the experimental evaluations, and hence recommends rejection.	0
a few comments that did not affect my evaluation but might be helpful to the authors: i might suggest to the authors to reconsider the title of their paper.	0
i don’t think this paper is strong enough for iclr as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written.	0
my second major concern with the paper is in the choice of evaluation metrics, which are based on the correlation between variance of predictions (over a sample of perturbations) and the value of the loss.	0
related to that, another question concerning the experimental evaluation arises.	0
2. the proposed method is simple but effective, which performs better or onpar with sotas on zsl benchmark datasets under the evaluation of the harmonic average of accuracies.	0
some comparisons to baseline domain adaptation in the evaluation (see the second concern) would help justify this decision.	0
the paper is poorly written (there are many simple grammar mistakes, the methods are not well motivated, and the methods are described in a confusing way), and more seriously the evaluation of the method is not complete.	0
but, i have serious concerns about the positioning of this paper and the experimental evaluation as follows.	0
however, i have concerns regarding the evaluation, as described below.	0
regarding the evaluation, my first concern is that the experiments are all on atari, in which the visual variation is extremely limited, compared to realworld observations.	0
a good start would be to mitigate the concerns i expressed for the evaluation of the model.	0
however, my major concern is in the empirical evaluation, especially i'm observing that the comparison with (yang et al., 2016) is not presented.	0
applications of this new network are shown and evaluated on edgepreserving smoothing, image denoising and image restoration, where in all cases the evaluations show a small but consistent gain in psrn compared to baseline models.	0
as an outsider to the lowprecision dnn literature, the proposed soft quantization strategy for batch normalization layers looks interesting, but the submission suffers from clarity issues (see clarityrelated comments), and the evaluation is insufficient to support some claimed contributions (see evaluationrelated comments).	0
evaluationrelated comments:  i don’t understand the reasoning behind the bolding of entries in table 2. in the mnist section, why is the addnet entry bolded but not the twn and sgm entries, even though they share the same error rate?	0
furthermore, the paper is badly written with multiple typos and lacks a rigorous evaluation. '	0
https://2020.msrconf.org/track/msr2020papers#callforpapers ' the evaluation lacks any comparison with standard language models that have been employed in this field.	0
with respect to the evaluation on cifar and mnist, i have soem concerns.	0
the authors made an effort to show evaluation results on mnist and cifar10, but they are much less challenging tasks.	0
weaknesses: 1. except for the experimental evaluation, what is the advantage of rank statistics over directly comparing feature vectors?	0
c) why evaluation of incremental learning on cifar100 is not well (acc difference between old and new is larger than other datasets).	0
overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation.	0
it points out existing problems of the current evaluation scheme: (1) only compares the final result without testing the robustness under different random seeds; (2) lacking fair comparison with random baseline under different random seeds.	0
figure 6 and l34 of page 8 claim a better mse and qualitative results for a single example, but the difference is not obvious and to validate the claim that the proposed approach 'leads to superior reproduction of viewdependent appearance' (page 3 under imagebased rendering), i think a more systematic evaluation on multiple objects and views is needed.	0
i think, the paper lacks a thorough evaluation on more realistic data, having only a single example in fig. 14.	0
 a troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the entailed/refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion.	0
– but i think some of the existing ones could go to leave space for evaluations using more underlying vae/baselines/edgecase systems.	0
i am not familiar with the details of the dataset in their evaluation, but it would be helpful to show relevant results if it is easy to simulate using their dataset.	0
below are some concerns i had when i read the paper and also some suggestions on how to improve this paper: 1) i am slightly concerned about the evaluation of the fact completion task and its baselines.	0
review:###summary:  key problem: neural architecture search (nas) to improve both accuracy and runtime efficiency of deep nets for semantic segmentation;  contributions: 1) a novel nas search space leveraging multiresolution branches, efficient operators ('zoomed convolutions'), and parametrized expansion ratios, 2) a decomposition and normalization of the latency objective to avoid a bias towards very fast but weak architectures, 3) a natural extension of the optimization problem to simultaneously search for teacher and student architectures in one pass, 4) a novel stateoftheart efficient architecture (fasterseg) found by the aforementioned nas algorithm, 5) a detailed experimental evaluation on 3 datasets and an ablative analysis quantifying the benefits of the aforementioned contributions.	0
while it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationallydemanding models, is limiting.	0
pros  the extensive empirical evaluation shows that the proposed method is effective in preventing performance degradation in an asynchronous setup across tasks and models.	0
the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. note on related work: it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that betavae is far from optimal for “extrinsic” disentanglement, the text in section 4.1 should take these results into account.	0
technical quality: the evaluation was fairly thorough, but the paper can be strengthened massively with a few small changes and additions.	0
 the evaluation is thorough  the problem and solution are well motivated weaknesses:  while the authors demonstrates that genesis is able to model static scenes, it is not clear how straightforward it is to extend genesis to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation).	0
review:###this paper concerns the limitation of the qualityonly evaluation metric for text generation models.	0
instead, a desirable evaluation metric should not only measure the sample quality, but also the sample diversity, to prevent the mode collapse problem in ganbased models generation.	0
as a minor weakness, the evaluation seems lacking in that human evaluations are only performed on the audio quality, not any of the target properties that are being changed.	0
therefore the objective function used in the experimental validation violates assumption 1. now, i can imagine that perhaps in practice this does not matter too much, but surely the evaluation is not correct according to the theoretical claim.	0
the main strengths of this paper are extensive experimental evaluation using both quantitative and qualitative analysis, and significant improvement in performance on reallife data over previous works.	2
general comment motivated by the comment ('note...perfectly') mentioned 5 above: for all the adversarial evaluation in the paper, the authors should also try cw attacks/blackbox attacks to get a more confident estimate of their model’s robustness.	0
review:###update: my recommendation has been borderline because the discussion of the paper about the nature of locality and compositionality seems to be less indepth than i would have expected, but if the authors will revise the submission to shift the focus of the paper to more focused on analysis and evaluation and weaken the claims that their model exhibits locality and compositionality, i would lean towards an accept as the empirical evaluation is extensive.	0
you sample them during testing and evaluation, but are embeddings which encode them approximately onehot, so that the (gumbelsoftmax) sampling produces the same rules all over again or are these rules fairly continuous and sampling is just a way to visualise them, but they are internally much more continuous?	0
in summary, i do like the main idea and the paper has merits, but it requires more evaluation and analysis to be accepted.	0
i note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including iou, f1 score and cd and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. decision: weak reject because the idea is quite interesting, but i believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.	0
evaluations are conducted on the standard shapenet dataset and the yields results close to the stateoftheart, but using significantly less parameters.	0
shortcomings of this evaluation scheme and proposed alternatives.	0
4 shows to some extend the influence of k, but i would like to see a more extensive evaluation.	0
there are evaluations on paws for paraphrase detection which i appreciated, but that is a little different.	0
5) evaluation the evaluation is on synthetic settings and the results on the convergence seem convincing but a subtantial optimization problem remains for future work.	0
evaluation: the problem of discovering primitives is approached by the authors in a novel and interesting way, however in my opinion the paper should be rejected because: (a) the experimental section is not convincing enough to support the claim that the method captures the shared motions across different skills.	0
main comments to authors: pros: interesting combination of techniques (iaf flows and wgf/stein inference) to do metasampling empirically appealing results as pertaining to raw performance metrics like accuracy weaknesses:  the evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains.	2
very good evaluation on hotpotqa, but would be even stronger if this were applied to at least one other task/dataset.	0
i believe that there are a few things that need to be clarified within the evaluation, but i would argue that this paper should be accepted. '	0
i am positive with respect to accepting this work, but find that there are a few unclear points in the evaluation that should be clarified to strengthen the empirical results.	0
# concerns 1) first, my concern is the problem setting and evaluation methodology, and in particular table 3 with varied parameters.	0
to be honest, i do not like the way the authors frame their work (e.g. the way the method is motivated in section 2.3 or calling it a 'unified framework'), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.	0
review:###after rebuttal, i think the authors made a valid argument to address my concerns on evaluation.	0
— i have a major experimental concern: when comparing against acet, the baseline of performing adversarial style training on random noise inputs seems more appropriate since it’s closer to the evaluation metric (which picks random noise as out of domain and not 80m tiny images).	0
the manuscript is clearly written and i only have some concerns regarding the evaluation.	0
i have several confusions or concerns about the synthetic experiments 1. in the synthetic experiments, is the evaluation task different from the training task?	0
however, it lacks proper empirical evaluation and makes an impression of a work in progress.	0
cons:  no evaluation with respect to any reasonable gan setting.	0
the main idea of the paper is to propose an estimator for offpolicy evaluation, that computes the ratio of the stationary state distributions (similar to recent previous work liu et al.,) but when the behaviour policy may be unknown.	0
while this new metric is compared qualitatively to existing methods, quantitative evaluation is lacking.	0
the authors provide a large experimental section, however the key problem with the paper is that it blurs the evaluation issue.	0
however, i think that the paper lacks an appropriate comparison with similar methods in the literature, and the separation between the real evaluation in a downstream task (xnli) and the analysis on a rather artificial contextual word retrieval task (which favors the proposed system) is not clear enough.	0
on the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper.	0
the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use predefined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.	2
when efficiently implemented on a gpu, total time spent on learning the manifold is negligible in comparison to function evaluations.” this would mean that their iterations are performed in less computation time than the ars, but i would have personally liked to see a number attached to this.	0
review:###this paper studies the so called problem of derivativefree optimization, which is relevant for cases when the evaluation function is continuous but access to gradients is not possible.	0
strengths: the ablation study seems complete clear improvements over state of the art methods weaknesses/improvements: the description of the evaluation procedure was a bit vague.	2
due to the above weaknesses in the evaluation, i am not fully convinced that the claimed contributions are substantiated empirically.	0
empirical evaluations seems fair as far as i can tell, but i am not familiar with benchmarks for vaes.	0
though this work is well presented and indicates promising results, i think the paper should not yet be accepted due to the following main reasons: (i) the experimental evaluation indicates promising, but not yet convincing results; (ii) the computational complexity of nap seems to be a major limitation of rapp which is not addressed in the text; (iii) the added value/insights from the theoretical section 4 (motivation of rapp) are not clear.	0
[pros]  clearly written  clear motivation  correct derivations  interesting algorithm [cons]  experiments are a little weak (and focus on a single domain)  would have liked to see an explicit algorithm for the optimization procedure  small lack of clarity in the presentation of section 4.1notation q_t is not introduced for example  more discussion about the evaluation metric  linking it more to prior work	0
preliminary evaluation  clarity: the writing is fairly clear, though some details are lacking.	0
i maintain some concerns around the empirical evaluation in the paper (collating results from multiple sources with different experimental procedures).	0
5) i am not particularly up to date with evaluation of gan models but to me the presented results in the main paper looked mostly reasonable.	0
'additional feedback my concern is about evaluation.	0
the applied method seems conceptually quite simple (as admitted by the authors in section 5), and the neural rendering approach seems quite neat, but both method presentation (section 3) and evaluation (section 4) seem incomplete and leave significant open questions.	0
section 4 mostly focuses on quantitative evaluation and lots of picture examples, but fails to give insight into particular behaviors, failure cases, etc. the evaluation procedure is cast a bit into doubt by two things: 1) in figure 4, the initializations (blue circles) between the darnet method and the proposed method are very different in size.	0
pros:  there is an exhaustive evaluation and comparison across different types of data with the existing methods along with the sota.	2
overall, i think the paper proposes an interesting unification and generalization of existing stateoftheart approaches [6, 4], but i think the experimental evaluation needs to be more extensive and clarified to judge the potential significance of the results.	0
this might be a sufficient contribution (in the end, efficiency is very important for actually applying methods in practice), but then a thorough evaluation of the method would be expected (see further comments about it further).	0
1 & 2  appears to accomplish what it claims as contributions  demonstrates a rotationequivariant attention mechanism  shows that its introduction improves performance on some tasks weaknesses:  unclear how the proposed attention mechanism accomplishes the goal outlined in fig. 2d  performance of the authors' evaluations of the baselines is lower than reported in the original papers, casting some doubt on the performance evaluation  the notation is somewhat confusing and cumbersome, making it hard to understand what exactly the authors are doing  no visualisation or insights into the attention mechanism are provided there are three main issues detailed below that i'd like to see addressed in the authors' response and/or a revised version of the paper.	0
negatives : the experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : 1. it misses out a recent stateoftheart method (slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way.	0
i would be happy to adjust my evaluation if revisions to the paper can one/several of the concerns indicated above.	0
while reviewing this paper i went back and read the ende evaluation data for the last few years trying to see how often i could reason that images would help and i came up severely lacking.	0
my main concern about the work is that the experimental evaluation is limited.	0
but i think given the reliance of the field on these datasets it makes sense, plus i think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method.	0
however since i am not very familiar with this area, so there can be things i miss in the evaluation of this paper.	0
i would move the simulations of different storage strategies (figure 2, blue and black lines) into the introduction, as part of introducing the reader to the problem and evaluation, and explain how figure 2 is related to what you really want to minimize: 'total pallet distance moved over time x'.	0
weaknesses ' my primary concern are with the evaluation.	0
since this is not a vision conference, i am giving this a weak accept, but i would really like to see at least an evaluation of (2).	0
it's possible that this happens, but there is no evaluation that discusses this, and from all of the examples i'm led to believe that this is basically also just learning a few program templates, the same ones learned by previous methods.	0
it is unclear to me why the authors believe that their “model’s merit would be more salient” in a multiturn setting, and i think such an experiment would be good to show  or, at the very least, an indication that such an experiment was tried but results were not considered due to reasons x,y, z, etc. overall, the rough improvement that is being provided in the firststage of the two stage setting seems rather minor (23% > 26% accuracy; 2.21 > 2.35 human eval), and that the task remains extremely difficult questions ' i know the dinan et al. models, at human evaluation time, hardcoded to not pick the same knowledge twice.	0
reasons to accept:  original and wellmotivated idea  clearly written paper reasons to reject:  problematic empirical evaluation (e.g., lacking recent baselines)  several performance numbers appear to be below random gcn baseline performance  general applicability of the approach (e.g., to nonhomophilous networks) is not clear	0
 having said that, i like how the evaluation was performed on a variety of stochastic processes  previous literature only used gp  eq kernel, but here more challenging nonsmooth functions such as gp  matern kernels and sawtooth functions are explored  and it’s very convincing to see the outstanding performance of convcnps here.	0
i’m concerned that the subset of the test set the authors are using for evaluation isn’t representative of the entire test set.	0
however there are several major concerns with the paper: 1. since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision.	0
but this definition of domain distance is more meaningful in the domain adaptation setting (like amazonoffice dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains.	0
replace weighs with weights 5) [8] points out how selfsupervised learning on a large dataset but with a domain shift (yfcc100m) is not as effective for pretraining as it is doing selfsupervised learning on the downstream task's dataset (imagenet).	0
the latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the informationbottleneck objective).	0
but the paper is poorly written and i concern the most about the stability of the unsupervised data labeling process.	0
another way of phrasing this remark: many theoretical bounds have lots of unknown constant factors, but are ultimately just trying to expose some scaling with respect to one parameter, e.g. number of data points, and therefore the constants don't need to be known that well for the statement to have some value.	0
i believe if the strengths of weight decay is properly tuned then it should help even with data augmentation, by a noticeable margin.	2
(4) if we only see wrn and densenet on cifar datasets (allcnn is probably outdated and performs poorly, and imagenet is not convincing as said in (1)), we notice that actually wddropout do provides a small increase on the augmentation schemes.	0
yet, if dropout is applied not to some intermediate layer, but to the input layer, does it suddenly become more similar to data augmentation (think of, e.g. cutout augmentation), and therefore an 'implicit' regularization?	0
in summary, data augmentation clearly improves the generalization performance, but to formally characterize it, one needs an alternative approach from the default rademacher complexity way.	0
ideally, pros and cons of each regularization technique could be discussed and if the message that 'data augmentation is better than explicit regularization' could hold across multiple domains and tasks, then this will definitely be delivering a strong message.	2
i agree that it's useful to have a generic default similarity measure to try first, but clearly if prior data is available it should be used.	0
the paper refers to their resource as a knowledge graph, but i would say it is more accurate to call it a bibliographic database (it consists primarily of paper titles, authors, and keywords).	0
my current decision is a weak reject, for a wellwritten paper, but some concerns as follows: the results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from cifar10.	0
considering that the premise of the paper is that mlp’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” the mnist visualization of groupselect felt informative, but the xor example for grouping visualizations seemed too easy.	0
the aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of blackboxes.	0
by analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the blackbox model predictions as a target.	0
pros: ' considers an interesting dataset subsampling variant of the sample weighting metalearning problem. '	2
the rebuttal also addresses my concerns regarding datasets, and my concerns regarding implementation details of ‘y_train_hat’, as now it's included in sec 4. overall, after rebuttal, i'd like to recommend 'weak accept' for this paper.	0
however after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above).	0
furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets.	0
i wonder if this has also been considered/tested by the authors of this paper as i would consider the topology to be a hyperparameter which should not be optimized on training but rather validation data.	0
strengths of the paper:  the ideas are interesting, and appear to perform well on a simulated and real(ish) data.	2
q2: the title still hasn’t changed on the current draft q4: to be more precise: ‘a novel databased approach for analyzing the stability of the closedloop system is proposed by constructing a lyapunov function parameterized by deep neural network’  this alone is not novel, you would need to specify how your method of doing this is new ‘a practical learning algorithm is designed to search the stability guaranteed controller’  this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the lagrangian approach, if this is novel) ‘ the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent’  this is not a contribution, but an experimental result.	0
probably i have missed something, but based on the description, can’t the frequencies be directly calculated on a grid along the 1dimensional subspace defined by the mean and first principle component of the data?	0
overall, this paper raises an interesting point about missing data imputation via generative models, and wellwritten; however, there are number of concerns: 1 the predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables.	0
the authors use h in the notation section to denote the matrix, use x_i to denote the data input, but use _x0008_f{h} and x_i (lower cased) subsequently.	0
in sum, while i like the overall idea and find the work novel and potentially practical, it is difficult to properly evaluate the work due to lack of comparison against stateoftheart data augmentation methods for achieving robustness.	0
i don't know if there are any issues in the training but data augmentation with    ilde{n}=2 is expected to be ineffective to improve robustness.	0
so it is not only possible, but rather likely that the learned 'object detectors' are to some degree driven by the statistics of the data, not the labels  e.g., there could be a highly selective 'bird' unit which nevertheless has high false positive rate for any of the more specific bird species categories in the imagenet nomenclature.	0
the paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (omniglot) is shown, while the comparison on market1501 is missing; 4) lack of comparison and discussion of other related works.	0
= strong/weak points  the idea of interpreting the 'humanlikeness' of program behaviors is interesting, and could help substantially with augmenting the traditionally small clean datasets in program synthesis.	0
2. the details of how the training and testing datasets are obtained are also lacking.	0
strengths:  the methodology is rigorous and the datasets considered is extensive  the paper is well written concerns:  isomorphism is not necessarily a bad thing in graph classification tasks.	2
on a side note: the paper also incorrectly attributes the synthetic dataset to (morris et al, 2016), but it is in fact from [1].	0
(i) i think the experimental evaluation is the weakest part of the paper at the moment which i find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details.	0
technical soundness: [a] optimization of the elbo: (1) the ordering of data (i.e., the directional information) was mentioned repeatedly in the paper but its importance to the fast approximation was neither explained nor discussed.	0
i also have concerns on the experimental datasets.	0
review:######summary### this paper tackles the transfer learning problem with the doubleblind unsupervised domain adaptation, where either the source or the target domain cannot observe the data in the other domain, but data from both domains are used for training.	0
my background is not in machine learning with clinical data (so i do not know common datasets) but it is unclear to me why the datasets evaluated on only consist of a very small section of the overall dataset they’re taken from (in case 1).	0
this is all very clearly presented, with justifications for different modeling choices and how they preserve the axioms; my only criticism here is that many aspects of this are fairly obvious (i.e. an architecture which shares weights in computing the embeddings of both data points, followed by computing a squared l2 distance, will quite clearly get us in the ballpark of a pseudometric), but in my opinion 'excess' clarity is much better than the opposite.	0
this means that a trivial way of improving performance is to just get more representative data (which is not a bad way of improving performance, but not the way that requires scientific study).	0
### post discussion update ### one of my primary concerns was the benchmark didn't restrict representation learning to a fixed dataset.	0
i agree that solving representation learning might not just be about finding a better algorithm, but a better algorithm coupled with the right data.	0
however, i'm also concerned that the benchmark might push people to (directly or indirectly) start training on data more representative of vtab evaluation tasks, and the performance gains in the benchmark will not translate to better algorithms.	0
review:###the paper proposes an architecture that grounds words from a captioning model, but without requiring explicit perword grounding training data.	0
with data such as pushing, this might not be so different from the simple visual inspection, but again with real data this will vary.	0
minor details: there are some good contents in this work, but for it to be a strong 'empirical' contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show stateoftheart results.	0
as far as experiments are concerned, section 4.1 presents results on mnist, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705].	0
the new ablation study is especially difficult to understand, since the first set of results are mega without using both memory and new data, but it seems to me that mega 'needs' the memory data to work, so i have no idea what is being ablated here, and i also do not understand how this answers the question why megad performs so well.	0
solving this kind of forecasting problem seems important to ridesharing applications, but is highly specialized for origindestination demand data.	0
however there are some concerns: 1. there are many clustering methods based on geodesic distance of data points (manifold learning), which are supposed to be better at capturing the nonconvex data distribution.	0
exploiting object hierarchy to learn disentangled representation is a promising direction but i lean towards rejecting this submission due to 1. no results on commonly used disentanglement metrics (e.g. see [1]) 2. no comparison with existing supervised/unsupervised methods on disentangled representations (e.g. [2][3]) 3. the needs for full supervision on each level and manually designed fixed hierarchy require labels for the full hierarchy and make it not applicable to many existing data.	0
my main concern is the limited evaluation as all the experiments are performed on the cifar10 and synthetic data.	0
review:###this paper is concerned with how to determine whether a set of data points are from a given distribution.	0
the basic idea of using the typical set to check whether given data points are from a given distribution seems sensible, as guaranteed by theorem 2.1. my concern is about the performance of the proposed method compared to alternatives.	0
my major concerns about the proposed method are whether 'the typicality set' could be faithfully applied in the small data regime.	0
my main concerns are the novelty of the work, the theoretical soundness of the method for small data settings and its robustness in settings with model misspecification.	0
from the figure, it seemed that ldmgan improved the sample quality and diversity compared to gans, but it is still prone to characterizing only a single or a few modes of the data distribution.	0
results on selected architectures and datasets show some improvements compared to naive binary index representations on the positive side, i see an interesting approach to pruning a neural network, assuming the storage cost is guiding the pruning algorithm.	2
 while the authors cite lack of work in such data setting (which is actually incorrect as described below), they do not motivate or justify why such an approach of using either unifilar hsmm or bayesian inference techniques are good starting point to model such information.	0
specifically, it claims “with very little data, a twostate model shown in fig. 3 is deemed most likely; but as the amount of data increases, the correct fourstate model eventually takes precedence”, but in figure 3, the plot with the highest bic score when the training sample is less is the green curve which stands for the sixstate model according to the legend.	0
strengths: (1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (one potential improvement could be to try noise distributions other than gaussian) (2) simple objective and consistent improvements.	2
for instance, the paper gets off on a rather strange footing discussing large data (indeed, but evident to the vast majority of iclr readers), but little is said in terms of the specifics of the temporal localisation problem except via citations to other papers.	0
2. missing comparisons: proposed method is interesting, but i wonder if there were a more standard evaluation to test the efficiency of the method, perhaps something like testing if representations learned using such data augmentations were more robust to adversarial perturbations?	0
 summary:  key problem or question: assessing / improving the robustness of object detectors to image corruptions (simulated fog, frost, snow, dragonfire...);  contributions: 1) a benchmark (obtained by adding imagelevel corruptions to pascal, coco, and cityscapes) and an experimental protocol to measure detection robustness , 2) extensive experiments quantifying the severe lack of robustness of multiple stateoftheart models, 3) experiments showing that data augmentation via style transfer (geirhos et al, iclr'19) improves robustness at little cost (at most 2% performance degradation on clean coco images).	0
the observation is that in many realworld applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.	0
it will be also interesting to see how does the proposed method perform on largescale datasets such as domainnet and officehome dataset: domainnet: moment matching for multisource domain adaptation, iccv 2019. http://ai.bu.edu/domainnet/ officehome: deep hashing network for unsupervised domain adaptation, cvpr 2017. http://hemanthdv.org/officehomedataset/ 3) the novelty of this paper is incremental as the theoretical results are extended from cortes et al (2019) and zhao et al (2018).	0
their results seem to be much stronger than the prior neural stateoftheart, which also has the downside of requiring labelled data.	0
it also looks at squad translations (but, i'd have preferred a bit more depth on one of these datasets over having both, but i understand why you made this rhetorical choice).	0
the paper seems to correlate tasks and languagerelated hints; however getting a language hint is not always available, especially for datasets such as taskonomy [1, see refs below]  which is a dataset for multiple tasks.	0
what i can say about it is that it is considerable amount of work, not only implementing the simulator but also looking at what rl methods need, and how to optimize the allocation and exchange of the data so that everything would work on gpu more efficiently.	0
the paper does not seem to provide source code and part of the data used in this paper also does not seem to be public data like libritts (i would be wrong but i could read that the multispeaker training data are not public), although i appreciate the authors' efforts to provide the implementation and evaluation details as much as possible in the appendix.	0
integrators with dynamic step size defiantly do exist, but the step size does not depend on the data like in gru/lstm but based on approximation accuracy.	0
weaknesses  1. limited methodological novelty: the methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications.	0
additional comments  1. lack of novelty: the data representation, which is presented as one of the core contributions of the paper, is not new.	0
existing approaches to bert training use bpe with ~30k vocabulary size or roberta with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext103 or billion word.	0
random choicespecific shocks in reward is accommodated, which are only observed by the agent but not recorded in the data.	0
i think training a joint model on these two datasets is a completely natural experiment that many of us wanted to see, and so i appreciate the effort of the authors and the benefit to the community of having these numbers, but i'm not convinced there is that much meat otherwise in this paper.	0
in the unsupervised setting, a betavae is used to learn a disentangled representation of x as a proxy for c, but then the data is regenerated using a two step process.	0
one could argue that it would be expensive to precompute such data, but i think rl scales even worse to higher dimensional problems.	0
it works hard to achieve a good experimental result, through many tricks listed in the “additional implementation details” in page 4, and through the data augmentation used in page 5. however, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in oliver et al. (2018).	0
this paper just shows that, if we have parallel data (which we do for many resourcericher language pairs), it is better to do joint modeling instead of learning simple alignments, but that is a pretty trivial finding imho.	0
based on the results presented, it seems that the performance saturates by adding more parallel data, but the authors fail to fully understand their evaluation data in the first place.	0
for instance, xnli isn't mentioned but mldoc is, it isn't clear in the table which methods are mappings and which aren't (big data difference), in the mldoc experiment was more parallel data used for these specific languages than was used for laser?	0
they evaluate on monolingual and bilingual word similarity, bitext mining, and a zeroshot document classification task where a classifier is trained on data in one language but evaluated on another (using their embeddings as features in both cases).	0
i do realize comparing to laser is not really fair since it is trained on so much data, however the model is similar to previous versions of laser that were trained on europarl, which could also be used as the data for training your models for a more applestoapples comparison.	0
concern and additional experiments: 1. please use standard grid graph dataset as used in the literature  max |v| = 361. moreover, i was wondering how do one generate 500 grid graphs with just max 100 nodes ?	0
also, the authors claim q is updated with real 'transition', but later part states only initial tuple is from real data.	0
a black box model that yields distributions over classes expresses aleatoric uncertainty via that distribution, and uncertainty due to a shifted data distribution is epistemic as additional data from the new domain would reduce it.	0
cons my main concern is that the evaluation of proposed method is quite limited in the sense that the method is evaluated with only single dataset. only basic model is used to compare to proposed model.	2
 in the abstract you state the definition of continual learning says you can't have access to the data from previous tasks, but then mention that previous works use samples from previous data.	0
the only sentence that seems to talk about it is 'this way ... network won't be able to have an output similar to the outputs on data from its training set', but this is not clear, not enough explanation, and seems somewhat innaccurate.	0
feedback/suggestions/nits (not necessarily part of decision assessment): 1. cite the definition of continual learning (e.g. with a reference to a textbook or review) 2. a lot of the writing is unclear, wordy, and/or grammatically incorrect  inconsistent verb tense  e.g. 'if one would need .... it is required' should be 'if one would need .... it would require' i'd suggest rewording this sentence entirely, because it's misleading  it says 'retrain on this new dataset (which sounds like train just on the new data), but i guess you mean retrain on all data including the new data.	0
the proposed model has shown performance improvements in classification accuracy on mnist and cifar100 datasets in incremental class tasks.	0
the way i see it, flowbased models aren't autoencoders, but functions that reparameterize data (here molecules) in terms of simpler random variables (here gaussian noise).	0
the metric itself appeared to be the main contribution, but as the author said, the metric was based on an ideal hardware setup that ignores the memory hierarchy and data transfer cost, which could be the bottleneck in reality.	0
 was discarding all but the strength subclass from the persuasion dataset and empirically motivated decision or just something you did apriori?	0
 the human results are already hedged, but maybe the grain of salt should be bigger: it occurs to me that the comparison of the model performance and human performance could be unfair since the human performance is reported on the original tasks, before the filtering of data and changing label distribution like with pdtb and gum.	0
 particularly given the data filtering and restructuring of some of the tasks, getting a rough estimate of human performance for all the datasets would be quite valuable  i think saying the mnli does not help model performance on disceval is completely valid but claiming it hurts performance could be a stretch given the margins of error.	0
the paper is well written and the ideas presented are interesting, but in my opinion not novel enough or thoroughly demonstrated to justify acceptance:  there is a very relevant work that is not mentioned by the authors and that can be seen as a generalization of the model presented in this paper: 'disentangled sequential autoencoder' by li and mandt (icml 2018), which introduces a model that is also disentangling a content and a temporal representation of sequential data.	0
2. the paper mentions multiple times that it does not use statistical models tailored for a dataset or application but instead use deep neural networks.	0
i suppose this is to be expected, but taken with the previous point (little variance in generated samples) i think it seriously weakens the paper's claim that this is an 'accurate an efficient data generation method.'	0
 since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on highdimensional images with more realistic objects as in the clevr dataset other than 2dgrid objects.	0
> if this is basically the same i would expect that you can use,mention methods from there  the plots with tsne are obviously colorful but do not provide a lot of information  they should be removed  i do not see a benefit  it is very common that all these methods (like yours) are provided on some kind of image data ... is there a particular reason / limitation?	0
regarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains.	0
this is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper.	0
the setting is perhaps a bit unusual, but since the data considered is essentially sequence of observations, any model of sequences could be used to produce embeddings (for instance, recurrent vae, autoregressive models, timecontrastive methods) and compare the different trajectory representations.	0
authors claimed that “we let the number of communities grow with the data, a flexibility that we achieve using a dirichlet process mixture,” but this is not clear in the modeling in section 2.1. which parameters control such increases and how they were adjusted?	0
maybe i missed it, but how is the relevance score / predicted label determined for testing data given the graphs constructed in each class of training data?	0
in fang’s paper, at different iterations, the data are sampled from an unknown but fixed data distribution.	0
computing renyi’s entropy however necessitates to get a highdimensional density estimation over the data distribution  giraldo et.	0
there is not much of a discussion regarding the complexity of learning with the proposed losses but it looks like a simple algorithm for learning with equation (1) would have to use the entire dataset to compute it.	0
 most experiments in the paper are conducted on a small data set and this is a big downside of the paper.	0
in particular, the paper is concerned with posterior collapsei.e. posterior remains at the prior, limiting the model’s ability to reconstruct the data.	0
there has been much work on reduce ml system computation costs and memory storage requirements, but mostly by modifying the model instead of the data.	0
to me this is the fundamental tradeoff: not how many fewer examples can i learn from, but how much faster is the method than the 'standard' of using the full dataset?	0
(sorry if these are in the appendix, but some short summary is needed in the main paper) ## p3: title change recommended i don't think the presented method is really doing a 'distribution search'... i would suggest 'training data subset selection with ensemble active learning' minor method concerns  ## m1: what about regression?	0
review:###the paper takes the binary projection framework used for lshtype applications (based on dasgupta et al. 2017) and given training data from that framework, shows an efficient closedform solution update for the 'projection' matrix, and also derives an alternating minimization algorithm for when training data are not available (note that these are not training data in a typical supervised learning sense, i.e., they are not data features, but rather the output of a projection of the very specific type discussed in the paper).	0
the reasoning sounds fine (i.e. minimum and maximum would be too pessimistic) but no data is given.	0
to allow the classification of data from any domain, in the inference network the labels d are not used to infer the latent states, but only as an auxiliary loss that forces z_d to capture domainspecific information.	0
several metrics are used (rmse, mse, mape, mae, mre) while results on different datasets are shown in different but not all metrics.	0
they claim that using demonstration data in a supervised manner 'cannot generalize supervision signal over those states unseen in the demonstrations,' but most of these approaches are using neural networks and definitely are generalizing those signals to other states.	0
they're described as 'treating demonstration data as selfgenerated data,' but in fact they both add supervised losses to more closely match the demonstrated data.	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
if the input is missing, it cannot be transformed, it can be predicted using a conditional model over data given a representation of past (and/or future) observations, or it can be a (probably learned) dummy value, but please clarify this wording it’s essential.	0
my main concern is that, although the objective function is fairly intuitive, it is not clear to me what the properties are of the population minimizer (i.e. in the infinite data case).	0
for indomain noisy data, the dirichlet distribution should be sharp but in the middle of the simplex, while for the ood data, the dirichlet distribution should be flat.	0
one concern is that what if the ood dataset for test is not available at training.	0
 could you also compute the sum of the exponential of logits for the synthetic data, since it is the only metric that is evaluated in the real data experiments but not in the synthetic data?	0
i doubt that this would be a big effect, but there are easily ways to find highly similar articles between the pretraining data and test data to make sure.	0
review:###summary: this paper proposes a unified model for continual learning and aims to address the following problems: outoftraindomain dataset recognition catastrophic forgetting the outofdomain or open set recognition model is not only used to detect outliers but also for sampling “representative data” of previous tasks for forward (and backward) transfer.	0
4. major concern: it’s somewhat strange to observe that the vae model is able to generate multiclass data so fluidly with a simple gaussian prior.	0
the results shows robustness on different dataset include image and audio in the continual learning condition, where the new come data has a different distribution but the model still able to maintain reasonable quality for the previously and newly come examples.	0
overall i find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data.	0
they construct a pair of synthetic but somewhat realistic datasets—in one case, the bayesoptimal classifier is 'not' robust, demonstrating that the bayesoptimal classifier may not be robust for realworld datasets.	0
 a discussion of adversarial examples are not bugs they are features (https://distill.pub/2019/advexbugsdiscussion/): nakkiran (2019) actually constructs a dataset (called adversarial squares) where the bayesoptimal classifier is robust but neural networks learn a nonrobust classifier due to label noise and overfitting.	0
3. experimental setup:  one somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the bayesoptimal classifier has nonzero test error in expectation, and moreover the data variance is fullrank in the embedded space.	0
i am concerned that these properties are what drive the bayesoptimal classifier for the symmetric dataset to be robust (concretely, if 0.01 ' identity was not added to the covariance matrix of the symmetric model and the covariance was left to be lowrank, then any classifier which was bayesoptimal along the positivevariance directions would be bayesoptimal, and could behave arbitrarily poorly along the zerovariance directions, still being vulnerable).	0
this concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets.	0
 it is unclear if what is lacking from the nn is explicit regularization, or just more data.	0
while completely alleviating this concern may once again be quite difficult/impossible, it could be significantly alleviated by generating training samples dynamically (at every iteration) instead of generating a dataset in one shot and training on it.	0
4. a suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets.	0
1) authors write: 'for example in human activity logs, the video data can be missing in bathrooms by ethical reasons but can be supplied by environmental sensors which have less ethical problems.'.	0
labels for supervised learning might in many remote sensing tasks be scarce, but to get any old labels and pretrain a reasonable proxy task is still easy, as map data is abundant.	0
it makes no sense to state that standard pictures often have 'one subject'  imagenet does, but a look at standard datasets like cityscapes or depthinthewild shows that it is not true in general.	0
last but not least, what will the final performance will be given the potentially harmful consecutive reuse of data?	0
as per amdahl's law, upper bounds the potential benefits for data echoing; hence, failing to report is more than a little bit concerning.	0
regular em algorithm, this yields exponential speedup in the number of data points, but is worse in other factors (such as a k^4.5 dependence on the number of mixture components).	0
review:###<strengths>  this paper proposes a new dynamic model named hierarchical multitask dynamical systems (mtdss) as a latent sequence model that enables users to directly control the output of data sequence via a latent code z.	2
 this paper claims that convolutional filters in cnns are not the result of fitting to the input data distribution but they are the optimal solution to a spectral decomposition of the convolutional operator.	0
review:###this paper develops a random walkbased method on top of prototypical networks to address the semisupervised fewshot learning, i.e. when each classification task can access only a fewshot labeled data but many unlabeled data.	0
as the wt should be unitary, if the linear classifier method is reasonably trained, then both methods should lead to the same result, except if the data are poorly conditioned.	0
empirical experiments on couple of molecular graph data suggets that graphnvp approach performs as well as prior approach but albeit without any rule checker.	0
a related shortcoming is that the training data simultaneously comes from all the tasks, whereas prior work has looked at the more interesting setup where tasks arrive sequentially and incrementally.	0
pros: the reverse pass idea is similar to autoencoder paradigm that discards redundant information and only save the essential low dimensional one by comparing the original data and the decoding data.	2
the illustrated sshape example in figure 1 somehow demonstrate the difference of the proposed method with pca, tsne and umap, but the usage of the embedding is not clear since figure 1(d) looks like a 2d visualizing of the original 3d data visualized from certain angle.	0
then it compares uncertain points in the public and private dataset projections and only augments m (again using the black box finetune) using uncertain public examples with many nearby uncertain private samples.	0
diversepublic and nearprivate trade performances on mnist and svhn, but nearprivate does (as might be expected) better when the public dataset is polluted.	0
i agree that dataindependent privacy guarantees are preferable, but this should certainly be discussed.	0
review:###the paper proposes methods for improving the performance of differentially private models when additional related but public data is available.	0
the authors propose to finetune the private model with related but public data.	0
i can't see why such a low number was chosen given that the data is simulated, but it doesn't allow to assess whether the methods proposed would be practical in a real social network  there is work suggesting that misinformation spreads faster than information: https://science.sciencemag.org/content/359/6380/1146 thus it would make sense to take this into account in designing the graph theoretical model.	0
further, the idea of concatenating all tuples of a relational db to be passed through a particular feed forward layer is infeasible for all but the smallest datasets.	0
while i agree with the authors' explanations on the difference between trojan horse attack versus multitask learning (shared data or not), my main concern is the lack of comparison and discussion to another secrecybased attack scheme, the 'adversarial reprogramming of neural networks' published in 2019. in their adversarial reprogramming attack, the model weights also remain unchanged (and unpermuted).	0
in my perspective, they have the same threat model but adversarial reprogramming seems to be even stealthier as it does not use the secret data to jointly train the final model.	0
the main procedure for the task still follows a standard fewshot classification task, but in each episode, the data from a different distribution may be presented together with the query images.	0
on gnnfilm's training stability and regularization: i am interested in the negative result described in the following sentence: 'preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by shchur et al. (2018))' do the authors have any intuition about why the results are highly dependent on the random seed?	0
pros:  the paper incorporates robust optimization into bert training with more data, and shows that together it significantly improves bert's performance on downstream tasks.	2
the idea is good, but it seems like gans have been suggested for imbalanced data sequences before.	0
eq 5 is true from data processing but the conditional entropy h(h_l | x) is not fixed.	0
using sgd' but on what data?	0
review:###this paper studies the distributionally robust optimization (dro), in the sense that the weights assigned to the training data can change, but the training data itself remains unchanged.	0
### weaknesses  there are certain changes in the training pipeline that makes the comparison with prior work difficult (tables 1 and 2), and find the real contribution of dataaugmentation caused by knnbased information fetching (kif).	0
 it is mentioned that attention based modules scale poorly with large sized datasets.	0
it would be perfectly reasonable to think that most datasets have low bandwidth, but this not a claim that made the authors.	0
nyuv2 dataset is more realistic, but the reported results are not clear to show significant and meaningful differences (see table 2).	0
the added experiment on sun rgbd dataset is appreciated, but all results appear too close (table 2) to suggest one should adopt the proposed method.	0
adding the right inductive biases into each problem may also help mitigate the lack of specific domain data when aiming for high performance in each domain.	0
decision: overall, this paper handles an interesting collection of data, but does not add much interesting novelty to the field of representation learning and fails to leverage some opportunities in those datasets to do sth.	0
it is commendable to prepare the datasets for publication and the basic results the authors show make for good first baselines here, but ultimately do not meet the bar for novelty or scientific insights.	0
### weaknesses apart from the contributions of compilation of different datasets which are already present, and classification results on them, there is not much novelty in the paper.	0
''about datasets'': the paper emphasizes a lot on the lack of standard datasets and metrics in remote sensing.	0
the reviewer also disagrees with using only 1k samples for selecting best dataset to transfer from  explained in minor weaknesses.	0
however, after thinking about this issue quite some time, i am curious if it is possible to obtain 'zero' of the topological loss (so this term is perfectly optimized), but the encoder introduces, e.g., cavities in the data which were not present in the input (e.g., 1dim.	0
pros:  best generated video quality out there (in the kinetics600 dataset)  numerically outperforms baselines in all datasets weaknesses / comments: ' what is the message of the paper?	2
the authors claim that the fact that the kinetics600 dataset is large automatically removes the concern of overfitting.	0
 after lemma 1: generalization may not provide a strong argument here unless further discussed: the constant kernel is obviously bad for learning anything, but the eoc limiting kernel is also pretty bad for predicting anything outside the training data  section 3: typo 'ourselves'  prop 3: specify that phi is the relu  the proof of prop 3 should be more detailed.	0
p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.	0
you do something like this on p.7, but it would be neat to see a histogram like the one you have for eventmnist for one of the realworld datasets to see if it learns the domainrelevant time knowledge you claim that it should learn.	0
i guess this is why the data generation process is a common concern raised by both reviewers 1 and 2. i think the response of the authors is not enough to clarify this.	0
i made my decision mainly based on the following points: [limited task diversity]: i agree with the authors that having a fix offline data for testing different algorithms is important; however the suggestion (dqn logged data on atari2600) is a very limited dataset, in terms of 1) task diversity 2) data collection strategy.	0
[overfitting] i believe having a fixed dataset for testing offpolicy rl is great, however the current suggestion is very prone to overfitting.	0
i can foresee that if the community start to use this benchmark as a test case, soon we will overfit to this task, in a sense that i try algorithm x on offline data, test on atari see the results, tune algorithm x test on atari again, … this way i finally have an algorithm that only learned from offline data, but is performing great on online atari.	0
my concern is whether the authors had considered the cost due to this offline setting, as additional time needs to be spent on training another agent to obtain such logged data, which is not required in the online methods.	0
i understand the offline setting may not consider this issue, but am just thinking if such data can be better used.	0
i liked to see experiments performed on different tasks and datasets but overall that section could be significantly improved.	0
this has the disadvantage of making quantization dependent on query distribution, but in cases where such data is available it will be very valuable to know if incorporating data distributions in quantization process helps performance and to what extent.	0
the approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.	0
but there is lack of suitable data.	0
=========== my major concerns: collection new data using google search then run existing methods has limited contribution.	0
and the new data are related to noisy labels only, which does not cover any input noise such as low resolution, abnormal size, black/blank/carton background etc. the web data searched are of moderate or rather small scales, which doesn’t seem to contribute much to the community.	0
however this paper still contributes a dataset, hence this paper still is some sort of contribution.	0
strength • simple yet effective regularization technique to neural networks for graphstructured data weaknesses • weak technical contribution to the problem • the performance gain is from mostly the use of a semisupervised learning approach based on entropy minimization, which has been developed without consideration of graphstructured data.	0
specifically, the unsupervised progressive learning (upl) problem, requires a learner to consume a stream of data, where each data point is associated with a class, but only very few labels are provided.	0
this is highly concerning as this basically means the entire dataset has been seen multiple times before the final pass over the dataset (the one with the best hyperparameters) is performed.	0
on the other hand, there are shortcomings of the paper, one being the model evaluation on dataset such as mnist, svhn, and emnist.	0
the wbound), but also makes the regulation scalable to large networks and datasets.	0
the experimental results look reasonable and thorough, however the methods are sold on the idea of better representations for data missing from the training set, whereas the results are focused on anomaly detection.	0
 the paper presents a, to my knowledge, novel approach, to avoid the leakage of metadata in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)  the paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below) weaknesses: 1. experimental evaluation: 1.1. the paper only evaluates on the training set.	0
specifically, in experiment 1, the paper learns an embedding and supervises it to be orthogonal to given labels; this is good, but it also somewhat expected that this could be learned when tested on the same dataset.	0
missing assumptions about blackbox calibration approaches: 1. if we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description?	0
2. is it reasonable to assume some constraints on how much data we can get from the blackbox generator?	0
the authors analyze possible causes, and for the first time present two simple yet effective “blackbox” methods to calibrate the gan learned distribution, without accessing either model parameters or the original training data.	0
in my experience, on a single gpu they can verify small models over entire dataset (10,000 examples) within a few minutes; large models may take a few hours, but still quite reasonable.	0
my main concern about this paper is the assumption that the data is separable by a quadratic function, which seems to be very restricted (although this is indeed a progress from those who assume linear separability of the data, e.g. brutzkus et al. arxiv:1710.10174, wang et al. arxiv:1808.04685 and oymak & soltanolkotabi 2019 which has a suboptimal dependency on the condition number of the training data set).	0
minor: low loss vs perfect fitting: i couldn’t find any discussion about this but the authors seem to assume that a zero loss directly implies that the training data is fit perfectly.	0
it seems the data assumption of that paper is stronger (gaussian input), but in terms of the major claim of this paper on mild overparameterization, that paper already made an attempt.	0
theorem 2.5 of [ref1] assumed gaussian data, but that is not the major result of [ref1].	0
however the model they present achieves significantly lower test accuracy on clean data than a standard network, so the practical deployment of such a model seems unlikely in its current implementation.	0
 as said this hybrid approach not only aims to extend mcts to continuous domains but could also be considered as a way of integrating a planning ability into policy optimization that helps dataefficiency.	0
i also understand that when the authors use the word 'noise' they don't really mean noise, but changes in view point etc. some convincing demonstration that such characteristics of dataset adversely affects the training of generative models will be helpful (in addition to one passing sentence in the experiment section about it).	0
a major issuemainly due to the lack of technical details and the lack of promise to provide code/data (unless i missed this)is that the paper does not appear to be reproducible.	0
reasons to accept:  interesting new application of gnns reasons to reject:  incremental modeling contribution  lack of sufficient technical detail on models and dataset  does not appear to be reproducible	0
my main concern is that the comparison to other approaches seem unfair, because (1) substitute models typically use all training data and query the learned model once per training example; the proposed approach uses fewer training data but needs 1800 queries per example.	0
in summary: (i) the paper is wellpresented and provides hyperparameter sensitivity results; (ii) the paper is very interesting, but (imo) it should leave clearer message why one should use this method; (iii) the proposed method has tighter regret, but only in some (datadependent) cases and combines existing methods, limiting novelty.	0
moreover, the author applies the notion of “neuron’s center”, which is a neutral data point that is similar to actual input x, but with differences in particular objects to cause f(x) be positive.	0
here are a few comments and questions 1. my only concern is how scalable is this procedure to highdimensional data.	0
concerning the existing invertible flowbased models for graph structured data, madhawa’s work is one of the first attempts in the literature.	0
the authors dismiss prior work in the introduction but do not provide any direct evidence that prior work is unable to handle the datasets introduced in the paper.	0
2) given the core contributions of new architectures for disentanglement, the lack of any results on real, non synthetic, datasets in order to validate these (for instance, celeba) which significant prior work on disentanglement has been evaluated on is an unfortunate omission.	0
the idea seems wellmotivated and somewhat new though not revolutionary, and the new data sets are nice (though see comments below about how i'm not qualified to evaluate them), but i don't understand why the proposed algorithm wasn't evaluated on existing data sets as well, and i don't understand why it wasn't compared against other algorithms that purport to do the same thing.	0
it makes a little more sense to refer to a trained generator as having high fidelity to the training data set, but tbh i still don't even like the phrase in that context.	0
however i felt that i could not fully see the benefit of the application for medical data because the area, task, datasets etc are not well introduced.	0
pros:  the effectiveness of the methods is experimentally demonstrated using two relevant datasets  the inverted wireless experiment clearly shows the interest of the attention combination strategy cons:  the methods is very simple (combining multiple segment prediction to perform document classifications) making the contribution of this paper quite weak.	2
rnn are renn but restrained to a linear chain structure  in introduction: ... in the domain extremely complex data that is language ... > i'm not sure the sentence is correct  in introduction: the last sentence should be shorten and rephrased.	0
moreover, there is a large body of work in the drug discovery literature that uses sparse experimental data on the interactions of multiple target proteins and multiple ligands to build models that predict the outcome of biological assays for held out protein targets, where this problem is known as drugtarget interaction prediction, but these papers are not referenced in this work (e.g. reviewed in chen et al. molecules 23(9):115, 2018, ezzat et al. 2017, 2018, 2019).	0
some detail is provided about these collections, but a priori, it's not easy for me to judge the quality of these data sources since they haven't been benchmarked previously in the literature.	0
in section 3, the authors discuss the potential concerns regarding privacy, what are the measures taken by the authors when collecting & distributing this dataset?	0
“while cardiologists are able to see these differences, it is hard to conclude that they are ‘real’ by finding the same anomalous signal across multiple time points and patients without a data driven approach.” o in 3 privacy concerns: heartbeats as biometrics section/ paragraph 3: ?	0
the authors use randomized smoothing on a binary linear classifier with logistic loss and deduce a radius of certification that is a function of the probability of flipping labels in the training data and the probabilistic separation p. major concerns.	0
for instance, in figure 1, note that the percentage in the flip rate of the training dataset ranges from 0 to 14%, however the certified accuracy was for q that ranges from 30 to 47.5. will the certified accuracy at a given percentage of the data be the highest for q that match the flip rate?	0
resnets ensemble via the feynmankac formalism to improve natural and robust accuracies, arxiv:1811.10745, neurips, 2019 overall, this paper studies an interesting and important certified adversarial defense against labelflipping attack problems with a focus on certification on each test data, but more experimental verification is needed.	0
a couple questions and concerns: in section 3, the amount of training data (up to 160 pairs of formulas for predicting satisfiability) seems to be very small for a machine learning problem.	0
it would be good to include some real world image dataset, such as cifar, imagenet etc. 3. the algorithm is somehow incremental compared with sporf.	0
the experiments on the scan dataset concern a standard rnn model learned from data.	0
ensembles mainly help multiclass classifier _calibration_ on indistribution inclass data, but they have miniscule to nonexistent utility in classifier ood detection.	0
hendrycks et al. does assume access to ood data, but not ood data seen during evaluation; in this way, they do not assume data is 'known ahead of time,' which they reiterate throughout their paper.	0
they compare to odin, but odin assumes access to ood data from the test distribution.	0
the questions in what follows may help to resolve such concerns:  section 2.1: for imagenet dataset, data imbalance in training set might be the reason of such disparate impact?	0
some experiments are run with cifar and others with 'random' data, but random in which sense?	0
the result on the overlap and the 'specialisation' of the teacher to the student presented in the paper is rigorous (though i did not completely checked the proof), and seems general enough, but it seems a bit trivial: of course if i have no or little error on all my datapoints, i have overlap with the teacher, and since i'm overparameterised and it's a relu network, then the alignment will be manytoone.	0
presentation weaknesses: ' in the synthetic dataset the model cannot tell the difference between correct and incorrect signals at train time.	0
it could be that through datadriven sparse expansion, bioinspired hashing could help overcome the limitation of existing hashing based approaches, but without comparison, we do not know the answer.	0
the paper however looks premature for publication at iclr, for several reasons: ' the thorough discussion of the dataset might be put in supplementary material.	0
strengths:  the paper is clearly written, and the authors have taken great care to describe the unique structures of the data they are modeling.	2
weaknesses: there are three components to the evaluation, and each of them are problematic:  the first evaluation (fig 2) compares the average frechet distance between phrases generated by different models, and within the original dataset.	0
some brief argument is given for why frechet is a good choice here, but it still seems quite tenuous: what does this distance intuitively mean in terms of the data?	0
a better approach might be to create independent plots for each model's output (with the original data), but i'd generally advise against using tsne for this kind of analysis altogether.	0
i think the highlevel idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.	0
the authors rightly “note that for both datasets, the vast majority of words we used require clinical expertise to interpret”, but it doesn’t seem they have given the learned topics to clinicians to judge whether they make sense or are helpful in any ways.	0
in particular, the hessianvector computations and the need to iterate through the dataset scale poorly with model and dataset size, and efficient approximations for these are nontrivial and an area of active research.	0
review:###this paper is aimed at tackling a general issue in nlp: hardnegative training data (negative but very similar to positive) can easily confuse standard nlp model.	0
pros: 1. the data scale is huge, with 40 billion sentence pairs.	2
large scale pretraining like in bert usually benefits from very large datasets, but also very large models.	0
if that is the case, this is expected to see a drop in performance, bt is usually a very effective, but not so much when the monolingual data is noisy or out of domain.	0
i understand that they use the exponential mechanism, but i would have assumed that the dataset needs to be varied in addition to the model.	0
for the structural alert prediction results on diffpool, below i copied and pasted the results from the arxiv version table 3: structural alert prediction results  tox21 | chembl f1macro f1micro rocauc f1macro f1micro rocauc  gin 78.9 68.3 72.6 93.6 76.7 59.2 diffpool 79.2 68.0 75.6 94.5 83.3 59.3 graph unet 71.1 47.6 67.9 92.9 68.1 59.3 lapooldistance 80.6 74.2 73.5 95.2 81.3 59.5 lapoolunreg 81.3 72.8 74.1 94.1 75.8 58.9 lapool3hop 79.1 71.6 74.8 93.8 75.0 59.1  one example: (1) in the iclr version, the f1 score for diffpool is only 48.638 ± 9.916 on chembl data (about 50% of its previous level) but f1 for the proposed method is improved.	0
data augmentation is, however, performed to remedy the lack of training samples in general.	0
comments: 1. it is out of the area of my expertise, but the innovation of combining the pca and lpp to produce the dimensionally reduced representation of the longitudinal data seems not significant enough, especially when the author did not mention why they chose these two methods out of all the available projection/hashing approaches.	0
3.3) “for face recognition, ddl can easily improve the performance by concentrating the discriminative information and for video action recognition, ddl can further accelerate the pipeline by eliminating the frames with insufficient information.” this sounds like ddl behaves differently depends on the task, but perhaps the conclusion driven by the authors cannot be fully correct given that the number of used datasets are limited to make such a general statement.	0
4.2) in section 4, only ytf dataset is mentioned but authors also use iqiyividface challenge dataset.	0
4. the datafree derivation approach makes sense and i understand that this makes the approach theoretically applicable to arbitrary datasets, but the authors do not apply it to other datasets to show that it generalizes in practice.	0
this problem is practical, as it is often preferable to train online while data is collected to make uptodate predictions for tasks (such as in online advertising or recommendation systems), but it has been found that retraining is necessary in order to obtain optimal performance.	0
could one potentially add data more incrementally (rather than half of the training set) and potentially mitigate this drastic change in the problem?	0
i understand that it may be a less expressive model in general, but it is not clear to me why it would not be a competitive baseline on some of the smaller datasets considered here  was this baseline tried?	0
said otherwise, i think that conflicting gradients may appear on some neurons over some data, but will be mostly a stochastic phenomenon which is not necessarily harmful on the long run, much like the stochasticity of picking a sequence of data from different classes in sgd.	0
curious since the model gets around 92.5 with the largest amount of data shown in figure 5(b) but the number reported in table 3 is 92.8. ''''	0
3. how about training deep nets with replicas of the training data but replace the true labels with random labels?	0
however, the following concerns are important to clarify for the authors: 1) compared to existing work, the proposed method can preserve high data utility due to the fact that the proposed method doesn't ensure differential privacy for the discriminator.	0
i could agree with many of the problems that the authors describe, but the proposed solution seems to be a very specific solution that works on a given dataset (for which supervised training data is available), but i do not think it will generalize well to unseen test data in different domains.	0
paper weaknesses 1. although this paper shows the problem of fid for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data.	0
fjd simply follows the fid formula but concatenates the condition embedding to the original data embedding.	0
another drawback of the evaluation is the lack of a proper statistical analysis of the results, given the small data and model sizes.	0
3. is it possible to create a scenario where there are more labeled data from one cluster but less data from another cluster?	0
pros:  i like the paper and the idea behind being able to improve or even train a student network when the original data is not present.	2
pros:  while using synthetic data of gans to assist supervised learning has been shown to be failed in pervious works (e.g. [1] and also shown in this paper, this paper presents a new perspective to utilize gan as a successful dataaugmentation technique in teacher student paradigm.	2
it seems that the prototype learning is used to deal with the lack of data.	0
fig 4: would be useful to have a picture of the samples (right now they’re just in appendix) ' fig 4: in the data generating process, i believe i((x, y); z) = ln(5) = 1.6 nats, but none of your models converge to rates around there.	0
on the other hand, if the dataset remains closedsource then blind review isn’t violated but results aren’t reproducible and hard to follow by the community with the current level of description.	0
review:###this paper presents experimental data supporting the claim that the under aggressive hyperparameter tuning different optimizers are essentially ranked by inclusion  if the hyperparameters of method a can simulate any setting of the hyperparameters of method b then under aggressive hyperparameter tuning a will dominate b. one way to achieve this rather trivially is to do a hyperparameter search for b and then set the hyperparameters of a so that a is simulating b. but the point here is that direct and feasible tuning of a with dominate b even in the case where a has more hyperparameters and where hyperparameter optimization of a would seem to be more difficult.	0
weaknesses: many quantities that should be reported are not: quality of models, diversity of policies, etc. the source of datasets should be clarified.	0
i am not an expert in mibitof, but i guess all the results in section 6.2 can be easily achieved with the real data, right?	0
5. can the model be generalized well for data collected in different experiments (i.e., different tissues) but from the same machine?	0
of course, i know that ivanov et al. require the full data as input during training, but i’m interested in whether vsae can perform inpainting properly even if trained given imperfect images.	0
[the claim] one of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.	0
pros: ' generative modelling of partially observed data is a very important topic that would benefit from fresh ideas and new approaches ' i really like the idea of explicitly modelling the mask/missingness vector.	2
the paper states multiple times that vaeac [ivanov et al., 2019] cannot handle partially missing data, but i don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.	0
weaknesses:  how multimodal are the datasets provided by uci?	0
it seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?)	0
the experiment in 4.1 obviously is a toy data problem  i mean although the data is real, but the data generated was using noisy and rotations.	0
besides, the authors further evaluated the method on miniimagenet features and omniglot dataset, and showcased comparable performance to previous methods but much shorter running time.	0
finally, whether useful rules can be learned for clustering from labeled data is still quite open and authors may want to give some convincing examples of such rules'' for which existing clustering criterion will fail but with learned rules it can be resolved.	0
it means that the scale of the coefficients for these terms changes according to the number of training data, but the sensitivity analysis in fig. 2 does not show such effect.	0
 i am concerned about whether the proposed method works well with harder datasets such as officehome dataset, because each class data are modeled by a simple gaussian distribution in the proposed method. '	0
# concern 1: technical correctness the paper claims at multiple places that the geometry of euclidean space is 'trivial' or 'too simplistic' to meaningfully reflect the structure of the data.	0
the paper states that 'ideally, this step preserves the topology of the data [...]', but this is never analysed.	0
# concern 3: conceptual improvements while i enjoyed the didactic approach of the paper, which first introduces simple test data sets to illustrate the concepts, my main question is about the conceptual improvements that the charts provide in the end.	0
i slightly lean for acceptance but am very borderline, especially as the results on 'real data' are very weak.	0
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough for the following reasons: (a) weak datasets: authors have chosen some standard benchmarks but they do not seem to be convincing as the datasets are too easy.	2
the rumour classification dataset is relatively small, but even more importantly, the experimental results on that dataset are not thoroughly analysed, for instance through an ablation test.	0
(edit 11.8: ' regarding point (1), there is a quantity called observed fisher information in e.g. grunwald (2007) that coincide with eq (1) in the paper, but it is a function of the dataset instead of the model parameter, and can only used to study model parameters near the 'global optima' (as it is applied in grunwald (2007)); it cannot help with choosing between different local optimas as this work claims.	0
however, a part from the form, i have other concerns about the paper:  the main purpose of the paper is to tackle large image datasets that are hard to address by classical gmms.	0
the limitations of the three local curvatures are shown empirically and theoretically cons: experiments seem inconclusive, with no discussion of the results proposed global curvature characterizes the optimal embedding parameters but not a different, efficiently calculable discrete curvature to approximate them analysis of particular graph families doesn’t necessarily inform what to expect from embedding large graph data overall, i lean towards rejecting this paper.	0
strengths: the paper has nice proofs of the theorems, and they show a method with quadratic convergence (but fullbatch training) without having to invert the full jacobian matrix whose size depends on the number of parameters, but rather inverting the gram matrix, whose size depends on the number of training data.	2
(3) also due to (2), the experiments shown in the paper are on toy data and hence lack of strong empirical support.	0
they argue that their most significant benefit over gan priors is the lack of representation error that (1) enables invertible models to perform well on outofdistribution data and (2) results in a model that does not saturate with increased number of measurements (as observed with gans).	0
in my opinion the main advantage in this method is not on having low reconstruction error on observed pixels, which becomes less of a problem for more powerful gan models, but rather the good performance on out of domain data which is somewhat surprising.	0
2) for the uci experiments the comparison is only made against deepensembels or other vi methods, however to the best of my knowledge mcmc methods are superior in this setting given the small dataset size?	0
some of this can probably be attributed to differences in dataaugmentation and model architecture however in general it makes it very hard to compare with other methods when the baselines are not competitive.	0
a possible advantage of using laes to address the pca problem is that they play nicely with sgd, but again, the claim that the sgdlae approach is superior to, say, randomized svd on a data subsample requires evidence.	0
although seresnet shows good performance on some common data, but the squeezeexcitation block might bring in nonrobustness.	0
however, here, one is using a gaussian model to generate a feature space that is easier to train with a neural network (by filling the sparse data by gaussian process interpolation) but also augment the dataset.	0
pros: 1) they collected a new smallscale evaluation dataset for evaluating the quality of semantic representations of various existing embedding techniques for code identifiers.	2
cons: 1) the proposed datasets are very small.	0
1) authors claimed to 'propose an abstractive dialog summarization dataset based on multiwoz (budzianowski et al., 2018)' in the abstract and introduction, which sounds like part of their contribution is creating a new dataset, but in experiment section there's no discussion about how the dataset was created or used at all.	0
2) authors emphasized two drawbacks in the beginning of the paper, but didn't discuss or show any evidence of those drawbacks from data later.	0
overall, the paper has some incremental improvements on the existing methods that dealing with the nonuniform time series data.	0
review:###in the paper, the authors proposed a novel method adding graph architecture for collected data points to utilize not only features but also relative information.	0
i agree this is probably the case, but this would still be a useful empirical result to show the degree of data required for these alternative.	0
lee et al. didn't experiment on mnist variations but many natural image datasets.	0
2. clear writing, with sufficient but not redundant introduction of background knowledge and explanation of both the advantages and drawbacks of existing models (too large computational complexity on highdimensional data).	0
'rating' there are interesting bits of data in this paper, but the overall story is somewhat muddled and some inferences seem to be insufficiently supported by data (12 below).	0
the results on selfsupervised learning task (including the layerwise pruning results) and a subset of training dataset are reasonable and kind of expected, but it is still good that this paper provide solid experimental results to verify this.	0
the main application seems to be extracting lottery tickets faster by downsampling the data however this aspect is again fairly obvious.	0
(5) lacks reference to recent work on cpcstyle objectives for rl (see suggested references below) ## questions (a) why is the latent state variable s_t observed in the model depicted in fig 3 (i.e. part of the input data)?	0
the contribution of this paper is incremental in the context of prior adversarial debasing (ad) approach using essentially same gan for group fairness and prior work presenting ideas of utilizing clean validation data to defend against data poisoning.	0
there might be synergistic effects where the robustness aspects helps the fairness aspect but this comes at the cost of needing a clean validation set (and it only matters with poisoned data).	0
there are other differences in implementation and the choice of datasets, but those are (imo) minor details relative to the core similarity.	0
what’s more, the datasets chosen are all singleclass datasets with a massive amount of data—as far as generative modeling is concerned, these are very datasets with a minimal amount of variation.	0
accordingly, i have substantial concerns that this method will not work well on datasets outside of these highlyconstrained, nearlyunimodal, singleobject, veryhighdata datasets.	0
“but it requires the dimension dx of the data space to be identical to the dimension dz of the latent space” minor nitpick, but i would swap “dx of the data” with “dz of the latent space” in this sentence, to make it clear that the model’s latent dimensionality is constrained by the dimensionality of the data.	0
selfsupervision is way of creating loss from data without labels, but i am not sure what is meant by collecting data this way.	0
however this doesn't mean that shakespeare dataset 'does not provide any interesting insights'.	0
however, the reviewer is concerned with the following questions: the paper is mainly on analyzing the case when the true data has n points instead of on a continuous support.	0
the authors justify this formulation because it allows analysis via lyapunov functions, but it would be very useful to know if it itself is the maximum likelihood estimate under an alternate data model.	0
the experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures.	0
the study reveals that the attention patterns largely depend on the dataset, on some datasets they are sharp, but on others the attention patterns are almost uniform and not so different from the uniform aggregation weights in gnns that does not have attention.	0
the evaluation is done on two datasets, one with examples from nearoptimal players produced by mohex 2.0, and the other from randomly sampled but perfectly labelled examples generated by benzene.	0
the authors dance around this topic, just asserting that they are handling nonstochastic missing data but do not say precisely what is assumed about the relationship between the observed and missing data.	0
the issue is motivated by applications where one modality of data becomes unavailable in the target domain (e.g., when deciding which ads to serve to new users, the predictor may have access to behavior across other websites but not on a specific merchant's website).	0
by transfer, here they specifically mean the ability for pretrained networks to be 'updated' to new, similar datasets either completely (all parameters are updated while being initialised by the pretrained network parameters) or partially (all but the last few layers are kept constant at pretrained parameter values).	0
in the the section 'varying labels with fixed input', the authors mention the use of caltech101 and webcam data, but this isn't mentioned in figure 8, instead, it mentioned cub200, which isn't mentioned in the text.	0
they also mention conclusions from experiments using the food101 and places datasets, but don't show these results anywhere.	0
structural metadata of wikipedia, such as crosslingual document alignments, is deliberately not exploited (some discussion is provided but i would have preferred an empirical comparison of local vs global extraction).	0
however, i am concerned that iclr 2020 may not be the appropriate venue for this paper, as in my understanding dataset release papers are not explicitly solicited in the call for papers https://iclr.cc/conferences/2020/callforpapers .	0
however the experiments are quite limited, with only 2 training datasets.	0
review:###pros: 1. this work suggests a surprisingly elegant approach to identify data with corrupted labels.	2
cons: 1. the biggest concern is the generalizability of auls to datasets that are more challenging than cifar10 and cifar100.	0
the metric is also dependent on the considered noise level range; the plot of ece vs noise level can be illustrative and informative while the averaged value of ece over all noise levels can be misleading (at least, we may want to have some discount factor to account for the fact that with noise level 0 we strongly care about calibration, while for very high noise levels calibration doesn’t give us much information since the useful patterns in data may be corrupted and it may be impossible even for humans to classify such objects, e.g. on figure 1topleft for noise level 100, the digit 6 is corrupted so much that we wouldn’t care about calibration for such inputs but just want to maximize the entropy).	0
the property that the algorithm 'preserves exponential form of the original data distribution, if one exists' is interesting in principle, but it is unclear if any real data would anyway precisely have such a distribution; what happens if the data is not exactly in an exponential family?	0
section 1.5 claims the algorithm 'preserves the low dimensional sufficient statistics of the data': it is not clear what 'preserves' means here, certainly it seems the decoder in theorem 7 uses the same kinds of sufficient statistics as in the original data, but it is not clear that hat(x) would somehow preserve the same values of the sufficient statistics.	0
simulations are constructed to verify the arguments in the paper but there is no experiments on real dataset.	0
the paper compares the proposed method against behavior cloning, which is known to perform poorly under limited data.	0
the authors state “we believe this finding to have farreaching consequences as it directly contradicts the popular hypothesis about copying caes.” the paper definitely shows some empirical evidence that supports the claim that copying does not occur, but these findings are all done with a single seed and by considering small subsets of datasets (celeba and stl10).	0
the authors state “the loss curves and reconstruction samples do not appear to reflect the notion of dataset difficulty we defined in section 2.3” and “this lack of correspondence implies that the intuitive and neural network definitions of difficulty do not align.	0
experimental concerns  is there a good reason to not try to compare on publicly available datasets like those used in the pointcnn paper (focusing only on the nontemporal versions of the model)?	0
review:###the paper proposed an unsupervised anomaly detection method for the scenarios where the training data not only includes normal data but also a lot of anomaly data.	0
how to determine which data samples are anomalous is a key to the success of the model, but the proposed method based on the variance assumption is too intuitive and not convincing.	0
the method is based on autoencoders and reconstruction error, but instead of training the autoencoder using all the datapoints, the method iteratively uses some form of clustering to determine the points which presumably belong to the normal set, and uses them for training the autoencoder.	0
they try to deal with the main challenge of anomaly detection: a lack of certainty on what defines normal data and anomaly one.	0
general data augmentation techniques frequently do not draw examples form a true distribution (here the test distribution or a heldout development set) but rather manipulate real examples through some transformation function (like noise addition, rotation, scaling, warping, etc.).	0
in my recognition, fewshot learning is the scenario where we want to learn from small data, e.g., p can be 0.5 but we have a very small number of data but balanced (n_pos=n_neg).	0
in that case, it may be nice to also include an experiment on a smaller dataset, e.g., mnist, which i believe this has already been conducted but it was in the appendix, to the main body of the paper as well to strengthen the experimental results in the paper.	0
there are two key concerns about this paper: 1) it is well known that both gan and rl are hard to train, not to mention combining them together to joint train in order to deal with data indifferenceability issue of discrete triple generation.	0
review:###this paper presents a method called ‘function feature learning’ which do not learn the data distribution but the parameters distribution of several neural networks types.	0
(i realise the dataset already exists and was presented elsewhere, but this might be worth a footnote).	0
this prohibits the posteriors of two distinct data points to share the same mean but not share the same variance.	0
?when i first read the paper, and looking at the losses in equations 911, i thought that this wasn’t the case (also considering this paper is about unsupervised representation learning), but some sentences and figures make this quite unclear: a. in section 3.2, you say “we train the encoder so that c_i = 1 and c_j = 0 if the input data has ifeature and no jfeature”.	0
i gather that jones et al. established the existence of a neighbourhood wherein one can define a bilipschitz mapping to r^d for suitable d. but how does this relate to latent and data space mismatch?	0
the authors start from the observation that this problem has been approaches either using variational inference models, that scale very well but whose approximations may lead to degenerate results in practice, and diffusion maps, that scale poorly but are very effective in capturing the underlying data manifold.	0
although the experiment shows successes of the proposed method on several datasets, the major weakness of this paper is the lack of technical novelty and detailed analysis of the proposed method.	0
for example, the authors demonstrate the sensitivity analysis fails on the synthetic data, but never explain why.	0
### updated after author response ### the authors have partially addressed my concern on the scalability of the proposed algorithm to highdimensional data.	0
besides, my bigger concern is that the contribution of this work is highly limited, since there are a bunch of data augmentation techniques: cropping, flipping, color space transformation, rotation, noise injection, etc. given this broad selection of data augmentation, as far as i know, noise injection is not the most effective nor popular one.	0
pros: 1. rigorously studying how to augment training data for cnn is important.	2
the model is based on tucker decomposition but the core tensor is decomposed as cp decomposition so that it can be seen as an interpolation between tucker and cp. the performance is evaluated with several nlp data sets (e.g., subjectverbobject triplets).	0
obviously it is not possible to augment data for that, but it would be good to state this somewhere as the idealized goal of this work, which you approximate by data augmentation.	0
this is indeed the case, but it does not seem like here the authors fully overcome this issue: they do have additional weaklysupervised data, but they still strongly rely on linear 3dmm supervision (p6, “pairwise shape loss”, “adversarial loss”), and do not seem to provide experimental evidence that the model will work without it. '	0
this paper points out that they fully exploit the value of unlabeled face data, but there are few evidences in this paper to support that.	0
note that in the method, the algorithm is not doing effective exploration but just randomly explore until you collect sufficient data to solve for a new goal. '	0
thus, the paper is not about imitation learning, but rather about an optimization method that reuses data generated from multiple tasks.	0
rl, on the other hand, would gather the data incrementally, learn, and act right away.	0
similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speedups that allow it to scale to large real world datasets.	0
the test error given by the systems is comparable, but there are clear speed benefits to the proposed method oenn as the other techniques could not be run for a dataset size of 20k, 50k, or 100k.	0
pros: the numerical experimetns conducted in this paper are thorough, and they show interesting observations on the real datasets.	2
i think this paper is an interesting idea, and my only other main concern is the transfer results to other nli datasets.	0
their main results show transfer performance comparing their approach and using an mlp, but it seems that overall, on all datasets, their approach transfers more poorly.	0
some concerns are listed below, 1. if the generated dataset exhibits very good property as the real dataset, it means the data is to some extent perfectly foreseen, and there is little to no privacy, is it contrary to the aim of not leaking the real dataset?	0
2. it is more interesting to see the difference between the distribution of real data and the generated data, however the author only show a simple toy data distribution comparison, i would like to see more comprehensive results about the distribution differences on real dataset , e.g. the tsne embedding?	0
experiments: what does it mean to be application agnostic but restricted to particular datasets and losses?	0
the main concern of this paper is the results are only confirmed on synthetic data, where all the 4 datasets are generated from known bayesian networks (i.e., causal graphs).	0
even with learnt causal models, they were learning a bayesian net from too optimistic data that were indeed generated from bayesian nets, but these are usually not true for real world data.	0
# weak points ## unclear aspects  the authors argue that consistency regularization has had little success so far in semantic segmentation problems since low density regions in input data do not align well with class boundaries.	0
review:###this paper presents a method for subselecting training data in order to speed up incremental gradient (ig) methods (in terms of computation time).	0
but if the baselines weren't thoroughly tuned, it could be the case that ig on the craig subset performs similarly to ig on the full training data, but that neither is actually reaching satisfactory performance in a given domain.	0
2 definition of test loss: authors define the test loss to be crossentropy but in almost all these tasks, what we care about is the taskloss which is 0/1 classification error on the test data and not the crossentropy loss.	0
the second method, dual prior vae (dpvae), modifies the standard vae by introducing a second separate prior for the anomalous data, which is also gaussian but has different mean.	0
the idea of using unlabeled data for continual learning is interesting and to the best of my knowledge this is the first work that suggests using delayed feedback for continual learning but unfortunately they do not consider measuring forgetting and this work seems an online learning method using delayed feedback.	0
also, the used datasets intentionally restrict the possible values for the number of subgraph isomorphisms, but the counts would be exponentially large if we consider practical (dense) graphs.	0
2) the method fits the model using (r)mse loss, but minimizing log errors ((r)msle) would be better considering distributions of response values (counts) of the used datasets in figure 6. fitting the mse loss is not good for such highly skewed cases, and for example, might focus only on the few instances having very large count values.	0
the data in question is randomized but the algorithm does not have access to the identity of the intervention variable.	0
____ pros: this paper contributes some interesting ideas to a recent topic of interest in the communitynamely, that deep generative models assign high likelihood to outofdistribution (ood) data [nalisnick et al., iclr 2019] and how should we address this problem if we are to use them for anomaly detection, model validation [bishop, 1994], etc. this paper makes some careful distinctions between the true data process, the model, and the alternative distribution, which i have not seen done often in this literature.	2
besides the lack of experiments on real data, i find the paper’s material to be a bit disjointed and ununified.	0
the authors never comment or explore whether the problem may 'not' be that these data points are not in the image of the gan, but rather that the optimization procedure doesn't succeed.	0
the data used for rcnn is a pruned version of the data used to train the blackbox.	0
clarifications and concerns: 1. for the dataset considered here, i would like to see the distribution of the irrelevant, clearly relevant and unsure if they are relevant tokens as detected by the rcnn.	0
this assumption is standard in the analysis of fitted qiteration for offpolicy rl (eg, munos and szepesvari '08), but it implies that the algorithm does not need to solve a challenging exploration problem, since the datagathering policy has good coverage.	0
 the proposed method performs better than gduap in the datafree and blackbox setting.	0
ablation study is carried out to compare with baselines like perturbation maximizing only first layer activation, only last layer activation, etc. also on some other settings (blackbox testing, less data) the proposed method outperforms gduap.	0
the experiments are effectively done on toy data, which is fine, but the paper stops at that point.	0
the paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. the writing is also quite unclear, to the point that i often didn't understand exactly what argument was being made.	0
[reviewer's reply:] gem (lopezpaz et al., 2017) and its faster version (agem) (chaudhry, et al. 2018) and other memory based methods such as mer (riemer et al. 2018), erres (chaudhry et al. 2019), they use memory sizes of at most 6mb to store samples but they only do a ''single epoch'' through the data.	0
table2 does give some intuition, but authors should add another row with multitask wl  table3: how well does mdunet do with 9.4% sl data?	0
my main issue here is that at least on the ses set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite).	0
it seems to me that the data and thus the analyses lack the generality needed for the purpose of understanding behaviors of neural networks on real tasks/data.	0
besides, the lack of groundtruth data on nyc taxi dataset and the azure monitor dataset makes it hard to evaluate the effectiveness of the proposed algorithm.	0
weaknesses:  the data used (in particular the method of buggy code generation applied) seems very specific.	0
the second network is trained separately using selfsupervision (specifically, iic, but adapted to use temporal consistency of observations rather than data augmentation).	0
 the authors highlight that their system can be trained in 'less than 5 hours, on a single gpu', while 'xlm uses much more data and training time', but this is quite deceptive.	0
this can have some justification (it might be computationally prohibitive for the authors to pretrain their own models, which might be the reason why they use public models trained on different data) but they should be more upfront about this.	0
the authors claimed that the proposed new evaluation metrics are novel contributions of the paper but there is no discussion on why they are good metrics for evaluating the quality of synthetic datasets nor which metric should be used in what scenarios.	0
note that 5split mnist is not reported in [4], but a recent work has reported hat’s performance on this dataset (https://openreview.net/forum?id=hkluccvkdb) that achieves 99.59%.	0
while i accept the response for the remaining questions from authors but i am still concerned about the weak experiments and an issue brought up by r3 regarding lack of enough comparisons with frcl on any other datasets besides split mnist and pmnist.	0
additional experiments can on different environments structure (city grids, metro network) of complexity of the environments (the width of the cell in the grid) to measure the performance of the algorithm based on hyperparameters (3) realworld data brings more applicationbased matter into the subject, but requires more thorough investigation on the optimality of the solutions, proposed by rl method.	0
besides, the paper lacks details in some parts, like the usage of 3g cellular mobile data, which was not mentioned in this work, although it is emphasized as an essential part if not a significant contribution of this research.	0
related work if the main concern of the paper is still the limitation of other methods which use expert knowledge then it is better to state the usage of additional data (development index) in social equity metric as a reward design part in appendix to justify this reward engineering “we believe that rl has the ability to solve the metro expansion problem” is the statement, which should be substituted by extensive literature review on rl methods used for planning with constraints or specifically graphbased expansion methods.	0
this may not be possible in the coco experiment where the individual labels are not known but it seems quite unrealistic to have a dataset where only pairwise subset relationships are known.	0
simref also doesn't do data augmentation but there's no explanation why it is done for the proposed method and not for this baseline.	0
advantage of using it for unlabeled data is poorly motivated as unlabeled graphs can easily take statistics such as degree as the node labels, which was shown well in practice.	0
“this result extends nagarajan & kolter’s observation about linear interpolation beyond mnist to matching subnetworks found by imp at initialization on our cifar10 networks” > nagarajan & kolter’s observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order.	0
subsequent section are clearer but since i did not understand how is the data really represented it is impossible for me to gain insight on what is actually being done.	0
the authors found that on some datasets, using half of the scans is already enough to reach similar accuracy but speeded up the convergence by a factor of 2. detailed feedbacks:  the paper presents a simple idea that directly uses the nature of jpeg compression.	0
however, my main concern is whether the experiments can be representative enough for large scale experiments (e.g. using nonsubsampled imagenet dataset with parallel training using ssd storage).	0
in table 1, there are rows for just an lstm or just a treelstm, but they seem to be trained with labeled data whereas the proposed method is trained selfsupervised.	0
the authors now write in the paper 'we consider input data in dimension d = 50 and a scalar target function such that , but otherwise iid ’s.'	0
you mention (x.x’)^r being negligible or not depending on r,d etc, but i wounder if there is some kind of simple ‘geometrical’ interpretation (is it a coarse graining of the data in angular space, the ‘high energy’ eigenvalues corresponding to the high frequency, high resolution distinction between very close angles ?).	0
phrases like 'belonging to any random subset of the dataset' suggest a nondeterministic method of selecting an element of the power set of the training data, but it is unclear what to do if more training data arrives in this case.	0
the approaches for domain shift concern to improve robustness of cnns to such shifts in data distribution.	0
i see this somewhat has been empirically shown in fig 2 with accuracy on the original data, but what is the mechanism not to forget what have being learned so far?	0
the main concerns of this paper lie in several aspects: 1. it seems that the authors did not report their comparison to recent sotas (such as lin et al, 2019) comprehensively enough, nor were the benchmark measures (missing several other attacks, especially black box ones), datasets and backbones fully aligned.	0
pros: 1. the idea of using randomized tensor factorization for dense is novel 2. it seems that this defense is robust to large perturbation (epsilon), and the accuracy on clean data is high when combined with pgd adv.training.	2
cons: 1. i don't understand why using randomization in the latent space of the weights can retain the classification accuracy on clean data.	0
my main concern of this paper is that the proposed method was only evaluated on a single benchmark data.	0
taken together, the existing experimental setup potentially creates an unfair advantage for the neural networkbased methods  while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neuralnetwork based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of wikipedia).	0
in particular, it should consider the effect of training the model on one dataset, but evaluating it on another dataset; and discuss how differences in performance (if any) compare to standard methods.	0
review:###this paper proposed a new realistic setting for fewshot learning that we can obtain representations from a pretrained model trained on a largescale dataset, but cannot access its training details.	0
2) is the argument that these semantic attacks somehow capture a more realistic part of the data distribution over all natural images, and therefore it is good to have models that perform well on these semantic adversarial examples even if we're not concerned about an adversary (e.g., because the model might generalize better to other tasks or be more causally correct)?	0
 paper contains some interesting ideas to integrate causality into an autoencoder (but see weaknesses below)  paper proposes a new dataset for evaluating causal mechanisms (but the approach is not evaluated) weaknesses:  the quality of the writing is inappropriate for a scientific venue.	0
 the dataset is potentially interesting, but it is artificial.	0
one main issue with such methods is that the removal itself can throw the image out of the data distribution and therefore result in a drop in confidence, not because of the region's importance, but because of the network's unpredictable behavior in such unseen parts of the input space.	0
pros: owing to its generality (cpc assumes only a weak spatial prior in the input data), and cheap computational cost relative to earlier generative approaches, cpc is already a promising unsupervised representation learning technique.	2
the authors also make the observation that linear separability, the standard benchmark for evaluating unsupervised representations, correlates poorly with efficient prediction in the presence of limited labeled data.	0
cons: the improvements given in the paper are quite useful within their stated domain (image data), but aren't directly applicable to other types of input data.	0
i am sure that some of the methods used here could lead to improvements in the use of cpc for other types of data, but the authors currently don't provide any insight on this issue.	0
the authors should double check the characteristics of the datasets that are used, and see if they lack long history dependence properties in intuition.	0
compared to previous work, the learning curve model not only takes hyperparameter configurations into account, but by training it on offline generated data, it is able to model learning curves across different datasets.	0
this is a very nice study, but you should precise how much you augment the data here (or recall by how much, if you say it earlier).	0
the positive aspects of the work are as follows: first, the elephant in the room for data valuation methods, which is assessing how good or bad a data point is, is against privacy and this work addresses this question for the first time in the (very small) literature.	2
or, one can do such a thing but the method cannot be called an approximation for the true shapley value of data points.	0
all in all, although the paper's experimental and theoretical results are useful and interesting as the use case of 'a valuation method', but is technically incorrect as calling it an approximation for data shapley values makes it not publishable.	0
the motivation for the first method (entropy decreasing along a markov chain due the data processing inequality) seems to only be valid when y := f(x), but not necessarily when y := f(x)  e. for example, let f be the identity function and e be independent to x. how did you resolve this argument against the intuition??	0
concerns: 1) the central empirical result stated is that using this approach allows one to reduce amount of labelled data by 1020 %.	0
the vgg16 network is pretrained in imagenet and finetuned to relevant data but it is not clear for what task?	0
to solve the lack of 3d data, an rgbtodepth model is trained on external available dataset and then applied to images from visual relation dataset.	0
the paper suggests that this data was not used directly to train the model, but was instead used to build a template for generating natural language instructions.	0
while it is true, that is the signal is perfectly recovered, it would follow the structure from this data was obtained, however no such guarantees can be made for nonzero errors.	0
i think the paper is interesting but needs some polishing to make it easier to read and perhaps some improved experiments in real datasets.	0
in this highly synthetic dataset, this feels more debatable: objects are more entangled in the segmentation (which makes using segmentation more challenging), but only weakly, with many frames with no occlusion; furthermore, segmentation provides object shapes as information.	0
this paper introduces the vector quantization but doesn't mention the use of it in streaming data in related work, which kind of blurs the contribution a bit.	0
the idea is to combine online vector quantization with bayesian neural networks (bnn), so that only incremental computation is needed for a new data point.	0
strengths:  coherent formulation of datadependent pacbayes for a metalearning setting that partitions episodic data into a support set (used to compute the datadependent prior) and a query set (used to produce the posterior.	2
of note: 1. evaluate on the coco dataset, which is the current standard for segmentation ; 2. the scribble supervision method of lin et al (2016) is mentioned, but not compared against.	0
as the authors addressed, mahalanobis detector proposed by lee et al. (2018b) requires validation to determine weights for feature ensembling, but the validation can be done without ood data by generating adversarial samples as proposed in the same paper.	0
## strengths  bootstrapping a learned local distance metric to a global distance metric to reduce testtime planning cost is an interesting problem  the paper has nice visualizations / analysis on the toy dataset  the learning procedure for the local distance metric is clearly described  the paper uses a large variety of different visualizations to make concepts and results clearer ## weaknesses (1) missing links to related work: the author's treatment of related work does not address the connections to some relevant papers (e.g. [13]) or is only done in the appendix (especially for [4]).	2
review:###this paper studied the problem of learning the latent representation from a complex data set which followed the independent but not identically distributions.	0
 the paper main claim is the data equality and hyperplane equality are the main strengths of relu, but doesn't give any justification or even intuition into why this is the case.	2
the authors should consider evaluating their method on a dataset that contains nonhealthy subjects, and investigate the performance of the method when it’s evaluated on healthy subjects, but tested on unhealthy ones.	0
the authors first create their own dataset of singing voice data with accompaniments, then use a gan to generate singing voice waveforms in three different settings: 1) free singer  only noise as input, completely unconditional singing sampling 2) accompanied singer  providing the accompaniment 'waveform' (not symbolic data like a score  the model needs to learn how to transcribe to use this information) as a condition for the singing voice 3) solo singer  the same setting as 1 but the model first generates an accompaniment then, from that, generates singing voice firstly, the authors have done a lot of work  first making their own data, then designing their tasks and evaluating them.	0
it is true that this dataset is small in comparison to the training data the authors generate, but it will certainly be cleaner.	0
code is even provided, but data for training is not.	0
also, the authors mention they use k in range of {1,2,3,4} but do not provide details what was useful for each dataset and how is it useful.	0
similarly, they propose coattention mechanism with adaptive gate functions but does not provide any analysis of why they are useful and what characteristics they capture in the data that allows it to select most relevant neighbors.	0
the second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the rgb image and provide input to the system in a way that maintains stateoftheart performance but shrinks the performance gap between the seen and unseen data.	0
the shift in terminology is not an insignificant change, because using 'bias' to describe the problem incorrectly suggests that the data collection procedure is to blame, rather than a lack of data or an overparamatrized learning strategy; i imagine that more data in the training set (if it existed) could help to reduce the gap in performance the paper is concerned with.	0
though the performance on the unseen data does not change much, it raises some concerns about the generalizability of the learning approach they have used: in an ideal world with infinite training data, the network would perfectly accurately reproduce the ground truth results, and there should be no difference between the two.	0
the authors conjecture that this works because the generated samples come from near the data manifold, but indistribution samples and negative samples can be indistinguishable when the generative model is very well trained.	0
i think connecting the current learning rule to the activity of dopamine neurons requires quantitative comparisons with experimental data, otherwise although i agree that the method is biologically inspired, but whether it is biologically plausible is not clear.	0
my guess is that data augmentation also decreases the lipschitz constant of a neural network near the training data points, but regardless of whether this is true or not, it is not clear if and how rugosity is better than lipschitz constant for characterizing the implicit regularization of data augmentation.	0
the authors present the train and validation numbers on their dataset, but it is difficult to know the impact of the result without comparing to a baseline of some kind.	0
also, the singlecomponent solution doesn't seem like a local minimum  but rather the loglikelihood is unbounded here since you can put another component on one of the data points with infinitely small variance.	0
3. you showed the prover has better performance with more synthetic data, but why is your model (generator) better?	0
the labeled data can be tailored to ensure this, but one cannot make it happen for the unlabeled data.	0
in section 4.2, there is another assumption that 'in practice we can balance the dataset', which might be true for the labeled part through sampling, but not necessarily true for the unlabeled part.	0
section 4.2 also assumes that 'the probability can be estimated through the data' but did not mention how large the data needs to be for an accurate estimation.	0
pros:  the idea of training on data from varying quartiles, with the goal of preventing overlyconservative models, is quite intriguing and inspiring.	2
my concern is that it is impossible to compare the costs of these techniques given the current presentation of data.	0
it is usually fine to do store a small subset of examples in continual learning, but should be made explicit, because it may not always be possible (eg if there are data privacy laws).	0
i do however still have issues regarding the memory usage of the method, specifically which data needs to be stored from previous tasks.	0
my understanding based on the (somewhat conflicted responses) of the authors is they store a substantial amount of prior task data, but most of this is only used at test time.	0
pros: their idea of making an adaption to gan architecture by replacing the generator by a classifier to select p from u and using the discriminator to distinguish whether the selected data is from p or u for pu learning is interesting, and benefits from not relying on the class prior estimation.	2
in general, the paper proposed an interesting ganlike network architecture to learn from pu data, but some unclear parts need to be improved.	0
review:###summary: this paper looks at privacy concerns regarding data for a specific model before and after a single update.	0
eg h_i_j is defined to be the taskspecific representation of the source task i but is indexed over both tasks i and j where j is the target, or there is a e_j(x_j) where both indexes are j while e's index is over tasks and x's index is over datapoints.	0
0 for unlabeled data in the figure”, but this is never mentioned again.	0
the unbiased data setup is interesting but still did not provide any insights about generalization from the adversaries.	0
authors did cite peters 2018 and howard 2018 at the beginning of the paper, but may want to explicitly associate them with ‘elmo’ and ‘ulmfit’ when these two terms first occur respectively; 3) for table 2, does the ‘all data’ or ‘data with 100% agreement’ include training data (80%) or just the test data (20%)?	0
the paper glosses over this point by suggesting that 'data' makes this a nonissue, but the paper would be much stronger if these limitations were confronted and some bounds on the chances of meeting the requirements needed for learning provided.	0
the real data, similar to the image case is a bunch of samples from the data distribution (where the stochasticity comes not from the word embeddings but from the tfidf of thte document), but the generator also uses the tfidf representation, so essentially both the generator as well as the real world data use the document as an input.	0
although the paper presents an interesting application of ml, i vote for rejection since i) the paper is similar to previously published works and lacks methodological novelty, ii) the description about the data and methods is not written clearly enough, and iii) the evaluation needs to be strengthened by additional baseline models and evaluation metrics.	0
strengths:  the numerical common sense task and its related challenges are presented and motivated clearly  the data/annotations and their analysis are presented clearly.	2
the experiment can be replicated reasonably (provided that the data is released, as the authors state) weaknesses:  the dataset is too small (230 pairs of units and values).	0
i do not even know where to start, but to give an example: 1st sentence of the abstract reads: “for numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular spacetime sampling patterns and large missing data rates.”, a sentence that misses the verb.	0
this is not true because under the classical setting of differential privacy, the models are trained on clean data but the process of learning is anonymized (see dpsgd from abadi et al.).	0
with all the interesting results presented, i still have the concerns about the sensitivity of the proposed measure:  it is an average over the training data or the selected sample.	0
since the measure is an average over the training data, it has difficulty to differentiate between one network which has almost zero value for part of the training data but large values for the rest, and another network with roughly the same hat{f}(g) value but small values for all training data.	0
 it only concerns the generation of the training data, but not the sampled data from the network (at least not directly).	0
in particular it would be useful to explain in what sense the task considered is related to continual learning (switches between allocentric and egocentric tasks that are not informed by experimenter, but that the rodent has to figure out), because for now it is only briefly stated in the legend of figure 1. analysis of neural data: analysis of neural data using dpca reveals interesting results regarding the representations of correct/incorrect trials, allocentric/egocentric tasks, temporal structure.	0
the updated paper does look more convincing, but my main concern remains  all considered datasets and tasks are synthetic and specifically made for the method.	0
cons: 1) experiments are limited: 1a) the method is evaluated on custom and relatively toy datasets.	0
toy experiments are fine to illustrate the concept, but at some point, we need to move on to actual data.	0
the paper is 'only' looking at the topology, but neglecting the geometry of the data entirely.	0
 the claim about the scale for the vietorisrips complex is factually correct, but the statement is misleading insofar as the paper only extracts the complex at a single threshold, whereas persistent homology integrates the multiscale aspect of realworld data sets.	0
it is easy to train a model that achieves 0 training error but high test error (this is not specific to neural networks; any overparameterized linear model would do, or even consider a model that simply memorizes the training data and predicts a random number otherwise).	0
the central question is why training a deep neural network on real data often results in a model with 0 training error but low test error.	0
d4: why do we need to filter out both users & items (with interactions<10) for yelp, but only users for the amazon datasets?	0
the particular type of weakness explored heremisclassifying random input as 'acceptable'has not been done, as far as i know, but i don't think the results are particularly surprising as they are consistent with what we know pretty well: these models are thrown off by inputs that are outside the distribution of their training data.	0
importantly, these methods are not just techniques for collecting diverse data, but provide a single algorithm for learning the encoder and policy.	0
my most significant concerns have to do with the fact that this approach can essentially only work well if the selfsupervised training of the oneshot system is performed using data that resembles what the demonstration will look like.	0
ultimately, the very highlevel motivation for undertaking this work is fairly reasonable, and the narrow proposed algorithm (train a oneshot imitation system using exploration data) comes through, but it is not presented very well, and i think what precisely the authors are aiming for relative to other approaches is actually a bit confused.	0
further, the authors provided multiple techniques to remove dataset biases, but didn't provide any insights on why a particular technique perform better than the others.	0
pros: 1. the proposed method incorporates the clustering method into model training to get better representation to keep the original data structure in the target domain.	2
in the probably approximately correct (or pac) learning model, the assumption is that the data distribution over labeled examples is correctly classified by some fixed but unknown concept in some concept classes, e.g., by a linear separator.	0
in the agnostic setting, however, the assumption is weakened to the hope that most of the data is correctly classified by some fixed but unknown concept in some concept space, and the goal is to compete with the best concept in the class by an efficient algorithm.	0
this is a fine goal — using a computational model of the brain as a means to better understand its latent processes when analyzing experimental data — but it is worth bearing in mind that actual models of the brain do not have much in common with typical deep learning architectures, even recurrent networks.	0
also, multiple times the paper points out the goal is not to maximize performance, but rather to mimic the animal data.	0
i believe the task is important, but i am not sure if the datasets used are right for this task.	0
my main issues are:  the description of 'our sd dataset' is concerning: 'we do not perform explicit preprocessing, and thus the papers do not follow any standard format ...'  this makes me wonder if the data can be trusted, please describe this more carefully.	0
this is a strategy that looks like mult, but the proportion of data from source/target domain could vary across batches.	0
concerns:  a goal for the use of synthetic data is to enable an extensively large amount of data to train parametric models, which may otherwise not be available because of the constraints on capturing and creating real data.	0
the overall idea of the paper (using not only surface forms of word but also syntactic information for style transfer without parallel data) is very interesting.	0
i also have more foundational concerns with using purely unreferenced evaluation metrics in dialogue (i.e. metrics that aren’t trained on a dataset of (context, response, score) triples).	0
a formal definition of this seems necessary to advance the agenda of generating biased datasets, but the nearest thing provided is the eq in section 3; but this relies strongly on the assumption that is a labelagnostic feature.	0
more generally, my feeling is that this is a truly interesting direction, but i think for it to be a compelling approach the authors need to formally define bias (the sort they are trying to simulate) and then convincingly show that generated samples reflect the sort of biases that plague realworld datasets.	0
on the other hand, there may be an ethical concern with generating datasets that are biased w.r.t sensitive attributes.	0
recently proposed dataset, roomtoroom (r2r), is captured from real indoor environments but only english instructions are provided.	0
pros 1. multiple languages in vln datasets.	0
overall, the dataset gives a chance to study the vln task beyond english, but it is far below a 'crosslingual' vln dataset as it claims in the title.	0
from the mnist results i gained a sense that uniloss involves a lot more design effort but still is not able to even perform better than a straightforward crossentropy (even on this very artificial and abused dataset mnist).	0
 this paper treats the important problem of when a model should be retrained from scratch or incrementally trained as new data becomes available.	0
just to report a few:  fig 2 legend (which by the way is not referred anywhere in the text…) “a oneshot sgd settings as a function of ratio of initial to incremental data.”  are “n” and “n” used interchangably in the notation?	0
 this study proposed a heuristic approach to decide whether to use incremental training or full training of a pretrained model with the availability of new data.	0
the idea is that if the new data is consistent with the previous model (therefore, incremental learning achieves a bounded error), one can resort to incremental learning; otherwise, a full retraining is required.	0
to assess this need, the authors proposed to use reservoir sampling to select data for incremental training.	0
the proposed approach, 'parameterized incremental training', maintains a reservoir of ground truthlabeled data samples at each incremental training epoch, performance is tested against these examples, and if accuracy is below a userspecified threshold, the model is fully retrained.	0
for example even for the simpler unif noise, the proposed method performs poorly versus the existing approaches particularly when less trusted data is available.	0
i find the experiments inconclusive due to lack of statistical significance testing, insufficient data, and inadequate hyperparameter tuning.	0
this can violate privacy in some limited cases where attributes of the dataset were not known previously (e.g., the fact that points from a particular class possess in general a certain feature), but it does not violate the privacy of individuals whose data is used to train the model.	0
if it is too costly, then perhaps let us assume that we have not one but a set of images with missing parts for mi attack (call it the “hole” data).	0
strengths:  the rationale to make nets rely more on shape features is justified  the approach is relatively straightforward and easy to follow weaknesses:  solid tests for adversarial robustness are missing  thresholding in the 'robust' canny seems to perform gradient masking  use of gan after edge detection is questionable (data processing inequality) detailed comments: 1.)	2
to make the empirical evaluation more rigorous and convincing, i think the authors should: (1) repeat the adversarial robustness experiments on more commonlyused datasets such as imagenet and cifar, and also evaluate robustness of their models more thoroughly, using techniques like blackbox attacks.	0
the idea of exploiting generated data has been used by previous works [1], but the paper proposes a novel generation method, which is new and useful for universal domain adaptation.	0
2. only one classifier is shown for the celeba data so it is hard to compare different blackboxes in the evaluation.	0
pros: >modeling heterogeneous sequential data with missing value is an important task, especially in the healthcare domain.	2
besides clarity my main concern about the paper is that it might be overengineering a solution using the exact information provided in the gqa dataset: scene graphs, programs etc. is the model essentially being heavily supervised to infer backwards the data generating process of gqa, might this mean it will heavily overfit to the questions posed in this dataset.	0
incremental improvements are made on a smaller domainspecific dataset with feature engineering and minor architecture tweaks.	0
the authors claim that only focusing on maps better tests generalization (because of the lack of instrument overlap between train and test), but maps is composed of more synthetic data, and it is not clear if the specific biases introduced by the authors just align better to those peculiarities of the dataset, or if they would transfer to more realistic data such as maestro.	0
4. in section 2.2, the training, validation and eval sets for the maestro dataset are mentioned, but is this dataset used for training any of the proposed models?	0
below, i list the issues of the paper:  in this paper, it is suggested that using random matrices is better than learning them from data, but it is not clear what is the advantage.	0
the paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task  the two datasets weren't even combined!	0
 it also felt that the paper's exposition and techniques were somewhat unclear  they seemed to be focused on classification tasks (e.g. the superclass partitioning) but then were trying to generalize to nonclassification problems without satisfactory explanations of how these approaches would generlize  on a more minor note, i felt the discussion in the paper is very specific to vision tasks since language understanding tasks have very different trends and techniques (e.g. bert  where more pretraining data only helps the model).	0
the experiments, although look fine, lack the robustness/generalizability often desired of such complex architectures by trying it on more complex sequence data.	0
for da, this could be done by using a large f x n x m classifier, and selecting the fxn slice that corresponds to the augmentation used (just as was done for sda, but without the dataaugmentation loss term in eq. 2).	0
the opening of the abstract discusses applications to diverse architectures and data modalities, but the experiments are limited to very similar image classification models.	0
pros: 1. the setting is very useful as most of the public datasets are imbalanced.	2
cons: this involves a couple of questions at philosophical level and few at technical level: 1. i find the comparison between infogan and elastic infogan biased in terms of imbalanced dataset experiment.	0
[philosophical] even for an imbalanced dataset, i really dont find infogan performing poorly, especially in figure 1 (center).	0
detailed comments: 1. the first concern to me is the data augmentation since the data augmentation itself seems to incorporate human supervision regarding the predefined transformations.	0
what would probably make more sense if one can extract some more meaningful discrete factors of variation such as e.g. gender / hair color / etc. ' (minor) i find it a bit concerning that the metrics are actually computed on the output of the classifier, which is trained on the real data whereas at evaluation time it is fed with artificially generated images.	0
review:###summary & pros  this paper proposes a tasklevel data augmentation technique that augments labels while data augmentation.	0
the tables use different datasets but provide the same message.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
replacing the l2 norm of the mean with the l1 norm of the aggregated means (note that this is not merely replacing the l2 norm in the perdatapoint kl with the l1 norm) changes the objective significantly but is not discussed in enough detail in the paper.	0
this is not quite correct, each data point is mapped to a simple distribution but the data distribution is mapped to a more complex distribution (i.e. the aggregated posterior ).	0
it is an interesting 'discriminative replays' alternative to 'generative replay' models present in the literature (where 'discriminative' denotes that the replay is not created to generate original data, but rather to achieve loss decrease directly).	0
some material/presentation concerns: # overall  considering a 'fixed initialisation' scenario is arguably not important, as if one could store the initial network weights, one could also just store  heta' and have an empty 'synthetic dataset' to train on.	0
comments regarding experiments from sec 4.1 the results indeed show the performance is greatly improved over using random images from the dataset, however obtaining 45% on cifar10 would hardly be considered “distilling the dataset” as the full set of data yields performances of 90% on this task.	0
— the idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [didericksen, et al., collaborationbased user simulation for goaloriented dialog systems, convai neurips ws, 2017] — even if in a different setting and not clustered) in any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to followup without public datasets, etc. finally, i don’t think the scope of potential impact (if wellexecuted) makes iclr the ideal venue.	0
the idea to use dwt is interesting but not new and the results are only tested on one small dataset.	0
the implementation details, such as the use of memory bank, and different data augmentation strategies are useful, but has very limited novelty.	0
strengths 1) their method achieves a significant improvement in performance over baseline selfsupervised video representation learning methods on 2 datasets: ucf101 and hmdb51 2) the paper presents though experiments with different architectures and training settings in order to be fair to the baselines.	2
weakness 1) comparison to other selfsupervised methods trained on kinetics data but tested on imagenet are missing.	0
in constructing these categories, the authors skim over separate definitions of race and ethnicity (schaefer, 2008) but decide to conflate the two, stating “in practice, these two terms are often used interchangeably.” while this practice may be fine in colloquial english this is not sufficient for defining data class boundaries.	0
### 3) insufficiently compelling experimental results ''the experimental results do not clearly convey the benefits of the two proposed approaches, due to a combination of insufficient/inappropriate baselines, limited number of benchmark datasets and metrics, inconsistencies/lack of detail in the way results are reported and not accounting for random variability.''	0
following on this observation, this work notes that the observation of an out of sample point being scored with a higher likelihood can also happen when the model lacks the capacity to learn the ground truth support patterns of the data.	0
a domain expert could qualitatively see that the model represents advancement for this type of data, but the paper gives no evidence to support this.	0
i am also a bit confused by the choice of architecture (attentional lstms) for nontime series data and the lack of explanation how and why these are used for the different datasets.	0
weaknesses:  datasets are very uncommon.	0
= strong/weak points  the method seems to work in practice  the writing is reasonably clear, and it feels sufficiently precise to reproduce the results (given enough time)  little novelty on modeling side (straightforward extension of seq2seq) and data extraction (straightforward application of control flow graphs)  the central novelty in the analysis (extracting 'kinds') seems to contribute very little to results (cf. tab.	0
rather, i believe the authors should focus on training the models in a way (a) that makes their decisions “uncorrelated” or “conditionally uncorrelated” with respect to unwanted/undesired tasks (to suppress unfairness) , or (b) that ensures that the data of users who participated in the training model is not leaked to an attacker who has a whiteor blackbox access the model.	0
have the authors tried a more conventional but exact alternative for such simple datasets ?	0
pros:  novel/original idea  endtoend learnable, allowing to utilize sgd, which better fits for the largescale dataset and multimodal dataset  well written and easy to follow  competitive results over gbdt, which is commonly used in relevant tasks.	2
review:###the paper proposes deep neural forest (dnf), which targets tabular data and integrating strong points of gradient boosting of decision trees (gbdt).	2
additionally, in figure 4, the hierarchy on imagenet dataset is reasonable, but in figure 6, the hierarchy on celeba dataset seems to be arbitrary and subjective.	0
it aims at calcium imaging data, but in fact only tests its proposed methods on two synthesized data sets.	0
second, the paper title promises 'an application to calcium imaging data', but that application is not actually presented.	0
i understand the two synthetic data sets are meant to assess the viability of the model for the task, but the title calls for an actual application to real data.	0
i also think that this seems to be an incremental method by simply introducing lgan into the traditional da model for data augmentation.	0
actually, i have some concern that because of the domain distribution difference, the similarity matrix wts between target and source data cannot well reflect the true similarity.	0
however, this reliance on ood data not only adds implementation costs (extra memory and computation) but also makes the choice of ood set a crucial modeling decision.	0
this paper makes some incremental but important contributions in relieving odin of its dependence on ood data.	0
the paper reports not only comparisons against sensible baselines but also performs ablation studies to isolate the effect of parametrization choices and their data augmentation strategy.	0
first, the authors proposed to use input preprocessing similarly to odin (liang et al., 2017) but the proposed scheme does not require outofdistribution data.	0
pros:  improved performance on benchmark dataset originality: the novelty of this work is limited.	2
note that in section 8, authors did try to repeat the experiment to askubuntu data, but the result is not convincing.	0
i have one concern with the new dataset used for evaluation, that i didn’t see addressed in the paper.	0
 this paper proposed a strong baseline defense, the knearest neighbor (knn) based data filtering against cleanlabel poisoning attack, which aims to add small noises to (a subset of) training data but maintain the original training labels such that some targeted images at inference will be misclassified.	0
it could be the case that adding the second objective in eq 2 will tend to generate sharp local minima (since by construction it works well for a subset of the data and works poorly on another subset).	0
to make a case for a video sr method that is applicable not only to cgi rendered videos but also to a more general class of natural videos (e.g. camera acquired) additional results on the ntire 2019 video sr challenge dataset should have been provided.	0
the experiment proves that data points in mnist can be outliers for cifar, but minist and cifar basically are very different from each other.	0
implementing this program requires (i) to formulate the tasks for unlabelled and partially labelled training data, (2) to use an approximate but differentiable sampler for the encoder model and (3) to compute the kldivergence for markov chain models.	0
while the method is specifically designed to image classification  which in a way reduces a bit the scope the possible applications but this is a minor remark  i find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets.	0
i may have missed, but i did not really understand in the experimental setup how many additional target instances are used to train the classifier because i guess that the random aspect of the generator can help to generate multiple additional data but in this context the quality of the model can dramatically decrease.	0
perhaps a fairer comparison would be to [7], where a metalearning algorithm is learned in an online setting (i.e. with the tasks coming in sequentially) but where there is access to all previous data. '	0
in particular: seems like the imputed value of a soft label in equation (1) takes into account only the closeness of the unseen label to seen labels (in terms of attributes), but not that if the specific seen label is active or inactive for the data point.	0
2. the proposed method is simple but effective, which performs better or onpar with sotas on zsl benchmark datasets under the evaluation of the harmonic average of accuracies.	0
for example, the quantitative measures being used are “internal” to the problem and seem to presuppose that this is useful, but it’s not clear if this helps do classification better or generate fake data better, or what?	0
the data processing in 2.2 is not a contribution, but borrowed from [1] cons:  it seems the dual discriminators are not necessary.	0
experiments on simple synthetic data are performed to show the effectiveness, but these experiments are too simple, more thorough experiments on real data sets should be conducted.	0
certainly we want estimators that can handle highdimensional gaussians, but can this estimator deal with very highdimensional, very structured data that mi estimators are currently being asked to handle (and not doing a great job of)?	0
it seems to me that copycat is very similar to fgsm, but it is computed over an entire training dataset as opposed to for one image.	0
(3) the results of experiments demonstrate the effectiveness of noisy ssc and noisydr ssc, but this paper not only lacks the description of experimental data, but also lacks the largescale experimental data.	0
further, the theory only guarantees that the uncertainty will be present as the input goes to infinity, but doesn’t guarantee that the uncertainty will increase monotonically as the input moves away from the data distribution.	0
 you mention accumulating error multiple times, but it'd be nice to see the data for that: in tables 13, add another 2 rows each for obj pos/orientation after 5 steps or something along those lines.	0
for example, the authors should be able to directly use one of the bair robotic interaction datasets (https://sites.google.com/berkeley.edu/roboticinteractiondatasets) with their model, which replicates fairly well the synthetic environment used to test the model but has real data, and compare the accuracy of the predictions to a 2d video prediction model such as svglp (denton and fergus, icml 2018) that does not make any assumptions about the 3d world nor has an objectcentric representation.	0
one would think they augmented the clevr dataset with 3d information, but then it appears that each scene is rendered from multiple azimuths and camera positions.	0
5. the paper admits that according to theorem 6.1, “it is impossible to approximate an arbitrary data manifold with latent space being r^{d}”, but in practice, deep generative models like gans can generate goodlooking images.	0
this concept of binarizing information in scrnaseq data potentially enables to process the data with new methods that were previously not applicable weaknesses: 1. only very specific genes work in a switchlike on off state with a viable threshold, there are many genes whose mode of operation are more analog with the level of the expression affecting responses.	0
decision the paper proposes a potentially promising approach, but is poorly presented, evaluates mostly on toy data, shows modest improvements over baselines, and lacks strong baselines.	0
now, the actual issue is that early stopping is not applied consistently between wd and mixreview, as the wd approach has seen the pretraining data only 20 times (“we stop the pretraining after the ccnews data is swept 20 time”), but mixreview continues to be shown ccnews data even after 20 epochs, so there is effectively no early stopping for mixreview.	0
this gets fixed over time thanks to the mixdecay parameter, but as repetitive decaying of the pretraining data occurs, that pretraining data essentially vanishes to ultimately be empty.	0
i found it very interesting, but i also don't think using a synthetic distribution makes sense but i do not have any proposals for how operationalize the paper's proposed definition for measuring exposure bias with real data.	0
i am not sure about this point, since in the literature there are many datadependent preconditioning methods, including adagrad, adam, adareg, etc., but none of them is free of the localminima problem.	0
2 main concerns here: 1. the paper uses the word 'distributed' to mean multisource or clustered data inputs.	0
among the existing directions in this area, it falls into the sample selection direction, but it also takes semisupervised learning based on the likely noisy data into account.	0
furthermore, the data likely to have incorrect labels are not thrown away but used in a semisupervised manner.	0
c) why evaluation of incremental learning on cifar100 is not well (acc difference between old and new is larger than other datasets).	0
the paper contributes three main ideas to succeed at this task, namely 1) use of selfsupervised learning to initialize the representations in a way that doesn't bias them to the labeled data, 2) a robust rankbased metric to generate estimates of similarity/dissimilarity along with consistencybased regularization to improve optimization, and 3) joint optimization/refinement using a combination of labeled/unlabeled losses, as well as ability to learn incrementally without forgeting the original labeled classes.	0
also, the assumption that the system cannot access the source data but must access all source feature seems a significant limitation in terms of privacy issues and communication cost between the target node and the source nodes.	0
the weak points of this paper are as following: 1. the assumption of modeling all tasks as qa might be strong; 2. the baseline from using real data is missing; 3. there are many components that are missing from the discussion, such as the complexity of the language model, etc. for instance, when the model complexity is high, topk sampling could be expensive.	0
cons:  network architecture used for experiments as well as the exact details of the datasets are nonstandard, making results very hard to compare with other papers, so one needs to rely on provided baselines only.	0
for incremental learning, i would personally think of imagenet as a good example and for continual learning of multiple datasets you can consider the existing sequence of 8 tasks benchmarked in [2] and [3] where you can evaluate your method on more realistic images with a total of over 400k images and significant shift in the distributions.	0
we agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period.	0
for example, exploiting well partitioned datasets into different tasks and known task labels is a good starting point, but once such information is not available all bets are off.	0
 it's perfectly tractable to store some small amount of old data for these problems in an 'episodic memory', so one could claim that an entire class of relevant baselines is missing, e.g. agem, icarl, etc. luckily, i am not an adversarial reviewer, but i want to see progress across a more diverse and widely relevant set of continual learning challenges.	0
also, the paper currently lacks intuition about the effectiveness of their policy gradient approach on top of the data collected from an dqnbased agent.	0
the experimental results are very good, however you are using offpolicy replay data and therefore have the issue that using the same data many times can (often) improve performance for any offpolicy algorithm.	0
there are some minor typos, then some key terms of are not sufficiently explained/defined (e.g. g_ heta in sec.2.2 is said to project “the latent data to unimportant features”, but then there is discussion of “in settings where does matter” – i’m confused.)	0
i think, the paper lacks a thorough evaluation on more realistic data, having only a single example in fig. 14.	0
a few remarks/concerns are: 1. usefulness of the dataset: it seems limiting for a factverification dataset to restrict itself to a binary space i.e. entailed vs refuted.	0
i think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ml and nlp are overstated in my opinion.	0
 a troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the entailed/refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion.	0
the paper states that a uniform random policy in the continuous action space of the agent works well, and offsets the data collection to 'any exploration policy', but this is a key requirement if the proposed approach is to be useful.	0
moreover, i appreciate that experiments were conducted on real robot datasets, but this seems to be more of an exercise in latent space anthropomorphism than practical evidence of a feasible control policy.	0
the idea of the proposed method is to essentially keep the 'atom' distribution unchanged between the training and test data, but maximize the divergence between the 'compound' distributions between them.	0
the paper lacks technical novelty other than the training and test data generation approach, but having one available to the community with these apparently desirable characteristics as benchmark data for measuring complex, compositional generalization capabilities, and that could be invaluable to the research community.	0
remarks: sec 3.1: it seems that    au is an important hyperparameter for dividing the noisy data into labeled and unlabeled sets, but in sec 4 i only see that    ao is set to be 0.5 or 0.6 for cifar, what about other experiments?	0
here are a few questions and concerns:  how much does the image matter for the singleimage data set?	0
some are better with 100 labeled examples but don’t scale as well as others when an order of magnitude more labeled data is available.	0
in summary, this paper proposes an interesting test suite for languageconditioned reinforcement learning, but i wish it is more realistic in the language data and more complex in the rl tasks.	0
###pros### 1) the paper proposes an incremental domain adaptation scenario where one domain appears another and only the data from the current domain can be accessed, which is interesting and heuristic to the domain adaptation research community.	0
closely related to the above if i have understood correctly all the experiments including the gene networks are on synthetic data, it would be good however to see if synthetic data can help generalize to real data.	0
however i found the high level motivations given in introduction/sec 3 are similar (at times even the wording) to those of belilovsky et al 2017 which introduced/motivate the data driven approach to this problem.	0
other comments:  in equation (9) theta^{'(i)} should there be an (i) index there, i suspect this is a typo but if it is not can the authors explain how the target differs for different (i)  the data generating process described in sec 5, how does it assure spd, the described procedure (sampling off diagonal entries u(1,1) then randomly setting zeros) does not appear to me to assure spd without further constraints.	0
paper weaknesses 1. experimental settings are clear, however, what makes me confused is that the construction for p_{_x0008_ar{d}} is straightforward for simple distribution like 2d points dataset, however, it might be intractable for complex high dimensional data such as images.	0
the “unseen data” is the one that appears in p_{hat d} but not in p_d.	0
overall, the method looks incremental and experimental results are mixed on small datasets so i vote for rejection.	0
the level of robustness is surprising, but doesn’t seem to square with intuition that more data ought to help.	0
overall, while the authors provide evidence of a better predictive performance with respect to several baselines, i am left a bit unconvinced by the study for the following reasons: 1. lack of relevant baselines: it seems clear that the overall purpose of the approach and the nature of the dataset require fitting a state space model, however, baseline are overall focused on recurrent and autoregressive models which seem underequipped to address these problems.	0
you have shown that your method achieves better performance in a contrived setting, but in order for it to actually be _useful_, it needs to work on real data, which hasn't been shown.	0
 unlike the pretraining task in concurrent work, the model was pretrained not just on multimodal datasets like conceptual captions but also on textonly corpus like bookcorpus and english wikipedia.	0
i do not have much background in this field, but i found the ideas of dks very interesting and novel (assuming this is the first work to make the filter data dependent and learnable.)	0
the outstanding issue is that the final results are not consistent across datasets, baselines, and metrics which is concerning.	0
========= strengths and weaknesses =========  the paper studies the interesting problem of decoupling data/knowledge uncertainty in the commonlyused framework of knowledge distillation.	2
the toy dataset, qualitatively, evaluates the decomposition quality with interesting results but is not enough to make conclusions for realworld datasets.	0
on compressing ensembles:  model compression, https://dl.acm.org/citation.cfm?id=1150464  compact approximations to bayesian predictive distributions, https://dl.acm.org/citation.cfm?id=1102457  distilling model knowledge, https://arxiv.org/abs/1510.02437 on informationtheoretic measures for decomposing total uncertainty as in eq. (4):  decomposition of uncertainty in bayesian deep learning for efficient and risksensitive learning, https://arxiv.org/abs/1710.07283 '[knowledge uncertainty] arises when the test input comes from a different distribution than the one that generated the training data' this is only one way knowledge uncertainty can arise, it could also arise when the test input comes from the same distribution that generated training data, but there aren't enough training data available.	0
e.g., not all physics simulation must have objects  for instance, fluid dynamics or radiative transfer do not have individual objects, but are nevertheless relevant in the context of visual data.	0
a weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40ish trained nns for every dataset and training hyperparameters, but i do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.	0
issues to address:  fitting 6 parameters to 4249 data points raises concerns about overfitting.	0
table 8 also provides such comparison, but please clarify the testing dataset.	0
strengths 1) the introduced dataset is timely and is more challenging than existing reading comprehension datasets, based on the examples presented in the paper.	2
strengths: —the dataset, which is extracted from standardized tests such as gmat and lsat, requires the ability to perform complex logical reasoning.	2
weaknesses: —the dataset seems small to acquire the ability to perform complex logical reasoning.	0
although my concern about the size of the dataset is not satisfied, i decided to increase the score of the paper (weak reject > weak accept) upon looking at the author response about transfer learning.	0
i'm recommending a weak accept for this paper, but only because of my background knowledge about the lsat and gmat, and my prior understanding of reading comprehension and why this would be a really interesting dataset.	0
you'd want to have separate models that create the split than what you're evaluating them with, but even then, you're training them on the same data, so it's still not really clear what to conclude.	0
of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated cv error but as far as i am aware there is a general agreement that getting “closer” to leaveoneout setup is not a good idea, furthermore, in my anecdotal experience 10fold cv for a dataset with only 600 samples can really be problematic.	0
 the method never actually uses the assumption that the datapoints are sampled i.i.d.. the algorithm should still work if the datapoints are not examined in a random order, e.g. consider seeing all the images with label '0' and then all the images with label '1' etc.. this would likely degrade the performance significantly but the method should still work.	0
one place where i lack confidence in the results: i am not very familiar with the datasets used (triangles and letterhigh) nor how standard they are in the graphlearning literature.	0
i concerned about performances of different gnn baseline methods in letter data due to small graph size (with 4.6 nodes in average).	0
the contribution is broadly applicable to other areas in which data collation is more difficult; the authors additionally do a good job of pointing out that their knowledge encoder is not limited to text but can also use other knowledge grounding including images, videos, or a knowledgebase.	0
weaknesses: 1. the first observation says that 'diminishing returns in expected accuracy when more support data is added without altering phi'.	0
the paper obliquely comments that the groth paper gets better results with a more complex backbone network and blames it on lack of augmentation, use of a subset of the data, and a reduced resolution.	0
for the experiments on longtext generation using emnlp news 2017, it is not clear how the data is partitioned as training data, validation data and test data to get the results in figures 4(a), moreover for the results for leakgan, rankgan seqgan, maligan, it seems that they are copied from other papers, but again how the data set is partitioned is not clear for example in seqgan's paper, and most likely, the data is partitioned in a different way, so the results are not comparable.	0
also, it is not clear how is the process to train the segmentation network after the inclusion of each new batch of labeled data (do the segmentation network is trained incrementally with just the new data?).	0
for instance, since cnns are designed to capture invariances in natural images, it is unsurprising that they can generalize better on image data, but it would be quite astonishing if the same still holds true for acoustic data.	0
for example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from section 4. it would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.	0
on the synthetic dataset (3depn), the quantitative results are not as good as those of pcn (which is a supervised method, unlike the proposed method), but the qualitative results look plausible and comparable to pcn results.	0
the description of the left table in table 1 first says that it shows performance on reallife scans, but performance on synthetic data also appears there.	0
the main strengths of this paper are extensive experimental evaluation using both quantitative and qualitative analysis, and significant improvement in performance on reallife data over previous works.	2
one thing that i do wonder is: given that the supervised approaches work better, but that there is not always data available for supervising, could a transfer learning approach be developed, adapting a supervised problem to a non supervised problem.	0
however the experimental investigation on real data does not compare to a baseline, which seems like an essential part of investigating the proposed method.	0
the problem is wellmotivated: since in meta learning we want to leverage information from similar tasks to increate data efficiency, there may be privacy concerns for each of the task owners about both other task owners and the aggregator (metalearner).	0
the paper claims that l2 pruning requires more data, but it's unclear if this really matters since the whole dataset was used to train both methods initially.	0
pros:  compared with previous datasets (e.g., miniimagenet, omniglot), the constructed metadataset is larger and more realistic, which contains several datasets collected from different sources  several competitive baselines are compared on this dataset under different scenarios (e.g., different number of shot) with reasonable analysis.	2
 lack of experiments in larger contour detection datasets (like semantic border dataset or cityscapes).	0
strengths:  stateoftheart performance on datalimited contour detection tasks  nice illustration of how the network refines its predictions over time  demonstrates a contextual visual illusion in a tasktrained neural network  shows that tilt illusion is actually necessary for optimal performance in their model weaknesses:  architecture seems very complicated (unnecessarily so?)	2
 extensive experiments across various settings, e.g. varying the teacher model size, varying the decoding strategies, investigating ban/moe/sequencelevel interpolation, etc.  weaknesses:   it would have been interesting to consider a synthetic data setting such that one has access to the true underlying data distribution, such that approximations are not necessary.	0
while this is an interesting analysis to present, showing that most of the generated queries are nonsensical to humans and there is low interannotator agreement, i have an issue with the experimental procedure here: the victim model is finetuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators).	0
through interviews, the authors learn that the annotators were using word overlap heuristics, but perhaps training the annotators on a small set of the original data would draw a closer example to the victim model.	0
however, all datasets selected in experiment are black and white images with low resolution (28 x 28 ?).	0
selecting a subset (~22.7% of drop dataset) based on the design of the proposed model ('heuristically chosen based on their first ngram such that they are covered by our designed modules'), and compare to other models, which can actually handle a broader set of questions, only on the selected subset seems incomplete and raises concerns about how generally applicable the proposed model is.	0
i like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; i would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more indepth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large?	0
first, this work is somewhat incremental on rgcn ('modeling relational data with graph convolutional networks').	0
to this end, i would encourage the authors to repeat their experiments for synthetic data, but generated under linear and generalized linear scms.	0
3. parallel to the previous point, all experimental results in the manuscript so far concern somewhat lowdimensional data and relatively abundant data .	0
artefacts specific to small numbers of data points however will not have this property and thus have a substantially smaller impact on the learning.	0
main comments to authors: pros: interesting combination of techniques (iaf flows and wgf/stein inference) to do metasampling empirically appealing results as pertaining to raw performance metrics like accuracy weaknesses:  the evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains.	2
i understand this is per datapoint and the metalearning scenario is focused on the perdataset setting, but i find them related enough to consider a discussion.	0
 i am missing a plot similar to figure 4 but instead of being on the synthetic data, being on the application of nlp.	0
i agree that the authors extend a lot on this papers, especially in terms of dataset and completeness of experiments, but they are definitely closely related, and the fact that it is not cited is a serious flaw to the current version.	0
my major concern is related to the last part with experiments on the movielens data.	0
in experiment qta2 the authors (with a small issue, see weaknesses) show that their proposed modification to the loss function causes their outputs to be better matched to the speech conditioning as measured by the fixed embedding network trained in stage 1. finally in qt3 they measure how well their generated faces can be used with the original face embedding network to perform image retrieval (i am a bit unclear on the details of this experiment, see questions) strengths  ' the authors' proposed pipeline and modified loss function offers a generalized framework for jumpstarting conditional gan training ' their pipeline does not assume humanspecified labels, but instead access to paired data from different modalities, which is can easily be obtained for many conditional image generation tasks (inpainting, superresolution, colorization, etc.) ' their pipeline also doesn't seem particularly finetuned for the speechdriven image synthesis task, so it seems reasonable to believe it could be adapted to other tasks ' their results are qualitatively compelling, and they make a convincing efforts in experiments qta13 to quantitatively show that the conditioning information is affecting the output ' the paper is generally wellwritten and easy to follow weaknesses  ' without completing the loop, and showing that the second stage of this pipeline helps with identity matching for speakers, i'm not clear on what the motivation for this particular form of conditional image generation is (this is unfortunate, because their framework is quite general, and it seems feasible that the authors could have applied it directly to a task where the output of conditional image generation is directly useful) ' i interpret the main purpose of experiment qta 2 as validating the effectiveness of their proposed loss modification, it seems like a natural experiment to make this claim more convincing is to compare a generator trained with eq 4. against real images (hopefully getting a much lower matching probability than the 76.65% reported in the first experiment of this section) ' it seems like an important ablation study is testing the effect of jumpstarting the gan training with the pretrained networks of stage 1, and reporting what happens when one or both of these networks are initialized randomly (assuming that initializing randomly hurts performance significantly, this would be further evidence of their framework's strengths) ' the novelty of the proposed approach is limited so far as i can tell (slight architectural modifications, and adding a negative sampling term to the discriminator loss) initial rating  weak accept questions  ' in experiment qta3 i'm confused by how including multiple faces from the same video clip affects measuring retrieval accuracy.	2
it would have been useful also to assess a pretraining on pure rgbd data however, i have two (somehow related) concerns.	0
the approach is similar to recent work on geometryaware rnns (tung et al. 2019), but is applied to more realistic scenes (urban landscapes datasets generated using the carla simulator as opposed to simple tabletop arrangements of shapenet objects) and makes fewer assumptions about the camera pose (6 dof camera parameterization as opposed to 2 dof cameras placed around the objects).	0
review:#### summary the paper compares 3 ways to account for the variability of the dynamics model in the td error computation: (i) be robust (take the lowest target value obtained when evaluating the valuefunction on the training distribution of models); (ii) be bayesian (average over models); (iii) domain randomization (compute td error as usual but randomize the dynamics to obtain a more diverse data set).	0
i think this motivation is fine but it is not directly related to label corruption because we do not add outofdistribution data but rather the label noise.	0
in these settings the modeler can not inspect the raw data samples from the user (due to privacy concerns) and hence all modeling tasks (from data wrangling to hypothesis generation to labeling to model class selection to validation) become far more challenging.	0
pros: given the growing computational power of mobile devices and the importance of privacy for largescale deployment of machine learning, this work is a timely contribution that could augment the ml pipeline for atscale applications dealing with sensitive data.	2
pros:  dynamic graphs are an important but challenging data structure for many problems.	2
on one hand, if we were to assume the margin condition holds for all possible data points (i.e. assumption 3.1), then there is no concern about polynomial dependence on the number of samples, and this is certainly a reasonable assumption in some applications.	0
4. a side question: in realworld systems, the observations are not only governed by pde, but also unknown noisy factors, missing physics, etc. can your method handle the noisy data/labels?	0
1. this is an interesting model to do transfer or lifelong learning but only for convnet architectures with image data.	0
the word 'empirical' is also confusing to me in 'b is approximated by empirical data' because b is not an expectation, but an 'integral' which has no 'empirical' opposite of it.	0
later on it becomes clear that you use constantu training data and that u is part of the input to the model, so it has to be constrained dynamics, but at this stage it is still unclear to the reader.	0
adding gaussian noise makes sense if the data represents quantized continuous data, e.g. bitquantized image data, but i have concerns about the validity of using this method for categorical data (both edge type and node features are categorical in this application).	0
'discussion points and concerns from the reviewer:'  dataset / batching details please mention how the datasets were split for training and testing the models.	0
cons of the paper: 1) the experiments did not display a proper comparison against the hybrid method mentioned in section 1. the experiments also did not compare against adjoint methods in the multiagent example, or in the lowdata regime for the singleagent example.	2
overall, i am currently voting for weak accept, for solid presentation and content, but with with the following problems: it seems that neural odes are most beneficial, over the alternatives, when used with irregular data times and or sparse number of time points.	0
decision: weak reject motivation: the method is incremental and presented in general in a clear way and easy to follow, the authors present a simple but interesting trick to make the triplet loss more effective on a random forest in the case of generalization to a new unlabeled dataset.	0
this method looks incremental to me because it is addressing the problem of pseudolabelling for learning on a new dataset and instead of using confidence measures uses a random forest to assign labels.	0
#2 lack of data sets: only one experiment on one data set is reported.	0
the results instead seems to be hinting that neural nets don't indeed perform combinatorial generalization on their own, but can be forced towards it by supplying them huge amounts of diverse data (which is not true for humans).	0
the authors claim the second factor as the key aspect enabling better generalization, but 1) is equally likely (this would be more in line with a standard data augmentation type result).	0
here, i don’t necessarily mean “prove” in a mathematical sense, but just analyzing the learned representations a bit more rigorously and being able to say something along the lines of: here’s exactly how the trained model represents “lift”; because of reason x, y, z, this representation is completely disentangled from all object representations in the dataset (and ideally from all possible object representations, because that’s really what true systematicity entails, although i highly doubt that any generic model of the type studied in this paper will be able to achieve this, regardless of the amount and type of input it receives).	0
in the comparisons sections, some details on datasets are given, but these seem to refer to data for inference.	0
i recommend this paper for publication, but have some comments that should be addressed: major comments:  it would be good to evaluate how well the learned policies transfer between datasets and architectures.	0
i can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.	0
the main message of the paper is that for distributionally robust learning, regularization plays an important role by avoiding perfect fitting to the training data, at the cost of poor generalization (thus lack of robustness) in some of the possible distributions.	0
learning generative model in online fashion may work well in simple structured data such as mnist, but i highly doubt that the generative model could be trained properly for cifar10 or cifar100, especially in online setting.	0
the authors report brier score for their synthetic calibration method these datasets, but do not report any modeling results.	0
i believe many machine learning datasets have nmar (not missing at random) type of missing data, but not mcar.	0
it is argued in the paper that with high dimensional data, your algorithm is more acceptable, but how would it be with in other cases?	0
pros: 1. the proposed method seems to generalize well to the different datasets with the same network architecture and hyperparameters compared to previous works.	2
 authors have made a strong claim that neural networks can easily fit any training data, but it may be not be true for many datasets.	0
it is not straightforward to apply 3d convolutions on such a data structure, however this paper proposes a method to apply the same regulargrid kernels used in gridbased convolutions to the particle structure.	0
however, one major concern i had was that it seems all of the training data was generated in boxlike environments, which could easily lead to overfitting.	0
 in terms of the generalization ability of regel, the paper has clearly shown that regel is able to generalize to differently shaped graphs, with acceptable cost, but i am wondering for the same dataflow graph, how regel generalizes to different input data configurations (size, modality, etc.)?	0
minor comments: the main message of the paper is clear but some parts still confuse me: 1. i suggest the author to merge the figure 3 and data generation (page 4) part for a better presentation.	0
pros: a new theoretical analysis for multitask learning, which can give insight of how to improve it through data selection.	2
on the other hand, mln are quite powerful for logical reasoning and dealing with noisy data but its inference process is computationally intensive and does not scale.	0
(2) concern 2: fig 3(e), 'grid' data, your algo with kl projection does worse in reward than trpo, which is not unexpected since trpo ignores constraints.	0
review:###i found this paper very easy and clear to follow  the authors present, what i believe to be an elegant, approach to training a gan in the presence of missing data or where many marginal samples might be available but very few complete (e.g. paired) samples.	0
3. one concern i have with discrete representation is how robust they are wrt different dataset.	0
or is the argument that the data entropy term scales with the dimensionality, but the label term does not leading to an imbalance?	0
for the icing dataset, you mention that it is a contribution but do not provide enough details regarding it to allow further research with it.	0
in a real life biological setting, the data obtained at each batch will be more likely different both in term of sequences but also in term of labels.	0
weaknesses:  doesn't the multistep return render the update onpolicy, since the reward sequence is tied to the data collecting policy?	0
the method does not yield clear gains over previous work, but rather a similar performance for classification and segmentation of shapenet and s3dis data is shown.	0
this submission is lacking experiments comparing fedavg to the proposed method under these settings (which can be simulated using available datasets).	0
(eg, why not just take an average or sum over q) say what q_s and q_t are (student and teacher network from context) 'thus, at test time, these scaling factors will not be fixed but rather inferred from data' nit: would not generally call this an inference process.	0
i give a weak accept of this paper due to the following reasons: pros:  the idea of converting a set of data points to one point and rehearse at a meta level is a smart and novel idea.	2
cons:  this works assumes a task incremental setting, during training process task is received one by one, within each task we could assume i.i.d shuffling of the data.	0
the design of the experiment is again very simple (e.g., changing the size of data, switching two setups in different ways) but clear to understand.	0
the synthetic data experiments agreed with theory, but the experiment on cifar10 did have some gap between theory and experiment, although the cifar10 with relu experiment agreed with theory.	0
pros:  there is an exhaustive evaluation and comparison across different types of data with the existing methods along with the sota.	2
cons:  while i liked that an analysis was done to see the robustness of the method on the contaminated data, i would be interested to see a more rigorous comparison in this fully unsupervised setting.	0
concerns: 1) you note that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems to decrease as the dataset becomes more difficult (i.e. cifar100).	0
negatives : the experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : 1. it misses out a recent stateoftheart method (slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way.	0
furthermore, [1] also compares against many other sota methods missed out in this paper on the many other datasets datasets including those in this paper but in a more general multilabel setting.	0
my point is mainly that the presented work is not really a generalization of rsr as it claims to be, but rather it is just using rsr on a leaned embedding of the data.	0
the downside to their generalization bound is that it requires the covariance matrix of the input data must be positive definitive, and it explodes when the smallest eigenvalue is close to zero.	0
one question i have for the authors: in the experiments on the imdb dataset, the authors claim that the generalization error is worst at sigma_{epsilon} = 0, but it appears that the error is actually larger for sigma_{epsilon} = 0.4?	0
review:###1. define/explain data manifold: in the abstract, ``data moments' suggests the data is treated as random variables; the authors could explain more on how they make the connection between random variables and manifolds 2. introduction: include an example of ``models having different representation space' 3. introduction: elaborate more on ``unaligned data manifolds' 4. introduction: the distinction between ``intrinsic' and ``extrinsic' can be made more explicit; as currently written, the difference between ``extrinsic'/``multiscale'/``utilizing higher moments' isn't clear 5. the authors mentioned the hypothesis that highdimensional data lies on a lowdimensional manifold, but in (3.1) they only considered without justifying this restriction 6.	0
one would expect them to perform a bit better than lstms, but that might be contingent on the size of the dataset more than the structure of the inputs.	0
advantage of using it for unlabeled data is poorly motivated: why we can learn smth useful when maximizing the mutual information between graphlevel and patchlevel representations obtained via gnn?	0
graph interpolating activation improves both natural and robust accuracies in dataefficient deep learning, arxiv:1907.06800 ======================= please address the previously mentioned concerns in rebuttal.	0
similar to figure 8, but with actual learnt distributions), and preferably with some meaningful explanations as to e.g. how the additional modes capture or reflect what we know about the data.	0
however image captioning datasets are not mentioned.	0
while reviewing this paper i went back and read the ende evaluation data for the last few years trying to see how often i could reason that images would help and i came up severely lacking.	0
 easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
yes, one of the main strengths of the paper is the task and dataset that will be released.	2
the experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness) disclaimer: i am completely out of this area but it is an easy read and an interesting angle.	0
a pic with an architecture would be really helpful probably more datasets are required to have a more convincing empirical story section 3.4: even though you can't directly optimize for ber, there are ways that can work, instead of just replacing it with ce, for example this https://arxiv.org/pdf/1608.04802.pdf one critique is based on 3.4 i don't understand how can this be extended to multiple axis of intersecting groups e.g. not just mutually exclusive race values, but also gender for example.	0
the imc problem is used to model recommender systems where there can be users/items with few (or even no) observed ratings, but where each user/item is associated with additional metadata or other features.	0
the current paper demonstrates an interesting new approach for imc that does not require any metadata about new users/items, but rather, exploits the structure of the observed ratings themselves.	0
i think you can even use the 19 point cdf output layer and evaluate the data likelihood under this, but i 'don't' think the 19 point l2 distance to the empirical probability distributions is the correct thing to do.	0
comments:  the paper is well written, but it is unclear how the questions are generated during dataset creation process.	0
the optimal choice would be a similar compromise for spatiotemporal data, but of course this would be a huge effort and it would be up to impossible to have access to counterfactuals.	0
this is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to.	0
pros: this is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community.	2
a few small comments: ' there was some analysis of the augmented imdb dataset, but none of the snli dataset.	0
experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.	0
for example, the authors may use 'x sim p_data(x)' (specify variable) or 'z sim p_z' (omit variable), but not both.	0
1 », « together with an analysis parses this data », « anonimize », « bsuite environments by implementing », « even if require » review update: the authors have addressed my concerns, and i look forward to using bsuite in my research => review score increased to 'accept'	0
for some dataset, ntk can be worse than baselines but for some other dataset, ntk can be significantly better than baselines.	0
i do understand that these models are expensive to train and evaluate, but perhaps a smaller dataset might still suffice to demonstrate the value of these choices.	0
they focus on three main points: the need to use raw sensory data collected by the robot, the difficulty of handcrafted reward functions without external feedback, the lack of algorithms which are robust outside of episodic learning.	0
review:###in case of a lack of labeled data, humandesigned rules can be used to label the unlabelled data.	0
this may be out of scope for this current work, but one way to answer that would be to leverage datasets with more fully labelled factorised data (e.g. dsprites [1]) and present how one should leverage these with , in order to identify the original generative factors.	0
2. related, using emnist was interesting, but given the lack of “accepted” generative factors to be recovered, it is hard to tell if the 22 variables found are “correct” or more similar to using a generative model which would entangled the data more (and hence would falsely introduce extra latent variables).	0
q1.4) i highly value enlightening small scale experiments and do understand that computational resources are not available everywhere however i think it would benefit the paper greatly if the proposed method is demonstrated on some reasonably sized dataset (at the very least one of full mnist, fashion mnist, freyfaces).	0
the construction of the dataset focuses on demonstrating that compositional action classification and longterm temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregationbased methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues.	0
the authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with realworld data, but still it can serve a useful and complementary role similar to the one clevr has served in image understanding.	0
review:###this paper presents convergence rates for straggleraware averaged sgd for nonidentically but independent distributed data.	0
it is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive transformer baselines.	0
it can additionally be trained on nonparallel data in an approach similar to iterated backtranslation at sentencelevel granularity, but with language and translation model probabilities for observed sentences coupled by the latent variable.	0
the authors are also to be commended on their use of not just the shah and barber baseline, but also the backtranslationbased techniques, which are generally stronger competitors when monolingual data is incorporated.	0
a few additional suggestions for related work: noisy channel approaches (eg, the neural noisy channel, yu et al, iclr 2017); decipherment (eg, beyond parallel data: joint word alignment and decipherment improves machine translation, emnlp 2014  yes, from smt days, but still); other joint modeling work (kermit: generative insertionbased modeling for sequences, chan et al, 2019).	0
there is only a 3 point gap between wideresnet and the proposed model (92.9% vs. 95.8%) … but on what dataset?	0
3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported.	0
these skills can be used in a modelbased planning (modelpredictive control) with zero 'supervised' training data (for which the rewards are given), but using calls to the reward function to evaluate candidate sequences of skills and actions.	0
the takeaway seems to be that while context is crucial to generalization on these validation tasks, adaptation is not but can indeed be improved in a dataefficient manner with finetuning.	0
''the authors make the following contributions:'' 1. they show that qlearning trained on multiple tasks with a context variable as an input (an rnn state summarizing previous transitions) is competitive to related work when evaluated on a test task even though no adaptation is performed 2. based on these observations, they introduce a new method for offpolicy rl that does not directly optimize for adaptation but instead uses a fixed adaptation scheme 3. the new method leverages data during metatesting that was collected during metatraining using importance weights for increased sample efficiency ''overall, we believe the contributions are significant and sufficiently empirically justified.''	0
architecture parameters has a strong correlation with the generalization ability (via loss of test dataset), and shows this lambda will first decrease but then drastically increase after a certain epoch number on 4 different search spaces.	0
this is especially concerning because claims of statistical significance are made via ttesting, which assumes that the data is normally distributed.	0
 the models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application.	0
since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, ondevice ml) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).	0
main concerns: the experimental section should make experiments on standard datasets (i.e., uscd, trancos, mall, pklot, shangai, penguins) using standard evaluation protocols (mae, game).	0
but this definition of domain distance is more meaningful in the domain adaptation setting (like amazonoffice dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains.	0
replace weighs with weights 5) [8] points out how selfsupervised learning on a large dataset but with a domain shift (yfcc100m) is not as effective for pretraining as it is doing selfsupervised learning on the downstream task's dataset (imagenet).	0
i would say that the method does not have to outperform stateofthe art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability.	0
my current decision is a weak reject, for a wellwritten paper, but some concerns as follows: the results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from cifar10.	0
considering that the premise of the paper is that mlp’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” the mnist visualization of groupselect felt informative, but the xor example for grouping visualizations seemed too easy.	0
the aim of this work is to address this issue by learning how to select representative samples from a dataset for training local linear models to reproduce predictions of blackboxes.	0
by analogy, in this paper, the samples are taken from the entire dataset (i.e. the dataset is subsampled), while the validation loss is effectively an imitation loss formed by treating the blackbox model predictions as a target.	0
pros: ' considers an interesting dataset subsampling variant of the sample weighting metalearning problem. '	2
current ablation shows that eta should be 2.0 for best performance but without a second dataset it is hard to say this value is general.	0
the methods were evaluated on several baseline datasets (e.g., glue, hans) strengths: ' the paper is easy to follow. '	2
strengths 'this paper claims the importance of the local context of a word and shows an effect of their method on the various datasets: glue, and hans.	2
the paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (omniglot) is shown, while the comparison on market1501 is missing; 4) lack of comparison and discussion of other related works.	0
this can be interpreted as follows: the proposed algorithm performs well on easy datasets, however the performance is not much better than other algorithms on fashion mnist, which is little more challenging.	0
on a side note: the paper also incorrectly attributes the synthetic dataset to (morris et al, 2016), but it is in fact from [1].	0
weaknesses: while the authors claim that the need for hyperparameter tuning is reduced, they use a cyclic step size with parameter n, a laplace prior with parameter b, a momentum noise with parameter 0.9, and datasetspecific parameters h0, m, and c. this weakens (or even contradicts) the claim, and raises the question of how much the choice of these hyperparameters affects performance.	0
the label refinery algorithm for the noisy labeled dataset is interesting, but it is not evaluated thoroughly enough to demonstrate its superiority over existing algorithms.	0
(i) i think the experimental evaluation is the weakest part of the paper at the moment which i find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details.	0
my background is not in machine learning with clinical data (so i do not know common datasets) but it is unclear to me why the datasets evaluated on only consist of a very small section of the overall dataset they’re taken from (in case 1).	0
the theorem 2 and 3 are based on the fulldataset statistics, but because they are infeasible in practice, the authors propose to use the batch statistics.	0
### post discussion update ### one of my primary concerns was the benchmark didn't restrict representation learning to a fixed dataset.	0
those ground truth frames are defined as the frames where the movement in the image changes, which is easy on the brownian movement dataset but what about the others?	0
mainstream efficient networks, such as mobilenet and shufflenet, are not designed for cifar10 datasets, but imagenet datasets.	0
particularly, mobilenetv2 is compared against on the cifar10 dataset, but not the imagenet dataset.	0
given my concerns on how this unsupervised approach can scale to reallife datasets, i suggest a weak reject, but i think the proposed method has some interest for the community and i strongly encourage the authors to provide further evidence of performance of their method on more complex vision tasks.	0
as far as experiments are concerned, section 4.1 presents results on mnist, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705].	0
another idea for experiments is doing crossdataset evaluations where different datasets may have different leaf categories but shared high level ones.	0
for the synthetic dataset examples the comparison against veegan appears to be unfair—it’s one thing to report robustness to hyperparameters, but this seems more like the authors have intentionally picked settings for which veegan happens to fail (by halving the width).	0
one main concern of the proposed method is the choice of seed learning rate and duration of explore phase is still somewhat arbitrary and requires tuning for specific model/dataset.	0
please do not use the office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets. '	0
the observation is that in many realworld applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.	0
it will be also interesting to see how does the proposed method perform on largescale datasets such as domainnet and officehome dataset: domainnet: moment matching for multisource domain adaptation, iccv 2019. http://ai.bu.edu/domainnet/ officehome: deep hashing network for unsupervised domain adaptation, cvpr 2017. http://hemanthdv.org/officehomedataset/ 3) the novelty of this paper is incremental as the theoretical results are extended from cortes et al (2019) and zhao et al (2018).	0
weaknesses: 1. comparison to prior work 1.1. to allow a better comparison to prior work, i am wondering why the author did not compare in a setting and dataset prior work evaluated sat solvers.	0
it also looks at squad translations (but, i'd have preferred a bit more depth on one of these datasets over having both, but i understand why you made this rhetorical choice).	0
the paper seems to correlate tasks and languagerelated hints; however getting a language hint is not always available, especially for datasets such as taskonomy [1, see refs below]  which is a dataset for multiple tasks.	0
existing approaches to bert training use bpe with ~30k vocabulary size or roberta with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext103 or billion word.	0
i am open to eventually consider acceptance given the value of the datasets, but the paper would then require a significant overhaul to remove all confusing aspects.	0
concern and additional experiments: 1. please use standard grid graph dataset as used in the literature  max |v| = 361. moreover, i was wondering how do one generate 500 grid graphs with just max 100 nodes ?	0
only 1 dataset and only 64 models  the narrative is lacking, what are the key points that people using nas should be aware of?	0
cons my main concern is that the evaluation of proposed method is quite limited in the sense that the method is evaluated with only single dataset. only basic model is used to compare to proposed model.	2
feedback/suggestions/nits (not necessarily part of decision assessment): 1. cite the definition of continual learning (e.g. with a reference to a textbook or review) 2. a lot of the writing is unclear, wordy, and/or grammatically incorrect  inconsistent verb tense  e.g. 'if one would need .... it is required' should be 'if one would need .... it would require' i'd suggest rewording this sentence entirely, because it's misleading  it says 'retrain on this new dataset (which sounds like train just on the new data), but i guess you mean retrain on all data including the new data.	0
 was discarding all but the strength subclass from the persuasion dataset and empirically motivated decision or just something you did apriori?	0
 particularly given the data filtering and restructuring of some of the tasks, getting a rough estimate of human performance for all the datasets would be quite valuable  i think saying the mnli does not help model performance on disceval is completely valid but claiming it hurts performance could be a stretch given the margins of error.	0
2. the paper mentions multiple times that it does not use statistical models tailored for a dataset or application but instead use deep neural networks.	0
 since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on highdimensional images with more realistic objects as in the clevr dataset other than 2dgrid objects.	0
regarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains.	0
this is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper.	0
there is not much of a discussion regarding the complexity of learning with the proposed losses but it looks like a simple algorithm for learning with equation (1) would have to use the entire dataset to compute it.	0
i do appreciate that the authors conduction various experiments using different dataset and architecture but i do not understand why the authors want to separate the experiments into parts: adaptive methods and adaptive momentum methods.	0
according to my own experience, the artificial convolutional kernels with some prior knowledge may work well on small datasets but tend to fail on larger ones.	0
to me this is the fundamental tradeoff: not how many fewer examples can i learn from, but how much faster is the method than the 'standard' of using the full dataset?	0
several metrics are used (rmse, mse, mape, mae, mre) while results on different datasets are shown in different but not all metrics.	0
specifically i have the following three concerns: 1. defining elbo using samples from the entire dataset may bring in some benefits, but it complicates the calculation of elbo and related distributions when minibatch or single sample are used in learning and inference.	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
the paper proposes to combine the implicit function idea with output mode estimation, however the problem definition is vague, competing bayesian methods and density estimation methods are ignored, the experiments are insufficient with little stateoftheart comparisons, few datasets and clear problems in the learning.	0
there are some technical concerns:  in section 2.1, i’m only familiar with the mnist dataset.	0
one concern is that what if the ood dataset for test is not available at training.	0
review:###summary: this paper proposes a unified model for continual learning and aims to address the following problems: outoftraindomain dataset recognition catastrophic forgetting the outofdomain or open set recognition model is not only used to detect outliers but also for sampling “representative data” of previous tasks for forward (and backward) transfer.	0
the results shows robustness on different dataset include image and audio in the continual learning condition, where the new come data has a different distribution but the model still able to maintain reasonable quality for the previously and newly come examples.	0
while the extension is incremental, they are able to reduce the overall complexity and achieve new stateoftheart on wikitext 103 dataset.	0
they construct a pair of synthetic but somewhat realistic datasets—in one case, the bayesoptimal classifier is 'not' robust, demonstrating that the bayesoptimal classifier may not be robust for realworld datasets.	0
 a discussion of adversarial examples are not bugs they are features (https://distill.pub/2019/advexbugsdiscussion/): nakkiran (2019) actually constructs a dataset (called adversarial squares) where the bayesoptimal classifier is robust but neural networks learn a nonrobust classifier due to label noise and overfitting.	0
3. experimental setup:  one somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the bayesoptimal classifier has nonzero test error in expectation, and moreover the data variance is fullrank in the embedded space.	0
i am concerned that these properties are what drive the bayesoptimal classifier for the symmetric dataset to be robust (concretely, if 0.01 ' identity was not added to the covariance matrix of the symmetric model and the covariance was left to be lowrank, then any classifier which was bayesoptimal along the positivevariance directions would be bayesoptimal, and could behave arbitrarily poorly along the zerovariance directions, still being vulnerable).	0
this concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets.	0
while completely alleviating this concern may once again be quite difficult/impossible, it could be significantly alleviated by generating training samples dynamically (at every iteration) instead of generating a dataset in one shot and training on it.	0
4. a suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets.	0
i think it lacks a more thorough evaluation and description of the dynamics observed during the genetic evolution, and the performance of the baikal loss on other datasets (my quick experients with it on imagenet diverged i did not have the time necessary to tune the hyperparameters).	0
it makes no sense to state that standard pictures often have 'one subject'  imagenet does, but a look at standard datasets like cityscapes or depthinthewild shows that it is not true in general.	0
 authors say that 'polynomial dependence on the rank, the error, and the condition number, make these algorithms impractical on interesting datasets'  but it is not clear whether they mean that their algorithm is also impractical?	0
then it compares uncertain points in the public and private dataset projections and only augments m (again using the black box finetune) using uncertain public examples with many nearby uncertain private samples.	0
diversepublic and nearprivate trade performances on mnist and svhn, but nearprivate does (as might be expected) better when the public dataset is polluted.	0
quantitatively, they demonstrate that framereversal is meaningful for the somethingsomething dataset but less for kth because those actions rely more on spatial information than temporal (i.e., running, clapping).	0
2. lack of support for qualitative claims the paper makes 3 claims based on qualitative examples shown (see above bullets in summary); these claims could be easily substantiated qualitatively (by evaluating over the dataset or a subset of it).	0
the distinction is that ooe samples are from same dataset but come from classes not represented by the support set.	0
strengths the paper proposes benchmark datasets for outofdistribution detection of fewshot classification.	2
it is clear that shaw et al. (2019) didn't experiment on overnight dataset, but setting up the baseline on a dataset should not be classified as ``our method’’.	0
 it is mentioned that attention based modules scale poorly with large sized datasets.	0
weaknesses:  there are some details that are missing from the paper, for example details about the mapping operator, and the specific representations of e_i for the datasets used.	0
it would be perfectly reasonable to think that most datasets have low bandwidth, but this not a claim that made the authors.	0
nyuv2 dataset is more realistic, but the reported results are not clear to show significant and meaningful differences (see table 2).	0
the added experiment on sun rgbd dataset is appreciated, but all results appear too close (table 2) to suggest one should adopt the proposed method.	0
decision: overall, this paper handles an interesting collection of data, but does not add much interesting novelty to the field of representation learning and fails to leverage some opportunities in those datasets to do sth.	0
it is commendable to prepare the datasets for publication and the basic results the authors show make for good first baselines here, but ultimately do not meet the bar for novelty or scientific insights.	0
### weaknesses apart from the contributions of compilation of different datasets which are already present, and classification results on them, there is not much novelty in the paper.	0
''about datasets'': the paper emphasizes a lot on the lack of standard datasets and metrics in remote sensing.	0
the reviewer also disagrees with using only 1k samples for selecting best dataset to transfer from  explained in minor weaknesses.	0
pros:  best generated video quality out there (in the kinetics600 dataset)  numerically outperforms baselines in all datasets weaknesses / comments: ' what is the message of the paper?	2
the authors claim that the fact that the kinetics600 dataset is large automatically removes the concern of overfitting.	0
for example, some datasets would need more computational resources for some steps, but not others.	0
additional comments: active learning methods gives labels to unlabeled samples in different epochs until the budget is used up, but it would be interesting to give the final labeled and unlabeled dataset after budget is used up as a fixed dataset, and then train the traditional passive mixmatch with this.	0
i made my decision mainly based on the following points: [limited task diversity]: i agree with the authors that having a fix offline data for testing different algorithms is important; however the suggestion (dqn logged data on atari2600) is a very limited dataset, in terms of 1) task diversity 2) data collection strategy.	0
[overfitting] i believe having a fixed dataset for testing offpolicy rl is great, however the current suggestion is very prone to overfitting.	0
1. the major concern is with the dataset.	0
2. also, antivirus softwares are applicable on any pdf files, but the model trained with the dataset collected may not be useful in other circumstances.	0
it should compare the same model/software but on multiple different datasets to demonstrate the model's general applicability.	0
however this paper still contributes a dataset, hence this paper still is some sort of contribution.	0
decision i would recommend to reject this submission mainly due to the shortcomings of the proposed dataset,which make the analysis and conclusion of the paper unconvincing.	0
this is highly concerning as this basically means the entire dataset has been seen multiple times before the final pass over the dataset (the one with the best hyperparameters) is performed.	0
on the other hand, there are shortcomings of the paper, one being the model evaluation on dataset such as mnist, svhn, and emnist.	0
 the paper presents a, to my knowledge, novel approach, to avoid the leakage of metadata in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)  the paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below) weaknesses: 1. experimental evaluation: 1.1. the paper only evaluates on the training set.	0
specifically, in experiment 1, the paper learns an embedding and supervises it to be orthogonal to given labels; this is good, but it also somewhat expected that this could be learned when tested on the same dataset.	0
second, it seems like a dataset is built but the stats are missing.	0
the description of experimental results is however missing important details required to evaluate the results presented: ' what are the details of the dataset used to evaluate the approach?	0
 the evaluation of the idea is not complete while it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other blackbox attack methods in the limited query budget regime, i would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.	0
in my experience, on a single gpu they can verify small models over entire dataset (10,000 examples) within a few minutes; large models may take a few hours, but still quite reasonable.	0
i also understand that when the authors use the word 'noise' they don't really mean noise, but changes in view point etc. some convincing demonstration that such characteristics of dataset adversely affects the training of generative models will be helpful (in addition to one passing sentence in the experiment section about it).	0
quality: the paper is easy to follow and nicely written, but with a few minor typo issues: 1. page 1, refers to appendix a.3 but should be for a.4 2. page 2, 'section' is inconsistently capitalized 3. page 6, mentions three commonly used datasets but only mentions mnist and california housing.	0
5. claims are made about how many layers a certain dataset needs for sufficient classification through heuristic experiments; however they are not thorough enough in terms of ablation to fully make this claim.	0
6. extensiveness of experiments  i do like the toy dataset as an example, but to show effectiveness of this framework, a larger breadth of datasets could have been used.	0
reasons to accept:  interesting new application of gnns reasons to reject:  incremental modeling contribution  lack of sufficient technical detail on models and dataset  does not appear to be reproducible	0
i have two concerns as follows: 1. the authors conduct experiments on multiple small kg datasets such as fb15k and wk15k.	0
# minor presentation weaknesses some parts of the notation/explanation don't make sense to me: ' 'let lambda=... be the number of object classes in the dataset.'	0
the paper includes an explanation about the dataset and the task but does not include the experimental environment (used machines and the time to conduct the experiment).	0
unfortunately, there are many concerns which are still not convincingly answered: (i) considering the focus of this work (as admitted by the authors in the rebuttal) is empirical and with limited technical novelty, more comprehensive results on many datasets are required.	0
in section 3, the authors discuss the potential concerns regarding privacy, what are the measures taken by the authors when collecting & distributing this dataset?	0
for instance, in figure 1, note that the percentage in the flip rate of the training dataset ranges from 0 to 14%, however the certified accuracy was for q that ranges from 30 to 47.5. will the certified accuracy at a given percentage of the data be the highest for q that match the flip rate?	0
from the results, i don't think any of the conclusions will change significantly from the dataset size, but still its better to use larger datasets.	0
the specific numbers used in the experiments are reported, but it is not clear how these were chosen, and how one would choose them for a new dataset or model.	0
this already becomes more evident in the mnist (which contains mostly aligned digits) where convnet clearly outperform morf but would likely become more significant when going to realworld datasets, e.g. cifar.	0
it would be good to include some real world image dataset, such as cifar, imagenet etc. 3. the algorithm is somehow incremental compared with sporf.	0
the experiments on the scan dataset concern a standard rnn model learned from data.	0
the questions in what follows may help to resolve such concerns:  section 2.1: for imagenet dataset, data imbalance in training set might be the reason of such disparate impact?	0
presentation weaknesses: ' in the synthetic dataset the model cannot tell the difference between correct and incorrect signals at train time.	0
the paper however looks premature for publication at iclr, for several reasons: ' the thorough discussion of the dataset might be put in supplementary material.	0
weaknesses: there are three components to the evaluation, and each of them are problematic:  the first evaluation (fig 2) compares the average frechet distance between phrases generated by different models, and within the original dataset.	0
i think the highlevel idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.	0
the authors rightly “note that for both datasets, the vast majority of words we used require clinical expertise to interpret”, but it doesn’t seem they have given the learned topics to clinicians to judge whether they make sense or are helpful in any ways.	0
in particular, the hessianvector computations and the need to iterate through the dataset scale poorly with model and dataset size, and efficient approximations for these are nontrivial and an area of active research.	0
large scale pretraining like in bert usually benefits from very large datasets, but also very large models.	0
i understand that they use the exponential mechanism, but i would have assumed that the dataset needs to be varied in addition to the model.	0
3.3) “for face recognition, ddl can easily improve the performance by concentrating the discriminative information and for video action recognition, ddl can further accelerate the pipeline by eliminating the frames with insufficient information.” this sounds like ddl behaves differently depends on the task, but perhaps the conclusion driven by the authors cannot be fully correct given that the number of used datasets are limited to make such a general statement.	0
4.2) in section 4, only ytf dataset is mentioned but authors also use iqiyividface challenge dataset.	0
1 positive aspects:  experiments are conducted on many datasets and show visually convincing results.	2
i understand that it may be a less expressive model in general, but it is not clear to me why it would not be a competitive baseline on some of the smaller datasets considered here  was this baseline tried?	0
i could agree with many of the problems that the authors describe, but the proposed solution seems to be a very specific solution that works on a given dataset (for which supervised training data is available), but i do not think it will generalize well to unseen test data in different domains.	0
paper weaknesses 1. although this paper shows the problem of fid for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data.	0
that is why i think it is crucial to not only show one arbitrarily picked level of domain shift for each dataset/perturbation, but calibration across all levels of domain shift, as for ts and tstarget; since no recalibration is required for those probabilistic approaches this is very straightforward and would be very informative  especially since e.g figure 5 shows that uts has only very minor advantages over ts in many settings.	0
i am not familar with this stock prediction dataset, but the differences shown are often less than 1%.	0
however, i have some concerns: 1. the experiments on the celeba dataset are mainly subjective.	0
i encourage the authors to keep improving their writing of section 3.4. the description of the dataset is much clearer than the previous version, but i still think it is worthwhile to use more space for a better description.	0
on the other hand, if the dataset remains closedsource then blind review isn’t violated but results aren’t reproducible and hard to follow by the community with the current level of description.	0
the nli model results are interesting but for a more fair comparison, i would expect the proposed method compare with a model trained with vqa and coco caption dataset, such as vqae.	0
weaknesses: many quantities that should be reported are not: quality of models, diversity of policies, etc. the source of datasets should be clarified.	0
first there is no performance comparison to stateoftheart gnn models, such as dgcnn, diffpool and gin, etc. at least on the d&d dataset, many existing models report graph classification accuracy over 78.0, but the baseline method used in this paper only achieves 72.5. thus it is not fair to claim the proposed method can retain or improve the performance of existing gnn models.	0
besides, the authors further evaluated the method on miniimagenet features and omniglot dataset, and showcased comparable performance to previous methods but much shorter running time.	0
 i am concerned about whether the proposed method works well with harder datasets such as officehome dataset, because each class data are modeled by a simple gaussian distribution in the proposed method. '	0
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough for the following reasons: (a) weak datasets: authors have chosen some standard benchmarks but they do not seem to be convincing as the datasets are too easy.	2
the rumour classification dataset is relatively small, but even more importantly, the experimental results on that dataset are not thoroughly analysed, for instance through an ablation test.	0
(edit 11.8: ' regarding point (1), there is a quantity called observed fisher information in e.g. grunwald (2007) that coincide with eq (1) in the paper, but it is a function of the dataset instead of the model parameter, and can only used to study model parameters near the 'global optima' (as it is applied in grunwald (2007)); it cannot help with choosing between different local optimas as this work claims.	0
2) for the uci experiments the comparison is only made against deepensembels or other vi methods, however to the best of my knowledge mcmc methods are superior in this setting given the small dataset size?	0
however, here, one is using a gaussian model to generate a feature space that is easier to train with a neural network (by filling the sparse data by gaussian process interpolation) but also augment the dataset.	0
pros: 1) they collected a new smallscale evaluation dataset for evaluating the quality of semantic representations of various existing embedding techniques for code identifiers.	2
cons: 1) the proposed datasets are very small.	0
this paper is well written and executed, but unfortunately, i lean towards rejecting this paper because of a fundamental flaw in the nature of the proposed dataset that limits its applicability to the task of abstractive dialog summarization (more below).	0
my key concern is that the references in the dataset are generated from a small number of templates (budzianowski et.	0
1) authors claimed to 'propose an abstractive dialog summarization dataset based on multiwoz (budzianowski et al., 2018)' in the abstract and introduction, which sounds like part of their contribution is creating a new dataset, but in experiment section there's no discussion about how the dataset was created or used at all.	0
in appendix, table 1 tabulates the training details, but nowhere is it clearly mentioned if the n=500,000 latent codes are sampled from a gan model that was trained on a mixture of datasets (i.e., bedroom, living rooms, kitchen etc.) or individual datasets.	0
pros   nice, novel method  good experimental results cons   paper is poorly written  very few details of the scaling factor variations  only one dataset considered although the paper is written badly and the narrative is muddled, the underlying idea is a nice one, which is executed well experimentally.	2
it is concerned with the examination of pruning experiments for a lenet on the mnist dataset.	0
the results on selfsupervised learning task (including the layerwise pruning results) and a subset of training dataset are reasonable and kind of expected, but it is still good that this paper provide solid experimental results to verify this.	0
wmt ende (authors do not include the sizes of the datasets, but this is 4.5 million sentences) is the only large scale dataset, and the bleu drop is quite large on this dataset compared to the smaller ones such as iwslt.	0
(b) datasets and architectures: authors have used mnist and fashionmnist when discussing the shortcomings of ece.	0
if the concern is their training time on large datasets, they should be at least presented as comparison for the smaller datasets.	0
however this doesn't mean that shakespeare dataset 'does not provide any interesting insights'.	0
the experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures.	0
the study reveals that the attention patterns largely depend on the dataset, on some datasets they are sharp, but on others the attention patterns are almost uniform and not so different from the uniform aggregation weights in gnns that does not have attention.	0
the evaluation is done on two datasets, one with examples from nearoptimal players produced by mohex 2.0, and the other from randomly sampled but perfectly labelled examples generated by benzene.	0
strong points  the proposed model achieved improvement with fewer flops on large scale image classification dataset.	2
— soundness: my major concern is with the datasets that are used for “natural” distribution shift.	0
does robustness on synthetic datasets and synthetic generations of such perturbations transfer to perturbations (of the same kind) but occurring in natural world.	0
pros: 1) an interesting topic of trying to understand generalization and transfer in deep learning 2) multiple types of experiments, including visualizations of loss landscapes at convregence and at initialization, plots of hessian eigenvalues, measuring the deviation of the weights from their initial values, measuring the variance of the gradients of the weights, measuring the transfer between different datasets, measuring the transfer performance depending on the durarion of pretraining.	2
2a) in table 1 and figure 2, it is unclear if difference in generalization between the dataesets is due to similarity to imagenet (by the way, imagenet is only mentioned in the caption of table 1, but not in the text) or due to the inherent properties of the datasets (perhaps some are naturally more difficult or prone to overfitting).	0
by transfer, here they specifically mean the ability for pretrained networks to be 'updated' to new, similar datasets either completely (all parameters are updated while being initialised by the pretrained network parameters) or partially (all but the last few layers are kept constant at pretrained parameter values).	0
they also mention conclusions from experiments using the food101 and places datasets, but don't show these results anywhere.	0
however, i am concerned that iclr 2020 may not be the appropriate venue for this paper, as in my understanding dataset release papers are not explicitly solicited in the call for papers https://iclr.cc/conferences/2020/callforpapers .	0
cons: 1. the biggest concern is the generalizability of auls to datasets that are more challenging than cifar10 and cifar100.	0
simulations are constructed to verify the arguments in the paper but there is no experiments on real dataset.	0
'however, having a model that does not overfit the dataset can be useful, but in this case the decoder of a standard vae should not be regarded as a generative model—that is not its purpose.	0
the authors state “we believe this finding to have farreaching consequences as it directly contradicts the popular hypothesis about copying caes.” the paper definitely shows some empirical evidence that supports the claim that copying does not occur, but these findings are all done with a single seed and by considering small subsets of datasets (celeba and stl10).	0
the authors state “the loss curves and reconstruction samples do not appear to reflect the notion of dataset difficulty we defined in section 2.3” and “this lack of correspondence implies that the intuitive and neural network definitions of difficulty do not align.	0
1: the claim of robustness to camera intrinsics is not solved in principle but due to training using ground truth from multiple dataset .	0
2. the experimental section is entirely composed of nonstandard datasets, and  in general  the manuscript lacks almost entirely in critical details regarding how the datasets are generated; e.g. what is the pilot policy?	0
in that case, it may be nice to also include an experiment on a smaller dataset, e.g., mnist, which i believe this has already been conducted but it was in the appendix, to the main body of the paper as well to strengthen the experimental results in the paper.	0
(i realise the dataset already exists and was presented elsewhere, but this might be worth a footnote).	0
second, the authors do not provide an insight, why pcomplex is better than the complex baseline on svo dataset, but performs similarly on other datasets.	0
my decision is to reject the paper due to methodological issues with the experiments and lack of evidence wrt/ dataset variety.	0
on the positive side, the general point about the necessity of learning rate tuning for adversarial training (described in the fourth paragraph of the introduction) is a very good one, and there may be an opportunity for a more focused application of the proposed algorithm perhaps among further datasets and considering additional, alternative attacks. '''	2
similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speedups that allow it to scale to large real world datasets.	0
the test error given by the systems is comparable, but there are clear speed benefits to the proposed method oenn as the other techniques could not be run for a dataset size of 20k, 50k, or 100k.	0
some concerns are listed below, 1. if the generated dataset exhibits very good property as the real dataset, it means the data is to some extent perfectly foreseen, and there is little to no privacy, is it contrary to the aim of not leaking the real dataset?	0
2. it is more interesting to see the difference between the distribution of real data and the generated data, however the author only show a simple toy data distribution comparison, i would like to see more comprehensive results about the distribution differences on real dataset , e.g. the tsne embedding?	0
the main concern of this paper is the results are only confirmed on synthetic data, where all the 4 datasets are generated from known bayesian networks (i.e., causal graphs).	0
what happens if you do the same but for a dataset with more classes but have the minibatch be smaller than the number of classes.	0
this is not terrible but also not great; for example, achieving 1% error requires a dataset size of 10^4 (realistically, even larger datasets would be required to achieve results with high probability).	0
i know it has a long and important history in machine learning, but if you are interested in biologically plausible learning then it is simply the wrong dataset to start with from both an evolutionary and developmental perspective.	0
the remaining regression datasets are real but also lowdimensional.	0
also, in this equation, the gradient of the loss wrt/ z samples, average of gradients over z samples times..., does not seem to match what the gradient would be given the algorithmic description in the supplementary material, a gradient of the (sample) average z times... ' the abstract states the paper is proposing a method for highdimensional feature selection, but all of the experiments have datasets with max.	0
clarifications and concerns: 1. for the dataset considered here, i would like to see the distribution of the irrelevant, clearly relevant and unsure if they are relevant tokens as detected by the rcnn.	0
my main issue here is that at least on the ses set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite).	0
besides, the lack of groundtruth data on nyc taxi dataset and the azure monitor dataset makes it hard to evaluate the effectiveness of the proposed algorithm.	0
# weak points  there are two real datasets considered for ood evaluation, but i consider there are some flaws in their utilization for ood detection.	0
this argument applies to idd, although to a smaller extent as the datasets are both automotive, but again there are strong visual differences between samples of the same class across the two datasets.	0
the authors argue that this lack of realism of the inserted images make this dataset insufficient for ood detection.	0
however in the first version of their paper, blum et al. propose a mix of foggy driving, foggy zurich, wilddash and mapillary as dataset for odd detection, which is similar with the setup proposed here.	0
## other less important weak points  the choice of dataset in figure 1 , i.e. lost & found, can be misleading.	0
i would propose some sanity checks and look at the classification performances over other id classes, as suggested in the previous section 2) evaluate on a setting similar to fishyscapes lost and found, in which the dataset does not change much, but there are some novel objects.	0
strengths:  the paper is well written with high quality visuals and plots  the paper studies an important problem weaknesses:  the contribution seems to be rather incremental (evaluating existing methods on 2 dataset) and some related work might be missing  although the analysis is well executed, it is not clear what the community learns from the paper although i enjoyed reading the paper, i'd lean towards rejection of the paper.	2
 the experiments are run on 3 different random splits, based on the mean(std) of the evaluation metric, the performance of the proposed method does not vary much compared to baselines, especially on the second dataset, eg., gnn 0.25 / 0.02 compared with genn 0.26. also, glenn < gnn seems to imply including energy is not the most important part for helping the task, but rather the semisupervised joint training truly improves the performance.	0
pros: ' the proposed approach improves over the sot of competitive recent methods for anomaly detection on four image datasets. '	2
note that 5split mnist is not reported in [4], but a recent work has reported hat’s performance on this dataset (https://openreview.net/forum?id=hkluccvkdb) that achieves 99.59%.	0
while i accept the response for the remaining questions from authors but i am still concerned about the weak experiments and an issue brought up by r3 regarding lack of enough comparisons with frcl on any other datasets besides split mnist and pmnist.	0
the goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on fig4.	2
eq 3 : spearman not defined ' 4.3 ' metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them ' 'usually where a, b represent two different models and usually model a has a higher performance (by datasetlevel metric)' : unclear 5 experimental setting table3 : the encoding of the model name is not clear a metric on all the dataset for each model could be computed to decide which one is the best overall how did you choose the tested combinaisons ?	0
review:###the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
as said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
for example, it seems utterance u_i is from speaker z_i in the dataset, but there are n speakers and m utterances.	0
this may not be possible in the coco experiment where the individual labels are not known but it seems quite unrealistic to have a dataset where only pairwise subset relationships are known.	0
skiptzdataset should then offer a better comparison, but here the method struggles to compete with croitoru et al. 2019, which has a 11point higher fmeasure.	0
on handheld190k, the dataset proposed in this paper, is where the method finally shines, but still only offers a 1point fmeasure improvement over croitoru et al. that being said, considering how different the methods are, and how croitoru et al. requires twostage training, there are many benefits to this method.	0
“this result extends nagarajan & kolter’s observation about linear interpolation beyond mnist to matching subnetworks found by imp at initialization on our cifar10 networks” > nagarajan & kolter’s observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order.	0
i suggest to the authors to address the weaknesses pointed out to make the paper more stronger, especially adding few more attributes datasets such as person attributes datasets as noted below in the weakness section.	0
the authors found that on some datasets, using half of the scans is already enough to reach similar accuracy but speeded up the convergence by a factor of 2. detailed feedbacks:  the paper presents a simple idea that directly uses the nature of jpeg compression.	0
however, my main concern is whether the experiments can be representative enough for large scale experiments (e.g. using nonsubsampled imagenet dataset with parallel training using ssd storage).	0
examples: http://arogozhnikov.github.io/2015/06/26/learningtoranksoftwaredatasets.html, http://quickrank.isti.cnr.it/istelladataset/, https://www.cl.uniheidelberg.de/statnlpgroup/nfcorpus/, minor comments concerning section 3.3, in what sense is sgd used to “calibrate” the model?	0
the datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking.	0
it says k was 'chosen to capture 99.5% of the variance within the dataset', but i could not find how exactly it was chosen and what value of k was used (i apologize if i missed).	0
phrases like 'belonging to any random subset of the dataset' suggest a nondeterministic method of selecting an element of the power set of the training data, but it is unclear what to do if more training data arrives in this case.	0
i'm not sure i totally believe that this is a method of outofdistribution generalization, but rather it helps adjust for corruptions and modest dataset shifts which is an important problem itself.	0
strengths 1) the authors present extensive experiments on many datasets.	2
taken together, the existing experimental setup potentially creates an unfair advantage for the neural networkbased methods  while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neuralnetwork based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of wikipedia).	0
in particular, it should consider the effect of training the model on one dataset, but evaluating it on another dataset; and discuss how differences in performance (if any) compare to standard methods.	0
review:###this paper proposed a new realistic setting for fewshot learning that we can obtain representations from a pretrained model trained on a largescale dataset, but cannot access its training details.	0
2. in terms of experiments, this approach is only evaluated on the mathematics dataset, but the argument for relational encoding is pretty general.	0
while this work shows superior performance on the mathematics dataset, i have a few concerns about the generalizability of this proposed architectural change to other problems, as well as the fairness of comparison to baseline.	0
for this, though, it is important that the paper assess its strengths and limitations in comparison to alternative datasets.	2
 paper contains some interesting ideas to integrate causality into an autoencoder (but see weaknesses below)  paper proposes a new dataset for evaluating causal mechanisms (but the approach is not evaluated) weaknesses:  the quality of the writing is inappropriate for a scientific venue.	0
 the dataset is potentially interesting, but it is artificial.	0
in summary, the proposed weaklysupervised moment alignment network (wman) utilizes a multilevel coattention mechanism to learn richer multimodal representations for language based video retrieval.. pros: 1. significant performance improvement on didemo and charadessta datasets.	2
as an example, for the experiments in charadessta dataset, they include scores for different ious levels, but they do not repeat this for didemo dataset.	0
however it is not clear if this dataset will be publicly released or is only for internal experiments.	0
i am concerned how effective this approach could work for largescale dataset.	0
the description of the model was somewhat confusing to me, but my understanding is that the model does the following:  the dataset contains states, and each state has a label that says whether it is safe or unsafe  however, we don’t know the implicit constraints that determine whether a state is safe or unsafe  we learn a latent representation of the states, where we want the safe states and unsafe states to be separated in the latent space  we use a wasserstein distance metric in the latent space to construct a safety cost function the paper gives pseudocode for the algorithm, and experiments in a traffic simulator of the task of driving through a twolane roundabout with four exits.	0
to solve the lack of 3d data, an rgbtodepth model is trained on external available dataset and then applied to images from visual relation dataset.	0
in this highly synthetic dataset, this feels more debatable: objects are more entangled in the segmentation (which makes using segmentation more challenging), but only weakly, with many frames with no occlusion; furthermore, segmentation provides object shapes as information.	0
of note: 1. evaluate on the coco dataset, which is the current standard for segmentation ; 2. the scribble supervision method of lin et al (2016) is mentioned, but not compared against.	0
## strengths  bootstrapping a learned local distance metric to a global distance metric to reduce testtime planning cost is an interesting problem  the paper has nice visualizations / analysis on the toy dataset  the learning procedure for the local distance metric is clearly described  the paper uses a large variety of different visualizations to make concepts and results clearer ## weaknesses (1) missing links to related work: the author's treatment of related work does not address the connections to some relevant papers (e.g. [13]) or is only done in the appendix (especially for [4]).	2
the authors should consider evaluating their method on a dataset that contains nonhealthy subjects, and investigate the performance of the method when it’s evaluated on healthy subjects, but tested on unhealthy ones.	0
strengths:  the paper studies an interesting problem of adaptive mri reconstruction  the review of mri reconstruction techniques is well scoped weaknesses:  the evaluation is rather limited and performed on one, proprietary, relatively small sized dataset  some simple baselines might be missing i like the idea of adaptive sampling in mri.	2
the authors first create their own dataset of singing voice data with accompaniments, then use a gan to generate singing voice waveforms in three different settings: 1) free singer  only noise as input, completely unconditional singing sampling 2) accompanied singer  providing the accompaniment 'waveform' (not symbolic data like a score  the model needs to learn how to transcribe to use this information) as a condition for the singing voice 3) solo singer  the same setting as 1 but the model first generates an accompaniment then, from that, generates singing voice firstly, the authors have done a lot of work  first making their own data, then designing their tasks and evaluating them.	0
it is true that this dataset is small in comparison to the training data the authors generate, but it will certainly be cleaner.	0
also, the authors mention they use k in range of {1,2,3,4} but do not provide details what was useful for each dataset and how is it useful.	0
experiments were conducted on the kodak dataset based on the model of balle et al. the proposed method works only slightly worse than balle et al. at low bitrate region, but the gap becomes larger in higher bit rate regions.	0
furthermore, the paper does not show compression results aggregated over the entire kodak dataset, but rather picks 2 images for the main part of the paper, and shows 3 in the appendix.	0
the authors present the train and validation numbers on their dataset, but it is difficult to know the impact of the result without comparing to a baseline of some kind.	0
the released source code includes the lidar, scallop, silverman datasets, but not the phosphorus soil.	0
in section 4.2, there is another assumption that 'in practice we can balance the dataset', which might be true for the labeled part through sampling, but not necessarily true for the unlabeled part.	0
a single model trained for all 15 languages in the xnli dataset can achieve better results than 15 individually trained models, and get even better when unrelated poorlytranslated languages are removed from the multilingual tuning scheme.	0
 consider the creation of an adversarial dataset (of ~3 words in ~3 contexts) where techniques that optimize for the wrong thing will catastrophically fail but the proposed approach succeeds.	0
the experiment can be replicated reasonably (provided that the data is released, as the authors state) weaknesses:  the dataset is too small (230 pairs of units and values).	0
the updated paper does look more convincing, but my main concern remains  all considered datasets and tasks are synthetic and specifically made for the method.	0
cons: 1) experiments are limited: 1a) the method is evaluated on custom and relatively toy datasets.	0
i understand this could exceed the computational capabilities of authors, but why, for example, not to try on a different dataset, as cifar?	0
the authors show that fast weights can be applied in the continual learning setting, but alone they do not perform that well on the more challenging datasets, with mixed results on how much better they are as compared to a naive finetuning baseline, and they definitely lag behind synaptic consolidation methods.	0
the submission:  briefly describes the neural semantic encoder (nse) model (munkhdalai and yu, 2017)  claims that the dotproduct attention mechanism in nse is too simplistic for the task of summarization because dotproduct attention does not capture wordsentence and sentencesentence correlations, the latter two being important to summarize a document, and suggests utilizing the additive attention mechanism along with the pointergenerator mechanism instead  introduces the “hierarchical nse” model and a selfcritical sequence training scheme to optimize for rouge  provides empirical results for these models and for other relevant baseline models on the cnn/daily mail dataset at a highlevel, this submission is unpolished, imprecise, lacks technical (both experimental and theoretical) rigor in its description and explanation of its methods, and generally does not have a significant contribution, which is why i believe it should be rejected.	0
further, the authors provided multiple techniques to remove dataset biases, but didn't provide any insights on why a particular technique perform better than the others.	0
one possible reason is that [1] uses a dictionary of size |s| (the size of the seen dataset s), but the proposed method uses 23 times of them, as stated in appendix c.1 and figure a8.	0
the paper brings results on a densenet trained on imagenet, but it would also have been interesting to see the resulting visualizations with a pretrained model on an xray dataset (for example chestxray from nih).	0
my main issues are:  the description of 'our sd dataset' is concerning: 'we do not perform explicit preprocessing, and thus the papers do not follow any standard format ...'  this makes me wonder if the data can be trusted, please describe this more carefully.	0
on the positive side, the results are promising (especially in terms of rougel), even if limited to a single dataset, and previous summarization maed (memoryaugmented encoder/decoder) models have mostly focused on short documents or extractive summarization.	2
i also have more foundational concerns with using purely unreferenced evaluation metrics in dialogue (i.e. metrics that aren’t trained on a dataset of (context, response, score) triples).	0
recently proposed dataset, roomtoroom (r2r), is captured from real indoor environments but only english instructions are provided.	0
pros 1. multiple languages in vln datasets.	0
overall, the dataset gives a chance to study the vln task beyond english, but it is far below a 'crosslingual' vln dataset as it claims in the title.	0
from the mnist results i gained a sense that uniloss involves a lot more design effort but still is not able to even perform better than a straightforward crossentropy (even on this very artificial and abused dataset mnist).	0
this can violate privacy in some limited cases where attributes of the dataset were not known previously (e.g., the fact that points from a particular class possess in general a certain feature), but it does not violate the privacy of individuals whose data is used to train the model.	0
 type: '... dataset, we ...'  figure 4 caption says 'dynamically', but my understanding is that the kernel weights are just learned (adaptive) and doesn't dynamically change with inputs.	0
besides clarity my main concern about the paper is that it might be overengineering a solution using the exact information provided in the gqa dataset: scene graphs, programs etc. is the model essentially being heavily supervised to infer backwards the data generating process of gqa, might this mean it will heavily overfit to the questions posed in this dataset.	0
in summary, i am confused by the inconsistent results on the different datasets and under different attack scenarios (white and black).	0
incremental improvements are made on a smaller domainspecific dataset with feature engineering and minor architecture tweaks.	0
the authors claim that only focusing on maps better tests generalization (because of the lack of instrument overlap between train and test), but maps is composed of more synthetic data, and it is not clear if the specific biases introduced by the authors just align better to those peculiarities of the dataset, or if they would transfer to more realistic data such as maestro.	0
4. in section 2.2, the training, validation and eval sets for the maestro dataset are mentioned, but is this dataset used for training any of the proposed models?	0
results are also presented on the mutag, proteins etc graph classification datasets, but these are really too small to be relevant today.	0
the authors also tried an error correcting code dataset, but the way the results are reported it is hard to understand how good they are.	0
the paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task  the two datasets weren't even combined!	0
1.2) sampling images from the same class as a form of corruption seems novel to me but i feel like it could be dataset specific ?	0
pros: 1. the setting is very useful as most of the public datasets are imbalanced.	2
cons: this involves a couple of questions at philosophical level and few at technical level: 1. i find the comparison between infogan and elastic infogan biased in terms of imbalanced dataset experiment.	0
[philosophical] even for an imbalanced dataset, i really dont find infogan performing poorly, especially in figure 1 (center).	0
2. the regularizer for maml seems interesting but only tested in omniglot dataset, i would like to see more empirical results.	0
experimental weaknesses: a) there are claims in the paper: 'report stateoftheart results on two datasets for longrange action recognition: charades and breakfast actions' however, there is no comparison to any other work, of which there are many.	0
of these two, the top performance is only around 26. the graph also has the yaxis labeled as % accuracy, but the standard evaluation metric on charades is meanaverage precision as it is a multilabel dataset.	0
charades dataset is ok, but charades is a mutlilabeled video dataset, which is not appropriate to showcase clip selection.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
i understand there is no readily available dataset for such evaluation, however the ball is in the camp of the authors to solve this problem. '	0
some material/presentation concerns: # overall  considering a 'fixed initialisation' scenario is arguably not important, as if one could store the initial network weights, one could also just store  heta' and have an empty 'synthetic dataset' to train on.	0
comments regarding experiments from sec 4.1 the results indeed show the performance is greatly improved over using random images from the dataset, however obtaining 45% on cifar10 would hardly be considered “distilling the dataset” as the full set of data yields performances of 90% on this task.	0
— the idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [didericksen, et al., collaborationbased user simulation for goaloriented dialog systems, convai neurips ws, 2017] — even if in a different setting and not clustered) in any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to followup without public datasets, etc. finally, i don’t think the scope of potential impact (if wellexecuted) makes iclr the ideal venue.	0
the idea to use dwt is interesting but not new and the results are only tested on one small dataset.	0
the experiment results are somewhat strong, but i wonder how it can generalize to different tasks, different network structures and different dataset.	0
strengths 1) their method achieves a significant improvement in performance over baseline selfsupervised video representation learning methods on 2 datasets: ucf101 and hmdb51 2) the paper presents though experiments with different architectures and training settings in order to be fair to the baselines.	2
on the other hand, the main contribution of this paper is not about any specific representation learning techniques or applications, but rather a novel dataset.	0
### 3) insufficiently compelling experimental results ''the experimental results do not clearly convey the benefits of the two proposed approaches, due to a combination of insufficient/inappropriate baselines, limited number of benchmark datasets and metrics, inconsistencies/lack of detail in the way results are reported and not accounting for random variability.''	0
5. in the caption of table 1, it says 'four electronic health records datasets' but the number seems to be three everywhere else?	0
i am also a bit confused by the choice of architecture (attentional lstms) for nontime series data and the lack of explanation how and why these are used for the different datasets.	0
weaknesses:  datasets are very uncommon.	0
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
pros:  novel/original idea  endtoend learnable, allowing to utilize sgd, which better fits for the largescale dataset and multimodal dataset  well written and easy to follow  competitive results over gbdt, which is commonly used in relevant tasks.	2
additionally, in figure 4, the hierarchy on imagenet dataset is reasonable, but in figure 6, the hierarchy on celeba dataset seems to be arbitrary and subjective.	0
pros:  improved performance on benchmark dataset originality: the novelty of this work is limited.	2
i have one concern with the new dataset used for evaluation, that i didn’t see addressed in the paper.	0
to make a case for a video sr method that is applicable not only to cgi rendered videos but also to a more general class of natural videos (e.g. camera acquired) additional results on the ntire 2019 video sr challenge dataset should have been provided.	0
while the method is specifically designed to image classification  which in a way reduces a bit the scope the possible applications but this is a minor remark  i find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets.	0
weaknesses and questions: for visual recognition datasets, the attention targets are defined such that we want to maximize the weights between two overlapping objects of differing classes.	0
concerns #3: incremental improvements  the performance gains of l2tl over finetuning seem to be incremental even many samples in the source dataset are irrelevant to the target task (table 1 & 4).	0
however it is not clear why teb is having such an improvement on such dataset but not having an improvement when coupled with the transformer net.	0
the model is evaluated on several datasets (e.g., gqa, clevr, vqacp) strengths: 1. the model is novel.	2
it seems to me that copycat is very similar to fgsm, but it is computed over an entire training dataset as opposed to for one image.	0
suggestion: it would be great if the authors could come up with a toy dataset with ambiguous but known bounding boxes (i.e. overlapping objects with particular poses) to study how well the proposed model recovers those structures.	0
realworld datasets are great, but we have little understanding of what effect exactly is helping the classification here and an additional toy setting would increase the clarity of the modeling ideas.	0
the authors find that different datasets behave differently, which makes sense if more or less occlusion is present, but unfortunately the problem mentioned above undermines the results here.	0
weaknesses:  beyond a proof of concept that 3d feature representations are a useful way of incorporating spatial common sense into language understanding, the dataset will potentially be useful for other applications too.	0
one would think they augmented the clevr dataset with 3d information, but then it appears that each scene is rendered from multiple azimuths and camera positions.	0
the authors show the results on latest network architectures on imagenet dataset  resnet18, resnet50 and googlenet and the results either compare to other quantization methodologies and in the case of googlenet they surpass the state of the art admm method by 1.7% top1 accuracy the current experimental section does not have any runtime or model size numbers, it would be great if authors could talk about that and maybe add that information to get a more apples to apples comparison on not just accuracy but other factors.	0
since i am not from the field of forecasting, i can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field.	0
for incremental learning, i would personally think of imagenet as a good example and for continual learning of multiple datasets you can consider the existing sequence of 8 tasks benchmarked in [2] and [3] where you can evaluate your method on more realistic images with a total of over 400k images and significant shift in the distributions.	0
we agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period.	0
claw fundamentally based on variational continual learning, but it outperforms benchmarks on diverse dataset even without additional coreset.	0
a few remarks/concerns are: 1. usefulness of the dataset: it seems limiting for a factverification dataset to restrict itself to a binary space i.e. entailed vs refuted.	0
i think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ml and nlp are overstated in my opinion.	0
 a troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the entailed/refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion.	0
the experiments seem to cover a lot of ground:  toy polygon dataset demonstrated to be hard for existing sota in set representations  sets of points from mnist images  graph classification (competitive with a recent graph convolution approach)  integrate with relation nets for deep set prediction the authors acknowledge (sec 7) the limitation imposed by requiring the input argsort (and size) at the decoder, but point out that even as a representational pretraining or regularization, this strategy can help to improve set prediction strategies not subject to the same constraint (like rn, as in 6.5).	0
finally, the authors evaluate three sequencetosequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.	0
i am not familiar with the details of the dataset in their evaluation, but it would be helpful to show relevant results if it is easy to simulate using their dataset.	0
a similar observation was found in wikihop dataset, which is a multichoice qa dataset that all negative candidates are guaranteed to be the same type, but still questioncandidatesonly baseline (without context paragraphs) outperforms stateoftheart models (https://openreview.net/forum?id=b1lf43a5y7).	0
paper weaknesses 1. experimental settings are clear, however, what makes me confused is that the construction for p_{_x0008_ar{d}} is straightforward for simple distribution like 2d points dataset, however, it might be intractable for complex high dimensional data such as images.	0
(1) reviewer’s major concern is that this method is not very scalable to largescale realworld datasets such as imagenet and sun database.	0
overall, while the authors provide evidence of a better predictive performance with respect to several baselines, i am left a bit unconvinced by the study for the following reasons: 1. lack of relevant baselines: it seems clear that the overall purpose of the approach and the nature of the dataset require fitting a state space model, however, baseline are overall focused on recurrent and autoregressive models which seem underequipped to address these problems.	0
the outstanding issue is that the final results are not consistent across datasets, baselines, and metrics which is concerning.	0
the toy dataset, qualitatively, evaluates the decomposition quality with interesting results but is not enough to make conclusions for realworld datasets.	0
a weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40ish trained nns for every dataset and training hyperparameters, but i do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.	0
table 8 also provides such comparison, but please clarify the testing dataset.	0
strengths 1) the introduced dataset is timely and is more challenging than existing reading comprehension datasets, based on the examples presented in the paper.	2
strengths: —the dataset, which is extracted from standardized tests such as gmat and lsat, requires the ability to perform complex logical reasoning.	2
weaknesses: —the dataset seems small to acquire the ability to perform complex logical reasoning.	0
although my concern about the size of the dataset is not satisfied, i decided to increase the score of the paper (weak reject > weak accept) upon looking at the author response about transfer learning.	0
i'm recommending a weak accept for this paper, but only because of my background knowledge about the lsat and gmat, and my prior understanding of reading comprehension and why this would be a really interesting dataset.	0
of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated cv error but as far as i am aware there is a general agreement that getting “closer” to leaveoneout setup is not a good idea, furthermore, in my anecdotal experience 10fold cv for a dataset with only 600 samples can really be problematic.	0
one place where i lack confidence in the results: i am not very familiar with the datasets used (triangles and letterhigh) nor how standard they are in the graphlearning literature.	0
good analysis on the performance vs dataset size and time complexity cons: ' heavy notations which could be more concise. '	0
on the synthetic dataset (3depn), the quantitative results are not as good as those of pcn (which is a supervised method, unlike the proposed method), but the qualitative results look plausible and comparable to pcn results.	0
2) in the paper, authors mentioned that “note that there exist training schemes in the literature that train on even larger batch sizes such as 32k (you et al., 2017; jia et al., 2018), but these methods require a lot of hyperparameter tuning specific to the dataset.” as far as i know, they just need to tune warmup steps and peak learning rate, which is also required in the paper.	0
the paper claims that l2 pruning requires more data, but it's unclear if this really matters since the whole dataset was used to train both methods initially.	0
pros:  compared with previous datasets (e.g., miniimagenet, omniglot), the constructed metadataset is larger and more realistic, which contains several datasets collected from different sources  several competitive baselines are compared on this dataset under different scenarios (e.g., different number of shot) with reasonable analysis.	2
 lack of experiments in larger contour detection datasets (like semantic border dataset or cityscapes).	0
because of the extensive nature of the research problem, the coverage of each piece of this research problem in this paper could have been more thorough: for example, i would be really excited for a more indepth analysis on methods for enforcing locality, but the authors consider only one method, which is the auxiliary loss, and this method does not seem to be applicable for datasets that do not have attribute labels.	0
i think the analysis using the synthetic dataset is nice, but i am not sure how surprising the results are.	0
however, all datasets selected in experiment are black and white images with low resolution (28 x 28 ?).	0
examples include:  singular/plural nounverb mismatches (rules..and does not change > do, methods ...achieves > achieve)  q_r^(t)  choice of left operands > right  an onehot > a onehot minor issues:  the claim that the neurallp is the only model able to scale to large datasets is a bit too strong given [1]  you say “we could adopt” but you “do adopt” clarification questions:  are 10 times longer rules of any use?	0
selecting a subset (~22.7% of drop dataset) based on the design of the proposed model ('heuristically chosen based on their first ngram such that they are covered by our designed modules'), and compare to other models, which can actually handle a broader set of questions, only on the selected subset seems incomplete and raises concerns about how generally applicable the proposed model is.	0
to be specific, the results in table 2 are very close between mtmsn and the bertbased model proposed and it's not clear if the difference is because (1) the model is generally better; (2) this is a subset of the dataset that this model performs better; (3) this is because of the additional supervision signals provided (e.g. the results of fig 2a without the auxsup is almost the same as mtmsn) and if we provided similar auxiliary supervision for other models they would equally do well; (4) due to lack of reporting variance and errorbars across runs we see a small increase which may not be significant; ... again, the paper is very interesting, but i don't think it's clear and thorough to experimentally prove that the overall approach is working better.	0
evaluations are conducted on the standard shapenet dataset and the yields results close to the stateoftheart, but using significantly less parameters.	0
i have the following concerns: 1.fsnet achieves a small model (fsnet2wq in table 5) with only 0.68m parameters and an map of 70.00% on the voc2007 dataset.	0
also, for wn18rr dataset, mr of convkb is better but it's not in boldface!	0
minor comments:  in table 2 sacn method outperforms compgcn based on h@10 (0.54 vs 0.535) for fb15k237 dataset, but not highlighted with bold.	0
i’m also a little bit concerned about the fairness of the clustering experiment, in that the elasticitymotivated clustering algorithm utilizes an auxiliary dataset whereas simple baselines such as kmeans and pca kmeans are not able to use that.	0
i understand this is per datapoint and the metalearning scenario is focused on the perdataset setting, but i find them related enough to consider a discussion.	0
very good evaluation on hotpotqa, but would be even stronger if this were applied to at least one other task/dataset.	0
i agree that the authors extend a lot on this papers, especially in terms of dataset and completeness of experiments, but they are definitely closely related, and the fact that it is not cited is a serious flaw to the current version.	0
the approach is similar to recent work on geometryaware rnns (tung et al. 2019), but is applied to more realistic scenes (urban landscapes datasets generated using the carla simulator as opposed to simple tabletop arrangements of shapenet objects) and makes fewer assumptions about the camera pose (6 dof camera parameterization as opposed to 2 dof cameras placed around the objects).	0
better test errors and within 57x slower than the conventional ibp methods with worse errors cons: > extensive experiments with more advanced networks/datasets would have been more convincing, esp.	0
4) in section 2.2, second paragraph, the dsprite dataset is mentioned but not cited.	0
'discussion points and concerns from the reviewer:'  dataset / batching details please mention how the datasets were split for training and testing the models.	0
 a more comprehensive description of the dataset is lacking.	0
decision: weak reject motivation: the method is incremental and presented in general in a clear way and easy to follow, the authors present a simple but interesting trick to make the triplet loss more effective on a random forest in the case of generalization to a new unlabeled dataset.	0
this method looks incremental to me because it is addressing the problem of pseudolabelling for learning on a new dataset and instead of using confidence measures uses a random forest to assign labels.	0
from proofs i had expected epsilon tiny (1e9) but you use 0.5 ' would be useful to show how model performs on smaller dataset to gain intuition	0
here, i don’t necessarily mean “prove” in a mathematical sense, but just analyzing the learned representations a bit more rigorously and being able to say something along the lines of: here’s exactly how the trained model represents “lift”; because of reason x, y, z, this representation is completely disentangled from all object representations in the dataset (and ideally from all possible object representations, because that’s really what true systematicity entails, although i highly doubt that any generic model of the type studied in this paper will be able to achieve this, regardless of the amount and type of input it receives).	0
in the comparisons sections, some details on datasets are given, but these seem to refer to data for inference.	0
the proposed method is relatively simple, but is wellmotivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset  quantitative measure for saliency methods, so i would give this paper an accept.	0
indeed, a given measure might very correlate with generalization gap on this specific dataset but not on others.	0
for example, one experiment would be to train the framework on the current 3drooms dataset but then test on new environments (e.g., other room layout) or new objects (e.g. other shapes such as shapenet objects).	0
3. one concern i have with discrete representation is how robust they are wrt different dataset.	0
i suspect a bernouilli likelhood was used for the binarized dataset, but what about the other ones, and notably cifar?	0
for the icing dataset, you mention that it is a contribution but do not provide enough details regarding it to allow further research with it.	0
this submission is lacking experiments comparing fedavg to the proposed method under these settings (which can be simulated using available datasets).	0
experiments on graph classification lack diversity, where collab is the only nonchemical dataset in the experiment. '	0
table 1 and corresponding text say that a significant advantage of the approach is that it can handle arbitrary scale values, but there was no explicit exploration of the effects of using this beyond one set of scales per experiment/dataset.	0
(perhaps this is not too surprising in retrospect, as images may have limited scale variation from camera position in this dataset, but significant withinclass viewpoint variation.)	0
concerns: 1) you note that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems to decrease as the dataset becomes more difficult (i.e. cifar100).	0
it also states that the rotated mnist dataset is rotated on the entire circle, but not how many fractions of the circle are allowed, which is equivalent to r_max.	0
negatives : the experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : 1. it misses out a recent stateoftheart method (slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way.	0
furthermore, [1] also compares against many other sota methods missed out in this paper on the many other datasets datasets including those in this paper but in a more general multilabel setting.	0
one question i have for the authors: in the experiments on the imdb dataset, the authors claim that the generalization error is worst at sigma_{epsilon} = 0, but it appears that the error is actually larger for sigma_{epsilon} = 0.4?	0
however, i have a bit of a concern as to its practical necessity in comparison to simple finetuning of the final softmax layer (referred to in table 1 as  heta_2) on a new dataset.	0
one would expect them to perform a bit better than lstms, but that might be contingent on the size of the dataset more than the structure of the inputs.	0
strengths:  simple approach that seems to be giving good results  large number of adversarial attack scenarios tested  good related work review weaknesses:  results are reported without variance information  there are some details missing on how the decay factor is selected  results are reported only on one dataset (imagenet) the paper is well written, the authors have identified a 'problem' of resnetlike models and proposed an approach that can exploit the problem in adversarial attacks scenarios (sgm).	2
of course, it's already implied by the current paper that the ntk is not deterministic and can move a lot when beta is large, but it's unclear whether the reverse is true, i.e., what is the regime when the ntk becomes deterministic and frozen when an entire dataset is involved.	0
 easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
yes, one of the main strengths of the paper is the task and dataset that will be released.	2
comments:  the paper is well written, but it is unclear how the questions are generated during dataset creation process.	0
the fact that no actual difference in performance between acgnns and acrgnns was noticed in the only nonsynthetic dataset used in the experiment should prompt the author to run experiments with more real life datasets, in order to empirically verify the results, but this is a minor point.	0
pros: this is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community.	2
a few small comments: ' there was some analysis of the augmented imdb dataset, but none of the snli dataset.	0
experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.	0
 the author claim in the experimental section that their method perform betters on both datasets, however when looking at table 1, wgangp performs better on cifar10 and when looking at the standard deviation we can see that the improvment is not significative.	0
i think this paper will be valuable to the research community for these reasons: (1) the dataset contains a more geologically complex search space comparing to the original nasbench101, whose search space is restrained in certain ways; (2) released metrics include more meaningful information rather than single point value in nasbench; (3) it uses 3 datasets rather than 1. my major concerns, which i will detail later, is the phrasing 'algorithmagnostic' does not truly reflect the difference between their approach and nasbench101, and about the architecture search space design details.	0
 extensive experiments on evaluating 15k architectures over 3 datasets  detailed statistics on the search space  good baseline experiments comparison main concerns about this dataset:  comparing to nasbench101 in terms of 'algorithm agnostic', it is in a 'moreorless' game but not a 'yesorno' one, so that aanasbench does not seem appropriate.	0
on the other hand, for some other stateoftheart algorithms, like proxylessnas on imagenet, the search space is also different from the one proposed in this paper, but likewise, it does not indicate evaluating proxylessnas on this dataset is impossible.	0
the models achieve sota on glue (by time the paper was submitted; it is not the best model now but that does not affect the contributions), arc, and the commonsenseqa dataset.	0
for some dataset, ntk can be worse than baselines but for some other dataset, ntk can be significantly better than baselines.	0
i do understand that these models are expensive to train and evaluate, but perhaps a smaller dataset might still suffice to demonstrate the value of these choices.	0
the choice of tasks to evaluate on is broad, which is a strength, but is missing simpler tasks that one would expect to see, such as a text classification dataset, or simple bagofvectors style models.	0
this may be out of scope for this current work, but one way to answer that would be to leverage datasets with more fully labelled factorised data (e.g. dsprites [1]) and present how one should leverage these with , in order to identify the original generative factors.	0
weaknesses: 1. regarding the iswlt translation task result: with this dataset, it's a bit of a stretch to say there was 'only a 1 point drop in bleu score'.	0
general advice/feedback:  should provide an explanation of the row in table 2 showing that a simple linear transformation is able to achieve accuracy and convergence times comparable to those of the nau  should provide an explanation of the universal 0% success rate on the u[1.1,1.2] sampling interval in figure 3  inconsistent captioning in figure 2c, missing 'nac• with'  should clarify in section 4.1 that the 'arithmetic dataset' task involves summing only 'contiguous' vector entries; this is implied by the summation notation, and made explicit in appendix section c, but not specified in section 4.1  it is unclear what experiments you performed to obtain figure 3, and the additional explanation in appendix section c.4 regarding interpolation/extrapolation intervals only adds to the confusion; please clarify the explanation of figure 3, or else move it to the appendix  the ordering of some of the sections/figures is confusing and nonstandard: section 1.1 presents results before explaining what exactly is being measured, figure 1 shows an illustration of an nmu 2 pages before it is defined, section 3 could be merged with the introduction grammatical/typesetting errors:  'an theoretical' : bottom of pg 2  'also found empirically in (see trask et al. (2018)' : top of pg 4  'seamlessly randomly' : middle of pg 5  'we choice' : middle of pg 6  inconsistent typesetting of 'nac' : bottom of pg 6  'hindre' : middle of pg 8  'to backpropergation' : bottom of pg 8  '=?'	0
q1.4) i highly value enlightening small scale experiments and do understand that computational resources are not available everywhere however i think it would benefit the paper greatly if the proposed method is demonstrated on some reasonably sized dataset (at the very least one of full mnist, fashion mnist, freyfaces).	0
my primary concern is to what extent can the new dataset (cater) add to existing video datasets that are also explicitly designed for long term spatialtemporal reasoning, such as video vqa datasets tgifqa[1]/svqa[2].	0
the construction of the dataset focuses on demonstrating that compositional action classification and longterm temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregationbased methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues.	0
the authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with realworld data, but still it can serve a useful and complementary role similar to the one clevr has served in image understanding.	0
there is only a 3 point gap between wideresnet and the proposed model (92.9% vs. 95.8%) … but on what dataset?	0
3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported.	0
below i have a couple of (minor) comments and questions: 1. out of curiosity, it seems that it is standard in the literature, but isn’t the assumption that one can go over the whole dataset, u, at each iteration of the active learning algorithm, limiting?	0
architecture parameters has a strong correlation with the generalization ability (via loss of test dataset), and shows this lambda will first decrease but then drastically increase after a certain epoch number on 4 different search spaces.	0
(i see though that for the second dataset pretraining seems to be done on a different part of wikipedia, i guess, to address these concerns).	0
main concerns: the experimental section should make experiments on standard datasets (i.e., uscd, trancos, mall, pklot, shangai, penguins) using standard evaluation protocols (mae, game).	0
(4) if we only see wrn and densenet on cifar datasets (allcnn is probably outdated and performs poorly, and imagenet is not convincing as said in (1)), we notice that actually wddropout do provides a small increase on the augmentation schemes.	0
i would say that the method does not have to outperform stateofthe art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability.	0
my current decision is a weak reject, for a wellwritten paper, but some concerns as follows: the results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from cifar10.	0
considering that the premise of the paper is that mlp’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” the mnist visualization of groupselect felt informative, but the xor example for grouping visualizations seemed too easy.	0
the rebuttal also addresses my concerns regarding datasets, and my concerns regarding implementation details of ‘y_train_hat’, as now it's included in sec 4. overall, after rebuttal, i'd like to recommend 'weak accept' for this paper.	0
furthermore, convolutions encode translational symmetry which may be beneficial for the current application but may not be desirable for other datasets.	0
the methods were evaluated on several baseline datasets (e.g., glue, hans) strengths: ' the paper is easy to follow. '	2
strengths 'this paper claims the importance of the local context of a word and shows an effect of their method on the various datasets: glue, and hans.	2
= strong/weak points  the idea of interpreting the 'humanlikeness' of program behaviors is interesting, and could help substantially with augmenting the traditionally small clean datasets in program synthesis.	0
this can be interpreted as follows: the proposed algorithm performs well on easy datasets, however the performance is not much better than other algorithms on fashion mnist, which is little more challenging.	0
i have some concerns about the method as well: what happens if the quality of edge maps is not good for some datasets?	0
2. the details of how the training and testing datasets are obtained are also lacking.	0
strengths:  the methodology is rigorous and the datasets considered is extensive  the paper is well written concerns:  isomorphism is not necessarily a bad thing in graph classification tasks.	2
(i) i think the experimental evaluation is the weakest part of the paper at the moment which i find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details.	0
due to the lack of such expensive annotations, this paper can be only evaluated on cub and flowers datasets, results on other popular zeroshot learning datasets e.g., awa, sun, imagenet, are essentially missing.	0
i also have concerns on the experimental datasets.	0
my background is not in machine learning with clinical data (so i do not know common datasets) but it is unclear to me why the datasets evaluated on only consist of a very small section of the overall dataset they’re taken from (in case 1).	0
strong/weak points simple but useful extension of the existing supernode idea experiments on a number of datasets and baseline gnn architectures,providing ample experimental evidence of the usefulness of the method.	0
pros: the authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification.	2
mainstream efficient networks, such as mobilenet and shufflenet, are not designed for cifar10 datasets, but imagenet datasets.	0
given my concerns on how this unsupervised approach can scale to reallife datasets, i suggest a weak reject, but i think the proposed method has some interest for the community and i strongly encourage the authors to provide further evidence of performance of their method on more complex vision tasks.	0
another idea for experiments is doing crossdataset evaluations where different datasets may have different leaf categories but shared high level ones.	0
the datasets are mostly smallscale, but this is in keeping with the goal of the paper, viz.	0
results on selected architectures and datasets show some improvements compared to naive binary index representations on the positive side, i see an interesting approach to pruning a neural network, assuming the storage cost is guiding the pruning algorithm.	2
the claim that 'this paper is the first in demonstrating that halfprecision can be used for a very large portion of dnns training and still reach stateoftheart accuracy' may not correct, in fact, nvidia's apex has already supported using mixedprecision or entirely halfprecision to train dnns, and there is no clear evidence that the proposed method is better than theirs due to the lack of experiments on more tasks and datasets.	0
strengths: (1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (one potential improvement could be to try noise distributions other than gaussian) (2) simple objective and consistent improvements.	2
the observation is that in many realworld applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.	0
it will be also interesting to see how does the proposed method perform on largescale datasets such as domainnet and officehome dataset: domainnet: moment matching for multisource domain adaptation, iccv 2019. http://ai.bu.edu/domainnet/ officehome: deep hashing network for unsupervised domain adaptation, cvpr 2017. http://hemanthdv.org/officehomedataset/ 3) the novelty of this paper is incremental as the theoretical results are extended from cortes et al (2019) and zhao et al (2018).	0
it also looks at squad translations (but, i'd have preferred a bit more depth on one of these datasets over having both, but i understand why you made this rhetorical choice).	0
it's claimed that these are the only datasets with user and item content, but why are both needed to run an experiment?	0
since some phenomena only appear in larger datasets [2], there is a concern that the proposed method also works on other datasets.	0
the paper seems to correlate tasks and languagerelated hints; however getting a language hint is not always available, especially for datasets such as taskonomy [1, see refs below]  which is a dataset for multiple tasks.	0
existing approaches to bert training use bpe with ~30k vocabulary size or roberta with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext103 or billion word.	0
i think training a joint model on these two datasets is a completely natural experiment that many of us wanted to see, and so i appreciate the effort of the authors and the benefit to the community of having these numbers, but i'm not convinced there is that much meat otherwise in this paper.	0
based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of batchnorm on unperturbed datasets.	0
the paper does compare against other methods for stereo matching, but i was wondering why the ktti leaderboard excerpt does not include the better results from http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo ?	0
the thorough evaluation over the curated datasets shows pretty well that the changes in the model prediction are indeed due to a lack of robustness of the models themselves rather than the difference occurring from one frame to the other (occlusion etc).	0
i am open to eventually consider acceptance given the value of the datasets, but the paper would then require a significant overhaul to remove all confusing aspects.	0
results provided use somewhat nonstandard datasets for deep learning, and as such it is hard to asses statistical significance of the differences reported; while the test datasets sizes are big enough to trust error to 1e3 level, they are heavily imbalanced and relatively low dimensional problems, which have proven to be hard for neural network many times in the past, consequently i would expect to see confidence intervals or stds of each result, at least for table 3. the problem becomes even more severe for metrics such as f1 which do not decompose additively and so can be very sensitive to the (false) positive rates values  reported improvements might disappear once these are introduced, but even if one does not outperform handcrafted proxies for specific objectives, having a unified method that is onpart with those in blackbox scenario is a good result (i would argue that even if all the results are slightly worse, it is still worth publishing).	0
the proposed model has shown performance improvements in classification accuracy on mnist and cifar100 datasets in incremental class tasks.	0
 particularly given the data filtering and restructuring of some of the tasks, getting a rough estimate of human performance for all the datasets would be quite valuable  i think saying the mnli does not help model performance on disceval is completely valid but claiming it hurts performance could be a stretch given the margins of error.	0
however i am not entirely sure that the impact of these results will attract the interest of a broad audience since the experiments presented are on “small datasets” and with some rather shallow neural nets.	0
3. the results are shown in some very recent datasets including taskonomy cons: 1. there is not much novelty in this work.	0
the experimental task is interesting (i'm okay with synthetic tasks of this form for unusual new architectures like this), but i'm not sure what it tests that isn't tested in the clevr and sortof clevr datasets which rely on similar relational reasoning to solve.	0
regarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains.	0
this is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper.	0
according to my own experience, the artificial convolutional kernels with some prior knowledge may work well on small datasets but tend to fail on larger ones.	0
several metrics are used (rmse, mse, mape, mae, mre) while results on different datasets are shown in different but not all metrics.	0
the authors claim that there are no clear classification tasks for the datasets  but this is not accurate as both celeba has clear classification tasks in the form of predicting attributes.	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
running each model with different random seeds and presenting statistical significance results would be ideal, but such runs can be expensive given the size of the datasets.	0
the paper proposes to combine the implicit function idea with output mode estimation, however the problem definition is vague, competing bayesian methods and density estimation methods are ignored, the experiments are insufficient with little stateoftheart comparisons, few datasets and clear problems in the learning.	0
they construct a pair of synthetic but somewhat realistic datasets—in one case, the bayesoptimal classifier is 'not' robust, demonstrating that the bayesoptimal classifier may not be robust for realworld datasets.	0
3. experimental setup:  one somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the bayesoptimal classifier has nonzero test error in expectation, and moreover the data variance is fullrank in the embedded space.	0
this concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets.	0
4. a suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets.	0
i think it lacks a more thorough evaluation and description of the dynamics observed during the genetic evolution, and the performance of the baikal loss on other datasets (my quick experients with it on imagenet diverged i did not have the time necessary to tune the hyperparameters).	0
it makes no sense to state that standard pictures often have 'one subject'  imagenet does, but a look at standard datasets like cityscapes or depthinthewild shows that it is not true in general.	0
 authors say that 'polynomial dependence on the rank, the error, and the condition number, make these algorithms impractical on interesting datasets'  but it is not clear whether they mean that their algorithm is also impractical?	0
further, the idea of concatenating all tuples of a relational db to be passed through a particular feed forward layer is infeasible for all but the smallest datasets.	0
strengths the paper proposes benchmark datasets for outofdistribution detection of fewshot classification.	2
the authors combine well established methods in a straightforward manner and while the resulting increase in performance for some datasets may be relevant in the domain, the conceptual advances are too incremental for a machine learning audience.	0
the experiments are a little insufficient, as the authors used very small datasets, and there is no comparison with other carefully tuned optimization algorithms, which, circle back to my previous concern, is that whether such an algorithm could actually outperform a carefully tuned sgd, adam.	0
 it is mentioned that attention based modules scale poorly with large sized datasets.	0
weaknesses:  there are some details that are missing from the paper, for example details about the mapping operator, and the specific representations of e_i for the datasets used.	0
it would be perfectly reasonable to think that most datasets have low bandwidth, but this not a claim that made the authors.	0
decision: overall, this paper handles an interesting collection of data, but does not add much interesting novelty to the field of representation learning and fails to leverage some opportunities in those datasets to do sth.	0
it is commendable to prepare the datasets for publication and the basic results the authors show make for good first baselines here, but ultimately do not meet the bar for novelty or scientific insights.	0
### weaknesses apart from the contributions of compilation of different datasets which are already present, and classification results on them, there is not much novelty in the paper.	0
''about datasets'': the paper emphasizes a lot on the lack of standard datasets and metrics in remote sensing.	0
pros:  best generated video quality out there (in the kinetics600 dataset)  numerically outperforms baselines in all datasets weaknesses / comments: ' what is the message of the paper?	2
for example, some datasets would need more computational resources for some steps, but not others.	0
p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.	0
you do something like this on p.7, but it would be neat to see a histogram like the one you have for eventmnist for one of the realworld datasets to see if it learns the domainrelevant time knowledge you claim that it should learn.	0
i liked to see experiments performed on different tasks and datasets but overall that section could be significantly improved.	0
it should compare the same model/software but on multiple different datasets to demonstrate the model's general applicability.	0
the wbound), but also makes the regulation scalable to large networks and datasets.	0
 the paper presents a, to my knowledge, novel approach, to avoid the leakage of metadata in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)  the paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below) weaknesses: 1. experimental evaluation: 1.1. the paper only evaluates on the training set.	0
 the evaluation of the idea is not complete while it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other blackbox attack methods in the limited query budget regime, i would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.	0
i trust the results because i performed exactly the same experiments for cifar10 with longer nonregularization periods and found that there is no effect (this is also that the authors show in the paper) but i didn't test on other datasets and obviously didn't think about potential benefits for compression.	0
section 3 on perception claims to give results on mnist and celeba datasets, but there only seem to be results on celeba.	0
quality: the paper is easy to follow and nicely written, but with a few minor typo issues: 1. page 1, refers to appendix a.3 but should be for a.4 2. page 2, 'section' is inconsistently capitalized 3. page 6, mentions three commonly used datasets but only mentions mnist and california housing.	0
6. extensiveness of experiments  i do like the toy dataset as an example, but to show effectiveness of this framework, a larger breadth of datasets could have been used.	0
i have two concerns as follows: 1. the authors conduct experiments on multiple small kg datasets such as fb15k and wk15k.	0
the authors dismiss prior work in the introduction but do not provide any direct evidence that prior work is unable to handle the datasets introduced in the paper.	0
2) given the core contributions of new architectures for disentanglement, the lack of any results on real, non synthetic, datasets in order to validate these (for instance, celeba) which significant prior work on disentanglement has been evaluated on is an unfortunate omission.	0
however i felt that i could not fully see the benefit of the application for medical data because the area, task, datasets etc are not well introduced.	0
pros:  the effectiveness of the methods is experimentally demonstrated using two relevant datasets  the inverted wireless experiment clearly shows the interest of the attention combination strategy cons:  the methods is very simple (combining multiple segment prediction to perform document classifications) making the contribution of this paper quite weak.	2
unfortunately, there are many concerns which are still not convincingly answered: (i) considering the focus of this work (as admitted by the authors in the rebuttal) is empirical and with limited technical novelty, more comprehensive results on many datasets are required.	0
from the results, i don't think any of the conclusions will change significantly from the dataset size, but still its better to use larger datasets.	0
this already becomes more evident in the mnist (which contains mostly aligned digits) where convnet clearly outperform morf but would likely become more significant when going to realworld datasets, e.g. cifar.	0
in sum, although i believe that the paper proposes a very practical method that is easy to implement and is promising, due to lack of experimental validation against a similar approach, stateoftheart sparsification methods, and results on more datasets, i temporarily provide the rating of weak reject.	0
the most useful comparison are the results given in table 3, but only three datasets are used, the margins are small, error bars are not provided, and no significance testing is performed.	0
i think the highlevel idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.	0
the authors rightly “note that for both datasets, the vast majority of words we used require clinical expertise to interpret”, but it doesn’t seem they have given the learned topics to clinicians to judge whether they make sense or are helpful in any ways.	0
large scale pretraining like in bert usually benefits from very large datasets, but also very large models.	0
 the introduction doesn't clearly state what are the contributions of the method, an example of possible applications would be appreciated (the following about robotic for example but with more details)  3.1 i really enjoy the fact that different datasets are used for their different properties (foreground, background, sparsity), this is a nice touch figure 4 results are a little underwhelming, smoe's scores are very close to the other methods  3.2 the authors refer to smoothgrad squared method, it is indeed a good process to refine saliency maps, however why it is used could be detailed, just as the parameters chosen for its implementation.	0
3.3) “for face recognition, ddl can easily improve the performance by concentrating the discriminative information and for video action recognition, ddl can further accelerate the pipeline by eliminating the frames with insufficient information.” this sounds like ddl behaves differently depends on the task, but perhaps the conclusion driven by the authors cannot be fully correct given that the number of used datasets are limited to make such a general statement.	0
1 positive aspects:  experiments are conducted on many datasets and show visually convincing results.	2
4. the datafree derivation approach makes sense and i understand that this makes the approach theoretically applicable to arbitrary datasets, but the authors do not apply it to other datasets to show that it generalizes in practice.	0
i understand that it may be a less expressive model in general, but it is not clear to me why it would not be a competitive baseline on some of the smaller datasets considered here  was this baseline tried?	0
in their proposed weight reptile algorithm (algorithm 3), the authors have also assumed access to the blackbox functions of the related problems or meta tasks, which is not typical of other meta bo works that only require the existing observations or datasets of the related problems/meta tasks.	0
one improvement i could suggest to better motivate the proposed approach is to experiment it not only on convolutionalbased networks with image classification tasks but also on recurrentbased networks with text datasets.	0
strengths: the proposed model is well motivated and shows strong performance and generalization ability on several datasets.	2
weaknesses: many quantities that should be reported are not: quality of models, diversity of policies, etc. the source of datasets should be clarified.	0
weaknesses:  how multimodal are the datasets provided by uci?	0
it seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?)	0
 i am concerned about whether the proposed method works well with harder datasets such as officehome dataset, because each class data are modeled by a simple gaussian distribution in the proposed method. '	0
pros: 1. the proposed method seems to work well on different shape i2i datasets without using semantic masks compared to previous works.	2
cons that significantly affected my score and resulted in rejecting the paper are as follows: 1  experimental setting and evaluations: the biggest drawback in this paper is the experimental setting which is not rigorous enough for the following reasons: (a) weak datasets: authors have chosen some standard benchmarks but they do not seem to be convincing as the datasets are too easy.	2
however, a part from the form, i have other concerns about the paper:  the main purpose of the paper is to tackle large image datasets that are hard to address by classical gmms.	0
however, i found the writing / notation imprecise at times and the experimental section too small (lacking an extensive set of baselines, and only on two datasets).	0
in the paper's defense, it seems that they were following the experimental setup of wang et al. (2019), but the paper should elaborate more on the choice of evaluation datasets.	0
cons: 1) the proposed datasets are very small.	0
in appendix, table 1 tabulates the training details, but nowhere is it clearly mentioned if the n=500,000 latent codes are sampled from a gan model that was trained on a mixture of datasets (i.e., bedroom, living rooms, kitchen etc.) or individual datasets.	0
lee et al. didn't experiment on mnist variations but many natural image datasets.	0
wmt ende (authors do not include the sizes of the datasets, but this is 4.5 million sentences) is the only large scale dataset, and the bleu drop is quite large on this dataset compared to the smaller ones such as iwslt.	0
(b) datasets and architectures: authors have used mnist and fashionmnist when discussing the shortcomings of ece.	0
weaknesses: ' usually, svhn is also among the tested datasets ' the pseudo labelling part is a bit unclear.for example, do you just refresh the pseudolabels at the end of each epoch ? '	0
there are other differences in implementation and the choice of datasets, but those are (imo) minor details relative to the core similarity.	0
what’s more, the datasets chosen are all singleclass datasets with a massive amount of data—as far as generative modeling is concerned, these are very datasets with a minimal amount of variation.	0
accordingly, i have substantial concerns that this method will not work well on datasets outside of these highlyconstrained, nearlyunimodal, singleobject, veryhighdata datasets.	0
if the concern is their training time on large datasets, they should be at least presented as comparison for the smaller datasets.	0
anyway, the experimental evaluation shows good results on all three datasets for your models, but its hard to be sure since you only have one comparison, an old lda, and nothing recent.	0
the experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures.	0
the study reveals that the attention patterns largely depend on the dataset, on some datasets they are sharp, but on others the attention patterns are almost uniform and not so different from the uniform aggregation weights in gnns that does not have attention.	0
the evaluation is done on two datasets, one with examples from nearoptimal players produced by mohex 2.0, and the other from randomly sampled but perfectly labelled examples generated by benzene.	0
second, why not test on more challenging datasets (cifar, celeba, etc) as opposed to simple black/white datasets such as mnist/fashionmnist?	0
— soundness: my major concern is with the datasets that are used for “natural” distribution shift.	0
does robustness on synthetic datasets and synthetic generations of such perturbations transfer to perturbations (of the same kind) but occurring in natural world.	0
pros: 1) an interesting topic of trying to understand generalization and transfer in deep learning 2) multiple types of experiments, including visualizations of loss landscapes at convregence and at initialization, plots of hessian eigenvalues, measuring the deviation of the weights from their initial values, measuring the variance of the gradients of the weights, measuring the transfer between different datasets, measuring the transfer performance depending on the durarion of pretraining.	2
2a) in table 1 and figure 2, it is unclear if difference in generalization between the dataesets is due to similarity to imagenet (by the way, imagenet is only mentioned in the caption of table 1, but not in the text) or due to the inherent properties of the datasets (perhaps some are naturally more difficult or prone to overfitting).	0
by transfer, here they specifically mean the ability for pretrained networks to be 'updated' to new, similar datasets either completely (all parameters are updated while being initialised by the pretrained network parameters) or partially (all but the last few layers are kept constant at pretrained parameter values).	0
they also mention conclusions from experiments using the food101 and places datasets, but don't show these results anywhere.	0
however the experiments are quite limited, with only 2 training datasets.	0
cons: 1. the biggest concern is the generalizability of auls to datasets that are more challenging than cifar10 and cifar100.	0
pros:  extensive experiments have been done on several datasets, both synthetic and real.	2
 comparison to cplex: the paper mentions that “we observe that our algorithm outperforms the cplex solver on er(100, 200) and er(400, 500) datasets, while consuming a smaller amount of time”, and “it is remarkable to observe that adp outperforms the cplex solver on both datasets under reasonably limited time.” note that cplex not only optimizes the objective, but also proves a bound on the objective, while adp only does the former.	0
minimizing a lower bound on a loss can be justified, but requires more discussion  the tradeoff inherent in the choice of n_w in algorithm 1 needs to be further discussed, especially in the case of large datasets where a full epoch of sgd in the inner loop of the optimization process is impractical  inaccuracies: the graphical models in figures 1 and 2 and conditional independences written in the text are not consistent:  in figure 1, x is not independent of s given y (neither is y') (see: v structures in a directed graphical model)  in figure 2, x' is independent of s regardless of conditioning on y considering all of the above issues, the paper is not currently ready for publication	0
the authors state “we believe this finding to have farreaching consequences as it directly contradicts the popular hypothesis about copying caes.” the paper definitely shows some empirical evidence that supports the claim that copying does not occur, but these findings are all done with a single seed and by considering small subsets of datasets (celeba and stl10).	0
for example, fig 2 (b) shows that the proposed dynamics model is better than an mlp or gru on deterministic moving mnist, but is this also true on real datasets, which have much more complex dynamics?	0
experimental concerns  is there a good reason to not try to compare on publicly available datasets like those used in the pointcnn paper (focusing only on the nontemporal versions of the model)?	0
2. the experimental section is entirely composed of nonstandard datasets, and  in general  the manuscript lacks almost entirely in critical details regarding how the datasets are generated; e.g. what is the pilot policy?	0
although the experiment shows successes of the proposed method on several datasets, the major weakness of this paper is the lack of technical novelty and detailed analysis of the proposed method.	0
finetuning dextr and deeplabv3 on the synthetic datasets can only show the models' weakness, but can't show your findings will help generalize the model to natural images.	0
second, the authors do not provide an insight, why pcomplex is better than the complex baseline on svo dataset, but performs similarly on other datasets.	0
on the positive side, the general point about the necessity of learning rate tuning for adversarial training (described in the fourth paragraph of the introduction) is a very good one, and there may be an opportunity for a more focused application of the proposed algorithm perhaps among further datasets and considering additional, alternative attacks. '''	2
 feature maps to show robustness of method is a good point  cifar10 and cifar100 are certainly a good start, but might not be the best datasets to test for image classification, in lieu of imagenet and others.	0
similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speedups that allow it to scale to large real world datasets.	0
pros: the numerical experimetns conducted in this paper are thorough, and they show interesting observations on the real datasets.	2
i think this paper is an interesting idea, and my only other main concern is the transfer results to other nli datasets.	0
their main results show transfer performance comparing their approach and using an mlp, but it seems that overall, on all datasets, their approach transfers more poorly.	0
experiments: what does it mean to be application agnostic but restricted to particular datasets and losses?	0
the main concern of this paper is the results are only confirmed on synthetic data, where all the 4 datasets are generated from known bayesian networks (i.e., causal graphs).	0
this is not terrible but also not great; for example, achieving 1% error requires a dataset size of 10^4 (realistically, even larger datasets would be required to achieve results with high probability).	0
the experiments show superior performance of the proposed methods, but the datasets are only two for each tasks.	0
notes: the results on the 2d mog toy datasets are good but are also suspect—the authors state that they use a 32dimensional latent space, but the original code provided for veegan uses a 2dimensional latent space.	0
also, the used datasets intentionally restrict the possible values for the number of subgraph isomorphisms, but the counts would be exponentially large if we consider practical (dense) graphs.	0
2) the method fits the model using (r)mse loss, but minimizing log errors ((r)msle) would be better considering distributions of response values (counts) of the used datasets in figure 6. fitting the mse loss is not good for such highly skewed cases, and for example, might focus only on the few instances having very large count values.	0
the remaining regression datasets are real but also lowdimensional.	0
also, in this equation, the gradient of the loss wrt/ z samples, average of gradients over z samples times..., does not seem to match what the gradient would be given the algorithmic description in the supplementary material, a gradient of the (sample) average z times... ' the abstract states the paper is proposing a method for highdimensional feature selection, but all of the experiments have datasets with max.	0
authors evaluate their model against traditional seq2seq models with copy actions over a set of tasks: ' code correction tasks: two bugfix pair (bfp) datasets of tufano et al. (2019) ' grammar error correction (bryant et al., 2017) ' learning edit representations pros: overall i am in favour of this work acceptance it represents a neat modelling for copying sequences that integrated simply with seq2seq models, especially the transformer model.	2
the model is trained on synthetic datasets generated by a black box rendering engine, and generalizes well to realworld datasets.	0
# weak points  there are two real datasets considered for ood evaluation, but i consider there are some flaws in their utilization for ood detection.	0
this argument applies to idd, although to a smaller extent as the datasets are both automotive, but again there are strong visual differences between samples of the same class across the two datasets.	0
pros: ' the proposed approach improves over the sot of competitive recent methods for anomaly detection on four image datasets. '	2
the authors claimed that the proposed new evaluation metrics are novel contributions of the paper but there is no discussion on why they are good metrics for evaluating the quality of synthetic datasets nor which metric should be used in what scenarios.	0
while i accept the response for the remaining questions from authors but i am still concerned about the weak experiments and an issue brought up by r3 regarding lack of enough comparisons with frcl on any other datasets besides split mnist and pmnist.	0
review:###the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
as said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets.	0
as imagenet is the most challenging of the datasets used, this is cause for concern.	0
i suggest to the authors to address the weaknesses pointed out to make the paper more stronger, especially adding few more attributes datasets such as person attributes datasets as noted below in the weakness section.	0
the paper shows somewhat thorough experiments on many datasets justifying this observation, but the theoretical part is rather weak since it doesn't seem to address this issue with the focal loss.	0
the authors found that on some datasets, using half of the scans is already enough to reach similar accuracy but speeded up the convergence by a factor of 2. detailed feedbacks:  the paper presents a simple idea that directly uses the nature of jpeg compression.	0
examples: http://arogozhnikov.github.io/2015/06/26/learningtoranksoftwaredatasets.html, http://quickrank.isti.cnr.it/istelladataset/, https://www.cl.uniheidelberg.de/statnlpgroup/nfcorpus/, minor comments concerning section 3.3, in what sense is sgd used to “calibrate” the model?	0
the datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking.	0
the main concerns of this paper lie in several aspects: 1. it seems that the authors did not report their comparison to recent sotas (such as lin et al, 2019) comprehensively enough, nor were the benchmark measures (missing several other attacks, especially black box ones), datasets and backbones fully aligned.	0
strengths 1) the authors present extensive experiments on many datasets.	2
taken together, the existing experimental setup potentially creates an unfair advantage for the neural networkbased methods  while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neuralnetwork based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of wikipedia).	0
concerns #2: insufficient experiments  to evaluate ood detection quality, id/ood datasets should be stated and various metrics (e.g., auroc) should be measured like other literature, e.g., table 2 in [2].	0
for this, though, it is important that the paper assess its strengths and limitations in comparison to alternative datasets.	2
 lack of evaluations with variety of datasets (cifar10/mnist)/configurations (other bitwidth)  lack of the citation and comparison to many most recent works on binarized networks (except xnornet) comments: i consider this a wellwritten paper with great clarity and good empirical performance.	0
the results on imagenet under 4bit quantization are strong and convincing, but the paper could benefit from conducting additional experiments on different datasets and bitwidth configurations.	0
the major drawback of the paper is the lack of experimentation with real datasets.	0
in summary, the proposed weaklysupervised moment alignment network (wman) utilizes a multilevel coattention mechanism to learn richer multimodal representations for language based video retrieval.. pros: 1. significant performance improvement on didemo and charadessta datasets.	2
the authors should double check the characteristics of the datasets that are used, and see if they lack long history dependence properties in intuition.	0
compared to previous work, the learning curve model not only takes hyperparameter configurations into account, but by training it on offline generated data, it is able to model learning curves across different datasets.	0
i think the paper is interesting but needs some polishing to make it easier to read and perhaps some improved experiments in real datasets.	0
pros 1. pretty high pruning ratios (80%) can be used for many datasets (except squad).	0
the released source code includes the lidar, scallop, silverman datasets, but not the phosphorus soil.	0
it feels like they want to learn something big, but they only focus on benchmark datasets.	0
this has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces.	0
another option would be evaluating on the same document membership task, but on datasets that were not seen during training.	0
i do not even know where to start, but to give an example: 1st sentence of the abstract reads: “for numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular spacetime sampling patterns and large missing data rates.”, a sentence that misses the verb.	0
cons:  lacking more realistic experiments on larger datasets such as imagenet and models like resnets.	0
the updated paper does look more convincing, but my main concern remains  all considered datasets and tasks are synthetic and specifically made for the method.	0
cons: 1) experiments are limited: 1a) the method is evaluated on custom and relatively toy datasets.	0
after reading the rebuttal, my concerns remain that: (1) lack of empirical evaluation on more complex real / synthetic datasets; (2) it is still unclear to me how such hierarchy is inferred from single images.	0
the authors show that fast weights can be applied in the continual learning setting, but alone they do not perform that well on the more challenging datasets, with mixed results on how much better they are as compared to a naive finetuning baseline, and they definitely lag behind synaptic consolidation methods.	0
d4: why do we need to filter out both users & items (with interactions<10) for yelp, but only users for the amazon datasets?	0
the evaluated datasets also only contain 27 classes and lack domain diversity.	0
i believe the task is important, but i am not sure if the datasets used are right for this task.	0
with a new train and test set, it is unclear whether the environments were selected to highlight the strengths of the proposed method or the results generalize to other datasets as well.	2
a formal definition of this seems necessary to advance the agenda of generating biased datasets, but the nearest thing provided is the eq in section 3; but this relies strongly on the assumption that is a labelagnostic feature.	0
more generally, my feeling is that this is a truly interesting direction, but i think for it to be a compelling approach the authors need to formally define bias (the sort they are trying to simulate) and then convincingly show that generated samples reflect the sort of biases that plague realworld datasets.	0
on the other hand, there may be an ethical concern with generating datasets that are biased w.r.t sensitive attributes.	0
pros 1. multiple languages in vln datasets.	0
the task assumes that there is no labeled training images at all, but in these days imagelevel class labels are very cheap and readily available in a largescale wellestablished datasets like imagenet; there is no reason to avoid them during training.	0
to make the empirical evaluation more rigorous and convincing, i think the authors should: (1) repeat the adversarial robustness experiments on more commonlyused datasets such as imagenet and cifar, and also evaluate robustness of their models more thoroughly, using techniques like blackbox attacks.	0
more concretely, some of the concerns regarding the paper are the following : 1. the proposed method of using bert as an encoder does not appear really novel as it has been proposed in other works such as xbert [1] and to datasets at a much bigger scale.	0
specifically, i propose the authors to address the following points: 1. the comparison to previous approaches is lacking, important recent works are missing from the comparison tables, while having better results on all of the 3 tested datasets.	0
the method is evaluated on many datasets, however the stateoftheart comparisons are missing several works, and the claim of outperforming stateoftheart is not correct.	0
in summary, i am confused by the inconsistent results on the different datasets and under different attack scenarios (white and black).	0
results are also presented on the mutag, proteins etc graph classification datasets, but these are really too small to be relevant today.	0
2. there're experiments on a variety of datasets, but there is no proper analysis of the obtained results  why it works in some cases but does not work in the other.	0
after reading the paper, my biggest concern lies in the fact that the proposed graph representation does not outperform the existing approaches, especially on the social network datasets.	0
the paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task  the two datasets weren't even combined!	0
pros: 1. the setting is very useful as most of the public datasets are imbalanced.	2
experimental weaknesses: a) there are claims in the paper: 'report stateoftheart results on two datasets for longrange action recognition: charades and breakfast actions' however, there is no comparison to any other work, of which there are many.	0
the tables use different datasets but provide the same message.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
weaknesses: missing experiments on larger datasets and other problems, missing comparisons to more competitive compression techniques.	0
— the idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [didericksen, et al., collaborationbased user simulation for goaloriented dialog systems, convai neurips ws, 2017] — even if in a different setting and not clustered) in any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to followup without public datasets, etc. finally, i don’t think the scope of potential impact (if wellexecuted) makes iclr the ideal venue.	0
strengths 1) their method achieves a significant improvement in performance over baseline selfsupervised video representation learning methods on 2 datasets: ucf101 and hmdb51 2) the paper presents though experiments with different architectures and training settings in order to be fair to the baselines.	2
### 3) insufficiently compelling experimental results ''the experimental results do not clearly convey the benefits of the two proposed approaches, due to a combination of insufficient/inappropriate baselines, limited number of benchmark datasets and metrics, inconsistencies/lack of detail in the way results are reported and not accounting for random variability.''	0
9) you claim to have 5 datasets, but the way i would interpret your setup, you have 5 tasks from 3 datasets.	0
5. in the caption of table 1, it says 'four electronic health records datasets' but the number seems to be three everywhere else?	0
i am also a bit confused by the choice of architecture (attentional lstms) for nontime series data and the lack of explanation how and why these are used for the different datasets.	0
weaknesses:  datasets are very uncommon.	0
have the authors tried a more conventional but exact alternative for such simple datasets ?	0
pros:  experiments in section 2 are novel and interesting, as far as i am aware  experiments are conducted across different datasets and networks, suggesting generalisation  paper is well written (apart from the title.	2
while the method is specifically designed to image classification  which in a way reduces a bit the scope the possible applications but this is a minor remark  i find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets.	0
pros: addresses adaptation of local context and feature dependent modification of receptive field size, experiments are shown to improve semantic segmentation performance in a number of datasets.	2
2. the proposed method is simple but effective, which performs better or onpar with sotas on zsl benchmark datasets under the evaluation of the harmonic average of accuracies.	0
weaknesses and questions: for visual recognition datasets, the attention targets are defined such that we want to maximize the weights between two overlapping objects of differing classes.	0
the model is evaluated on several datasets (e.g., gqa, clevr, vqacp) strengths: 1. the model is novel.	2
realworld datasets are great, but we have little understanding of what effect exactly is helping the classification here and an additional toy setting would increase the clarity of the modeling ideas.	0
the authors find that different datasets behave differently, which makes sense if more or less occlusion is present, but unfortunately the problem mentioned above undermines the results here.	0
for example, the authors should be able to directly use one of the bair robotic interaction datasets (https://sites.google.com/berkeley.edu/roboticinteractiondatasets) with their model, which replicates fairly well the synthetic environment used to test the model but has real data, and compare the accuracy of the predictions to a 2d video prediction model such as svglp (denton and fergus, icml 2018) that does not make any assumptions about the 3d world nor has an objectcentric representation.	0
for example, in figure 4, it's not explicitly said which datasets are used (e.g., i am 'guessing' it's cifar and mnist, but it's not clear).	0
although the proposed approach is interesting and works well on different datasets, one strong concern i have is the high computational complexity of the algorithm, which is as high as o(n^3) (n is the number of nodes).	0
c) why evaluation of incremental learning on cifar100 is not well (acc difference between old and new is larger than other datasets).	0
the method is shown to have comparable results on well known datasets but higher efficiency than 2 rival methods: ensemble anns and bayesian neural networks.	0
since i am not from the field of forecasting, i can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field.	0
cons:  network architecture used for experiments as well as the exact details of the datasets are nonstandard, making results very hard to compare with other papers, so one needs to rely on provided baselines only.	0
for incremental learning, i would personally think of imagenet as a good example and for continual learning of multiple datasets you can consider the existing sequence of 8 tasks benchmarked in [2] and [3] where you can evaluate your method on more realistic images with a total of over 400k images and significant shift in the distributions.	0
we agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period.	0
for example, exploiting well partitioned datasets into different tasks and known task labels is a good starting point, but once such information is not available all bets are off.	0
minor concerns: 1. does the proposed method show meaningful results when the tasks are different but the datasets are the same?	0
or the same task but different datasets.	0
moreover, i appreciate that experiments were conducted on real robot datasets, but this seems to be more of an exercise in latent space anthropomorphism than practical evidence of a feasible control policy.	0
finally, the authors evaluate three sequencetosequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.	0
review:###summary:  key problem: neural architecture search (nas) to improve both accuracy and runtime efficiency of deep nets for semantic segmentation;  contributions: 1) a novel nas search space leveraging multiresolution branches, efficient operators ('zoomed convolutions'), and parametrized expansion ratios, 2) a decomposition and normalization of the latency objective to avoid a bias towards very fast but weak architectures, 3) a natural extension of the optimization problem to simultaneously search for teacher and student architectures in one pass, 4) a novel stateoftheart efficient architecture (fasterseg) found by the aforementioned nas algorithm, 5) a detailed experimental evaluation on 3 datasets and an ablative analysis quantifying the benefits of the aforementioned contributions.	0
overall, the method looks incremental and experimental results are mixed on small datasets so i vote for rejection.	0
(1) reviewer’s major concern is that this method is not very scalable to largescale realworld datasets such as imagenet and sun database.	0
 unlike the pretraining task in concurrent work, the model was pretrained not just on multimodal datasets like conceptual captions but also on textonly corpus like bookcorpus and english wikipedia.	0
the outstanding issue is that the final results are not consistent across datasets, baselines, and metrics which is concerning.	0
the toy dataset, qualitatively, evaluates the decomposition quality with interesting results but is not enough to make conclusions for realworld datasets.	0
a weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40ish trained nns for every dataset and training hyperparameters, but i do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.	0
strengths 1) the introduced dataset is timely and is more challenging than existing reading comprehension datasets, based on the examples presented in the paper.	2
one place where i lack confidence in the results: i am not very familiar with the datasets used (triangles and letterhigh) nor how standard they are in the graphlearning literature.	0
pros:  compared with previous datasets (e.g., miniimagenet, omniglot), the constructed metadataset is larger and more realistic, which contains several datasets collected from different sources  several competitive baselines are compared on this dataset under different scenarios (e.g., different number of shot) with reasonable analysis.	2
introducing features and an actor/critic setup isn't an amazing innovation, but the method works well, has computational benefits, and has compared to many other models on 3 datasets.	0
 lack of experiments in larger contour detection datasets (like semantic border dataset or cityscapes).	0
because of the extensive nature of the research problem, the coverage of each piece of this research problem in this paper could have been more thorough: for example, i would be really excited for a more indepth analysis on methods for enforcing locality, but the authors consider only one method, which is the auxiliary loss, and this method does not seem to be applicable for datasets that do not have attribute labels.	0
however, all datasets selected in experiment are black and white images with low resolution (28 x 28 ?).	0
examples include:  singular/plural nounverb mismatches (rules..and does not change > do, methods ...achieves > achieve)  q_r^(t)  choice of left operands > right  an onehot > a onehot minor issues:  the claim that the neurallp is the only model able to scale to large datasets is a bit too strong given [1]  you say “we could adopt” but you “do adopt” clarification questions:  are 10 times longer rules of any use?	0
i note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including iou, f1 score and cd and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. decision: weak reject because the idea is quite interesting, but i believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.	0
the approach is similar to recent work on geometryaware rnns (tung et al. 2019), but is applied to more realistic scenes (urban landscapes datasets generated using the carla simulator as opposed to simple tabletop arrangements of shapenet objects) and makes fewer assumptions about the camera pose (6 dof camera parameterization as opposed to 2 dof cameras placed around the objects).	0
pros: ' very well presented and clear ' thorough experiments with baselines and comparisons with competitors ' novel and efficient approach of redesigning the classifier as a postprocessing step after the representation training cons: ' i did not find any single value of the 'temperature' coefficient that you use for the different datasets!	2
according to fig 2, it should be around 0.7 for imagenetlt but you should clearly specify the used values for all the datasets.	0
better test errors and within 57x slower than the conventional ibp methods with worse errors cons: > extensive experiments with more advanced networks/datasets would have been more convincing, esp.	0
'discussion points and concerns from the reviewer:'  dataset / batching details please mention how the datasets were split for training and testing the models.	0
the conclusion is reasonable, but the authors may need a more reliable method to compare the similarity between datasets.	0
strengths: — the proposed method has demonstrated strong results on 2 datasets in challenging opendomain settings.	2
in the comparisons sections, some details on datasets are given, but these seem to refer to data for inference.	0
i recommend this paper for publication, but have some comments that should be addressed: major comments:  it would be good to evaluate how well the learned policies transfer between datasets and architectures.	0
to be more convincing, the author needs to post not only bounds but also time costs cross several methods on one or two datasets.	0
the authors report brier score for their synthetic calibration method these datasets, but do not report any modeling results.	0
i believe many machine learning datasets have nmar (not missing at random) type of missing data, but not mcar.	0
pros: 1. the proposed method seems to generalize well to the different datasets with the same network architecture and hyperparameters compared to previous works.	2
 authors have made a strong claim that neural networks can easily fit any training data, but it may be not be true for many datasets.	0
anyway, this is not a big issue, but i encourage the authors can test the proposed method on more challenging datasets and make fair comparisons.	0
this submission is lacking experiments comparing fedavg to the proposed method under these settings (which can be simulated using available datasets).	0
weaknesses: no comparison on downstream tasks for more datasets except mnist.	0
strengths:  the authors show that the method can model various synthetic and realworld datasets that show the efficacy of the method  the method can also generalize to more objects and longer timesteps than trained on.	2
negatives : the experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : 1. it misses out a recent stateoftheart method (slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way.	0
furthermore, [1] also compares against many other sota methods missed out in this paper on the many other datasets datasets including those in this paper but in a more general multilabel setting.	0
the authors conduct experiments in three datasets, and the results show that their proposed meta attacker is more efficient than other blackbox attack methods.	0
a more accurate takeaway would be that on some datasets adversarial training helps but still leaves a gap, while on others it does not help at all and perhaps hurts.	0
the authors use the two tools to visualize the bias learned by various classifiers, cnn, mlp, and logistic regression, trained on three datasets mnist, fashionmnist and cifar 10. there are interesting patterns that emerged from the classifiers trained on mnist and fashionmnist by the classification image technique, but the result from the cifar 10 is not explainable.	0
however image captioning datasets are not mentioned.	0
 easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
but i think given the reliance of the field on these datasets it makes sense, plus i think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method.	0
the experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness) disclaimer: i am completely out of this area but it is an easy read and an interesting angle.	0
a pic with an architecture would be really helpful probably more datasets are required to have a more convincing empirical story section 3.4: even though you can't directly optimize for ber, there are ways that can work, instead of just replacing it with ce, for example this https://arxiv.org/pdf/1608.04802.pdf one critique is based on 3.4 i don't understand how can this be extended to multiple axis of intersecting groups e.g. not just mutually exclusive race values, but also gender for example.	0
the fact that no actual difference in performance between acgnns and acrgnns was noticed in the only nonsynthetic dataset used in the experiment should prompt the author to run experiments with more real life datasets, in order to empirically verify the results, but this is a minor point.	0
experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.	0
 the author claim in the experimental section that their method perform betters on both datasets, however when looking at table 1, wgangp performs better on cifar10 and when looking at the standard deviation we can see that the improvment is not significative.	0
i think this paper will be valuable to the research community for these reasons: (1) the dataset contains a more geologically complex search space comparing to the original nasbench101, whose search space is restrained in certain ways; (2) released metrics include more meaningful information rather than single point value in nasbench; (3) it uses 3 datasets rather than 1. my major concerns, which i will detail later, is the phrasing 'algorithmagnostic' does not truly reflect the difference between their approach and nasbench101, and about the architecture search space design details.	0
 extensive experiments on evaluating 15k architectures over 3 datasets  detailed statistics on the search space  good baseline experiments comparison main concerns about this dataset:  comparing to nasbench101 in terms of 'algorithm agnostic', it is in a 'moreorless' game but not a 'yesorno' one, so that aanasbench does not seem appropriate.	0
however pooling seems extremely useful for ntks and nngps on image datasets.	0
this may be out of scope for this current work, but one way to answer that would be to leverage datasets with more fully labelled factorised data (e.g. dsprites [1]) and present how one should leverage these with , in order to identify the original generative factors.	0
i am leaning towards acceptance, but am concerned with the following: 1. the authors experiment with a limited set of datasets (cifar10 is a relatively easy task), and with a set of noncompetitive baselines (sota for cifar10/100 is 99%/91.3%, see https://benchmarks.ai/cifar10{,0}).	0
my primary concern is to what extent can the new dataset (cater) add to existing video datasets that are also explicitly designed for long term spatialtemporal reasoning, such as video vqa datasets tgifqa[1]/svqa[2].	0
the construction of the dataset focuses on demonstrating that compositional action classification and longterm temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregationbased methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues.	0
pros:  significant reductions in average running time across the various datasets.	2
3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported.	0
the experiments are logically ordered with the initial set covering the standard fewshot learning benchmarks with appropriate baselines (though the results for fewshot tieredimagenet are lacking in this respect), with most essential details given in the main text and full details, including those related to the datasets in question and hyperparameter selection, documented in appendix h. metalearning does seem uniquely wellpositioned for tackling the task of continual learning and it's heartening to see this being explored here with a degree of success  it would be interested to see how its performance compares with standard continual learning methods (such as ewc) on the same task.	0
 the models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application.	0
since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, ondevice ml) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).	0
i'm concerned that the paper only present results on one, small (200200 images) data set.	0
in particular, choosing the split of the data set that achieves the best result, as quoted below, is concerning.	0
 most experiments in the paper are conducted on a small data set and this is a big downside of the paper.	0
my main concern about this paper is the assumption that the data is separable by a quadratic function, which seems to be very restricted (although this is indeed a progress from those who assume linear separability of the data, e.g. brutzkus et al. arxiv:1710.10174, wang et al. arxiv:1808.04685 and oymak & soltanolkotabi 2019 which has a suboptimal dependency on the condition number of the training data set).	0
the idea seems wellmotivated and somewhat new though not revolutionary, and the new data sets are nice (though see comments below about how i'm not qualified to evaluate them), but i don't understand why the proposed algorithm wasn't evaluated on existing data sets as well, and i don't understand why it wasn't compared against other algorithms that purport to do the same thing.	0
it makes a little more sense to refer to a trained generator as having high fidelity to the training data set, but tbh i still don't even like the phrase in that context.	0
it is mentioned in section 4.2 that “we also compared pn and spe using a 64d embedding, but with high dimensional embeddings, both methods are near ceiling on this data set, resulting in comparable performance between the two methods.	0
moreover, the evaluation is completely limited to a single data set (which is good as a strawman but not sufficient to make significant claims).	0
# concern 3: conceptual improvements while i enjoyed the didactic approach of the paper, which first introduces simple test data sets to illustrate the concepts, my main question is about the conceptual improvements that the charts provide in the end.	0
review:###this paper studied the problem of learning the latent representation from a complex data set which followed the independent but not identically distributions.	0
overall, the data sets are interesting but are not especially large (2300 total [low, high] pairs of 230 different quantities).	0
 the claim about the scale for the vietorisrips complex is factually correct, but the statement is misleading insofar as the paper only extracts the complex at a single threshold, whereas persistent homology integrates the multiscale aspect of realworld data sets.	0
pros: they have experimentally shown some improvements in the different mltc metrics with (bertsgm) and mixed methods, compared to the vanilla bert, specially, for data sets with hierarchically structured classes.	2
cons: their experimental improvement for their mixed method, is not as significant for public data sets, less than 1% on average.	0
on one data set they have got 1.6% better accuracy but that is not on average, as how they have reported.	0
here are a few questions and concerns:  how much does the image matter for the singleimage data set?	0
for the experiments on longtext generation using emnlp news 2017, it is not clear how the data is partitioned as training data, validation data and test data to get the results in figures 4(a), moreover for the results for leakgan, rankgan seqgan, maligan, it seems that they are copied from other papers, but again how the data set is partitioned is not clear for example in seqgan's paper, and most likely, the data is partitioned in a different way, so the results are not comparable.	0
i like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; i would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more indepth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large?	0
review:#### summary the paper compares 3 ways to account for the variability of the dynamics model in the td error computation: (i) be robust (take the lowest target value obtained when evaluating the valuefunction on the training distribution of models); (ii) be bayesian (average over models); (iii) domain randomization (compute td error as usual but randomize the dynamics to obtain a more diverse data set).	0
z_{delta t} is initialized as z_0 and then never changed but always appended into the data set.	0
#2 lack of data sets: only one experiment on one data set is reported.	0
 strengths:  the paper is well written and easy to follow  learning graph representation learning is a very important problem  the performance of the proposed approach are strong on the existing data sets weakness  the novelty of the proposed method is marginal  some realcase study on why the model works are not presented.	2
the approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.	0
the idea seems wellmotivated and somewhat new though not revolutionary, and the new data sets are nice (though see comments below about how i'm not qualified to evaluate them), but i don't understand why the proposed algorithm wasn't evaluated on existing data sets as well, and i don't understand why it wasn't compared against other algorithms that purport to do the same thing.	0
the proposed algorithm looks very simple, but it appears that it could be effective through some experiments on two data sets.	0
# concern 3: conceptual improvements while i enjoyed the didactic approach of the paper, which first introduces simple test data sets to illustrate the concepts, my main question is about the conceptual improvements that the charts provide in the end.	0
some nonparametric but nonneural models not implemented in gpus substantially beat lda, and will run on all the big data sets you list, though perhaps not quickly!	0
the model is based on tucker decomposition but the core tensor is decomposed as cp decomposition so that it can be seen as an interpolation between tucker and cp. the performance is evaluated with several nlp data sets (e.g., subjectverbobject triplets).	0
it is understandable that given the premature stage of the causal inference research might not grant standardized data sets at a larger scale, but at least lack of this quantitative scalability test could be acknowledged and the related claims could be a little bit softened.	0
if it contained at least partial results on more realistic data sets, i would vote for strong accept, but in its current form, i find it borderline acceptanceworthy.	0
bsg is acknowledged in the paper, but only small scale comparisons are performed (15 million) while the bsg paper uses a lot larger data sets.	0
overall, the data sets are interesting but are not especially large (2300 total [low, high] pairs of 230 different quantities).	0
 the claim about the scale for the vietorisrips complex is factually correct, but the statement is misleading insofar as the paper only extracts the complex at a single threshold, whereas persistent homology integrates the multiscale aspect of realworld data sets.	0
pros: they have experimentally shown some improvements in the different mltc metrics with (bertsgm) and mixed methods, compared to the vanilla bert, specially, for data sets with hierarchically structured classes.	2
cons: their experimental improvement for their mixed method, is not as significant for public data sets, less than 1% on average.	0
i’m not a retrieval expert, but it seems that retrieval networks are typically trained on other data sets and do additional tricks than just using a pretrained imagenet embedding.	0
it aims at calcium imaging data, but in fact only tests its proposed methods on two synthesized data sets.	0
i understand the two synthetic data sets are meant to assess the viability of the model for the task, but the title calls for an actual application to real data.	0
experiments on simple synthetic data are performed to show the effectiveness, but these experiments are too simple, more thorough experiments on real data sets should be conducted.	0
i like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; i would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more indepth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large?	0
#2 lack of data sets: only one experiment on one data set is reported.	0
edit: post rebuttal, authors have provided evidence of good performance on data sets with k=4 and k=5, addressing my main concern, and have addressed the smaller notes.	0
could the authors report results on at least two more data sets (however small or simple) during the rebuttal? '	0
pros:  an interesting and feasible approach to metalearning  competitive results and proper comparison to stateoftheart  good recommendations for practical systems.	2
 cons:  the analogy would be closer to grus than lstms  the description of the data separation in meta sets is hard to follow and could be visualized  the experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest  fig 2 doesn't have much value remarks:  small typo in 3.2: 'this means each coordinate has it' > its > we plan on releasing the code used in our evaluation experiments.	0
 the proposed approach is competitive with and outperforms vinyals 2016 in 1shot and 5shot miniimagenet experiments.	2
 are the experiments only run once for each configuration?	1
 have you tried on more complex datasets such as cifar?	1
 have you tried on more complex datasets such as cifar?	1
however, not much experiments are shown to justify the advantage of qprop over ddpg.	0
however, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.	0
however, the experiments don’t show a big improvement compared with knowledge distillation alone and i think more experiments are required in imagenet section.	0
 it would be nice to report teacher train and validation loss in figure 7 b)  from the experiments, it is not clear what at the pros/cons of the different attention maps  at does not lead to better result than the teacher.	0
however, the paper could use stronger experiments as suggested to earlier to bolster its claims.	0
 experiment the experiments are welldesigned and thorough.	2
the experiments are performed in cifar10 and imagenet classification problems using models such as alexnet, resnet, and googlerenet.	1
 how does one even draw a 'fair comparison' on these standard datasets at this point?	1
 how does one even draw a 'fair comparison' on these standard datasets at this point?	1
the experiments feel a bit rushed.	0
 how does the performance is impacted when one varies tmax from 1 to 5 (which the chosen value for the experiments i assume)?	1
the paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links.	1
in empirical experiments, samples generated by the baseline composed by multiscale and random filters sometime rival the vggbased model which has multilayer and pretrained filters.	1
cons:  the paper could benefit substantially from additional experiments on different datasets.	0
they also used ablation experiments to show each of them is important.	1
the major experiments are image classification and language models trained on mutations of characteraware neural language models.	1
however, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive.	0
 how is the 'pseudovalidation' data, target to the policy, chosen?	1
 i do not know enough about spns and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt random # remaining questions after the paper updates  table 3: random structure as baseline ok, but how were the parameters here learned?	1
 i do not know enough about spns and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt random # remaining questions after the paper updates  table 3: random structure as baseline ok, but how were the parameters here learned?	1
the experiments show mixed performance versus neural clm approaches to modeling linux kernel data and wikipedia text, however the proposed dsl models are slightly more compact and fast to query as compared with neural clms.	0
 it might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network  is there a reason the subscript on the jacobian changes to a_l in the <gl,v> definition?	1
 other than visual inspection, do you observe improvement in classification using proposed algorithm on mnist experiments?	1
 the experiments lack some details: how are the expert trajectories obtained?	1
however for all the experiments, they 'include a small number of full samples' (full == 'samples with full program traces').	0
 the many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?	1
 what loss do the authors use in their experiments?	1
while this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on cifar.	0
however, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers.	0
 experiments are well done and solid.	1
 experiments are well thought out and highlight the key advantages of the method over synchronous sgd (with and without bn).	2
 why is the neural cache model worse than lstm on ctrl (lambada dataset)?	1
the experiments explore a practical dataset and achieve fine numbers.	2
 paper is wellwritten with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.	2
the experiments in the paper seem thorough but the results are a bit underwhelming.	0
in summary, the idea is a good one, but the experiments are weak.	0
the experiments on two datasets (mnist for vision and timit for speech) shows that the method achieves very good compression rates without loss of performance.	1
overall evaluation === this paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions.	2
however, this work does include more indepth experiments in case of bouncing balls compared to battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio).	2
# contributions  1) `the model can be trained without having to engineer spatiotemporal features'  you would need to collect training data from humans though.. # section 3.1 the number of fixation points is controlled to be fixed for each frame  how is this done?	1
it is not clear to me that an architecture’s ability to model random data should be beneficial in modeling realworld data; indeed, the experiments in section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in section 3 show that these same architectures do not significantly vary in their capacity to model realworld data.	0
it is interesting that the ugrnn can achieve comparable bits per parameter as the ungated rnn and that the deep rnns are more easily trainable than other architectures, but the only experiments on a realworld task (language modeling on text8) do not show these architectures to be significantly better than gru or lstm.	0
while these experiments and results are interesting, the contribution is unclear.	0
2. well written paper, with clear description of the method and thorough experiments.	2
the paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept.	2
however, the paper under delivers: the experiments do not convince me (see 1) and 3)).	0
a well written paper that proposes to use mmd to distinguish generated and reference data.	2
pros:  the organization is generally very clear  novel metalearning approach that is different than the previous learning to learn approach.	2
 cons:  the paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than cnns such as fully connected and recurrent neural networks.	0
this seems problematic but doesn't seem to affect your experiments.	2
 otherwise the experiments seem adequate and i enjoyed this paper.	2
the experiments are good proofs of concept, but do not go beyond that i.m.h.o.	1
this paper derives this socalled 'a2t' learning algorithm for policy transfer and value transfer for reinforce and actorcritic algorithms and experiments with synthetic chain world and puddle world simulation and atari 2600 game pong.	1
the paper is clearly written and both the approach and experiments seem reasonable in terms of execution.	2
cons:  although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multiclass classification task.	0
(issue fixed with updated data) why sparsity for table3 and table5 are different?	1
several experiments are conducted on benchmark datasets (mnist, cifar10, cifar100, svhn) and improvements are demonstrated in most cases.	1
the only thing holding this paper back was unconvincing experiments, which now has been corrected.	0
? x50 less memory usage than alexnet, keeping similar accuracy ? strong experimental results weaknesses ?would be nice to test sqeezenet on multiple tasks ?lack of insights and rigorous analysis into what factors are responsible for the success of squeezenet.	1
[1] http://www.msmarco.org/dataset.aspx [2] https://datasets.maluuba.com/newsqa [3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf	1
the experiments are on toy examples, but show promise.	2
experiments show the proposed method is better able to learn meaningful representations of sequence data.	2
the main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments.	2
the experiments settings are clearly explained and the analysis appears to be complete.	2
more experiments were added, so i'm updating my score.	1
experiments are conducted to show that it leads to improved generalisation error and faster convergence.	1
pros  shows batch normalization to work for rnns where previous works have not succeeded  good empirical analysis of hyperparameter choices and of the activations  experiments on multiple tasks  clarity cons  relatively incremental  several ‘hacks’ for the method (pertime step statistics, adding noise for exploding variance, sequencewise normalization)  no mention of computational overhead  only character or pixellevel tasks, what about wordlevel?	2
[1] http://www.msmarco.org/dataset.aspx [2] https://datasets.maluuba.com/newsqa [3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf	1
the experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments.	0
=== question: in the experiments, what kind of classifier is used?	1
however, from the experiments it's not clear if this will extend to other types of q&a tasks where the answer may be free form text and not be a substring in the document.	0
not only in text but also in the experiments  as the authors partially do in their reply to the reviewers question.	1
summary: this paper describes a set of experiments evaluating techniques for training a dialogue agent via reinforcement learning.	1
but the setting of the experiments is not really explained.	0
it is unclear to me why the paper includes independence tests in the experiments.	0
1. if the point of using the algorithm is to be scalable, why release such a small data set?	1
the superiority of bpn is however becoming more clearly apparent in the subsequent lstm experiments.	0
1. if the point of using the algorithm is to be scalable, why release such a small data set?	1
1. when reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain sgd.	1
however, for each thread this is much smaller compared to earlier experiments on atari games.	0
it is an interesting and novel approach that could generalize well to interesting datastructures and algorithms.	2
the experiments section starts on page 12 whereas for han 2015 the experiments start on page 5. therefore, i believe much of the introductory text is redundant and can be efficiently cut.	2
the artificial experiments are instructive, and the realworld experiments were performed very thoroughly although the results show only modest improvement.	2
2. is there a way to relate the organization of the data to the behavior of this method?	1
overall good paper with detailed and welldesigned experiments.	2
cons: although the conclusions of the paper are based on the experiments on imagenet, the title of the paper seems a little misleading.	0
3. not comparing to stateoftheart: the stateoftheart however is not the basic phog cited here, but “probabilistic model for code with decision trees”, (oopsla 2016) which appeared before the submission deadline for iclr’17: http://dl.acm.org/citation.cfm?id=2984041 on the same dataset, oopsla’16 has accuracy of 83.9%, and on the more difficult task than considered here (see above point).	1
however, the current version of the paper could use some more work: the experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.	0
cons the paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less).	0
4 did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?	1
this paper introduces an actorcritic approach for sequence prediction, and shows experiments on spelling correction and machine translation.	1
4 did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?	1
pros this work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments.	2
4. can you comment on the comparison of your method to those who modelled the extrapolation data with ''uncertainty''?	1
this ability of the rn is thoroughly investigated through a series of experiments on a controlled dataset.	1
however, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.	0
4. in the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?	1
the approach is reasonable and intuitive however, experiments do not show superiority of their approach.	0
however, the experiments only serve as proof of concept.	0
however, i'm hesitant to strongly recommend acceptance because the experiments are weak.	0
however, more detailed experiments and analysis are needed to make this paper significant enough for an iclr paper.	0
however, more controls and detailed analysis are needed to make strong conclusions from these experiments.	0
however the experiments provide limited analysis past the main result (see comments above).	0
4: 'for each experiments' > 'for each experiment' how sensitive are your results to infusion rate?	1
5. for human evaluation, were the outputs of the proposed model and that of the qalstm model judged each judged by both the human experts or one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system?	1
additional questions: 'in section 5, how many datapoints are there in each dataset?	1
overall, the paper addresses a clearcut question with a wellmotivated approach, and has interesting findings on some toy datasets.	2
'all experiments were run with 10 different random seeds': does the environment change as well between the runs, i.e. are the full examples different between the runs?	1
but in the experiments of the paper, the learning rates are not picked according to the theorem.	0
this paper presents design decisions of terpret [1] and experiments about learning simple loop programs and list manipulation tasks.	1
but i still think there is lack of experiments and the results are not conclusive.	0
this paper falls in the category two, but fails to prove it with more throughout and rigorous experiments.	0
 convincing quantitative and qualitative experiments on three challenging datasets (youtube2text, mvad, msrvtt) showing clearly the benefit of the proposed attention mechanisms.	1
overall, i recommend acceptance, but encourage the authors to perform experiments on more datasets.	2
 4) the author shows experiments on variants of resnet.	1
cons:1. the experiments part is kind of weak in comparison with other sketching/projection methods.	0
however it's not shown in experiments.	0
the experiments give some insights into the advantages of the proposed approach, but are very limited.	0
the ideas presented have a good basis of being true, but the experiments are rather too simple.	0
cons  the experiments are on very simple dataset norb.	0
in the quantitative experiments the paper assumes available access to the neural network and its parameters.	1
pro: ' well written ' exhaustive set of experiments ' learning algorithms with decimal representation ' available source code cons: ' no coherent hypothesis/premise advanced ' two or three bold statements without explanation or references ' some unclarity in experimental details ' limited novelty and originality factor typos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).	2
the paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations.	1
however, i lean toward rejecting the paper because the following reasons: 1) all experiments are conducted in very small scale.	0
the paper also experiments with few other baseline models (ablations of the proposed model).	1
probably performing similar experiments but on more layers may be more useful (as the networks are already trained). '	2
however, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.	0
pros  the paper is the first of my knowledge to explicitly measure the bits per parameter that rnns can store  the paper experimentally confirms several intuitive ideas about rnns:  rnns of any architecture can store about one number per hidden unit from the input  different rnn architectures should be compared by their parameter count, not their hidden unit count  with very careful hyperparameter tuning, all rnn architectures perform about the same on text8 language modeling  gated architectures are easier to train than nongated rnns cons  experiments do not reveal anything particularly surprising or unexpected  the ugrnn and rnn architectures do not feel wellmotivated  the utility of the ugrnn and rnn architectures is not wellestablished	1
the paper is clearly written and the experiments seem relatively thorough.	2
interesting set of experiments.	2
pros: ' clear description ' well built experiments ' simple yet effective idea ' no overclaiming ' detailed comparison with related work architectures cons: ' idea somewhat incremental (e.g. can be seen as derivative from bell 2016) ' results are good, but do not improve over state of the art quality: the ideas are sound, experiments well built and analysed.	2
the experiments are modest but sufficient.	2
also how many experiments have you run?	1
also, could you not for example simply distort the dataset along different factors and then look for the quality of the reconstructed images (as in the introduction of reference [2])?	1
and even if ckms could scale to such datasets would they have as good predictive accuracy as convnets on those applications?	1
the purpose is to validate and analyze why the proposed sga is preferred rather than group lasso, e.g. joint training could improve, or the proposed groupsparse regularization outperforms l_21 norm, etc. however, we can't see it from the current experiments.	0
 the experiments, though limited in scope, are relatively thorough.	1
however, the paper can be improved by showing that the model can predict more nontrivial motion flows and the experiments can be strengthened by adding more classifiers besides than c3d.	0
and even if ckms could scale to such datasets would they have as good predictive accuracy as convnets on those applications?	1
however, as it stands, the experiments are not convincing enough.	0
pros:  extensive experiments cons:  sample importance is a heuristic, not entirely well justified  si yields limited insight into training of neural nets  si does not inform curriculum learning	2
and even if ckms could scale to such datasets would they have as good predictive accuracy as convnets on those applications?	1
but this should come in addition to experiments on existing data.	1
however, i think the main flaw is that the advantages of having that architecture are not convincingly demonstrated in the experiments.	0
pros:  interesting and novel idea cons:  improper experimental protocols  missing baselines  missing diagnostic experiments [r1] heess, n., williams, c. k. i., and hinton, g. e.	2
the experiments presented in the paper are nice illustrations but unfortunately insufficient.	0
however, beyond showing the validity of the kl term, one can't conclude much about the overall merit of the method as a multiview learning approach, given the above experiments.	0
the experiments are performed on a 1d mixture of gaussian distribution and bayesian logistic regression tasks.	1
however, the writing is confusing, and as far as i can tell, the experiments and discussion are inadequate.	0
and now just repeating questions from prereview section:  if, instead of swapping, you were to simply train k gans on k splits of the data, or k gans with differing initial conditions (but without swapping) do you see any improvement in results?	1
pros: ' interesting topic ' blackbox setup is most relevant ' multiple experiments ' shows that with flipping only 1~5% of pixels, adversarial images can be created cons: ' too long, yet key details are not well addressed ' some of the experiments are of little interest ' main experiments lack key measures or additional baselines ' limited technical novelty quality: the method description and experimental setup leave to be desired.	2
and the word/entity/ngrams distributions overlap between the 3 datasets?	1
and the word/entity/ngrams distributions overlap between the 3 datasets?	1
mnist and cifar10 are very simple baselines and more extensive experiments are required.	0
and the word/entity/ngrams distributions overlap between the 3 datasets?	1
the results of experiments themselves are for existing algorithms.	1
and will the gan example (figure 7) converge when optimised with real data?	1
the experiments are conducted on the omniglot dataset and are quite convincing.	2
another concern that i have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?	1
pros: there are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.	2
experiments are performed in multiple domains  vision and nlp.	1
the experiments on dialog modeling are mostly negative results, quantitatively.	0
pros:  paper is wellmotivated, exceptionally wellcomposed  provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation  the annealing trick with the straightthrough estimator also seems potentially useful for other tasks containing discrete variables, and the tradeoff in the flush operation is innovative.	1
however, the experiments all seek to demonstrate improved generalization performance.	0
however, the paper lacks sufficiently convincing experimental results, and i encourage the authors to do further experiments that prove significant improvements, at least on cifar10, perhaps on larger problems.	0
simple experiments are proposed using 2d grid worlds to demonstrate skills.	1
the paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks.	0
these experiments also lack sufficient details for replication.	0
however, the details and results of these experiments are not included in the paper, making it difficult to assess the utility of the proposed method and the significance of the results.	0
the paper is however very superficial in terms of experiments, or applications of the proposed scheme.	0
also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.	0
experiments shows the proposed method has better speedup than previous methods like hogwild!	2
experiments are given on 'lowshot' settting.	1
are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse?	1
they also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.	2
besides additional experiments, the paper could also use some reorganization and revision for clarity.	1
cons: (1) the paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation (2) no comparison with an ensemble (3) full experiments on imagenet under the 'partial privileged information' setting would be more impactful this paper is promising and i would be willing to accept an improved version.	0
in summary, here are the pros and cons of this paper: cons  the approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task  only one experimental setup that evaluates learned policy with multitask state representation  no experiments on more realistic scenarios, such 3d environments or highdimensional control problems pros:  this approach enables using the same network for multiple tasks, which is often not true for transfer and multitask learning approaches  novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful  experimentally validated on two toy tasks.	2
however, the current version lacks focus and clean experiments.	0
pros:  reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks  generally very well written and well presented results, with interesting discussion of eigenvalues of hessian as a way to characterize “flat” minima  promising mathematical arguments suggest that esgd has generalization error bounded below by sgd, motivating further research in the area cons / points suggested for a rebuttal: (1) one claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that entropysgd leads to improved generalization and has the potential to accelerate training.“ this does not appear to be supported by the current set of experiments.	1
as an open question to the authors: what breakthroughs should we pursue to derive a gan objective where the discriminator is an estimate of the data density function, after training?	1
the experiments are sound, clear and easy to interpret.	2
the experiments are solid, comprehensive and very useful in practical terms.	2
however, i feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope  i would have liked to see more experiments here.	0
however, there is little novelty in this work: the authors further mostly extend the work of (mi et al., 2016) with more vocabulary selection strategies and thorough experiments.	0
however, the main weakness of the paper is the experiments.	0
at least for omniglot experiments and synthetic task experiments?	1
experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.	1
besides the theoretical analysis, is there any empirical study to justify this trick?	1
however, the way this intuition is empirically evaluated is a bit weak.	0
but, did these issues not exist in the dataset used by tan et al. 2016?	1
but, i didn't learn much from the paper: glove is empirically less good for semantic similarity than other embeddings?	1
this, however could be studied in more detail, for instance showing empirically the trade offs.	0
however, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.	0
can ckms scale to datasets with millions of training and test instances?	1
can ckms scale to datasets with millions of training and test instances?	1
can ckms scale to datasets with millions of training and test instances?	1
however, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.	0
however, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss.	0
however, i think the empirical results are not thorough and convincing enough yet.	0
my bigger concern, however, is that the empirical evaluation is still quite limited.	0
these are two interesting hypotheses, however i don’t see that they have been verified in the presented empirical results.	0
more comprehensive empirical validation could clearly strengthen the paper.	2
however, the way this intuition is empirically evaluated is a bit weak.	0
can we train the model with less data?	1
can you maybe generate synthetic datasets with different dirichlet distributions and assess when the proposed method recovers the true parameters?	1
it is original, clearly presented, and strongly supported by empirical evidence.	2
authors also claim that droppath to provide improvement compared to layer dropping procedure in huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when wellknown data augmentation techniques applied.	0
the empirical results are satisfactorily convincing.	2
fig 4 is anecdotal, fig 5 is essentially a negative result (tsne is only in some places interpretable), so that leaves table 1. i recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.	0
 empirical results are solid with a strong win for rnns over convincing baselines.	1
can you please comment on the differences between your dataset and those as well?	1
however, the only empirical comparison in the paper is to a modelfree approach (cmaes).	0
however, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see section 6).	0
however this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true.	0
however, the empirical validation seems preliminary.	0
however, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method.	0
the paper splits itself in two very different parts  the empirical study of equivariances in existing cnns, and the proposal of equivariance objectives.	1
the paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.	1
however, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.	0
the paper however is quite weak on the empirical studies.	0
comments on evaluation: why does the dashed line in the result figures correspond to twice the depth scale instead of just the depth scale?	1
this paper adopts dib to res nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.	0
however, as an empirical study, this paper comes up somewhat short: 1. exactly one algorithm is shown for the deep learning example.	0
the empirical evidence that this was necessary is provided, however the question of 'why' it is necessary remains open.	0
pros:  an interesting and feasible approach to metalearning  competitive results and proper comparison to stateoftheart  good recommendations for practical systems cons:  the analogy would be closer to grus than lstms  the description of the data separation in meta sets is hard to follow and could be visualized  the experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest  fig 2 doesn't have much value remarks:  small typo in 3.2: 'this means each coordinate has it' > its > we plan on releasing the code used in our evaluation experiments.	2
however, as the paper is purely experimental, another baseline (worse than cca) would be to just have the random projections for 'f' and 'g' (the embedding functions on the two domains), to check that the bad performance of the 'no transfer' version of the model is due to overspecialisation of these embeddings.	0
pros:  the author evaluated the proposed methods on various computer vision dataset  the paper is in general wellwritten cons:  the method seems to be limited to the convolutional architecture  the attention terminology is misleading in the paper.	2
pros:  the paper is generally well organized and written  the qualitative analysis in the experimental section is very comprehensive.	2
could experiments be run on problems relating to the netflix challenge, which is the classic example of a prediction problem with missing data?	1
on the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results.	0
the experimental results however confound model vs inference which makes it hard to understand the significance of the results.	0
quality:  the shown experiments are solid and well done.	2
simple experiments suggest it works well.	1
cons:  the experimental baselines do not appear to be entirely complete.	0
paper strengths:  elegant use of moe for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner  the effective batch size for training the moe drastically increased also  interesting experimental results on the effects of increasing the number of moes, which is expected.	2
the experimental results are quite promising strengths:  the problem setting is very appropriately framed as online sequence prediction via bayesian transfer learning.	2
cons: some experimental details are not clear.	0
strong points:  well written, interesting idea of combining various sources of information in a bayesian framework for seq2seq models handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here  which is definitely of interest to the community  strong experimental section, with some strong results (though not complete: see weak points)	2
 weak points:  authors do not improve on computational complexity (w.r.t tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)  what about the baseline where you only combine direct, lm and bias contributions (no channel)?	0
the experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments.	0
pros:  the idea is novel  the approach is described clearly cons:  the experimental evaluation is not convincing, e.g. no improvement on svhn  number of parameters should be mentioned for all models for fair comparison  the effect of droppath seems to vanish with data augmentation	2
could experiments be run on problems relating to the netflix challenge, which is the classic example of a prediction problem with missing data?	1
could it be possible to show results with larger dataset?	1
however, i am not convinced by the experimental results presented in the paper.	0
cons:  weak experimental results (do not really support the claim of the authors).	0
could it be possible to show results with larger dataset?	1
could the authors analyze the effect empirically, on the distribution of the gradient norms?	1
the experimental results are solid and provide new insights.	2
overall: experimental paper with interesting results.	2
however, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated.	0
in general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the iclr conference.	0
i like the idea of the paper, however, the experimental evaluation is not convincing.	0
in summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.	0
overall this paper presents a strong and novel model with promising experimental results.	2
could the authors analyze the effect empirically, on the distribution of the gradient norms?	1
the experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing.	0
significance: the paper introduces a nice idea, and present nice experimental results.	2
cons:  lack of good enough experimental results.	0
could the authors show experiments in this setting?	1
however, for some of the basic tenets of network morphing, experimental evidence is not given in the paper.	0
however, i have doubts on some aspects of the experimental and analytical settings adopted in this paper, which (in my opinion) might limit its impact in both theoretical understanding and practical applicability.	0
overall: experimental paper.	1
the experimental side, however, is somewhat lacking.	0
however, the significance of the work is restricted by the limited experimental settings (both datasets and network architectures).	0
however, i think the paper is lacking in experimental results.	0
including experimental details in a supplementary section could help assuage these fears.	2
the biggest concern w the paper though is experimental results.	1
pros:  good experimental results cons:  ideas are quite trivial  the experiment on ptb was carried out improperly	2
could the proposed metric be compared to a direct measure of crosscorrelation between latent factors estimated over the 2d shapes dataset? '	1
the experimental results are unconvincing, and i suspect compare log likelihoods in bits against competing algorithms in nats.	0
the experimental section presents misleading results.	0
the experimental results on wn18 and fb15k seem pretty good.	2
however experimental results seem to fail to be convincing in that regard.	0
3. without further explanations and analyses about the experimental results, the contribution of the paper seems limited.	0
weak points:  experimental results are only proofofconcept in toy setups and do not clearly demonstrate benefits of the proposed architecture.	0
however, some details and the main experimental results are not convincing enough to me.	0
pros:  interesting and novel idea cons:  improper experimental protocols  missing baselines  missing diagnostic experiments [r1] heess, n., williams, c. k. i., and hinton, g. e.	2
in summary, here are the pros and cons of this paper: cons  the approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task  only one experimental setup that evaluates learned policy with multitask state representation  no experiments on more realistic scenarios, such 3d environments or highdimensional control problems pros:  this approach enables using the same network for multiple tasks, which is often not true for transfer and multitask learning approaches  novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful  experimentally validated on two toy tasks.	2
could you please add some experiments like fig 2 and 3 for the perturbated gan for comparison?	1
the experiments are made on different continous domains and show interesting results the paper is well written, and easy to understand.	2
however, the experimental and theoretical justification of this method need to be improved before publication: 1. experiments.	0
could you verify this in your experiments?	1
the experiments are well carried out and strongly support the presented idea.	2
however, this section lacks measurements and experimental results showing the effects of these choices.	0
however, the experimental results section can be made better, for example, by matching the results on cifar10 as reported in springenberg et al. and trying to improve on those using information dropout.	0
however, the paper lacks sufficiently convincing experimental results, and i encourage the authors to do further experiments that prove significant improvements, at least on cifar10, perhaps on larger problems.	0
did it work best empirically?	1
however, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things.	0
pros:  sound model for sentence ordering  strong experimental results cons:  might be a bit incremental  usefulness of sentence ordering	2
the experiments are well chosen, and the fewshot learning results seem pretty solid given the simplicity of the method.	2
the experiments are well conducted and clearly exposed.	2
however, the experimental results are not significant enough to compensate the lack of conceptual novelty.	0
pros:  an interesting and feasible approach to metalearning  competitive results and proper comparison to stateoftheart  good recommendations for practical systems cons:  the analogy would be closer to grus than lstms  the description of the data separation in meta sets is hard to follow and could be visualized  the experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest  fig 2 doesn't have much value remarks:  small typo in 3.2: 'this means each coordinate has it' > its > we plan on releasing the code used in our evaluation experiments.	2
pros:  creates 'structured' sparsity, which automatically improves performance without changing the underlying convolution implementation  very simple to implement cons:  no evaluation of how pruning impacts transfer learning i'm generally positive about this work.	2
my bigger concern, however, is that the empirical evaluation is still quite limited.	0
did you compute the graph of the figure 4 on the bird dataset?	1
however, the authors cite bytenet, which builds upon pixelcnn, only at the end of their manuscript and do not include it in the evaluation.	0
the goal is clearly defined and well carried out, as well as the experiments.	2
however, several details of the pattern extraction process are not very clear, and the evaluation is conducted on a very specific task, where predictions are made at every word.	0
did you use it on real datasets?	1
however, in my opinion the evaluations in the paper are not convincing.	0
however, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated.	0
however, quantitative evaluation is rarer.	0
i like the idea of the paper, however, the experimental evaluation is not convincing.	0
in summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.	0
the paper is well written, clear in its presentation and backed up by good experiments.	2
however, dynamic evaluation can improve the other methods as well.	0
however, i won’t champion the paper as the evaluation could be improved.	0
cons:  no evaluation of the architecture choices.	0
did you use it on real datasets?	1
cons: ' the evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. '	0
the evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.	0
however, the description of the authors' approach is merely a page, and its evaluation is only another page.	0
however, there are few problems with the evaluation:  in the highway network experiment, the author does not compare with a baseline.	0
on a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended.	0
however, i won’t champion the paper as the overall clarity and evaluation could be improved.	0
however, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (chen et al., microsoft coco captions: data collection and evaluation server, corr abs/1504.00325 (2015)).	0
however, my biggest concern is still with evaluation.	0
in summary: pros:  interesting idea  seems to improve performances cons:  paper writing  weak evaluation (only one dataset)  compare only with approaches that does not use the lasttimestep error signal	2
pros:  an interesting and feasible approach to metalearning  competitive results and proper comparison to stateoftheart  good recommendations for practical systems cons:  the analogy would be closer to grus than lstms  the description of the data separation in meta sets is hard to follow and could be visualized  the experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest  fig 2 doesn't have much value remarks:  small typo in 3.2: 'this means each coordinate has it' > its > we plan on releasing the code used in our evaluation experiments.	2
do they think this approach would scale when the data bases are huge (millions of rows)?	1
however, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.	0
 4.1: the authors state: “the number n of teachers is limited by a tradeoff between the classification task’s complexity and the available data.” however, since this tradeoff is not formalized, the statement is imprecise.	0
however maximum entropy approaches are an equally valid modelling scenario, where information is given in terms of constraints rather than data.	0
does it work on nonface datasets?	1
however, in data such as bird songs high correlations might exist between nonadjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions.	0
however, comparisons on this level are also difficult as a more complex models needs more data.	0
does this assume the data about individuals is extensive, so such sharing is not necessary?	1
does this correspond to a measurable characteristic of the dataset?	1
doesn't this really depend on the empirically learned distribution of training samples (i.e. p(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)?	1
the experiments show mixed performance versus neural clm approaches to modeling linux kernel data and wikipedia text, however the proposed dsl models are slightly more compact and fast to query as compared with neural clms.	0
in the introduction, the author states that 'existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.'	0
finally, i have a simple question: where is input data x (not sampled data) is used in algorithm 1?	1
finally, i'm not familiar with state of the art on this dataset... do the comparisons accurately reflect it?	1
finally, i'm not familiar with state of the art on this dataset... do the comparisons accurately reflect it?	1
the paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear.	2
for example, can regularizer parameter lambda be predicted given the data  is there a property in the data that can help guessing the right lambda?	1
for realworld data, how are the training data (y, sigma) generated?	1
for most applications of rnns, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of realworld tasks, we hope that rnns can learn to model data that is anything but random.	0
for the concierge data, what would happen if ‘correct’ meant being the best, not among the 5best?	1
for the results on realworld data, in table 1, how is the nll computed?	1
fourth, they argue johnson et al. (2016) can overcome this issue partly due to svi; how does data subsampling affect this behavior?	1
given that eeg data contain mostly frequential information, how is this properly handled in persample embeddings?	1
given that gans are not easy to train, how often does the training fail/were you able to reuse the hyperparameters across all experiments?	1
given the nature of bottomup topdown evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials?	1
ifttt dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures.	0
however, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, i feel that this contribution carries substantial weight in terms of expected future rewards.	0
however, since the processing of sequential data seems to be a broad and general area of application, it is conceivable that this work will be useful in the design and application of future cnns.	0
have the authors tried applying vlae to such datasets?	1
have the authors tried applying vlae to such datasets?	1
have these experiments been run?	1
how can we assess how much the model is 'replaying' training data?	1
how do you choose r_0 in you experiments?	1
how is this baseline applied to this data?	1
however the presented vib assumed p(x, y) as the ''underlying data distribution'' (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x).	0
how was the test set for this dataset for the table 1 results created?	1
pros:  the idea is novel  the approach is described clearly cons:  the experimental evaluation is not convincing, e.g. no improvement on svhn  number of parameters should be mentioned for all models for fair comparison  the effect of droppath seems to vanish with data augmentation	2
authors also claim that droppath to provide improvement compared to layer dropping procedure in huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when wellknown data augmentation techniques applied.	0
how would these differ from the labels of 'what will happen' in the proposed dataset?	1
the idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive.	0
however, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in eqs.	0
i guess is it because this representation generalises better when training data is not that representative for the future?	1
my only negative point is that this work might be more relevant for a data science or medical venue rather than at iclr.	0
al 2015 as a model which 'learns ... indirectly rather than from explicit information of where humans look', however the their model has been trained on fixation data using maximumlikelihood.	0
i have a few qualms with the experimental setting:  is figure 2 obtained from a single (i.e. one per setup) experiment?	1
however, the current version of the paper could use some more work: the experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.	0
i have only a few comments regarding experiments and link to prior resarch: experiments:  in table 2 (and for other datasets as well), could you include an svm baseline?	1
i have only a few comments regarding experiments and link to prior resarch: experiments:  in table 2 (and for other datasets as well), could you include an svm baseline?	1
i have only a few comments regarding experiments and link to prior resarch: experiments:  in table 2 (and for other datasets as well), could you include an svm baseline?	1
i have three main issues with the paper in its current form, if these can be addressed i believe the paper would be significantly strengthened: 1) although the recursive splitting approach for extracting the 'keyframes' seems reasonable and the feature selection is well motivated i am missing two baselines in the experiments:  what happens if the feature selection is disabled and the distance between all features is used ?	1
i think the lm may be also the key to explain why noisy channel is much better than direct model in table 3. a couple minor questions are 1. it is not very clear to me is your direct model in the experiments ssnt or sequencetosequence model?	1
i understand it takes o(n) instead of o(1) for pixelcnn method to do sampling, but is that the main reason that the experiments can only be conducted in low resolution (32 x 32)?	1
i understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement lsr1 to train their deep models?	1
i would like clarification on the following: (a) are the data point dependent vectors z generated from the forward model or taken from the approximate posterior?	1
if so, to what size in all the experiments?	1
if the data is unbalanced, namely, some classes have very few examples, how would that affect the model?	1
however, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.	0
if the dataset contains only redcapped mushrooms, would the color selectivity index for this neuron be high or low?	1
however, in practice the high dimensionality is typically dealt with using a lookuptable structure (encoding), which is not difficult to train at all, because the gradient for each data point only involves one entry in the lookuptable.	0
if this is true, then it is also difficult to answer the question like: will the data dependence be a problem?	1
pros:  thoughtful methodology with sensible design choices  potentially useful for smaller (n < 10000) datasets with a lot of statistical structure  nice connections with sumproduct literature cons:  claims about scalability are very unclear  generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.	2
in other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?	1
in particular:  the training datasets are tiny, from sets of 1 image to 56. what is the reason for not using larger sets?	1
in particular:  the training datasets are tiny, from sets of 1 image to 56. what is the reason for not using larger sets?	1
in the abstract the authors mention “extensive experimental results …”, but i find the experiments not very convincing: with experiments on the usps handwritten digits dataset (why not mnist?	1
however this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true.	0
however, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature).	0
in the abstract the authors mention “extensive experimental results …”, but i find the experiments not very convincing: with experiments on the usps handwritten digits dataset (why not mnist?	1
however, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.	0
 the reliance on the scanned text dataset does not help; however the imagenet results are definitely very encouraging.	0
with 10m product pairs, however, this doesn't seem to be the case for the amazon dataset (although i haven't worked with this dataset myself so perhaps i'm missing something... either way it's not discussed at all in the paper).	0
in the abstract the authors mention “extensive experimental results …”, but i find the experiments not very convincing: with experiments on the usps handwritten digits dataset (why not mnist?	1
however in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains.	0
in the experiments, a few things need to be discussed further:  what is the running time of the proposed approach?	1
paper strengths:  i agree that models can benefit from diverse set of datasets.	2
in the related work, a discussion of the relation to semimdps would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the prereview questions) experiments: can you provide error bars on the experimental results?	1
the paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.	2
that is, this manipulation does not achieve the goal of concentrating “most of the data points in very few linear regions.” a far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer.	0
most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics.	0
the experiment 'bad initialization on mnist' shows that for very negative biases or weights drawn from a noncentered distribution, all relu activations are 'off' for all data points, and thus, optimization is prevented.	0
in the related work, a discussion of the relation to semimdps would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the prereview questions) experiments: can you provide error bars on the experimental results?	1
in what sense is it fair to simulate data using another neural network?	1
in your data, do you calibrate the mfcc scale to be closer to the auditory systems of the animals that generated the song?	1
perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in rezende & mohamed (2015), the main decision choice is the flow length; tran et al. (2015), the size of the variational data; ranganath et al. (2015), the flow length on the auxiliary variable space.	0
it might be true that the floatingpoint unit itself might consume less energy due to smaller bitwidth of the operands, however a large fraction of the total energy is spent in data movement to/from the memories.	0
is it because your model is difficult to scale to large datasets?	1
the analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. still: this is a nice work!	0
is it because your model is difficult to scale to large datasets?	1
 limited data results look promising.	1
strengths: 1. the paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowdworkers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.	2
however, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (chen et al., microsoft coco captions: data collection and evaluation server, corr abs/1504.00325 (2015)).	0
however, i would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowdsourced questions.	0
is it necessary to discuss exploding/vanishing gradients when the rnn experiments are carried out by an lstm, and handled by the cell error carousel?	1
however, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.	0
is the “truncated wikipedia dataset” used for training the standard text8 dataset?	1
however, rbm is primarily used to model binary data, realvalued rbm (gaussianrbm) is not a wellrecognized model for realvalued data.	0
is the “truncated wikipedia dataset” used for training the standard text8 dataset?	1
however, when the missing data consists of iid randomly missing pixels, it seems that every region will be missing some information.	0
is the learning rate schedule the same in all experiments of each table ?	1
is the link in footnote 4 the one that is used in the experiments?	1
icml 2011 positive points  using more data and an improved cnn architecture, this paper improves on prior work for bidirectional image/audio retrieval  the presented method performs efficient acoustic pattern discovery  the audiovisual grounding combined with the image and acoustic cluster analysis is successful at discovering audiovisual cluster pairs negative points  limited novelty, especially compared with harwath et al, nips 2016  although it gives good results, the clustering method has limited novelty and feels heuristic  the proposed method includes many hyperparameters (patch size, acoustic duration, vad threshold, iou threshold, number of kmeans clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices	0
however, the dataset for this task is nonstandard and results are provided for only a single baseline.	0
strengths: a method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data.	2
is the rnn finetuned on the labelled imdb data?	1
is there a reason to construct the training data similar to cheng & lapata, if that turns out to be a better method?	1
is there some differences in the experimental setting?	1
however, for the case where a cnn is trained it seems that it is likely to be in a largescale data and iterative training regime, and it is not obvious that kissme would be more efficient than other metric learning objectives based on a pairwise or triplet loss.	0
i am also confused about the dimensionality reduction role of matrix w. in eq. (5) w^t is used to take the data down from dimension d to dimension d. however, when applied to a cnn, the dimensionality reduction is assumed to be incorporated in the network itself, such that the dimension of the cnn representation is d from the outset.	0
is this confirmed over different models and maybe datasets?	1
is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?	1
strengths  interesting to explore the connection between relu dnn and simplified sfnn  small task (mnist) is used to demonstrate the usefulness of the proposed training methods experimentally  the proposed, multistage training methods are simple to implement (despite lacking theoretical rigor) weaknesses no results are reported on real tasks with large training set not clear exploration on the scalability of the learning methods when training data becomes larger when the hidden layers become stochastic, the model shares uncertainty representation with deep bayes networks or deep generative models (deep discriminative and generative models for pattern recognition , book chapter in “pattern recognition and computer vision”, november 2015, download pdf).	2
it looks like the number of biclusters used for this method in the experiments is only 35?	1
it seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations?	1
the authors claim stateoftheart, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches.	0
just a sample: 'independant' (fig.1), 'we evaluate an handwritten', ', hand written words [..], an the results', 'their approach include', 'the letter bigrams of a word w is', 'for the two considered database'  wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process?	1
more importantly i suspect methodological problems with the experimental comparisons: the paper mentions using 'default' values for learningrate and momentum, and having (arbitrarily?)	1
o what data do you train on?	1
of course, more experiments would be even nicer, but is it ever not the case?	1
however instead another objective is added again without justification, and the conditional entropy of z is left disconnected from the data it is to be conditioned on.	0
on the duc dataset, it compares the model with other uptodate models, while on the gigaword dataset paper only compares the model with the abs rush et al. (2015) and the gru (?	1
however, i'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.	0
the paper is wellwritten, and the experiments extensively evaluate the approach with 3 different rl algorithms in 3 different domains (atari, mujoco, and torcs).	2
on the experimental side, i would have liked to see something more than nll measurements and samples  maybe show this is useful for other tasks such as classification?	1
one last question is cifar has three channels and mnist only one: how it this handled when pairing the datasets in the second set of experiments?	1
cons:  the paper could benefit substantially from additional experiments on different datasets.	0
however, if the authors can show better results on cifar10, imagenet, mscoco or some other more diverse and challenging dataset, i would be more convinced of the value of the proposed method.	0
one last question is cifar has three channels and mnist only one: how it this handled when pairing the datasets in the second set of experiments?	1
one of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms?	1
ifttt dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures.	0
however, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, i feel that this contribution carries substantial weight in terms of expected future rewards.	0
only the activations of the last layer are evaluated, but on what data ?	1
overall, the change of adding a termination gate is rather modest, and it leads to rather small gains on the tested cnn/ daily mail dataset (is it possible to include significance here?).	1
however, the advantage will be greater when the number of classes are huge, which doesn't seem to be the case in a simple dataset like mnist.	0
good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.	0
page 6, 6.3: 'in experiences with vgg': in experiments?	1
however, mtl is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``deep learning' for these simple datasets.	0
pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.	2
perhaps you could run the algorithm on a random subset of data and extrapolate from that?	1
weak points:  cifar is still somewhat toy dataset with only 10 classes.	0
fig 4 is anecdotal, fig 5 is essentially a negative result (tsne is only in some places interpretable), so that leaves table 1. i recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.	0
they do a reasonably extensive evaluation with similar approaches and motivate their approach well.	2
pros  shows batch normalization to work for rnns where previous works have not succeeded  good empirical analysis of hyperparameter choices and of the activations  experiments on multiple tasks  clarity cons  relatively incremental  several ‘hacks’ for the method (pertime step statistics, adding noise for exploding variance, sequencewise normalization)  no mention of computational overhead  only character or pixellevel tasks, what about wordlevel?	1
however, models trained on reallife datasets can often exploit simple object properties (not relationbased) to identify relations (eg: animals of bigger size are typically predators and smallsize animals are preys).	0
the new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper).	0
questions:  can you show empirically that the proposed higher order nonlinearity produces sparser representations than the complex modulus?	1
regarding the experiments,  could you present some baseline results on flashfill benchmark based on previous work?	1
second, they claim the kl regularizer forces the approximate posterior to be close to this uniform; this is only true for small data, certainly the energy term in the elbo (expected loglikelihood) will overpower the regularizer; is this not the case in a meanfield approximation to a mixture of gaussians model?	1
the manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.	2
section 4.2.1: why can't you use the original data?	1
pros:  thoughtful methodology with sensible design choices  potentially useful for smaller (n < 10000) datasets with a lot of statistical structure  nice connections with sumproduct literature cons:  claims about scalability are very unclear  generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.	2
section 4.24.3: when generating the datasets, did you verify that the test set is disjoint from the training set? '	1
section 4.24.3: when generating the datasets, did you verify that the test set is disjoint from the training set? '	1
cons  the experiments are on very simple dataset norb.	0
section 8: there is a typo in 'experiments' section 8.1: 'we simplicity, we binarized' i think there's a problem with the english language in this sentence section 8.3: 'we report that dropout helps'.. this is quite general statement, only tested on a synthetic dataset section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time?	1
however, the paper did not provide the results on lfw dataset.	0
section 8: there is a typo in 'experiments' section 8.1: 'we simplicity, we binarized' i think there's a problem with the english language in this sentence section 8.3: 'we report that dropout helps'.. this is quite general statement, only tested on a synthetic dataset section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time?	1
so what is the distribution of t in the testing data?	1
 the reliance on the scanned text dataset does not help; however the imagenet results are definitely very encouraging.	0
so, could the authors please explain how does the reduction in number of parameters help experimentally?	1
with 10m product pairs, however, this doesn't seem to be the case for the amazon dataset (although i haven't worked with this dataset myself so perhaps i'm missing something... either way it's not discussed at all in the paper).	0
specifically, a couple important relevant experiments would have been: ' how do the performance and visualizations change as the number of attention vectors (r) varies? '	1
summary : in this paper a variational inference is adapted to deep generative models, showing improvement for nonnegative sparse dataset.	0
pros:  the proposed approach demonstrates strong performance on fb15k dataset.	2
paper strengths:  i agree that models can benefit from diverse set of datasets.	2
strengths: 1. the paper presents a large scale dataset for machine comprehension.	2
however, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.	0
however, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets.	1
review summary: while the dataset collection method seems interesting and promising, i would be more convinced after i see the following  1. human performance on all (or significant percentage of the dataset).	1
the proposed method improves upon (cnn, daily mail, whodidwhat datasets) or is comparable to (cbt dataset) the stateoftheart results.	1
the proposed model is tested on 4 different dataset.	1
overall the paper is ok, but it has a flavor of 'we ran lstms on an existing dataset'.	0
that makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?	1
however, the datasets seem to only contain single trees or forests, and the stackbased method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks).	0
given that the authors want to analyze the structures between sentences, is the argumentation mining the best dataset?	1
section 4.24.3: when generating the datasets, did you verify that the test set is disjoint from the training set? '	1
most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics.	0
the author should compare with the stoa methods such as https://arxiv.org/abs/1609.04802 2. can the experiments based on ae support the idea that artificial neural networks can perceive an image from low fidelity?	1
the usefulness of the gan discriminative features for semisupervised learning is already established in previous works such as catgan, dcgan and salimans et al. however this paper does a good job in actually getting the semisupervised gan framework working on larger images such as stl10 and pascal datasets using the proposed context conditioning approach, and achieves the stateoftheart on these datasets.	0
however, i lean toward rejecting the paper because the following: 1) no other dataset reported.	0
pros:  interesting dataset and application.	2
paper strengths:  the questions in the dataset are real queries from users instead of humans writing questions given some context.	2
strengths: 1. the paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowdworkers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.	2
however, i would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowdsourced questions.	0
however, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.	0
the authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i, must it only contains pixels above and to the left of x_i (similar to pixelcnn) minor: in equation 8, should there be an expectation over the data distribution?	1
the authors state that they “did not observe a large difference in preliminary experiments”  so if that is the case, then why not choose the correct objective?	1
the clarity of the presentation could be improved maybe by simplifying the experimental setup?	1
experiments on three datasets are presented, however the results are mostly not stateoftheart.	0
the dataset used in this work cannot be shared?	1
the experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. minor comments: third line in abstract: where model > where the model second line in section 7: into space > into the space shouldn't the rhs in eq 3.5 be sum    ilde{y_{t,i}}(frac{hat{y}_t}{  ilde{y_{t,i}}}  e_i) ?	1
they say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is — is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?	1
pros: there are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.	2
cons: lacking baseline such as discriminatively trained convolutional network on standard dataset such as cifar10.	0
however, the dataset for this task is nonstandard and results are provided for only a single baseline.	0
this also suggests that your “preliminary experiments with direct exploration in the parameter space” may not have followed best practices in neuroevolution?	1
this doesn’t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this?	1
this seems to be the case only on mnist dataset and not on cifar?	1
the authors claim stateoftheart, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches.	0
however, it will be much more convincing if a well known dataset and experiment set up are used, such as graves (2013) or sutskever et al (2014), and actual training, validation and test performances are reported.	0
to be more convincing i would like to see the following experiments  comparison with outer product in the identical model  comparison with mcb in the identical model  comparison with elementwise sum instead of elementwise product  one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?	1
were there any experiments to validate that using 1 was a good choice?	1
what datasets would you use to compare the dpmm on for example?	1
what datasets would you use to compare the dpmm on for example?	1
however, the normal update of sgd might also benefit from simd, especially when the dataset is dense.	0
2. qualitative results: 2.1. i like the large number of qualitative results; however, i would have wished the focus would have been less on the “number” dataset and more on the visual genome dataset.	0
the mnist results are shown for a particular convolutional neural network architecture, however, most ladder network results for this dataset have been produced on standard fullyconnected architectures.	0
this is a well written paper with a variety of experiments that support the claims.	2
what hinders this paper from reporting quantitative numbers on real data (e.g., the 2d and 3d face data)?	1
what is the stateoftheart on that dataset?	1
what is the stateoftheart on that dataset?	1
what makes a data set 'good' or publishable?	1
paper strengths:  elegant use of moe for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner  the effective batch size for training the moe drastically increased also  interesting experimental results on the effects of increasing the number of moes, which is expected.	2
however, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, i feel that this contribution carries substantial weight in terms of expected future rewards.	0
where are the quantifications of modelhuman similarities for the data shown in figure 8?	1
however, mtl is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``deep learning' for these simple datasets.	0
cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.	0
however, the current version of the paper could use some more work: the experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.	0
where is the quantification of modelhuman similarity for the data show in figure 3?	1
while the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?	1
however, models trained on reallife datasets can often exploit simple object properties (not relationbased) to identify relations (eg: animals of bigger size are typically predators and smallsize animals are preys).	0
why does the classification error rate of dataset 2 remain stubbornly at 24%?	1
why does the classification error rate of dataset 2 remain stubbornly at 24%?	1
the manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.	2
pros:  thoughtful methodology with sensible design choices  potentially useful for smaller (n < 10000) datasets with a lot of statistical structure  nice connections with sumproduct literature cons:  claims about scalability are very unclear  generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.	2
why does the model with zooming powers outdo the translationonly model on dataset 1 (where all target images are the same size) and tie the translationonly model dataset 2 (where the target images have different sizes, for which the zooming model should be tailormade?).	1
why does the model with zooming powers outdo the translationonly model on dataset 1 (where all target images are the same size) and tie the translationonly model dataset 2 (where the target images have different sizes, for which the zooming model should be tailormade?).	1
however, the significance of the work is restricted by the limited experimental settings (both datasets and network architectures).	0
why is your yelp 2013 dataset smaller than the original tang et al, 2015 paper that has ~300k documents?	1
paper strengths:  i agree that models can benefit from diverse set of datasets.	2
however, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets.	0
however, the datasets seem to only contain single trees or forests, and the stackbased method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks).	0
why not testing the model on a few more widely used datasets for short text classification, such as trec?	1
most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics.	0
why not testing the model on a few more widely used datasets for short text classification, such as trec?	1
the usefulness of the gan discriminative features for semisupervised learning is already established in previous works such as catgan, dcgan and salimans et al. however this paper does a good job in actually getting the semisupervised gan framework working on larger images such as stl10 and pascal datasets using the proposed context conditioning approach, and achieves the stateoftheart on these datasets.	0
however, the datasets that are used are very small.	0
will a randomly initialized deep model such as dbn or cnn perform poorly on these datasets?	1
strengths: 1. the paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowdworkers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.	2
however, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.	0
weakness: 1, as the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method.	0
will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?	1
will the dataset, or code to generate it, be released?	1
pros: introduces an energy function having the leakyrelu as an activation function introduces a novel sampling procedure based on annealing the leakiness parameter similar sampling scheme shown to outperform ais cons: results are somewhat out of date missing experiments on binary datasets (more comparable to prior rbm work) missing pcd baseline cost of projection method	2
however, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.	0
experiments on three datasets are presented, however the results are mostly not stateoftheart.	0
would it be possible to perform similar experiments on this dataset?	1
would it be possible to perform similar experiments on this dataset?	1
would it be possible to perform similar experiments on this dataset?	1
pros: there are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.	2
wouldn’t actionrecognition in videos, for example, not be a better illustrative dataset?	1
wouldn’t actionrecognition in videos, for example, not be a better illustrative dataset?	1
however, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.	0
experiments on two different network indicate that the proposed scheme is better than using handtuned weights for multitask neural networks.	1
 in the experiments, why are the input images downsampled to 320x320?	1
from the current manuscript, i do not feel confident that i could reimplement deep sensing or reproduce the experiments.	0
 another concern is related to experiments.	1
but, incomplete comparisons with similar existing work and limited experiments makes this a weak paper.	0
this paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task.	1
however, a few things are not fully explained, and the experiments are too limited to be convincing.	0
it unfortunately only experiments with ccnn architectures with a small number (eg 3) layers.	0
there is also some initial experiments in fig 3a.	1
the authors employ a reasonable evaluation criterion in their experiments: the median squared euclidean distance between the original and adversarially modified data point.	1
pros:  good new implementation of an existing idea  significant perplexity gains on character level language modeling  good at domain adaptation cons:  memory requirements of the method  wordlevel language modeling experiments need to be run on larger data sets (edit: the authors did respond satisfactorily to the original concern about the size of the wordlevel data set)	2
b) i had a lot of issues with the gan experiments already and i don't think the paper should be accepted unless those are addressed.	0
experiments show that this provides faster progress per iteration on the game of hex against a fixed third party opponent.	1
i would like to see simple targeted experiments aimed at testing how much and in what way tunneling is a problem in current methods before i see high dimensional non quantitative experiments.	1
the authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching.	1
in the experiments, the authors evaluated memorygan on three datasets, cifar10, affinemnist and fashionmnist, and demonstrated the superiority to previous models.	1
the methods and experiments are poorly explained.	0
answers to these questions can automatically determine suitable experiments to run as well.	1
experiments evaluate the ability to port the model learned in an unsupervised manner to semantic segmentation tasks, using a limited amount of supervision for the end task.	1
did you use this in the experiments?	1
many mistakes in the presentation and experiments.	0
since the provided experiments are mostly illustrations, i would argue that the significance of the paper is limited.	0
although authors argue in section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller, experiments could have been performed to show their impact.	0
weakness: several issues in terms of concept, methodology, experiments and analysis.	1
the paper is well written, derives cleanly from previous work, and has solid experiments.	2
experiments demonstrate the proposed scheme outperforms the stateoftheart methods.	2
beyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technical content, experiments, and related work: i'm wondering  can the reward shaping in equation 2 be made to satisfy the property of not affecting the final policy?	1
this is a simple and sensible idea, and empirical experiments convincingly demonstrate the advantage of the method in large scale distributed training.	2
"in those experiments, our model shows to be significantly less prone to overfitting than a traditional feedforward network of same size, despite having more parameters."""	0
"3d printing experiment transformations: while the 2d and 3d rendering experiments explicitly state that the sampled transformations were random, the 3d printing one says ""over a variety of viewpoints""."	1
it is not clear why the learned matrix should outperform gaussian kernel, but the experiments show that it does.	2
"""may not helpful"" > ""may not be helpful"" ""vocabularie"" > ""vocabulary"" ""system first retrieval"" > ""system first retrieves"" comments on revisions: i appreciate the authors including the new experiments against concatenation baselines."	2
 experiments on three different tasks indicating the potential of the proposed technique.	2
furthermore, the setting for the experiments is fairly simple, consisting of a gridworld with 1channel input features.	1
review: (1) pros the proposed optimization method considers the dynamic selfadjustment of the learning rate in the optimization based on the ratio between the l2norm of parameters and that of gradients on parameters when the batch size increases, and shows improvements in experiments compared with previous methods.	2
the experiments are conducted on text data sets to evaluate the proposed method.	1
experiments do demonstrate improved effectiveness in the chosen domains, and the authors do a nice job of illustrating the range of performance by their approach (which has low variance in some domains, but high variance in others).	2
 the mtfl experiments look most convincing (although this might be because i am not familiar with sota on the dataset), but still there is no control for the number of parameters, and the performance improvements are not huge  on cifar10  there is a marginal improvement in performance, which, as the authors admit, can also be reached by using a deeper model.	0
"experiments with inception models on imagenet show increased robustness both against ""blackbox"" attacks using heldout models not used in ensemble adversarial training."	2
i think that the paper need some more work, in particular to make more convincing experiments that the benefit lies in cia (baselines comparison), and that it really is robust across these defenses shown in the paper.	2
overall:  as is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of at.	0
the authors conduct experiments on several real but relative smallscale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets.	1
major weaknesses:  the main weakness of this paper lies in its weak experiments.	0
 in terms of substance, the experiments don't really add much value in terms of general lessons.	0
however, i don’t see any demonstration of this in the experiments section.	0
the experiments are ok but i would have liked a more thorough analysis.	0
2. cheap soft unitary constraint 3. efficient cuda implementation (not experimentally verified) cons  1. some experimental setups are unfair, and some other could be clearer 2. only small scale experiments (although this factorization has huge potential on larger scale experiments) 3. no wallclock time that show the speed of the proposed parametrization.	0
overall this is a clearly written paper that is easy to follow, with experiments that are well motivated.	2
while the experiments are compelling, as i explain below, i believe there is an underlying assumption that is not considered.	0
is this setting ever used in the experiments or is the negative set always the same?	1
the authors study, using experiments, what aspects of human priors are the important parts.	1
however, i am not sure it will bring a lot of benefits for readers except those who need review for some reports, introductions in phd thesis, etc. although the authors mentioned some approaches to combine different regularisations, they did not performed any experiments supporting their ideas.	0
experiments on various text classification and sentiment analysis datasets show that the proposed method is competitive with existing approaches.	2
 [experiments are super weak] the paper has a good motivation and a beautiful story and yet, the experiments are poor to verify them.	0
1) how many neighbors are used in the experiments?	1
it presents experiments during training and with varying network sizes.	1
what happens in your experiments as you increase the number of unrolling steps?	1
perhaps most critically, the paper omits much of the existing literature for online optimization of the learning rate and subsequently fails to compare to the appropriate baselines in the experiments, such that it is not possible to assess whether the approach actually provides benefits to existing approaches.	0
the experiments support the conjecture mentioned above and show that the proposed technique 'significantly' improves the detection accuracy compared to 2 other methods across all attacks and datasets (see table 1).	2
in their experiments, enas achieves test set metrics that are almost as good as nas, yet require significantly less computational resources and time.	2
this is fine, except they don’t convincingly pan out in experiments.	0
the experiments are convincing and solid.	2
experiments show improved generalization over minibatch sgd, which is the main positive aspect of this paper.	2
the resulted g2lstm is applied for language model and machine translation in the experiments.	1
the author(s) should do more experiments to various dataset to be more convincing.	0
the experiments are quite limited.	0
in summary, almost all the experiments in the paper are trying to establish improvement over basic gan, which would be ok if the gap between theory and practice is small.	2
overall, many people have shown success with experiments similar to your simple mixture of gaussians experiments, so in order to show something significant here, you will need to have a more challenging experiments and show a comparison to other models.	2
 overall the paper is good: good motivation, insight, the model makes sense, and the experiments / results are convincing.	2
 quality: the experiments are chosen to compare the method that the paper is proposing directly with the method from li et al. (2017).	1
is it possible to set similar experiments with the previously published material on this topic and compare the results?	1
first, i'd like to note that the empirical component of this paper is strong: i was impressed by the breadth of architectures and settings covered, and the experiments left me reasonably convinced that the classification layer can often be fixed, at least for image classification tasks, without significant loss of accuracy.	2
results on a second dataset are reported in the appendix, which greatly increases confidence in the generalizability of the experiments.	2
sadly these experiments are done on private data and it is thus hard to compare with benchmark models and datasets.	0
the experiments seem sound and the information in both the paper and the appendix seem to allow for replication.	2
paper provides numerical experiments for mnist and cirar10 on lenet300100 and lenet5.	1
the lstm experiments seems not a standard benchmark.	0
however, the experiments only focus on identifying important hidden units and fall short of actually providing an interpretation using these hidden units.	0
a few suggestions for experiments: a. i would recommend first doing comparisons between bagofwords representation and the dependencybigram representation, just using log(tf)idf as a distance metric.	1
 overall  the paper tackles an important problem, aims for important characteristics, and does extensive and various experiments.	2
experiments and evaluation have been conducted on the aqad corpus to show the effectiveness of the approach.	2
 the authors report a number of experiments using offtheshelf sentence embedding methods for performing extractive summarisation, using a number of simple methods for choosing the extracted sentences.	1
  pros:  the paper properly compares and discusses the connection between amgan and class conditional gans in the literature (acgan, labelgan)  the experiments are thorough  relation to activation maximization in neural visualization is also properly mentioned  the authors publish code and honestly share that they could not reproduce acgan's results and thus using to its best variant acgan' that they come up with.	2
 are the experiments single runs?	1
2. in nearly all the experiments, the classifier is built on top of the frozen unsupervised features.	1
overall, while i find there are some interesting theoretical bits in this paper, it lacks focus, the experiments do not offer any surprises, and there are no comparisons with prior literature.	0
the paper presents experiments on a variety of scenarios to show the performance of kivi.	1
as written, the paper to me looks like two separate white papers: {beginning  to end of section 3}: as a theoretical white paper that lacks experiments, and {section 4}: experiments with some recent methods / datasets (this part is almost like a cute course project).	1
the writeup is somewhat confusing, and in particular the reader has to constantly refer to the supplementary materials to make sense of the models and experiments.	0
given the incremental process, i find the presentation unnecessarily involved, and experiments not convincing enough.	0
given the incremental process, i find the presentation unnecessarily involved, and experiments not convincing enough.	0
if you tried richer things and found that they didn't help, you should show ablation experiments.	1
authors are suggested to perform experiments on more datasets to make the results more convincing.	0
i therefore am quite confused about the fact that the authors present experiments on classification tasks, with a method that writes for regression.	0
therefore a fair experiments it to find a universal set of parameters and use them across different datasets.	1
regarding the experiments, presenting results with multiple evaluation criteria and showing more qualitative results would improve the exposition.	1
"5. in the experiments section, it is mentioned that ""...inputs to the hidden neurons converge to a distinctly nonzero value."	1
more experiments with different number of layers and different architecture like resnet should be tried to show better results.	2
see e.g. yang et al. “joint unsupervised learning of deep representations and image clusters”, cvpr 2016. to conclude, the paper has some interesting ideas, but the presentation is not convincing, and the experiments are substandard.	0
the experiments on the linzen et al. (2016) dataset is also interesting, as it shows that for hard examples, the different models do have different behavior (even when the difference are not noticeable on the whole test set).	2
i will adjust my score accordingly if the experiments are presented.	1
the experiments show the proposed approach is effective on 14 uci datasets.	1
the experiments were performed on relatively shallow networks (8 to 26 layers).	0
in the experiments the authors generally show improved convergence over svae.	2
 how robust are the eigen options for the atari experiments?	1
 proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem  detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches.	1
moreover, to justify the effect of the randomness, the paper should have empirical experiments.	1
the experiments do not really convey how well this all will work in practice.	0
## quality/science of experiments the experimental results have been updated, and the performance of the baseline now seems much more reasonable.	2
it would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens.	1
> transfer >> my reading of the transfer experiments is that they are basically unsuccessful.	0
only two datasets are used in the experiments, which leaves questions on whether the results will generalize to other datasets.	1
however, their methods and experiments are only applied to guided policy search (gps), which seems like a specialized rl algorithm.	1
 i think your result in theorem 3.2 would be significantly stronger if you could provide an analysis of the bound you obtain: in which cases can we expect certain terms to be larger or smaller, etc.  your experiments in section 4.2 show a nontrivial degradation of the accuracy with feta.	1
pros:  baseline performance is exceeded by a large margin  novel use of adversarial perturbation and temperature  interesting analysis cons:  doesn't introduce and novel methods of its own  could do with additional experiments (as mentioned above)  minor grammatical errors	1
extensive and convincing experiments.	2
the introduction/experiments section of the paper is not well motivated.	0
moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work.	0
nevertheless, it is well written and i think it is solid work with reasonable convincing experiments and good results.	2
regarding the experiments: here i see a few problems.	1
the experiments are only limited to babi task which doesn’t tell you much.	1
this argument is not well supported by the experiments.	0
i don’t think is very explicit what k is in the experiments with bouncing balls.	0
follow up experiments extend the basic setup significantly.	2
the experiments are conducted on celeba and lsunbedroom datasets.	1
though the effect is most obvious from the speech recognition experiments in section 4.3, mp also achieves slightly higher performance than baseline for all imagenet models but inceptionv1 and for both object detection models; these results add support to the idea of fp16 as a regularizer.	2
pros:  the proposed model (layer architecture) is simple and easy to implement cons:  the novelty is low  no competitive baseline in experiments	1
cons  results are not surprising  the questions tackled by the experiments are not original.	0
without additional experiments on other datasets, it is hard for the reader to draw any meaningful conclusion about the proposed method in general.	0
also, the nearesthalfandlog pattern is not used in any latter visual recognition experiments, so i think it's better to just compare logdensenet with the two baselines instead.	1
first of all, the line between the authors' contribution and prior art is blurry, because they seem to be introducing new metrics for measuring hypernymy as part of the experiments.	1
4.4 processing and representation differences there is virtually no discussion of what makes the naturalistic setup naturalistic, and thus it's not clear which conclusions we should derive from the corresponding experiments.	1
 the experiments are largely conducted with very small scale datasets.	1
 experiments are conducted in relatively lowscale datasets.	1
in the experiments, the authors plotted the training progress w.r.t.	1
while the approach (as has been shown in the past) is very reasonable, i would have liked the experiments to be more thorough, with comparison to the state of the art models for the two datasets.	1
so i will focus on the omniglot experiments.	1
the experiments are still limited to simple smallscale tasks.	1
in the experiments the authors compare with classical backpropagation, but they do not compare with the explicit step of carreiraperpinan and wang?	1
the experiments show the benefit over the euclidean distance when applied to the datasets used in the paper.	1
 the experiments show that the geodesic correlation alignment outperforms the original alignment method.	1
" in general  experimental section should be extended, as currently the only convincing success story lies in convex hull experiments side notes:  dcn is already quite commonly used abbreviation for ""deep classifier network"" as well as ""dynamic capacity network"", thus might be a good idea to find different name."	1
while the experiments clearly show the advantage of this method, this is hardly surprising or novel.	1
how does this affect the experiments?	1
this aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the hyperlex dataset.	1
 the experiments are on small/limited datasets (mnist and cifar10).	1
finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for bleu or wer.	1
in the experiments, the efficacy of the proposed techniques has been demonstrated.	2
weakness:  most of the experiments in the paper are performed on small neural networks and simple datesets.	0
5. for the experiments on synthetic datasets, workers are randomly sampled with replacements.	1
cons:  all experiments use simulated workers; this is probably common but still not very convincing.	0
 it seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem  no comparison with overparameterization in experiments results is given, which makes me wonder why their method is better.	1
unfortunately, it's not just talk, they are also the point of comparison in the experiments, i.e., there are no comparison with modern deep learning methods.	0
experiments show significant reduction of training samples needed.	1
overall i think this paper lacks the breadth of experiments, and to really understand the significance of this work more experiments in more established domains should be performed.	0
the experiments are quite small scale, which i expect is due to the computational cost of generating the adversarial examples.	1
 experiments are performed on reasonably realistic and complex tasks (albeit in simulation).	1
however, the value function does not appear to be learned in the experiments section.	1
the paper is well written and the experiments are quite convincing.	2
however, for large datasets, the nucleusp norm is shown to be advantageous over l2 regularization only in one of the three datasets (svo)  this is fine (and not my concern) as i think it is important to show experiments where the algorithm is not advantageous too.	0
experiments were done on mnist, cifar10, and imagenet, which is very useful to see that the attack works with high dimensional images.	2
review: quality: the quality of the work is high  the experiments are extensive and thorough.	2
the experiments are missing comparison with the threshold based pruning proposed by han etal.	0
in the experiments, see line 4 of page 9, the authors instead used relu activation .	1
hence, the experiments need to make more convincingly, at least for some different complicated architecture of deep neural network.	1
also, in the experiments, the authors mention multiple attempt with the same settings  are these experiments differentiated only by their initialization?	1
general review:  when the generator is producing only 'one' discrete distribution the theory is presented in section 2.3. when we move to experiments, for image generation for example, we need to have a generator that produces a distribution by pixel.	1
the experiments could be improved to show the effectiveness of the proposed approach better.	2
their experiments clearly indicate that the combination of the two reranking components outperforms raw machine comprehension approaches.	1
overall, with these key things missing the paper falls a bit short making it more suitable for a re submission with further experiments.	0
the experiments show robustness to these types of noise.	1
in section 5.3, experiments look at inceptionresnetv2 on imagenet, showing the proposed approach can reach comparable accuracies to previous work at even fewer parameter updates (2500 here, vs. ∼14000 for goyal et al 2007)	1
 a wide range of experiments are conducted to demonstrate performance of the proposed method.	1
"in the first paragraph of section 4, it is stated that ""in all experiments if the competing techniques use an embedding of dimensionality l, g2g’s embedding is actually only half of this dimensionality so that the overall number of ’parameters’ per node (mean vector  variance terms) matches l."""	1
however, a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation.	0
by comparing only to onesample discriminators it leaves open the (a priori quite plausible) possibility that minibatch discrimination is generally a good idea but that other architectures might work equally well or better, i.e., the experiments do not demonstrate that the mmd machinery that forms the core of the paper has any real purchase.	0
this paper proposes to fix this by redesigning from ground up the deep learning engine placing particular focus on code generation, compilation (e.g., type checking), optimization (e.g., fusion, matrix chain reordering, common subexpression elimination) etc. unfortunately, the paper falls short in two significant respects: it does not adequately cite related work and it does not present any experiments to quantify the benefits they claim will be achieved by their new compiler.	0
while the experiments are well done, i don't think the substance of the algorithmic improvement is enough.	0
it is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate.	1
in the experiments section the authors evaluate the quality and meaningfulness of the induced riemannian metric.	1
in general, this is an interesting direction to explore, the idea is interesting, however, i would like to see more experiments 1. the authors tested out this new activation function on rnns.	2
the details on the experiments are also scarce.	0
the block sizes used in experiments are about 4x4, 8x8, up to 32 x 32. the relative performance degradation ranges between 10% to 96%, depending on the method, severity of compression, and task.	1
6) did you use data augmentation for both teacher and student models in the cifar10/100 and caltech256 experiments?	1
it is never explained how this reward is allocated even in the authors’ own experiments.	0
it might be good to emphasize that you don’t train on the iwae bound in any experiments.	2
i think these experiments should be elaborated on.	0
the experiments are standard and compare to numerous existing state of the art methods.	1
it conducts many experiments regarding this.	1
here's a few, but i'd suggest the authors to make a more thorough literature study on the aforementioned subjects: 1) learning without forgetting eccv 2016 2) incremental classifier and representation learning cvpr 2017 3) expert gate: lifelong learning with a network of experts cvpr 2017 4) incremental learning of object detectors without catastrophic forgetting iccv 2017  in the experiments section no baseline is included on the alternative approaches.	1
but the experiments are not convincing enough.	0
though experiments may not show show stateoftheart performance, i think that they still serve to demonstrate the utility of the skiprnn architecture when compared sidebyside with a similarly tuned nonskipping baseline.	1
experiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain.	2
unfortunately, the experiments are inconclusive.	0
quality: 1. it is unclear to me if the generated distribution in the experiments is similar to the original distribution d_tr given y = t^', either from inception accuracy or from pictorial illustration.	0
however the experiments do not directly support this conjecture.	0
the experimental section had many experiments, which is great.	2
have the authors conducted the corresponding experiments?	1
finally, experiments based on mnist illustrate the properties of the proposed approach.	2
the experiments shown in figure 7 are strange.	1
the experiments are weak.	0
third, the experiments with synthetic noise are significant to a reduced extend.	2
main concern would be the lack of experiments showing that the network learns meaningful representations in the hidden layer.	1
: during all experiments, only gaussians are used?	1
the authors demonstrate this using a taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference.	1
also, the experiments are not clearly explained.	1
the experiments were extensive and show that this is a solid new method for trying out for any adaptation problem.	2
3) the experiments show some promise.	2
it appears to consist of further experiments.	1
i do think that it is a solid contribution with thorough experiments.	2
all the experiments use large tasks  it would be helpful to have an experiment showing an improvement over characterlevel modelling on a smaller task.	1
the proposed method is evaluated on extensive experiments.	2
the experiments are well illustrative but rather small sized.	0
however, part of experiments are not comprehensive enough.	0
the manipulation experiments show nice results on nontrivial tasks.	2
however, the experiments are not entirely convincing.	0
the authors should explain the experiments and the settings in the table, as those are not very clear.	0
 with the above experiments i think this would be a good paper.	2
no clear experiments from which conclusions can be drawn. '	1
the additional experiments are nice.	2
but experiments at that scale have not been shown currently.	0
the paper mentions making predictions from “incomplete” input several times, but in all experiments, the input is an edge map, normal map, or lowresolution image.	0
moreover, claim [d] about repeatability is also invalidated by the fact that the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes.	0
the experiments are limited.	0
the experiments are also very weak.	0
the experiments in this paper are also lacking.	0
the experiments are also not enough.	0
the experiments only concern two slightly different versions of the proposed algorithm in order to show the importance of the deconvolution of both considered noises, but nothing indicates that the model performs fairly well compared to existing approaches.	0
they perform experiments on mnist and cifar10, doing classification by scaled regression.	1
what architecture did you use in your experiments?	1
the experiments are also well designed and executed.	2
experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent.	2
in particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper.	1
2) the experiments are restricted to a single dataset  mnist.	1
the paper follows with some fun experiments implementing these new game theory notions.	2
if not, please update the experiments to be consistent with the baselines.	1
in the experiments, authors only stated that “we fit the gps architecture using ups optimizer for varying degree of the neighborhood of the graph”, and then the graph is used to train existing models as the input of the graph.	1
they present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy.	1
experiments are based on the federated averaging algorithm.	1
"i would think this should be lessthanorequal  there is a sentence that doesn't end at the top of p.3: ""... the original gan paper showed that [ends here]""  should state in the abstract what your ""notion of generalization"" for gans is, instead of being vague about it  more experiments showing a comparison of the proposed metric to others (e.g. inception score, mturk assessments of sample quality, etc.) would be necessary to find the metric convincing  what is a ""pushforward measure""?"	1
i also did not find the experiments illuminating.	0
al. (2017) or the split cifar experiments in zenke et.	1
the neural networks in the experiments are shallow.	0
experiments and analysis are both presented clearly.	2
the experiments range over 4 video datasets.	1
pros  nice experiments, with very interesting results.	2
the work is extremely creative and packed with interesting experiments.	2
this is applicable to experiments in figure 4. i strongly recommend running experiments that test the predictive power of the roles found by graphwave.	2
 the other experiments are lacking important details.	0
the experiments in section 4.2 show that, using 25 workers, the best of these 5 variants obtains a 5fold speedup over sequential hyperband on cifar and an 8fold speedup on svhn.	1
my main criticism of this paper is the experiments.	0
the experiments were well carried through.	2
 some details of the experiments/methods are confusing.	0
for example: (1) the step number k is dynamically determined by a short line search as in section 4 ``dynamic rollout’’, but later in the experiments (section 6) the value of k is set to be 2 uniformly.	1
experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method  the experiments show that essentially, the latent defenders are stronger than the input defender in most cases.	1
for example, there are no experiments demonstrating the usefulness of this approach except for a toy mixture of gaussians and binarized mnist, explaining what is already known with the betavae and infogan.	1
the assumptions that large fourier peaks happen close to origin is probably welljustified from the empirical point of view, but it is a hack, not a well established wellgrounded theoretical method (the authors claim that in their experiments they found it easy to find informative peaks, even in hundreds of dimensions, but these experiments are limited to the svm setting, i have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features).	0
experiments are performed on citation textual datasets.	1
but i was less convinced by the empirical experiments.	0
the main flaw of the paper is in its experiments.	0
for the experiments, the following should be addressed.	1
i have two questions from authors: 1 what are the hyperparameters that you optimized in experiments?	1
then the paper is missing experiments to support the idea that the learned representations are in any way an improvement.	2
cons: ' in the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections.	0
experiments include both artificial data and real data.	1
extensive experiments could be added.	2
2. the experiments are unconvincing.	0
the experiments are performed on 15 small scale dataset and show that autostacker is performing better than random forest almost systematically and better than tpot, the external baseline, 13 times out of 15. also the speed comparison favor autostacker vs tpot.	1
in the paper up to 100 experiments were used.	1
the authors validated the need for their proposed regularizer through experiments on 4 datasets (3 text and 1 images).	1
beyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational em, and pure amortized inference as in the original, standard vae.	1
what is the exploration strategy in the experiments?	1
did the authors try experiments with less embedding matrices?	1
while this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more largescale experiments on real datasets.	0
experiments on realistic settings should be included.	1
if my assessment of the situation is correct, i would like to ask you to repeat your experiments with the following two settings:  resnet where after each block you multiply the result of the addition by 0.5, i.e. x_{l1} = 0.5.mathcal{f}(x_l)  0.5x_l  van with the following altered equation: x_{l1} = .mathcal{f}(x_l)  (1.alpha)x_l, i.e. please remove the alpha in front of .mathcal{f}.	1
9. experiments are unconvincing.	0
finally, the experiments are promising.	2
overall, i think this is interesting work, but i have a few concerns which i’ve listed below: 1. section 4, which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit “disjoint” from the rest of the paper.	0
i think these experiments would strengthen the paper.	2
the experiments (though limited to atari) are well carried out and the evaluation is performed on both sample efficiency and training time.	2
this model is then evaluated on extensive experiments.	1
the experiments offer no empirical comparison.	0
apologies if i missed it but i am not clear how this was optimized for the experiments reported.	0
also the generalizability impact of this method on the stateoftheart is not clear due to the fact that the vanilla networks used in the experiments are generally not the stateoftheart networks.	0
[the experiments are now more informative.	1
there are two issues with the experiments.	0
although the formal discussion is concerned with markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the ppd) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used).	1
"there is however a phenomenon i would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller ""effective"" d', you only have to figure out a generating system for this subspace and carry out optimisation inside)."	1
i would suggest the authors also run the experiments on cifar10.	2
the experiments show clearly that a) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks negative aspects: my main criticism to the paper is that the task learning achieved through self exploration seems relatively shallow.	0
reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs.	1
experiments  why/how would you have distorted test data?	1
did the authors run experiments on other datasets?	1
cons: 1. it is a nice idea and it seems to perform well in practice, but are there careful experiments justifying the 3stage training scheme?	0
cons  the experiments are on toy datasets only.	0
the experiments seem to be inconclusive.	0
in order to confirm the robustness of the ensembles to bim attacks, the authors can do more experiments by generating bimgrad2 attacks with higher number of iterations.	1
 so the main burden lies on experiments.	1
i think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods.	2
the experiments are thorough and the results promising.	2
the experiments were well carried through and very thorough.	2
the experiments are fairly thorough (albeit with some sizable gaps) and show that the proposed approach outperforms dtw, as well as embeddings learned using siamese networks.	2
second, the experiments are quite weak.	0
while i understand why the aol dataset was used, the document ranking experiments should also include runs on any of the conventional trec datasets of documents, queries and actual (not simulated) relevance assessments.	0
 the experiments are conducted on previously published datasets.	1
  summary and evaluation:  the paper presents a nice set of experiments on language emergence in a mutlimodal, multistep setting.	1
this again could be addressed with experiments on more challenging datasets.	2
the approach is illustrated by numerical experiments.	2
analogous experiments are clearly missing.	0
however, it is never empirically verified.	0
it would be very good to empirically evaluate this claim.	2
the authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input).	1
it would be very interesting to empirically explore this statement.	2
to avoid the expensive matrix operation for the random walk, it empirically shows that k = 3 hops of the random walk can give a good performance.	2
the authors provide answers, both theoretically and empirically.	2
first, the proposed procedure is quite empirically designed.	1
the paper falls a bit short both empirically and technically.	0
empirically?	1
as weak points: 1) the paper claims the selection of .alpha is critical but then, this is fixed empirically without proper sensitivity analysis.	0
the only con (which is more a suggestion than anything)it would be nice if the authors compared the training time/# of parameters of their model versus the closest competitors for the latter two empirical examples.	0
 the empirical evaluations are not convincing.	0
according to lemma 1 and its finite sample version in theorem 1, the risk on the target domain can be upper bounded by the combination of 1) the reweighted empirical risk on the source domain; and 2) the distributional discrepancy between the reweighted source domain and the target domain.	1
this is an empirical paper.	1
empirical results look natural.	1
i also find the empirical results encouraging.	2
the authors demonstrate the derivation steps of their theoretical model and present 2 empirical results.	1
2. good empirical evaluation with ablations.	1
for a primarily empirical paper, every stateoftheart algorithm should be used as a point of comparison on every dataset considered.	2
can we have some sort of empirical analysis that what you say is true?	1
is it simply by the empirical plots of figure 2?	1
while the paper reports superior performance, the empirical claims are not well substantiated.	0
3. cosine schedule is used in the experiments.	1
experiments show modest gains on the task of multiplehuman pose estimation.	2
the experiments are also a bit disappointing.	0
experiments show that a neural model outperforms an svm on both tasks, using proof states sampled from a proof of the feitthompson theorem as a dataset.	2
the experiments are well executed.	2
5) saddles for neural nets the authors claim they “have not encountered convergence to saddles” for the experiments with neural networks.	0
being a largely empirical contribution, its value hinges on the exhaustivity and clarity of the experiments carried out.	1
 the experiments are detailed and thorough with both synthetic and real data.	1
the experiments on mnist show that derived more bayesian variants of rmsprop and adam can improve generalization in terms of test likelihood and test error. '	1
only a few experiments have been done.	0
the experiments use neural networks with stochastic optimization, on the classic datasets mnist, cifar10 and 20 newsgroup.	1
minor question  what is the difference between negative ll's and reconstruction losses in experiments?	0
 extensive experiments.	1
it is unclear to me which model setup was used in experiments.	0
7) empirical evaluation a) there are not enough details provided to be able to reproduce the experiments.	0
the experiments shows that the proposed model can infer more accurate programs from the scenes, and those generated programs can be used to recover the input scenes more accurately.	2
bottom line: i like the proposed model and for what it is it's quite good but it would have been a much more convincing paper with more experiments demonstrating the power of the method and analyzing it.	2
is this actually verified with experiments / model inspection?	1
a set of experiments illustrates that this way of doing nonlinear sfa is meaningful.	1
2. experiments on mnist and cifar require some more clarification.	1
my major concerns about this paper might be related to the experiments.	0
"""however, we do not use the testhard split for our experiments."""	0
several experiments get into some unclear value judgements over what the behavior of an ideal metric should be.	0
the experiments are also solid.	2
the mnist experiments shown are also pretty far from standard baselines.	0
regarding the real data experiments: in table 1 the results for dae_uai are based on which budget b?	1
experiments are on toy domains with very few goals and subtask dependencies.	1
4. there seems to be no experiments conducted to support the practical use of the method proposed in the paper.	0
 experiments are not convincing.	0
convincing experiments are conducted both qualitatively and quantitatively.	2
i think the biggest weakness of this paper is the experiments.	0
3. in the experiments, you compared your method with a classical gan.	1
however, this estimation is still based on relatively long experiments.	0
3.1 the authors explain the training protocol for the experiments.	1
(2) experiments were conducted on a publicly available dataset.	1
(3) robustness to coldstart scenario was tested and evaluated in the experiments.	1
the paper also presented some numerical experiments.	1
 the experiments are well designed and benchmarked against the stateoftheart models.	2
after all, newton method and natural gradients method are not used in experiments.	1
moreover, experiments are well conducted and the results are also good.	2
the experiments do not convey if learning has significantly resulted in improved exploration.	0
 could you plot the evolution of .beta, .rho and .lambda for a couple of your experiments?	1
so the relevance of the experiments is completely unclear, let alone the fact that the description is too muddled up.	0
the reasons are 1) incremental novelty; 2) insufficient experiments.	1
2. good experiments to justify improved running time and fewer number of parameters.	2
second, i was missing any sort of ablation experiments.	0
i think further experiments would strengthen the paper.	1
2) it is not clear what the “replicates” refer to in the experiments.	0
here are comments about the experiments.	1
3. the datasets and experiments are not sufficient.	0
the number of experiments conducted and comparisons done is quite limited.	1
a suite of experiments is run on graphs from konect.	1
i found the experiments in section 5.3 and specifically fig 4 and fig 7 insightful.	2
2) the paper is well written 3) the experiments are quite thorough and convincing.	2
thus the experiments and the results are not convincing.	0
the tasks the proposal is able to consider is not easy to realize, at least before the experiments part.	0
however, mem2seq does not outperform seq2seq model in all experiments.	0
experiments show that some proposed models work better than baselines in classification and oversampling.	2
only experiments with small networks on mnist variants are presented.	0
 the experiments on single datasets of a very specific speaker profiling problem would be somewhat misleading.	1
they propose algorithms for both cases and evaluate these in basic experiments.	1
2) the paper can benefit from more detailed experiments (e.g. figs 1 and 2).	2
in summary, i found this paper is interesting but my concern is about the experiments.	1
pros:  interesting experiments  lots of different problems evaluated with the technique cons:  the groupsort activation is justified from the angle of approximating lipschitz transformations.	1
in the experiments, you do not always specify the number of groups (table 4)	1
for the experiments it seems a lot of the key improvements come from the diayn algorithm.	0
they run experiments on 3 opendomain qa datasets and achieve sota.	2
in the case of infinite mixtures, it is not clear what is done in the end in the experiments.	0
i wasn't convinced that appropriate baselines were used in experiments.	0
i also appreciated the illustrative choice of experiments that show the benefit of recursive reasoning.	2
the proposed method is interesting and novel, but the experiments are not convincing.	1
the experiments are useful to demonstrate the application and usefulness of the approach.	2
in the experiments, the editors sharpen image quality, but the tradeoffs are not explored.	1
these inconsistencies cast doubt on the subsequent experiments.	0
the experiments and analysis is very well written in the paper.	2
so in this light, the conducted experiments on real world data are close to meaningless as no ground truth is known.	0
'i would like to see more experiments on more distant language pairs.	2
so it is not clear if number of classes really plays a role from the available experiments.	1
it would be interesting to show some experiments with natural gradient methods.	1
 you do not compare your results to any other system or baseline in your experiments.	0
experiments on on europarl and multi30k datasets are carried out to verify the proposed algorithm.	2
in your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?	0
"please divide the results for these toy experiments in ""geodeep"" and ""geodeep no pooling""."	0
 numerical experiments.	1
the future stock price regression experiments are also poorly presented.	1
cons and questions 1. how did you obtain the results of vat in the experiments?	1
also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.	0
however, the experiments did not show any benefits for the rl tasks.	0
some experiments on these applications are also preferred.	1
some extensive experiments verify the model robustness with respect to different distance measure, with most stateoftheart attacking schemes, and compared against several baselines.	2
# weaknesses discrepancy between quantitative and qualitative results: the good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results.	1
the experiments are limited and are done only on one data set.	1
experiments against blackbox attack, random noise, whitebox attack, greybox are presented.	1
most of the experiments are on cifar10 but one experiment is also presented on mnist.	1
 experiments: the experiments are generally thorough and wellpresented.	1
evaluating the paper along the requested dimensions: = quality: the paper clearly states its motivation, proposes a model, discusses practical issues, and provides convincing experiments (given the constraints of proprietary data, etc.).	1
it seems the authors conducted a very limited set of experiments and concluded that the proposed delifisher gan is better.	2
cons: the weakest point of this paper is the experiments.	1
experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.	2
 pros and cons  pros: 1) strong experiments.	1
the experiments are extensive.	1
some experiments are carried out on the yelp dataset, including some qualitative study.	1
the organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear.	2
experiments are extensive.	1
2. all the experiments are on mnist.	1
experiments are performed on three, but small, datasets including the simple mnist dataset.	1
cons: 1. all the experiments are based on cnn.	1
4. about the experiments.	1
experiments on mnist and clevr are toy examples but illustrate that the model is indeed performing as expected.	1
 extensive experiments.	1
2. experiments: the experiments are on toy datasets.	1
as such, i encourage the authors to continue the work on this empirical analysis, and perhaps submit in again to future conferences.	1
was the empirical or true fisher used?	0
overall, the proposed method is empirical and the authors show its performance by experiments.	2
the empirical results of validation loss in figure 1 is reporting the behaviour of random initialization of embedding.	1
a discussion on this or some empirical evaluations would be nice.	0
pros:  lots of empirical studies.	1
cons: my main concerns are on the technical novelty and experimental comparison.	1
 limited experimental validation.	1
experimental section.	1
cons: the experimental section could be better.	1
i think the paper shows good results, but it could very much benefit from improved presentation and evaluation.	2
can you provide results where you use your method but without any replay buffers, i.e. by using the last batch of data points?	1
for instance, a noise image should also in your data space, but does it belong to an error set or not?	1
however federated learning discusses the scenario where the data is distributed between several parties.	1
4. data: the datasets used in the experiments are all wellaligned.	1
experiments are performed on three, but small, datasets including the simple mnist dataset.	1
the mnistrot data is not spherical. '	1
for ehr datasets, are you assuming that events are always categorical?	1
furthermore, comparison to pretraining on other datasets should be performed.	1
please also add some discussion on how good the synthetic data simulate the real data.	1
2. why not using cifar100, but with a new dataset svhn for the semisupervised learning in section 5.2?	1
experiments are performed on three, but small, datasets including the simple mnist dataset.	1
in section 4.1, the different dataset and their train / test splits are presented, but what about validation?	1
this is great, but could you also report the number when the full dataset is used?	2
however the training dataset is fixed, therefore this input is effectively a constant.	1
is the dataset shuffling different?	1
2. experiments: the experiments are on toy datasets.	1
experiments are performed on three, but small, datasets including the simple mnist dataset.	1
2. experiments: the experiments are on toy datasets.	1
classification on pairwise proximity data.	1
 if so i will hope you tuned your r on the training data and the reported values are from the test data!	1
this seems to give good risk bounds on certain known compressible nets using image data sets.	1
finally, could the authors find one or two additional datasets?	1
if the entire image is presented as a flattened input, and the time dimension is iterating through the dataset, then there is no reason to use an rnn here.	1
the only observation i gained as far as this is that nonsaturating loss would possibly be stable across various datasets.	2
maybe the authors can compare the results with less data for metatraining.	1
2. the authors perform numerical experiments to demonstrate the effectiveness of their framework in benchmark datasets.	1
the authors are encouraged to conduct 1 nlp dataset.	1
even more, it seems that forward does always better than ddgc with noise 60% on every dataset.	1
such a kind of data manipulation is unacceptable and unfair to other baseline methods.	1
again, the authors reported results on two other datasets.	1
 may want to add noise to augmentation data, to judge robustness of method.	1
from the text it appears that .lambda is manually selected to trade off accuracy against uncertainty on ood data.	1
hence, in principle, one could maximize when learning from unsupervised data.	1
the framework explicitly formulates data distribution, which has not been considered by previous works.	1
section 3.2, datasets of 9993 dialogs: are they done by humans?	0
it is also not clear what training data include: scenes only?	0
this is great, but could you also report the number when the full dataset is used?	2
the upsampling analysis is interesting but it is only done on synthetic data  will the result hold for natural images as well?	1
however the training dataset is fixed, therefore this input is effectively a constant.	1
intuitively, a very high inception score may indicate that we are not trying to generalize, but just memorize the training input data.	1
