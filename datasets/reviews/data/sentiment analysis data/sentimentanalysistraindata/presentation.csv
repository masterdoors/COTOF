text	label
it seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved.	0
details : the paper can be clearly understood if the basic frameworks (like gans) are known, but the presentation is not general and good enough for a broad public.	0
this work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared.	0
for the sake of clarity of the presentation i would drop parts of section 3 ('a combinator library for neural networks') which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper.	0
pros:  significant speed improvements through dynamic batching  source code provided cons:  the effect on a large realworld (asr, smt) would allow the reader to put the improvements better into context  presentation/vizualisation can be improved	2
i like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence.	0
however, i'd like to see more analysis on the 2d representations (as concerned by another reviewer) to be convinced.	0
the presentation of the method for neural networks lacks clarity in presentation.	0
the main downside to the current version of this paper is the presentation, which provides sufficient detail but could be more clear.	0
establishing protocol is one important methodology contribution of this paper, but the presentation of section 3 is still not good.	0
pros: the presentation is quite good and the paper is easy to follow.	2
minor point on presentation: speaking of the 'evolution' of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{';a} is the immutable input vector, and it's the justintroduced z and y variables that represent its socalled evolution, no?	0
i do not rule out this paper for acceptance based on the length but i do hold it as a negative because clarity of presentation is an important quality.	0
this paper has excellent results but suffer poor presentation, lack of a clear focus.	0
overall, i think the paper contains a useful contribution on a technical level, but the presentation needs to be significantly cleaned up before i can recommend acceptance.	0
i have spent several hours trying to read this paper  but it has not been possible for me to follow  partially due to my own limitations, but also i think due to an overly abstract level of presentation.	0
pros:  introduction of a nice filter banks and its implementation  good numerical results  refinement of the representation via back propagation, and a demonstration that it speeds up learning cons:  the algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper.	2
overall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately.	0
pros the paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units.	0
in general, the paper suffers greatly from a lack of clarity and issues of presentation.	0
in short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.	0
after the rebuttal: the paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal.	0
pros: 1. nice idea on heirarchical attention for modulating the context (document) representation by the taskspecific (query) representation.	2
perhaps i have misunderstood the contribution, but the presentation also lacks clarity, and i cannot recommend this paper for publication.	0
second, while the technical contributions/novelty are not a focus of the paper's presentation, i am concerned by the lack of methodological advance.	0
the authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time).	0
my main concerns with this paper are as follows:  the point of this paper is about using generative models for representation learning.	0
in short, the ideas of this paper are interesting and potentially useful, but i think the presentation of this paper should be improved so that it becomes more suitable for the iclr and machine learning community.	0
overall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.	0
this is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is.	0
i found no fault with the work or presentation, but did not follow the details or know the comparable literature.	0
in summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient  i wouldn't be able to recreate the algorithm used in experiments  and the paper contains a number of questionable claims.	0
(presentation) i think this distinction would usually be handled by the reinforcement learning framework, but the proposed method is not presented in that framework or related to an rl based approach.	0
the work is dense in nature, but i think the presentation could be improved.	0
the paper’s presentation is inefficient and muddled, and the results seem incremental.	0
weaknesses:  the presentation is a muddled, especially the model definition in sec.	0
the method seems potentially quite interesting but the paper has serious problems in the presentation.	0
weaknesses:  the writing is at times quite opaque.	0
clarity: the writing is generally good both in terms of the biology and ml, but more mathematical rigour would make it easier to understand precisely what was done.	0
it may be true that deepdsl is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this.	0
the authors keep referring to 'previously popular attention paradigms' without any citation and then, i believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches 'summarize each modality into a single vector.'	0
my main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method.	0
two writing comments:  i agree that the results with word order and cbow are surprising, but i think it's slightly misleading to say that cbow is predictive of word order.	0
cons:  the writing is unclear at times and the mathematical formulation of the ian is not very precise.	0
overall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately.	0
writing comments  the writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely.	0
pros:  good numerical results  interesting applications linked to convolutions on lie groups cons:  sometimes, the writing is not clear/poor.	2
paper strengths:  the questions in the dataset are real queries from users instead of humans writing questions given some context.	2
the writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas.	0
concerning the writing, sections concerning well known notions (e.g. svm) could be suppressed.	0
i wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.	0
highlights:  proofofconcept of a partiallytrainable implementation of the important “divide and conquer” paradigm  explicit reasoning about complexity of induced programs  the solution isn’t generic enough to be applicable to unknown problems  the networks require tricks specific to each problem  the writing style pictures the method as very general, but falls back on very low level details specific to each task	0
“gradients of coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing.	0
paper summary: the authors proposed to use edgeboxes  fastrcnn with batch normalization for pedestrian detection review summary: results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and overall the work lacks novelty.	0
the architecture appears sound, but the writing makes it hard to fully understand completely so i can not give a higher rating.	0
overall, the results presented in the paper are interesting, but the writing can be improved.	0
in summary: pros:  interesting idea  seems to improve performances cons:  paper writing  weak evaluation (only one dataset)  compare only with approaches that does not use the lasttimestep error signal	2
the paper has a clear structure, but clarity could be improved at some points.	0
also lines such as 'we did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important' and claiming the importance of adagrad over rmsprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the rts setup in this paper?	0
the current work provides a more principled approach that does not have such an adhoc multistage structure, but a single iterative optimisation process.	0
some other points are listed below originality: while there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, i like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches significance: the method achievs stateoftheart performance on the two shown examples on mnist, however these networks are far from the deep networks used in stateoftheart models.	0
we are still a bit concerned whether the npl would be directly applicable for noisy observations (e.g., human skeletons) in a continuous space with less explicit structure, so more discussions will be interesting.	0
(and it would be very valuable to the the community if the environment would be opensourced) the expression 'latent structure/dynamics' is used throughout the text and the connection with bandits is mentioned in section 4. it therefore seems that authors aspire for more generality with their approach but the paper doesn't quite fully ground the proposed approach formally in any existing framework nor does it provide a new one completely.	0
briefly (and slightly inaccurately) model starts with the lstm structure but removes all but the diagonal elements to the transition matrices.	0
i am a bit concerned that many of the tasks are too easy (e.g. the aq question paraphrase), and i am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots.	0
this paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations.	0
pros:  interesting idea: learning structures of sentences adapted for a downstream task.	2
in some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.	0
cons the paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less).	2
the description of the saen structure in section 2.2 was worded poorly.	0
it seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there's not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant.	0
pros :  new and clear formalism for invariance on signals with known structure  good numerical results cons :  the structure must be specified.	2
comparison to literature is severely lacking; eg 'several automated structure learning techniques have been proposed' followed by 6 citations but no discussion of any of them, which one is most related, which ideas carry over from the offline setting to this online setting, etc. also since this work presents both joint structure & 'parameter' learning, comparison to the online parameter learning papers (3 cited) would be appreciated, specifically since these prior approaches seem to be more principled with bayesian moment matching in jaini 2016 for example.	0
i do not know enough about spns and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt random # remaining questions after the paper updates  table 3: random structure as baseline ok, but how were the parameters here learned?	0
it is also concerning that although 1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: al model that has “loop” fails to solve “list length” task in 84% of the runs).	0
they tend to lack longrange structure, to repeat notes etc. to solve this problem the authors suggest that the model could be first trained as a pure lmstyle lstm and then trained with reinforcement learning to optimize an objective which includes some nondifferentiable musictheory related constraints.	0
pros:  thoughtful methodology with sensible design choices  potentially useful for smaller (n < 10000) datasets with a lot of statistical structure  nice connections with sumproduct literature cons:  claims about scalability are very unclear  generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.	2
the word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision.	0
given the tree model it may be natural to specify a tree model encoder, but the posterior distribution does not respect the structure of the prior (as the posterior distribution couples treedistant variables), so there is in fact no good reason for this form, and a more general network could be compared with.	0
significance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.	0
the paper states in section 4.1 “our proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space.” however, this seems to be true also for many/most other approaches, e.g. [venugopalan et al. 2015 iccv] summary: =============== while the paper makes strong claims w.r.t.	0
this work provides an initial rigorous framework to analyze better the inherent structure of the current state of art res net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (rather than always to attempting to improve the performance of res nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).	0
“the underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.” i believe this perspective is already contained in several of the works cited as not belonging to this perspective.	0
pros:  new formulation of spns  new inference algorithm cons:  the authors did not discuss how the sspn structure is learned and how the generative process chooses the a symbol (operation) at each level)  the evaluations is lacking.	2
it is realizable by zero padding, but the invariance structure, which is the advantage of cnn compared to feedforward neural network, will be lost.	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
there have been concerns about cnn/dailymail dataset (chen et al. acl’16) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds.	0
maybe it is bound to be rather about lack of logical structure.	0
in that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.	0
especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.	0
the concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the 'explicit reference readers' need not learn it, and the cnn/daily mail dataset has very little headroom left as demonstrated by chen et al. 2016. the desire for 'dramatic improvements in performance' mentioned in the discussion section probably cannot be achieved on these datasets.	0
the presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting.	0
overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.	0
pros:  the organization is generally very clear  novel metalearning approach that is different than the previous learning to learn approach cons:  the paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than cnns such as fully connected and recurrent neural networks.	2
2. the organization of the paper is clear weak points of this paper: 1. comparisons with stateofart methods (graph kernels) is missing.	0
pros:  creates 'structured' sparsity, which automatically improves performance without changing the underlying convolution implementation  very simple to implement cons:  no evaluation of how pruning impacts transfer learning i'm generally positive about this work.	2
the current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example longrange syntax constraints like brackets)	0
cons:  although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multiclass classification task.	0
i believe it was introduced in mikolov et al., 2010. missing references: “recurrent neural network based language model.”, mikolov et al. 2010 “learning longterm dependencies in narx recurrent neural networks”, lin et al. 1996 “sequence labelling in structured domains with hierarchical recurrent neural networks“, fernandez et al. 2007 “learning sequential tasks by incrementally adding higher orders”, ring, 1993	0
i was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really 'structured interpretation' of the generative model (title), but i didn't see any of these.	0
significance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.	0
the writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas.	0
on the positive side, the paper is wellstructured and easy to read.	2
the proposed tasks are explained at a rather high level, which is convenient to understand intuition, but i think some lower level of detail might be useful.	0
reproducibility: the formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix.	0
this is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully).	0
the weaknesses:  the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.	0
also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part.	0
clarity: some of the derivations and intuitions could be explained in more detail but the main story is well described.	0
i understand that the nonparametric property induced by the prior might result in better capacity control, however i feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated.	0
some of the key details in this paper are very poorly explained or not even explained at all.	0
pros: it's well written and straightforward to follow the algorithm has been explained clearly.	2
writing comments  the writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely.	0
the contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were wellexecuted and explained.	0
the pass score is mentioned a couple times but never explained at all.	0
the approach is based on minimizing some quantity called the 'cognitive effort', but this is confusingly explained, and not very precise.	0
overview: this work seems very promising, but i believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.	0
an explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.	0
the paper lacks clarity in the description of the approach: mfa is poorly explained with undefined notations (in eq. 4, what is a?	0
pros: the method is simple and clearly explained.	2
there is a reference to supplementary material where some of these choices are explained, but i could not find this in the posted version of the paper.	0
the deep q network approach need not be implemented, but differences between methods should be explained because of the uniqueness of the proposed approach. '	0
preliminary rating: it is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained.	0
apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready, this is a nice, fresh paper, that might spur more attacks (and of course new defenses) from the new class of decisionbased attacks.	0
for the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow.	0
overall, i think this is an interesting paper, but the presentation is too fuzzy to get it evaluated.	0
but i do feel the paper lakes some details to better justify/explain the design choices: 1. a multiclass gan is used but in the formal background presentation of gans, section 2.2 only the binary version of the discriminator is presented.	0
overall: i think the research work in the paper is interesting and significant, but given the current presentation and level of detail in the paper, i don’t think it will be helpful for the research community.	0
to sum up, the paper needs a lot work on many fronts, but most importantly, presentation should be improved.	0
overall, i found the main idea of the paper relatively straightforward, but the presentation is a bit awkward in places.	0
i have two main concerns with the presentation.	0
the method seems interesting, but the presentation has a severe lack of focus.	0
cons  concerns about story/presentation: ' the second use of cavs, to test relative importance of concepts, is basically an improved saliency method.	2
i do have some nitpicks here and there with the presentation and exposition, and i am concerned that at times the paper appears to be minimizing its weaknesses, but i think these are things that can be addressed in the next revision.	0
there are some other details in the appendix that i find concerning: section 8 describes how there is some taskspecific tuning of which function to compute on the encoder to produce the sentence representation for the task.	0
this presentation of the paper can be misleading concerning the true innovation in the model trained.	0
in addition to the incoherent presentation, the proposed method lacks proper justification.	0
pros:  the presentation is clear and easy to follow.	2
o the presentation suffers from severe overgeneralization and lack of clarity, which disabled my ability to understand the network and algorithms for a specific case.	0
to summarize, the following are the pros of the paper:  clarity and good presentation;  good overview of the related literature;  extensive experimental comparison and good experimental results.	0
cons:  a few presentation/clarity issues as below  this paper leaves me wonder why amgan rather than simply characterizing d as a 2kway classifier (1k real vs 1k fake).	0
"the presentation of this algo is a bit short and could deserve more space (in the supplementary) for the da application, the considered datasets are classic but not really ""large scale"", anyway this is a minor remark."	0
my main concern is with the presentation of the paper.	0
the presentation is clear but can be improved.	0
given the incremental process, i find the presentation unnecessarily involved, and experiments not convincing enough.	0
concerning the presentation, the paper dedicates two full pages on a review of the algorithm by yin et al. (2017).	0
given the incremental process, i find the presentation unnecessarily involved, and experiments not convincing enough.	0
concerning the presentation, the paper dedicates two full pages on a review of the algorithm by yin et al. (2017).	0
see e.g. yang et al. “joint unsupervised learning of deep representations and image clusters”, cvpr 2016. to conclude, the paper has some interesting ideas, but the presentation is not convincing, and the experiments are substandard.	0
studying the relation between predictive coding and deep learning makes sense, but i do not come to the same (strong) conclusions as the author(s) by considering the experimental results  and i do not see evidence for a sophisticated latent representation learned by the network.	0
in my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.	0
another concern that i have is that there are a lot of conditionaconjugacy assumptions baked into the algorithm that the authors only mention at the end of the presentation of their algorithm.	0
in general, the paper shows improvements on an image caption agreement task introduced in kuhnle and copestake, 2017. the paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for iclr.	0
'strengths'' i like the highlevel motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images.                  	2
4 mentions that the “only image” encoder is used to obtain the representation for the image, but the “only image” encoder is expected to capture the “indescribable component” from the image, then how is the attribute information from the image captured in this framework?	0
cons and questions: 1. the presentation of the model is not clear.	2
i have a few questions about the evaluation, but most of my comments are about presentation.	0
main concerns:  presentation: the experiments show that (1) dive outperforms ge, hyperscore and hfeature baselines; (2) which scoring function works best for dive; (3) dive outperforms sbow in many cases, but not always, though better on average; and (4) we can use dive for wsd (only shown qualitatively).	0
"pro:  thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work  easy to read and follow cons: small issues in presentation: ' figure 2 ""optimal learning rate"" > ""optimal greedy learning rate"", also reference to theorem 2 for increased clarity. '"	0
i find the contribution fairly significant but i lack some clarity in the presentation as well as in the experiments section.	0
(b) a further difference to hartono et al, 2015, are comparisons with multilayer networks, and the presentation and discussion of this comparison is my strongest concern.	0
they compare a conventional, quadratic metric to a neuralnetwork based representation and a simple hamming metric, and show that the neuralnetwork based on achieves higher performance, but that the quadratic metric does not substantially beat the simple hamming baseline.	0
the implication is that indeed densely connected convacs can have greater representational capacity, however the gain is limited to the case where hidden layers shrink exponentially with increasing depth.	0
the paper is generally easy to follow, but in several places the presentation can be further improved.	0
pros: 1. updating a traditional opendomain qa approach with neural models 2. experiments demonstrate solid positive results cons: 1. the idea seems incremental 2. presentation could be improved	2
a second concern is the presentation of the paper, which can be confusing at some points.	0
although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims.	0
the idea proposed in the paper is quite interesting, but the presentation is severely lacking.	0
the main issue with this paper is the lack of scientific analysis of the results, together with many local issues in the presentation of these results.	0
pros: ' its a good illustration of deepneural network machinery at work, and well put together ' the experimental results show it works very well ' good experimental work (different data sets, different algorithms) cons: ' sloppy mathematical presentation	2
the presentation of statespace models, filtering and smoothing shows some lack of familiarity with the literature.	0
comments: the word analogy task was developed as an interesting way to analyse and understand word embedding spaces, but motivation for learning word embeddings was as generalpurpose representations for language processing tasks (as in collobert et al, 2011), not as a way of resolving analogy questions.	0
the algorithm seems to be simple and intuitive, but the presentation is overly formal and unclear.	0
weaknesses:  the presentation of the paper could be improved.	0
the writing is fairly clear, however the presentation of tables and figures could be done better, for example, fig. 2 is referred to in page 3, table 2 which contains results is referred to on page 5, fig 4 is referred to in page 6 and appears in page 5, etc. what kind of minimal preprocessing is done on the text?	0
i am concerned by the strong similarity between this work and poincaré embeddings for learning hierarchical representations (https://arxiv.org/abs/1705.08039).	0
main comments: this could potentially be a very nice paper, but i feel the current presentation is not ready for acceptance.	0
the presentation is clear but i find lack of description of a key topic.	0
this is a potentially important observation, and the experiments were well worth performing, but i don't find them fully convincing (partly because i was confused by the presentation).	0
the idea presented seems to have merit , however, i found the presentation lacking.	0
compared to existing approaches for counting (or vqa in general), this approach not only produces lower error but also provides a more humanintuitive discrete, instancepointing representation of counting.	0
the presentation and experiments are okay overall but i have a few questions and requests below that i feel would strengthen the submission.	0
in terms of presentation, at several points, the paper argues that previous, pixeldomain methods are more limited than the proposed featurespace method, but little evidence is given to support these claims.	0
lack of clarity in presentation the paper tends to introduces new key words for existing one.	0
not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.	0
in this perspective, the proposed framework might be useful, but as noted in the original review, the presentation is not clear, and it's not convincing to me that the mi framework is indeed useful in the sense i described above.	0
positive aspects:  the idea of using gans for this goal is smart and interesting  the results seem interesting too weaknesses:  some aspects of the paper are not clear and presentation needs improvement.	2
however there are issues in the presentation.	0
pros:  clear presentation, easy to follow.	2
== presentation == the paper is readable but not well polished.	0
however, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained.	0
there are some other issues with the presentation of the method, but these don't affect the merit of the method: 1. returns are defined from an initial distribution that is stationary for the policy.	0
the presentation of the z latent variable used to simplify the calculation of the entropy h(y|c) is confusing and needs revision, but otherwise the paper is interesting.	0
the presentation is good overall, but many minor improvements could help with readability.	0
the experimental results seem promising, but the presentation can be improved.	0
it has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same.	0
pros: 1. well written paper with clear presentation of the method.	2
i like that the paper gives a bit of context, but presentation of results could be clearer, and i am missing some more explicit information on training and results (eg how long / how many training examples, how many testing, classification rates, etc).	0
a few minor presentation issues:  relu > relu  originality: the paper is incremental work upon previous research (tishby et al. 2017; argawal et al 2014).	0
for architectures: the attentionbased model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta pixelcnn model is a standard pixelcnn trained with the maml approach by finn et al. for experiments: the imagenet flipping task is clearly tailored to the strengths of the attentionbased model, and the presentation of the general omniglot results could be improved.	2
however the writing is not very clear and the paper is not selfcontained at all.	0
pros:  good application of gan models  good writing and clarity  solid experiments and explanations cons:  results weak relative to naive baseline (entropy)  weak comparisons  lack of comparison to density models [1] louizos, christos, and max welling.	2
weaknesses: 1. the paper writing about the model architecture can be improved.	0
this is a great idea and could be a strong paper, but it's really hard to glean useful recommendations from this for several reasons:  the writing of the paper makes it hard to understand exactly what's being compared and evaluated.	0
the writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work. '	0
i found that the paper suffers many shortcomings that must be addressed: 1) the writing and organization is quite cumbersome and should be improved.	0
however, an issue in the writing, usage of external component and lack of experimental justification of the design choices hinder the clear understanding of the proposed model.	0
an issue in the writing overall, the paper is well written and easy to understand, but section 3.2.3 (the write unit) has contradictory statements about their implementation.	0
variables with both superscripts and subscripts have the superscripts pushed off to the right; i think you're writing these like but they should just be (no space).	0
if it can be demonstrated that the approach provides advantages to other existing methods for adaptively setting the learning rate and the writing of the paper is improved, the work could make a solid contribution to a future venue, but in its current form, the paper falls comfortably below the acceptance threshold.	0
the clarity of writing can be improved (several typos in the manuscript, notation used before defining, missing words, poorly formatted citations, etc.).	0
the paper contains some interesting experimental results, but unfortunately lacks concise motivation and description of the method and quality of writing.	0
cons: 1. the writing of the paper can be significantly improved.	0
pros :  interesting topic cons :  experiments are not convincing  writing & clarity could be improved sometimes the paper is a bit hard to read sometimes, because it does not refer to usual notions.	2
pros:  sdr characterisation of the convolutional filters is interesting  the authors show that filters with different characteristics are responsible for different aspects of image modelling cons:  the authors do not actually demonstrate how their analysis can be used to improve vaes or gans  their proposed sdr analysis does not actually find much difference between the generator and the discriminator of the gan  the clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form).	2
i think this paper contains an interesting idea, but suffers from poor writing and unprincipled experimentation.	0
3. relatively minor: the writing of this paper is readable, but could be improved.	0
overall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper.	0
cons: 1) badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves comments: the introduction puzzled me: the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t.	0
it is a rewriting trick that many knew but never thought of publishing ' section 4.1 is either incorrect or clearly misleading.	0
in 4.2, writing that both rescal and kbtd explain a rdf triple through a similar latent form is not an observation that could explain intrinsic similarities between the methods but the direct consequence of the deliberate choice made for f(.)	0
as there are not enough experimental details to judge, it's hard to figure out the problem, but this ppaper is clearly not publishable at any of the quality machine learning venues, for weakness in originality, quality of the writing, and poor experiments.	0
further, if in the implementation you discuss masking mantissa, i have serious concern about whether the compression protocol is feasible to implement efficiently, without writing some extremely lowlevel code.	0
overall, i found that the paper has too many red flags, and the lack of clarity in the writing makes it hard to judge.	0
overall this paper seems borderline  a nice theoretical story, grounding out into a simple architecture that does seem to work in practice (the domain adaptation results are promising), but with somewhat sloppy writing and experimentation that doesn't clearly demonstrate the value of the proposed approach.	0
again, due to lack of equations, i don’t completely understand the last paragraph of 3.2, i suggest rewriting it (as well as some other parts) in a more explicit way.	0
the writing is ok but a bit redundant.	0
because of the quality of the writing (generally sloppy), it's hard to tell, but i believe the authors are basically arguing that: a. you can generally easily detect adversarial points because they are low confidence.	0
the writing is fairly clear, however the presentation of tables and figures could be done better, for example, fig. 2 is referred to in page 3, table 2 which contains results is referred to on page 5, fig 4 is referred to in page 6 and appears in page 5, etc. what kind of minimal preprocessing is done on the text?	0
in general, the paper lacks rigor in writing down what it optimizes, and laying out details of the model clearly.	0
clarity: the writing clarity is rough, but understandable, with numerous minor grammar mistakes.	0
cons: the writing is bad and hard to follow, with typos: for example what is a period just before section 3.1 for?	0
while the strength of this paper is clearly the good writing as well as rigorous experimentation, the main concern i have with this paper is novelty.	0
the clear writing make the paper easy to read, which also makes clear the various weaknesses and pitfalls of smash.	0
the writing is improved and my concerns have largely been addressed.	0
strengths:   clear writing and document structure.	2
the overall writing lacks rigor and the contribution is poor.	0
however the paper writing has many unclear points.	0
i know this is a kind of open ended question but i think it would greatly aid the reader in understanding the paper if more ‘guidance’ is provided instead of just writing “..by definition this is the posterior.’ q4) in a similar vein to the above.	0
this paper has many strengths: 1) the writing is clear, and the paper is wellmotivated 2) the proposed algorithm is described in excellent detail, which is essential to reproducibility 3) as stated previously, the approach is validated with a large number of real android projects 4) the fact that the language generated is nontrivial (javalike) is a substantial plus 5) good discussion of limitations overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.	2
my main concern regarding the paper is its clarity: i will gladly increase my score if the authors can improve the writing.	0
in other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.	0
cons:  poor writing, close to a first draft.	0
minor comments: —— 1. writing should be fixed: “it seems that the common failure case of mmd is when the mean pixel intensities are a better match than texture matches (see figure 5), and the common failure cases of is happens to be when the samples are recognizable textures, but the intensity of the samples are either brighter or darker (see figure 2).”	0
pros:  qualitatively the proposed method has good results in several tasks cons:  writing needs to be improved  lack of motivation  not easy to follow technique details the motivation part is missing.	2
some writing issues: 1. lack of support in arguments, 2. lack of referencing to previous works.	0
on the other hand, i have several concerns about writing and technical discussions.	0
"2) the writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (what is 'golden centroid augmentation"" or ""model centroid augmentation""?"	0
"cons: 1. the writing of the paper could be improved and more clear: the conclusions on inner product and fnorm can be integrated into ""theorem 5""."	0
i have some nitpicks about the writing, but in general, it's not a burden to read.	0
"() pros / cons:  simple yet powerful method for text classification  strong experimental results  ablation study / analysis of influence of parameters  writing of the paper  missing discussion to the ""attention is all you need paper"", which seems highly relevant () typos: page 1 ""a support vectors machines"" > ""a support vector machine"" ""performs good"" > ""performs well"" ""the ngrams was widely"" > ""ngrams were widely"" ""to apply large region size"" > ""to apply to large region size"" ""are trained separately"" > ""do not share parameters"" page 2 ""convolutional neural networks(cnn)"" > ""convolutional neural networks (cnn)"" ""related works"" > ""related work"" ""effective in wang and manning"" > ""effective by wang and manning"" ""applied on text classification"" > ""applied to text classification"" ""shard(word independent)"" > ""shard (word independent)"" page 3 ""can be treat"" > ""can be treated"" ""fixed length continues subsequence"" > ""fixed length contiguous subsequence"" ""w_i stands for the"" > ""w_i standing for the"" ""which both the unit"" > ""where both the unit"" ""in vocabulary"" > ""in the vocabulary"" etc..."	0
pros:  simple model  strong quantitative results cons:  notation (i.e. precise definition of r_{i,c})  qualitative analysis could be extended  writing could be improved	2
pros the novelty of this paper lies in using a neural network structure to solve a traditional statistical problem which was usually done by a bayesian approach or using the idea of the stochastic process.	0
the explanation of the structure of the combined rnns is rigorous but clear enough of understanding.	0
"for one thing, the paper talks about exploiting ""hierarchical / multiscale structure"", but this does not refer to the spatial multiscale structure that is naturally present in images."	0
the paper is wellstructured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel.	0
"minor: i think that the statement ""which only supports broadcasting communication of the sum of the signals"" is not quite fair: surely they have used a 1channel communication structure, but it would be easy to generalize that."	0
originality  the paper heavily builds upon prior work on hierarchical latent tree analysis and adds 'skip path' formulation to the architecture, however the structure learning is not performed endtoend and in conjunction with the parameters.	0
it's true that the original vin paper worked in a gridnavigation domain, but they also had a domain with a fairly different structure; i believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.	0
pros:  investigating the ability of distributed representation in encoding input structured is in general interesting.	2
that said i found the intuitive story a little bit difficult to follow  it's true that in figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem  the gradients should still pull the generator 'towards' the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finergrained details.	0
however the analysis concentrates in the structure of the generator and the particular case of inverting scattering coefficients.	0
pros: 1. a new dna structure gan is utilized to manipulate/disentangle attributes.	2
the authors propose to tackle the tree transduction learning problem using recursive nn architectures: the prediction of a node label is conditioned on the ancestors sequence and the nodes in the left sibling subtree (in a serialized order) pros:  they identify the issue of locality as important (sequential serialization distorts locality) and they move the architecture closer to the tree structure of the problem  the architecture proposed moves the bar forward in the tree processing field cons:  there is still a serialization step (depth first) that can potentially create sharp dips to null probabilities for marginal changes in the conditioning sequence (the issue is not addressed or commented by the authors)  the experimental setup lacks a perturbation test: rather than a copy task, it would be of greater interest to assess the capacity to recover from noise in the labels (as the noise magnitude increases)  a clearer and more articulated comparison of the pros/cons w.r.t.	2
minor typographical/grammatical errors:  sec 1: “... in lstms meanwhile maintains the dimension consistency.” → “... in lstms while maintaining the dimension consistency.”  sec 1: “... is public available” → “is publically available”  sec 2: please rephrase: “after learning those structures, compact lstm units remain original structural schematic but have the sizes reduced.”  sec 4.1: “the exactly same training scheme of the baseline ...” → “the same training scheme as the baseline ...”	0
3. it seems that in the experimental results, is at most 2. is it because of the data or because of the lack of efficient training procedures for the hierarchical structure?	0
the presented mist architecture certainly has got its merits, but in my opinion is not very novel, given the fact that narx rnns have been described 20 years ago, and clockwork rnns (which, as the authors point out in section 2, have a similar structure) have also been in use for several years.	0
"it may be true that ""one plot showing synthetic icu data would not provide enough information to evaluate its actual similarity to the real data"" because it could not rule out that case that the model has captured the marginal distribution in each dimension but not joint structure."	0
strengths: the idea of using latent syntactic structure, and computing crosssentence alignment over spans is very interesting.	2
the paper claimed “the model is able to recover tree structures that very closely mimic syntax”, but it’s hard to draw this conclusion from the two examples in figure 2.	0
pros: adapting structure and weights simultaneously appears to improve learning in some cases and is a promising direction for research.	2
relying on the temporal structure for the hierarchy, and forcing the master policy to be relearned from scratch for each new task may be problematic in general, but this work shows that in some complex settings, a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of metapolicies over these motorprimitives.	0
since the original algorithm was already formulated with a general utility function, the proposed algorithm is similar in structure but replaces the utility function so that it takes momentum into account.	0
since the original algorithm was already formulated with a general utility function, the proposed algorithm is similar in structure but replaces the utility function so that it takes momentum into account.	0
on the positive side, there are certainly some interesting ideas here: the notion of goalconditioned value functions as proxies for a model, and as a means of merging modelfree and modelbased approaches is very really interesting, and hints at a deeper structure to goalconditioned value functions in general.	2
on the positive side, the paper is very clear and wellwritten, the sru is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace lstms in most cases but yield fast training.	2
the idea of structured matrices in this context is not new, but the diagonal block structure appears to be.	0
pros  a new approach to analyzing the behavior of weight matrices during learning  a new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.	0
cons: i think that the main drawback of the paper is that the structure of the neural network and the deep learning techniques used for optimizing the loss function are not explained in sufficient detail.	0
[significance] in the experiments, the proposed method is compared with the vanilla model (i.e., the model having no lowrank structure) but with no other baseline using different compression techniques such as novikov et al., 2015. so i cannot judge whether this method is better in terms of compressionaccuracy tradeoff.	0
furthermore, it becomes clear, that without cnn structure no really good performance is achieved neither on cifar nor on imagenet pros:  the paper is nicely written and good to follow.	2
strengths:  it is a sensible idea to improve the euclidean distance by the geodesic logeuclidean distance to better explore the manifold structure of the psd matrices.	2
"the memory model is not an rnn, but it is a recurrently called structure (as the name ""phonological loop"" also implies)  so i would also not highlight this point much  why would the four properties of the proposed method (mid of p."	0
the assumption on data and structure of network is a bit strong, but this is the first result that achieves a number of desirable properties ``1. works for overparametrized network 2. finds global optimal solution for a nonconvex network.	0
such a perturbations are random (they have not structure) and lack of interpretation for a human user.	0
the proposed metric is not necessarily a diversity metric, it is also a quality metric: in a situation where all the models diverge and generate random noise, with high diversity, but without any structure.	0
it lacks again any formal proof that the proposed approach exploits scalefree structures or even a proper motivation why this regularization should improve results.	0
music is not merely a series of notes, but entails an overall structure of its own sure, but natural images are not merely a series of pixels either, and they certainly have structure, but we are making lots of good progress modeling them.	0
maybe the structure of the variational distribution relaxes the overfitting problem, but even it's the case, this choice is not justified well in the text.	0
cons: the authors formulate the problem in such a way that they are forced to use an algorithm for nonmarkovian models when they could have conserved the markovian structure by choosing the appropriate parameterization.	0
i am not so familiar with smiles strings  but could it be that the experimental success reported here is mainly a result of the very specific structure of valid smiles strings?	0
my main concern is that the structure of these domains is very similar  essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general pomdps (or, in their continuous counterparts, statespace models).	0
our results answer this question negatively.” [f] “we are the first to evaluate any drlbased navigation method on maps with unseen structures” the paper also conducts an extensive analysis of the performance of a different version of the nava3cd1d2l algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions).	0
the analysis of manifold structure of dnn is important direction, but i am afraid novelty and insight of this work is not enough for acceptance.	0
my impression is that the present line of work will not be relevant for deep learning and will not beat stateoftheart results because of the lack of a structured prior.	0
as a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in  connection between ipms and mmd gets a bit lost; a figure (e.g. flow chart) would help  wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline  figure 1 is a bit confusing; why is kid tested without replacement, and fid with?	0
"my key concern is with the results as described in 4.1.; the correlation structure breaks down completely for ""lowbudget"" smash in figure 5(a) as compared figure (4)."	0
overall the paper is wellstructured and related work covers the relevant papers, but the details of the paper seem hastily written.	0
the conclusion seems to be that not constraining the predicted high level structure to match the encoder’s output, but biasing the encoder’s output in the observed frames to represent groundtruth pose information, gives the best results.	0
the idea of “soft ordering” enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular.	0
strengths:   clear writing and document structure.	2
the method is correct but it is not clear whether the method can match the performance of stateoftheart methods such as graph convolution neural network of duvenaud et al. and structure2vec of dai et al. in large scale datasets.	0
these models are not only large in terms of parameters but also quite sensitive to modifications in the weight structure.	0
"first, the description is mapped to a ""sketch"" (y) containing high level program structure but no concrete details about, e.g., variable names."	0
in this setting, they propose an algorithm based on sketches abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names.	0
overall, i think the paper contains several novel ideas, but its structure requires a 'significant' rework and in the current form it is not ready for being published.	0
this seems like a missed opportunity to capitalize on structure to bias predictions (neutral sentiment is closer to positive than is negative, after all, but the model does not know this as currently specified).	0
"i found it interesting to see what it does with the mixed signals of the word ""but"": on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence."	0
the results look aesthetically more pleasing than the baselines, but the reader does not learn much about how the method actually behaves in practice; when does it break down, how sensitive it is to various choices (network structure, learning algorithm, amount of data, how well the content and view can be disentangled from each other, etc.).	0
the reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the deeplift method.	0
differently from (yogatama et al, 2017), this paper doesn’t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way.	0
vague claims of the gain in replacing fc with these structures, lack of comparison with methods targeting that claim.	0
with a smaller network than the proposed learned structure (4.2m vs 6m) here they achieve slightly worse (5.5% vs 4.58%) but with a slightly larger (7.1m vs 6m) they achieve slightly better results (4.47% vs 4.58%).	0
"untuned but not irrelevant: the role of untuned neurons in sensory information coding, https://www.biorxiv.org/content/early/2017/09/21/134379 ' correlated variability modifies working memory fidelity in primate prefrontal neuronal ensembles https://www.ncbi.nlm.nih.gov/pubmed/28275096 ' on the interpretation of weight vectors of linear models in multivariate neuroimaging http://www.sciencedirect.com/science/article/pii/s1053811913010914 ' see also learning how to explain neural networks https://openreview.net/forum?id=hkn7cbatw ' regarding the intuition in section 3.1, ""the minimal description length of the model should be larger for the memorizing network than for the structure finding network."	0
generally, the work is well articulated with sound structure but needs polish.	0
pros:  interesting concept of combining algebraic structure with a data driven method  clear idea development and well written  transparent model with enough information for reimplementation  honest pointers to scenarios where the method might not work well cons:  the method is only intrinsically evaluated (tables 2 and 3), but not compared with results from other motion estimation methods	2
pros:  interesting selfsupervised framework provided for highlighting relevant substructures for a given prediction task  the hard selection setting is encoded in input graph featurization cons:  it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks).	2
the authors are using images as input, but the images are all synthetic, and further, they are all synthesized to have highly regular structure.	0
again, the short walk may be capturing the communities but the highdimensional random walk sample path seems like a high price to pay to learn community structure.	0
i found that the paper suffers many shortcomings that must be addressed: 1) the writing and organization is quite cumbersome and should be improved.	0
2) lacks focus the paper lacks a good organization in my opinion.	0
a more reasonable cs style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.	0
summary of the reviews: pros: • a novel diffusion convolutional recurrent neural network framework for traffic forecasting • apply bidirectional random walks with nice theoretical analysis to capture the spatial dependency • novel applications of sequence to sequence architecture and the scheduled sampling technique into modeling the temporal dependency in the traffic domain • this work is very well written and easy to follow cons: • needs some minor reorganization of contents between the main sections and appendix detailed comments: d1: some minor reorganization of contents between the main sections and the appendix will help the reader reduce crosssection references.	2
these empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility.	0
the claim that lda works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true.	0
the paper is wellstructured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel.	0
pros:  investigating the ability of distributed representation in encoding input structured is in general interesting.	2
pros 1. the results on bandit structured prediction problems are pretty good 2. the idea of a learnt credit assignment function, and using that to separate credit assignment from the exploration/exploitation tradeoff is good.	0
my personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but i would like to hear the authors' thoughts.	0
the paper is also poorly written and structured, missing many key details about the algorithm.	0
• result section not well structured and results lack credibility: • long sections in the result section describe the actual algorithm.	0
even though the idea presented is a novel contribution and has potential the paper itself is highly unstructured and confusing and lacks a proper grammar check.	0
the main claim being argued in the paper is that the proposed stochastic policy has better final performance on average than a deterministic policy, but the only practical difference seems to be a slightly more structured approach to exploration.	0
the idea of structured matrices in this context is not new, but the diagonal block structure appears to be.	0
"there are many methods that can operate on prestructured data only, but also have the ability to incorporate text data when available, e.g. ""universal schema"" [riedel et al, 2014]."	0
regarding the limited set of problems: of course any given work can only explore so many tasks, but for this to have general implications in nlp i would maintain that a standard (structured) sequence tagging task/dataset should have been considered.	0
"it also implies that this would be true for any type of noise, and support this later claim using experiments on cifar and mnist with three noise types: (1) uniform label noise (2) nonuniform but imageindependent label noise, which is named ""structured noise"", and (3) samples from outofdataset classes."	0
clarity: the paper is well structured and written, but sections 14 could be significantly shorter to leave more space to additional and more conclusive experiments.	0
my impression is that the present line of work will not be relevant for deep learning and will not beat stateoftheart results because of the lack of a structured prior.	0
overall the paper is wellstructured and related work covers the relevant papers, but the details of the paper seem hastily written.	0
overall this paper examines interesting structured and randomized low communication updates for distributed fl, but lacks some important experimental comparisons.	0
the paper is overall clear and fairly well structured, but it suffers from several flaws, as next discussed.	0
the idea of incorporating learned representations with a structured bayesian filtering approach is interesting, but it's utility could be better motivated.	0
cons how and why the architecture is designed in this way should be further discussed or explained.	2
provides valuable insight into the maml objective and its relation to probabilistic models con:  the paper is generally wellwritten but i find (as a nonmetalearner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details).	0
"the learning algorithms used are not clearly explained: the authors simply state that they use ""accnet"" (from some unpublished prior work), but to readers unfamiliar with this algorithm, it is difficult to judge the contents of the paper."	0
the methods and experiments are poorly explained.	0
overall, the paper lacks any novel technical insight, contributions are not explained well, exposition is poor, and the evaluations are invalid.	0
10the vigenere cipher is explained again at the end of section 4.2 when it has already been presented in section 1.1 11 concerning results in table 2: i do not see why it would not be possible to compare the performance of the method with classical frequency analysis, at least for the character case.	0
3.6 claims that all information is local except d.hat{s}_k / dt, but this is not the case as d_i is not local (which is explained later).	0
the novel cell slstmi is meant to be different from lstm by addition of “input weight vector c_i”, but is not explained where c_i come from.	0
as explained below, the presented investigation is however insufficient to assess whether the proposed defense helps in a true whitebox scenario.	0
pros: () the paper is well written and the method is well explained () the authors ablate and experiment on large scale datasets cons: () the proposed method is a simple extension of resnext () the gains are reasonable, yet not sota, and come at a price of more complex training protocols (see below) () generalization to other tasks not shown the authors do a great job walking us through the formulation and intutition of their proposed approach.	2
the paper reports examples of the relative importance of certain concepts with respect to others in figure 5. pros  the paper proposes a simple and novel idea which could have a major impact on how deep networks are explained.	0
the authors do mention that the vae semantics allow to provide some weak form of regularization on q(z) during training, but the way in which the choice of decoder standard deviation alters the shape of q(z) is not explained, and there is no justification for choosing one standard deviation value in particular.	0
the superscripts indicating the dimensions of parameter matrices and vectors are quite helpful, but don't seem to be explained anywhere in the text.	0
line 7 of algorithm 4 also seems critical but is never explained – does this actually mean you have to evaluate the performance twice for each iteration?	0
pros:  the paper is clearly written  the method is new and somehow theoretically guaranteed by the proof of the proposition 1  the experiments are clearly explained with detailed configurations  the performance of the method in the model compression task is promising cons:  the “simple deduction” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis  it is confusing whether the output of the gate is sampled based on or computed directly by the function g  the experiments lack many recent baselines on the same dataset (penn treebank: melis et al. (2017) – on the state of the art of evaluation in neural language models; wmt: ashish et.al. (2017) – attention is all you need)  the experiment’s result is only slightly better than the baseline’s  to be more persuasive, the author should include in the baselines other method that can “binerize” the gate values such as the one sharpening the sigmoid function.	2
o the reward is a) not clear, and b) not well motivated when it is explained, and c) not explicitly stated anywhere: it is said that the actionspecific reward may be up to 10 times larger than the final reward, but the actual tradeoff parameter between them is not stated.	0
instead, i would recommend that in future versions of this document a single network, with a specific router and set of decisions, and with a single algorithm, will be explained with clear notation endtoend beyond the clarity issues, i suspect also that the novelty is minor (if the state does not include any information about the current output) and that the empirical baseline is lacking.	0
however, my main concern is the privacy aspect of the technique, as it is not explained clearly enough in the paper.	0
the main problem is that the work is poorly explained: starting from the task at hand, through the intuition behind the idea how to solve it.	0
however, the evaluation metrics and procedure are poorly explained; what are adjusted mutual information (ami) and normalized mutual information (nmi)?	0
cons  it is not explained why it makes sense to first convert the weight vectors into probability distributions by applying the softmax function, and then measuring distances using kl divergence between the probability distributions.	2
and compare that to han et al. 1. if this is what you try to do, it is never clearly stated it up to this point, and much of the preceding text is irrelevant and it is sufficient to just refer to existing work... i now see you have a similar statement in discussion, but if this is what you try to do and has to be explained at the beginning.	0
• when described, it is stated that it's a mix of different german and french doctrines that are name dropped but not explained.	0
the paper does propose to use an imageonly encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the introduction of the paper).	0
however as explained above the final conclusions are also significantly weaker than this prior literature so it’s a bit of apples vs oranges comparison.	0
please cite some work in that case) (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at runtime, or speed of a single iteration, not necessarily speed to train) [as i am not primarly a neural net researcher, i had always thought pruning was done to decrease overfitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix] (4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (martingale or markov chain theory) this is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above).	0
cons: i think that the main drawback of the paper is that the structure of the neural network and the deep learning techniques used for optimizing the loss function are not explained in sufficient detail.	0
the relative speedup of warpnet compared to resnet needs to be better explained — the authors break the computation of the warpnet onto two gpus, but it’s not clear if they do this for the (vanilla) resnet as well.	0
i think most readers, even those very familiar with probabilistic models and rl, would find reading the paper difficult due to jargon/terminology and poorly explained concepts.	0
i find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below.	0
i guess this can be explained by the fact that rotation equivariance makes sense for aerial images, where the scene is mostly frontoparallel, but less for cifar (especially in the upper layers), which exhibits 3d objects.	0
pros:  experimental results are clearly presented  most details of experiments are explained  the basic outline of the paper explores an interesting question cons  many sentences do not have a clear point, and there are many spelling and grammar errors  despite claims to the contrary, the experiments are far from extensive; they explore two models, with variations in context for embeddings, with 3 optimizers, on just one task.	2
"to be an extensive discussion of ""text"", i would expect to see languagemodeling and seqtoseq tasks as well  the majority of the manuscript is taken up by overly long explanations of background concepts (e.g. half a page on risk minimization, half a page on ""different kinds of neural networks"" with vague and occasionally misleading descriptions of rnn vs cnn, containing zero citations)  the related work is significantly lacking, and it appears that the authors are unaware of results and published work which would be directly relevant to theirs (e.g. arpit et al. a closer look at memorization in deep networks ; advani and saxe generalization error dynamics in high dimensions)  in the introduction, generalization is not clearly explained, which seems kind of like an important thing to do...  vc is a measure/set, not a norm?"	0
i'm on the fence about this work: i like the ideas and they are explained well, but i'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t.	0
they conduct experiments on imagenet1k with variants of resnets and multiple low precision regimes and compare performance with previous works pros: () the paper is well written, the schemes are well explained () ablations are thorough and comparisons are fair cons: () the gap with full precision models is still large () transferability of the learned low precision models to other tasks is not discussed the authors tackle a very important problem, the one of learning low precision models without comprosiming performance.	2
page 3 2.2.it is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all.	0
strengths: the model and the mixed objective is wellmotivated and clearly explained.	2
cons:  many of the details of the approach are not clearly explained or seem to be missing.	0
while the technique is novel as far as i know (eq. (1) in particular), many details in the paper are poorly explained (i am unable to understand) and experimental results do not demonstrate that the problem targeted is actually alleviated.	0
its limitations are that i) no novel machinelearning methodology is used (and relationship to prior work in machine learning is not clearly described) ii) comparisons with previously proposed similarity measures of spike trains are lacking, iii) the authors do not actually use their learned, network based metric, but the metric which performs no better than the baseline in their main results, and iv) it is not well explained how this improved metric could actually be used in the context of retinal prosthetics.	0
figure 1 is interesting but it could use better labelling (words instead of letters) overall: pros: wellwritten, good empirical results, wellmotivated and intuitively explained cons: not particularly novel, a modification of an existing idea, more sensitivity results would be nice	2
pros:  task of reducing computation by skipping inputs is interesting  model is novel and interesting  experiments on multiple tasks and datasets confirm the efficacy of the method  skipping behavior can be controlled via an auxiliary loss term  paper is clearly written cons:  missing comparison to prior work on sequential mnist  low performance on charades dataset, no comparison to prior work  no comparison to prior work on imdb sentiment analysis or ucf101 activity classification the task of reducing computation by skipping rnn inputs is interesting, and the proposed method is novel, interesting, and clearly explained.	2
"specifically, the following points need to be substantially clarified:  the note encoding described in section 3.1: ""w_i = (p_i, t_i, l_i)"" describes the pitch, timing, and duration of the i'th chord, but it is not explained how time and duration are represented."	0
maybe i’m showing my own lack of understanding here, but it’s worrying that the actual sampling technique is not explained anywhere.	0
sec 2 is poorly explained.	0
this paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at section 3.3. more specifically, if you generate the parameters .theta according to eq. 7 and posit a prior over .theta then you will have a problematic variational bound as there will be a kl divergence, kl(q(.theta) || p(.theta)), with distributions of different support (since q(.theta) is defined only along the directions spanned by u), which is infinite.	0
unfortunately, the proof of this result is poorly written:  equation (20) takes a long time to parse  more effort should be put into making this clear  give a reference for the expressions given for a(k,n) in 24 and 25  (27) and (28) should be explained in more detail.	0
weaknesses as explained in the summary, it is not clear to me why the abstraction to ncm is useful if one still needs to define specific subtrace losses for different neural abstract machines.	0
pros: () the idea introduced is simple and flexible to be used for any cnn architecture () experiments on imagenet1k prove demonstrate its effectiveness cons: () experiments are not thorougly explained () novelty is extremely limited () some baselines missing the experimental section of the paper was rather confusing.	2
in section 3.2, it sounds as if the exact hessian is used, and at the end of this section the authors say that figure 6 demonstrates that the effect of this second term is small, but i don't see why this is, and it isn't explained.	0
i would prefer less material but better explained.	0
furthermore, the possibility of using weak labels is put forth but not explained and no ideas on how this may be done are proposed.	0
however the thesis of this work should be better motivated and explained.	0
it would be helpful to include an equation in the paper for computing the rand index since it is explained but only with words.	0
overall: pros:  a nice idea with some novelty, based on a nontrivial observation  the experimental results how the idea holds some promise cons  the method is not presented clearly enough: the main component modeling the network activity is not explained (the hdda module used)  the results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)  experimental results are partial: results are not presented for multiple defenders, no ablation experiments after revision: some of my comments were addressed, and some were not.	2
it is intuitively compelling that cnns are memorizing patches of the training set, but i would like to see more analysis of why this might be expected, of how the results in this paper can be explained in terms of network architecture, learning dynamics, etc. for example, what properties of a net would give rise to this behavior?	0
not so much for how they explained the process of creating the dataset, that was very thorough, but the fact that this dataset was the only means to test the previous network models and their new proposed network model.	0
clarity: the language is clear, but the main contribution could be better explained.	0
3. experimental setup part is long but not wellexplained and is not selfcontained particularly for the evaluation metrics.	0
this is a reasonable procedure and it seems to work well, but also conceptually quite straightforward  this is quite likely how most people working in the field would solve this problem, standard gan techniques are used for training the generator and discriminator, and the network architecture is directly borrowed from radford et al. (2015) and not even explained at all in the paper.	0
section 4 starts with fig. 3 without explaining what the task is, how performance is measured, etc. it also claims that distributional retrace helps while this is not the case in fig. 3 (i realize it is explained afterwards, but it is confusing when reading the sentence « we can also see... »).	0
nothing groundbreaking here but still interesting enough and well explained.	0
experiments are very well described and performed, however as explained earlier some comparisons are needed.	0
what the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, i do not think that this paper is relevant for the iclr conference.	0
"considers a number of possible alternative models intuitive illustration in fig. 1 cons: misleading use of ""covariance"" the several important concepts including prototype mean/variance, distance, and loss are not well motivated or explained evaluation is too limited"	0
there are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be nonambiguous.	0
however, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions.	0
i would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained.	0
there has been a growing number of works that aim to find learning algorithms that learn to discover and disentangle objectlike representations without having so much prior put into the model, but rather through some general purpose objective.	0
overall, this paper tackles its tasks in an interesting but maybe too specific way; in addition, it could be improved in a variety of ways, both in terms of presentation and content.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
overall the generic idea of “quantile regression rl” from section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multiobjective statealigned rewards) makes it more a distraction than an asset.	0
pros novel treatment and formulation of metalearning from the perspective of fast and slow learning process cons some interesting cases not tested presentation could be improved [originality] the paper approaches the recently popular metalearning from a novel perspective by decomposing the learning process into slow and fast ones.	2
the lack of details especially for the technical presentation part make it very hard to read.	0
in addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established.	0
the paper is generally easy to follow but the presentation could be improved.	0
i am not stating that the proposed model for the first step is not achieving the intended purpose, but that from the current presentation in the text i was not able to understand that.	0
cons:  the presentation of the paper is sometimes confusing.	0
overall evaluation: the proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand.	0
more specifically, the paper lacks in clarity and the presentation of the main methodology needs improvement.	0
strengths: 1. nice presentation of the model.	2
pros:  interesting idea, nice results, mostly readable presentation.	2
my main concern with the paper is that the presentation is very hard to follow.	0
an unusual presentation could be justified if the connection to rd models were exploited in some useful way, but that does not seem to be the case here.	0
the presentation of lola can still be improved, but i see the main ideas.	0
in the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.	0
on the other hand, i have several concerns about the presentation of the results:  the abstract and the introduction sets up a misleading narrative around the results: the authors seem to suggest that their work somehow explains why certain learningrate schedules work better than others for deep learning applications / nonconvex optimization, although the actual results exclusively concern the classical problem of linear leastsquares regression.	0
also, i think that this presentation style is rather harmful as it suggests that learningtheory results concerning classical setups are somehow embarrassing, so they need to be sold through some madeup connections to trendy topics in deep learning.	0
clarity:' the goal is well stated but the presentation of the method is confusing.                 	0
the presentation of the approach makes sense, and experimental results using several different gans methods and competing regularization methods are extensive and good in general cons  i didn't find major issues of the paper.	2
adding a sentence explaining the intuition behind using satlu in equation (1) might be helpful to summarize my feedback: i think experimental results and analysis are strong, but the presentation is strongly lacking!	0
cons:  paper presentation needs some work on clarity.	0
to me it makes a lot of sense, but in the experimental part i could not clearly see if the improvement in performance is due to this representation of the binarized bn.	0
writing and presentation suggestions/questions: 1. if the proposed method is a breakthrough, i am fine with the title but i think the experiment results tell us that wasserstein is not all you need.	0
comments: i think the ideas presented in this paper are interesting, but i think their presentation could be a bit clearer.	0
in terms of model presentation, the authors can compare the model with a large set of deep recurrent models that have recently been proposed for modeling time series with nonlinear latent dynamics (e.g. variational sequential monte carlo, structured inference networks for nonlinear state space models, black box variational inference for state space models, composing graphical models with neural networks for structured representations and fast inference, etc.).	0
more details in terms of presentation:  section 2.1 and algorithm 1 reviewed gradient dropping which is the main related work to this paper but it is in the middle of the related work section.	0
(5/10) === pros ===  demonstrates that curiositybased reward works in simpler game environments  (implicitly) calls into question the value of these testbed environments  well written, with a large set of experiments and some interesting observations/discussions === cons ===  little methodological innovation or analytical explanations  offers minimal (but some) evidence that curiositybased reward works in more realistic settings  doesn’t answer the one question regarding observation representation that it set out to evaluate  the more interesting problem, rl  auxiliary loss isn’t evaluated in detail  presumably, the sample complexity is ridiculous overall, i am ambivalent.	2
it's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.	0
the authors evaluated different generative models including vae and various variants of the gans on the benchmark, but the current presentation leaves the details in the dark.	0
but the presentation of the work, especially the experiment section, only gives abundant number of results without detailed explanation regarding the pros and cons of the existing models, the efficacy of the proposed metrics, or the reason behind some nice generative properties of gans that are not able to learn the distribution well.	2
the analysis on the network's representation and convergence is nice but it does not bring much insights.	0
unfortunately, i think the work is slightly let down by the presentation (there are many typos, and the first couple of sections could do with a rewrite), as well as a lack of rigorous experimentation.	0
to summarize my feedback, i like most of the presentation and it is good to see effort towards reducing training times by selecting good training samples, but i think the manuscript requires significant effort to justify acceptance.	0
nevertheless the manuscript has a couple of weaknesses, one concerned with the presentation and another related to the design of the study.	0
clarity: the presentation for this paper is relatively clear, but it is quite technical, so some parts are hard to follow, without knowing the prior work in detail.	0
this paper is a slightly difficult read  not because of the language or the presentation of the material but more because there is not one main coherent argument or goal for the paper.	0
the authors state multiple times that “all our results apply to the multivariate case” but that they restrict themselves to the univariate case for simplicity of presentation.	0
i understand the nested optimization problem in section 6, but the presentation is somewhat unclearly written.	0
it is very likely that there is some merit to the proposed methods that introduce non linearity, but these points simply get lost in the mediocre presentation.	0
pros:  it is an interesting result that adding a weak visual imitation loss dramatically improves rl with sparse rewards  the idea of a visual imitation signal is wellmotivated and could be used to solve practical problems  the method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up rl in practice cons:  it seems possible that imitation only helps rl where imitation alone works pretty well already  some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal gail  the presentation borders on handwavy at parts and may benefit from a clean, formal description the submission tackles a real, wellmotivated problem that would appeal to many in the iclr community.	2
hard to parse “different and more difficult than graph generation designed only for learning the distribution of graph representations, for graph translation one needs to learn not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously.“  hard to parse “graph translation requires to learn”  hard to parse “in most of them the input signal is given over node with a static set of edge and their weights fixed for all samples”  “we propose an graph” > “we propose a graph”  hard to parse “the two components of the formula refers to direction filters as talked above”  hard to parse “next, graph translator requires to”  “as shown in equations equation 7 and equations equation 6,” > “as shown in equation 6 and equation 7”  hard to parse “the challenge is that we need not only to learn the”  figure 2 would seem to need more explanation.	0
######################### post rebuttal: the authors have addressed most of my concerns regarding the poor presentation of the earlier version.	0
in this paper the authors propose dl2 a system for training and querying neural networks with logical constraints the proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved.	0
even though the need of handling arbitrary input size is an interesting problem, i have several major concerns about this paper:  one of the main problems of this paper is its presentation, both the writing and methodology.	0
this presentation might contradict the timeline of the development of your approach but it might help to better connect your work to other works on the same topic.	0
i think this work is promising and interesting to the probabilistic modeling community, but needs some cleanup and some more compelling presentation (non image data?	0
due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.	0
this paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.	0
but more importantly, the reasons why i don’t recommend accepting the paper are the following ones: lack of clarity: i found that the paper lacks clarity in its presentation.	0
also, the paper lacks comparisons to the recently proposed disentangled representation learning models (fhvae and disentangled sequential autoencoder).	0
results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain.	0
presentation concerns  i have serious concerns about the presentation quality of this paper.	0
deepström networks quality: good originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: a number of open issues in the presentation  see detailed comments the paper is based on some concepts to link nonlinear data representations by means of nonlinear kernel mappings, with the deep learning approach.	2
i think the main idea of the paper is interesting but the presentation lacks a number of details in the description/parametrization and the experimental design.	0
there are some other problems with the presentation, including the fact that contrary to what is suggested in the introduction, the model seems to have access to the ground truth size of the blank (since positional encodings are given), making it all but useless in a real world application setting, but it is really difficult to evaluate the proposed task and the authors' conclusions without a much more detailed description of the experimental setting.	0
clarity: while the paper studies an important problem  it's important to move out of the norm ball based attack models and consider different attacks like spatial transformations, in the current version, the presentation lacks clarity in both the formulation of the attack model, attacks, defenses and explanation of the results.	0
while i have some questions about the evaluation and minor concerns about the presentation i would overall recommend acceptance of the submission.	0
i have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.	0
for example in section 5, the claim of “cnns learn semantics from images” is mainly proposed in a previous work, but the way of presentation sounds like this is a contribution of this work.	0
it could be lack of background on my part, but i find the presentation extremely confusing, and unnecessarily verbose/florid.	0
the first proposed estimator is unbiased, as shown by a proof, and accepts arbitrary losses, an improvement over prior approaches  the overall presentation is clear and clean cons:  one of the main claims of the paper is the proposal of an unbiased estimator.	0
if what the method does is an incremental version of the lowrank factorization proposed in earle at al (2018), i think the presentation can be better described in those terms.	0
i am still trying to evaluate the paper, but for now, my rating for this is low given that the main novelty in the paper: the environments, the evaluations, the tasks are so unclear because of the verbose presentation style on trying to tell us what we already know, such as goalconditioned learning, offpolicy learning, impala, etc.	0
cons:  presentation is not good.	0
the paper is generally well written, although the initial presentation of the model could be made a little clearer (it is not obvious from the text that the decoder takes the text as input  figure 2 helps, but comes a couple pages later).	0
the paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.	0
########################################### updated review: the authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time.	0
for references, please first check this seminal work and then follow the line of research: ng, andrew y and harada, daishi and russell, stuart, icml 1999, policy invariance under reward transformations: theory and application to reward shaping the method proposed in this paper seems a way of automatically shaping the reward, but loses the optimal policy invariance (for how this invariance is ensured in reward shaping, please check out this tutorial: http://wwwusers.cs.york.ac.uk/~devlin/presentations/pbrstut.pdf).	0
cons 1. unclear presentation of technical contributions, experimental results do not support the key contributions of faster attack generation 2. i am also unconvinced of the relevance of blackbox attack algorithms given the nascent stage of deeprl  since these agents are just being developed and their abilities need to improve significantly before they become deployable (and blackbox adversarial attacks are a real concern), i feel this work is premature and will need to be redone once more capable/robust agents can be trained for practical rl settings ### in light of the revision, i have revised my score given the rewriting of section 3 that addresses the second con i raised above.	2
however, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, i feel that the paper still require improvement before it can be accepted.	0
overall, i think the paper has some observations that may be slightly interesting; however, it lacks novelty and the analysis or presentation are unconvincing.	0
general evaluation ( pro/  con, more specific comments/questions below):  the paper is very wellwritten  the bbp presentation is light but very accessible.	0
however, despite my favorable opinion, i consider that the paper presentation lacks rigor at many levels.	0
"the presentation of the new forget gate in ""system 2"" is clear in terms of being able to implement it, but it's not intuitive to me what this actually looks like."	0
pros/cons summary:  the proposal yields good results in the provided experiments  minor contributions that are not convincing enough  muddled presentation of ideas  dubious or weakly motivated design choices  poorly written with plenty of typos  difficult to follow	2
cons: the biggest issue is that the overall presentation of the paper is weak, with several noticeable grammar mistakes and obscure sentences that are hard to follow.	0
in summary, the paper's quality has significantly improved, but some presentation issues keep it from being a great paper.	0
regarding more specific details, there are too many small presentation issues for me to list them all, but here are a few representative samples:  please use “tile” or “cell” to denote individual elements in the grid (not “grid” itself)  it is unclear how states are represented as inputs to the model for generals.io  the 77% win rate against flobot seems to be an important achievement of the paper since it is mentioned in the abstract and introduction... but not in experiments (!)	0
to make it acceptable, the clarity of presentation, especially of the results, must be improved, but more importantly, more work seems necessary to reduce the currently significant accuracy hit from the method, and the tradeoff of quantization level vs. robustness should be addressed.	0
pros and cons:  sound approach  good theoretical support of the approach (biasvariance analysis)  great results reported  of importance for optimizing without gradients  presentation of the method lacking many details and not very clear  overall quality of the paper is subpar, tend to be very textual and hard to follow in several parts  experiments are not exhaustive and detailed.	0
the presentation of the paper is excellent  clear, wellmotivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.	0
the presentation is pretty good as far as the english goes but suffers from serious problems on the level of formulating concepts.	0
it sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation.. my main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation).	0
however, there are presentation and clarity issues in the technical development, and the comparative analysis is lacking broader comparisons with the stateoftheart (to be fair, the authors recognize that layerwise max as a baseline is particularly susceptible to outliers).	0
pros:  good presentation and clear explanations.	2
however, i had a few concerns about the presentation which affected my understanding of the methodology.	0
in summary, even though i liked the idea and the approach to the problem, the presentation is quite lacking, and in its current form, the paper might lose impact just by virtue of being hard to understand and subsequently being hard to reproduce.	0
i think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.	0
the problem is very relevant, and the approach is interesting, but unfortunately, the presentation is very misleading (see details below).	0
therefore, i recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).	0
it seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ml venue. ''''''	0
in its current form, i do not recommend accepting this paper but i do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via rl experiments.	0
the presentation needs work in several areas, and the experimental results require more explanation, but otherwise this seems like a solid paper.	0
the overall presentation of the method is direct but slightly confusing.	0
the section 4.3. shows that the proposed method does find better representations of the mnist than vae, but does not mention that there are numerous extended vae methods (and others) that would perform better than the ltvae here.	0
overall the paper proposes a bnstyle structure on vae latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.	0
it would be great if the code associated to this could be released but the presentation allows for reproducibility.	0
however, i have some concerns about presentation and a number of specific questions about model implementation and evaluation.	0
i suspect that low mi means that input and latent representation are somewhat independent but i don't see the connection to compositional components.	0
pros  the presentation of the approach is clean and easytofollow.	0
i think the paper shows good results, but it could very much benefit from improved presentation and evaluation.	0
while i have a few concerns about presentation and experimentation, these are issues that can easily be remedied and i recommend acceptance.	0
strengths:  even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to mt is novel  the presentation is very clear  the choice of methods is well argued and justified  the experiments are well executed and analysed  thorough and varied analysis of the experimental findings i recommend this paper for the best paper award.	2
general presentation fairly clear and easy to read # cons ' would have been more impactful to focus experiments on realworld scenarios in which bandwidth is constrained and naturally contentious # other comments ' pg.	2
while the presentation of this paper is technical, it lacks intuition on the assumptions made as well as the conditions posed.	0
conclusion: despite my concerns on the first part of this paper, i think the very thorough experiments, clear presentation and the interesting results on learning rate warmups and model distillation merit its acceptance.	0
at the same time, i have some concerns about the novelty and the presentation.	0
the presentation of the work however lacks sufficient details and motivations, which makes it difficult to judge the proposed model.	0
issues/concerns  i assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which i discuss below.	0
however, the paper spends lots of effort explaining representations, but only a few sentences explaining about how the proposed representations/data structures can help find a somehow generic value iteration solution, which allows to efficiently compute/retrieve a particular solution once a .delta vector is specified.	0
in summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.	0
i have some following major concerns regarding to the quality and presentation of the paper.	0
the clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.	0
on the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology.	0
it may be true that a 2d embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …).	0
"the presentation is lacking in some regard and would benefit from some reworking i.e. section 5. unclear in the paper: section 5 describing the ""joint embedding model using compositional task descriptors"" is very sparse on detail."	0
evaluation: the paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing.	0
strengths  thorough analysis with a good set of questions weaknesses  some peculiar evaluation and presentation decisions  introduces 'yet another' synthetic visual reasoning dataset rather than reusing existing ones i think this paper would have been stronger if it investigated a slightly broader notion of generalization and had some additional modeling comparisons.	2
the paper lacks a lot in clarity and quality of presentation.	0
important issues: one of the biggest concerns is the presentation of the planning algorithm, and more importantly, a proper formalization of what is calculated, and thus a proper justification of this part.	0
concerns regarding the clarity of presentation and interpretation of the results.	0
overall the paper suffers from a lack of clarity in the presentation, especially in algorithm 1, and does not communicate well why the assumption of different dynamical processes should be important in practice.	0
the authors claim to learn functions to compute node representations, however the representations z^u seem to be direct embeddings of the nodes.	0
"in terms of presentation, the motivation in introduction is fine, but the following section named ""notations and pseudocodes"" is confusing and has many undefined notations which makes the paper very hard to read."	0
"the title of the paper in my opinion undersells the result which is not only that ""deep skinny neural networks"" are not universal approximators, but that the class of functions which cannot be approximated includes a set of practically relevant classifiers as illustrated by the figure on page 8. the presentation is extremely clear with helpful illustrations and toy but insightful experiments."	0
this is partly suggested when the authors mention that the representation could be learned from only a partial goalconditioned policy, but this idea definitely needs to be investigated further.	0
though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.	0
"the first paragraph in section 4 (""the presentation flows ..."") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of section 4 or to section 5."	0
comments: i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the numerical evaluation.	0
the only question i have is how does this hold on a real quantum computer such as ibmq/rigetti quantum computing etc.. or even under a noisy simulator although the paper is sounds and it is a good idea, the presentation is a bit lacking.	0
strengths:  good coverage of related work  clear presentation of the methods  evaluation using established semeval datasets weaknesses: 1. it is not entirely clear what is the connection between fuzzy bag of words and dynamax.	2
“… a lack of certain constraints on the generative capacity of current neuralnetwork based generative models make it challenging to infer structure from their latent generative representations.” what does “a lack of certain constraints” mean?	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
i think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired.	0
https://arxiv.org/abs/1810.01392 pros:  interesting observation of density modelling shortcoming  clear presentation cons:  lack of a strong explanation for the results or a solution to the problem  lack of an extensive exploration of datasets	2
my only concern is the incremental nature of the method, which is only partly offset by the good presentation.	0
'pros:'  easily accessible paper with good illustrations and a mostly fair presentation of the results (see suggestions below).	2
overall, i think explicitly taking into account different feature types in the lstm cell update rules is interesting, but the contributions of this paper compared to tlstm are not significant enough for acceptance, and the presentation can be made more clear.	0
the problem is of great importance but the theoretical results and presentation contain many issues that make the paper unfit for publication.	0
=========== update: authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.	0
pros: presentation of new application of representation learning models construction of a new dataset to the community for binary software vulnerability detection the proposed model shows a good performance cons: the presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.	2
however, while the method itself is promising, the weak baselines (in particular, the lack of evidence disentangling the benefits of larger scale / more frames vs the benefits of the proposed method) and unclear presentation make me unable to yet recommend the paper for acceptance.	0
there are basic concerns about how implicit and explicit rewards can be combined, and the technical presentation needs some improvement.	0
the presentation of the core idea is clear but imo there are some key missing details and experiments. '	0
pros:  a simple idea with good empirical results that would be of interest to the community cons:  (extremely) unclear presentation which hinders the message of the paper.	2
5. overall, i think this is a good paper, gives a good overview of an important problem; the matching networks idea is nice and simple; but the paper could be more broader in terms of writing than trying to portray the success of discern specifically.	0
partial conclusion: the description of the method contains relevant information and is functional, but the writing could be improved. '	0
cons:  writing could be improved, as the methodology currently reads as a summation of facts, and some parts are written out of order, resulting in various forward references to components that only become clear later.	0
cons: 1. the paper in its current form isn't very polished yet and clarity can still be improved in several ways: ' there were more than a few spelling and grammatical errors, please proofread the work and improve the writing.	0
cons:  the writing, in my opinion, needs to be improved [see my comments below].	0
i have multiple concerns with the papers: (i) the writing is informal and the ideas are not well explained.	0
they are many editing problems and the english is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.	0
cons 5. the paper wastes valuable space writing out in detail the equations for backpropagation in a standard feedforward mlp.	2
there is also a statement in the paper that is problematic but can be fixed by rewriting.	0
in section 6, the paper could give more details about random walk concepts such as 'stochastic', 'stationary distribution' etc., as it would make the paper more accessible (i am familiar with these concepts but since the writing of the paper is of high quality in the other sections, i am convinced this would improve its impact, and attract more readers).	0
figures 2 and 3 are the central architectural choices, but the writing around them does not clarify why all the choices have been made, and what the implications are.	0
the paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.	0
"2. it try to link it to information theory but most of study is just empirical (which is fine, but avoid it can simplify the writing and make it more readable), e.g. "" according to information theory and the attention mechanism (bahdanau et al., 2014), it is clear that we.."" i agree with the intuition but how it can be ""if and only if""?"	0
the paper might contain some interesting ideas, however, i am quite confused about the paper due to lack of clarity in writing.	0
the paragraph after eq 3 needs some rewriting  the explanations around and including equations 5 and 6 were quite poor: .pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in eq 6 to avoid ambiguity 2) the reference formatting is wrong (i.e. cite is used everywhere instead of citep) 3) i thought the motivation for the approach in the intro was very good 4) as the seemingly most related work, it would be good to elaborate more on the goyal et al paper and the differences of your approach to theirs.	0
# summary pros: ' useful dynamic batching trick that can lead to speedups ' empirical evaluation compares to two existing techniques and breaks down individual components of runtime cons: ' no critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage ' some questionable statements and assumptions ' lack of formalization and clear definitions ' paper reads longdrawnout, subpar writing hurts readability	2
"pros:  proposed regularizer is wellexplained and seems to work well, ablation study is helpful cons:  the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text  some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work specific comments / nits: (in reading order) 1. i think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative rl; i think ""forethinking"" or something like that that is an actual word would be better, but i can't think of a good word... maybe frugal?"	2
cons: while i appreciated the writing clarity of the paper, the paper misses the whole point of defensive ml research: in the model poisoning case, a minimal requirement for a defense mechanism is to be formally proven 'whatever is the behavior of the attacker' (within the threat model).	0
additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in section 3; (2) omitted details, e.g., how much overlap exists between kernels (figs.	0
weaknesses:  paper writing is good until section 3.1. this section is very confusing.	0
overall, i found the writing very clear, the main idea sound, and paper generally well executed, but i have serious concerns about the significance of the contributions that lead me to recommend rejection.	0
overall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).	0
e.g. the writing makes it seem like proposition 2 and theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size.	0
overall: pros: 1. clear writing 2. good motivation description.	2
pros:  the writing is clear.	2
writing and presentation suggestions/questions: 1. if the proposed method is a breakthrough, i am fine with the title but i think the experiment results tell us that wasserstein is not all you need.	0
overall this paper gives a useful but incremental improvement over the deep value network proposed by gygli et al. 2017. however, the writing should be substantially improved to make the paper more selfcontained and to include missing experiment details.	0
i am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but i am not overly concerned about this since proper due credit is assigned in the paper.	0
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
figure 2 is nice as a sales pitch, but it doesn't replace actually writing down the objective functions used for training!	0
clarity: some sections of the paper were harder to digest, but overall the quality of the writing is good and the authors have made efforts to present examples and diagrams where appropriate.	0
"the writing is mainly fine, though some sentences are written poorly and would benefit from a revision, e.g., 2nd paragraph, ""but none of them is suitable to train deep neural nets (dnns)."""	0
in terms of writing, the first few pages tend to be repetitive yet vague about what exactly will be done (generally ok for introduction, but a bit too vague).	0
cons:  writing can be improved.	0
beyond the highlevel lack of clarity about the contribution of the paper, the writing lacks precision and rigor, and many things are undefined (though one can figure them out after reading many times back and forth).	0
in short, a promising direction, but the contribution of the paper appears to be over claimed and the writing of the paper needs significant improvement before the paper can be accepted for publication.	0
quality: the introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing. '	0
not only is the writing hard to understand (some sentences lack a verb!	0
additionally, due to the lack of clarity in the writing and lack of mathematical rigor, theorem 1 does not seem to be true as stated.	0
the paper also has the following weak points: 1. the writing is a bit rough throughout, though not to extreme distraction.	0
# weaknesses ## writing i have to start with the most obvious one.	0
in addition to the lack of novelty or new insights, the writing needs serious attention.	0
using a generative model as the surrogate distribution for kernel twosample test is novel  an important and new application of deep generative models  strong experiments on synthetic and realworld time series data sets  very clear writing and explanation of the idea  reply sample segments from both directions (past and future) while in the practical setting, cpd is usually sequential and in one directional  lack theoretical understanding of the limit of the neuralgenerator in the kernel twosample test	0
but the execution is limited and the writing poor with critical details lacking.	0
3) weak points: the writing is sloppy.	0
as i mentioned, the paper has novel and interesting ideas, but it would be greatly improved with some important rewriting.	0
my recommendations to improve the article: (1) writing  i really enjoyed this work, but frankly, the writing is horrible.	0
there are a few areas where the writing could be more clear: .omega_t is introduced right after equation 5, but it is unclear what is that parameter.	0
pros: 1. the defense technique does not require knowledge of the attack method cons: 1. the paper is incredibly difficult to understand due to the writing.	2
even though the need of handling arbitrary input size is an interesting problem, i have several major concerns about this paper:  one of the main problems of this paper is its presentation, both the writing and methodology.	0
but there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc. the authors also rely mostly on the fid metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.) the writing is understandable for the most part, but the paper seems to lack focus  there is no clear take home message.	0
"it is comparable to and to my ears not better than existing polyphonic systems such as the ones below (links to sample audio are provided here): bachbot  https://soundcloud.com/bachbot (liang et al 2017)  tied parallel nets  http://www.hexahedria.com/2015/08/03/composingmusicwithrecurrentneuralnetworks/ (johnson 2015, ref below) performancernn  https://magenta.tensorflow.org/performancernn  (simon & oore 2017) ..others as well.. clarity  some of the writing is ""locally"" clear, but one large, poorlyorganized section makes the whole thing confusing (details below)."	0
pros:  the model is interesting and the motivation is quite clear  analysis is quite nice  writing is quite clear and decent cons:  extremely lacking experimental validation  there are literally no baseline models, no numbers or any kind of quantitative analysis.	2
strengths:  clear writing.	2
3) some writing issues: it would be better to 'clearly' demonstrate the final accuracy of different models (i.e. resnet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text.	0
the writing is generally clear, but the paper should be checked for typos.	0
### comments about writing ### the findings are in general interesting and inspiring, but the explanations need some further improvement.	0
in particular, the writing lacks some consistency and clarity in the wordings.	0
the submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.	0
i found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.	0
my major concern is the writing.	0
this could all be addressed with a change in the pitch and tone of the paper, but the authors should ask themselves “what are we adding beyond what can be found in the work of sutton et al (2011), modayil et al (2013), white (2015), and sutton’s numerous writings on predictive approaches to ai?” the connection between nexting (and thus gvfs) and successor features is well known, while theorem 1 follows directly from the original policy gradient theorem.	0
of course, an exhaustive discussion is not expected, but the writing makes it sound like there hasn't been much work at all.	0
evaluation  the writing of the paper is in general ok, but reading the introduction that categorizes the ssl by three streams seem somehow unnatural to me.	0
the writing is organized poorly and the formulas are sloppy and scattered.	0
weaknesses: the writing quality is somewhat weak.	0
overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.	0
> section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.	0
this is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like minibatches sizes , value of .lambda in the h derivation, how one calculates the .sigma^2_g within the algorithm presented in the appendix.	0
the technical approach (combining vae vectors to make new shapes) is not particularly novel[ overall: the paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.	0
clarity: ' the writing of the paper was clear for the most part, however the experimental section could have been clearer.	0
moreover, in spite of the authors writing that their goal is “completely different” from [lee at al 18a, ma et al 18a], i found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.	0
pros:  the writing is mostly clear and easy to follow.	2
the parer is in general interesting, however the clarity of the paper is hindered by the existence of several typos, and the writing in certain passages can be improved.	0
the writing has improved, but still has stylistic and grammatical issues.	0
instead of writing the chronological story of what you did, instead you should explain the problem, explain why current solutions are lacking, and then present your own solutions, and then quantify the improvements from your solution.	0
current, the method appears to be great, but the writing quality of the paper is not yet there.	0
the main weaknesses are poor writing, and that some details of the implementation required to reproduce the results are missing.	0
this function might seem a little ridiculous, but continuous or even lipschitz version of this counterexample could be constructed by smoothing things out around , and a function such as this could be obfuscated by writing it down with a long, complicated expression making it hard to identify by inspection.	0
i found the paper very difficult to read; not so much because of the topic, but due the writing.	0
current manns only support dense writing  presumably this means dense as in 'every timestep', but this terminology is overloaded  you could consider ntm / dnc as doing dense writing, and then work of rae et al 2016 doing sparse writing.	0
pros  highquality writing  very clear  complete experiments on a variety of tasks, some of which do not have optimal solvers  honest assessment of the model cons  the theoretical contributions are not groundbreaking (either the the tweak on reinforce or the model architecture)  the model is still far from obtaining meaningful results on tsp (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...) details  dai et al has been published at nips and is no longer an arxiv preprint  the comparison to alphago should either be expanded upon or scratched.	2
after reading the author response and the updated paper, i am satisfied on several of my concerns, many of which were due to the writing in the earlier submission.	0
"quality: the writing of the paper needs more polishing; i saw grammatical errors here and there: for example, at the first paragraph of page 2, ""alternating"" should be alternative and ""synthetical"" should be synthetic... clarity: i have not been able to fully understand why the proposed (uniform) sampling variant of bn is better than previous effort at making bn less computationally expensive in a gpubased training environment by reading the paper: 1. the authors argue that the ""summation"" operation is the one that makes bn expensive; however, the authors have not demonstrated enough evidence of this argument 2. if ""summation"" operation is what makes bn expensive, then in a gpubased environment, can we simply divide the data into smaller batch, and train on each gpu using a smaller batch (this is way, each gpu is essentially calculating the statistics based on a subsample) 3. the authors discussed microbn, which ""alleviate the diminishing of bn's effectiveness when the amount of data in each gpu node is too small..."" this seems to show that in practice, training with bn does not suffer from having a large batch, but instead suffers from having too small batch size on each gpu node."	0
"examples of imprecise / casual writing: ""good performance without discounting, but training was less stable."""	0
in its current form, i do not recommend accepting this paper but i do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via rl experiments.	0
these concern principally notation issues and some potential improvements in the writing.	0
cons: 1. the writing is imprecise and often hard to follow.	0
overall the writing is good... but i did find the main statement of the algorithm confusing!	0
i have other concerns which mainly stem from lack of clarity in writing: 1. in the line right above remark 1, it is not clear what “assumption” refer to.	0
the writing of the paper is good, but the writing of the captions could be improved.	0
i have no doubt to the novelty, but the writing could definitely be improved.	0
"other refs to mention: randomized relu (randomly set the leak threshold) https://arxiv.org/pdf/1505.00853.pdf noisy activation functions quality: decent (6.5/10), but i think the experiments, level of analysis, and quality of writing are more like a very good blog post than an academic paper (although i think that that line is becoming very blurry) clarity: decent (7/10), but there are many small grammar errors, especially to do with pluralization (e.g. ""activation functions .... its importance"" should be ""their importance"") and incorrect verb tense (e.g. ""was used"" should be ""is used"" or ""has been used""; past tense implies it no longer is used)."	0
"i pointed out the ones that were particularly confusing in the ""specific comments"" section, but the paper should be thoroughly reviewed for writing quality."	0
pros:  marginal improvement across the board  partial replacement strategy might be useful for other methods cons:  writing contains grammatical errors / change of tense / confusing sentence construction which make the paper somewhat hard to get through  not much exploration or insight about the learned functions or the activation function performs well  numbers seem quite incremental to me, and the method is not tested on larger/deeper networks where the gains might be mode significant.	2
while this could be an interesting result, i have several concerns regarding the assumptions, correctness, and writing.	0
pros:  natural, novel extension to gradientbased metalearning  state of the art results on two competitive fewshot benchmarks  good analysis  clear writing cons:  realistic, highdim data is only from the image domain minor questions for the authors:  relation networks are computationally intensive, although in fewshot learning the sets encoded are fairly small.	2
issues/concerns  i assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which i discuss below.	0
review versions of papers often lack polished writing.	0
based on the method, this paper could go either way, but given my concerns on its novelty and writing quality/clarity, i lean slightly towards reject.	0
pros:  the paper is well written, mostly selfcontained, and easy to read (for someone familiar with information theory);  all mathematical points are detailed and well explained, with sufficient introduction;  the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;)  information bottleneck is a topic of prime interest in the community these days;  the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning;  the solution brought to the ib lagrangian issues is simplistic though efficient (squaring i(x,t) so that it's not linear in i(x,t) anymore).	2
my main comment is that this work requires (a) more substantiation of the claim that attention shift is the phenomenon at play when it comes to lack of transferability, (b) improvement to the writing, and (c) more motivation behind the choice of mitigation mechanism.	0
however i do have a few concerns: 1. the writing can certainly be improved.	0
first, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in bartunov et al. the scientific question in bartunov et al. is not a matter of whether bp algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns.	0
the paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.	0
cons:  clarity of writing: lots of typos and bits of math that could be more clear (see detailed comments below) [fixed]  the plots in section 4 are all extremely jagged.	0
my concerns/questions are as following: (a) firstly, the writing is too verbose and vague without clarifying the details and there are too many references to laflaquiere et al. i would recommend the authors to be more precise, i.e. define topological/metric invariants, clarify how inconsistent sensory/motor pairs are sampled in mtm condition and how environment is perturbed in mmt condition.	0
my main concerns about this paper are its related work and its writing.	0
the paper is interesting however the benefit over the traditional maximum likelihood estimator is small and the writing needs a bunch of work.	0
in summary this is potentially interesting work but the writing should be sharper, their should be less ambiguity of interpretation of close set.	0
pros: 1. very clear writing.	2
strengths: attempts to solve a longstanding problem in modelfree rl (effective exploration in sparse reward environments) clear writing and structure, easy to understand (except for some minor details) novel, intuitive, and simple method building on ideas from previous works good empirical results (better than state of the art, in terms of performance) on some challenging tasks weaknesses: not very clear why (and when) the method works  more insight from experiments in less complex environments or some theoretical analysis would be helpful it would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) not clear how stable (to train) and robust (to different environment dynamics) the method is main comments / questions: the paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis.	2
strengths:  this work is probably the first to distill the structured manifold knowledge from a teacher network to a student network.	2
in the introduction they first claim “we cannot impose structure directly on the joint distribution of a gan’s outputs.” but after they claim “we submit that regularizing the structure of a gan’s generator and discriminator is generally more difficult than imposing meaningful structure directly on the model distribution, which we will refer to as q. these two statements conflict because the model distribution is a joint distribution of gan’s outputs.	0
strengths: the idea of using the hierarchical structure of the labels is innovative and wellmotivated.	2
next, considering additive demixing, the authors assume that the corruption/structured signal is unknown but it can be modelled using a convolutional network (using the architecture of dcgan).	0
however, the main technical contribution of this paper is otherwise not clear  the methodology section covers a very broad set of techniques but doesn't provide a clear picture of what is novel; furthermore, it makes a strong assumption about linear structure in the embedding space that may not hold.	0
this would be heavily dependent on the model structure (with more complex decision boundaries likely being harder to optimize) but they show empirically in 4 models that this method works well.	0
the idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence.	0
strengths:  modeling the structure via patches is an interesting idea.	2
weaknesses: method:  the idea of relying on patches to model the structure is not new.	0
more importantly, it seems that all experiments are performed on tasks where the underlying structure is known, however this is almost never the case in practice.	0
for the topological inference experiment it is assumed that one knows the structure, but how to address the more general problem?	0
pros: 1. introducing the tree structure is a neat way of extending the existing rslds model to multiscale scenarios.	2
it seems like this behavior is supported directly in the structure of the model, which is great — but i don’t think it can be called “zeroshot” inference.	0
"introduction, first paragraph, claims that ""such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability""  what do you mean by these concepts and how exactly is the current approach limited with respect to them?"	0
while the novelty and good structure of the paper are reasons to accept it, i have doubts concerning the soundness of the results due to the experimental setup.	0
reasons to accept the paper  novelty  works in an unsupervised setting  well written/structured reasons to reject  doubts concerning the experimental setting  (minor) related work is not complete  (minor) not all common performance measures are reported  [1] wang, peifeng, shuangyin li, and rong pan.	0
moreover, it is the prediction performance that matters to such task, but the authors remove the nonstructure features from the compared methods.	0
moreover, lstm is not just a neuron nonlinearity, but a recurrent network with a particular structure.	0
"technicalwise, the paper is relatively incremental  all of the building blocks for performing tractable inference are standard: since the posterior is intractable for nonlinear sparse coding, a recognition network is used; the prior is spike and slap, thus the recognition network will output parameters in a similar mixture structure with both a spike and a slap component; to apply reparametrization trick on the nondifferentiable latent code, a continuous relaxation, similar to the one used in concrete distribution/gamble trick, is applied to approximate the step selection function with a controllable ""temperature"" parameter."	0
the random graph experiments (table 3) show the effect of good structure in gcn, but i felt that it is not enough to demonstrate an improvement by gcns.	0
pros:  improving joint training of nondifferentiable pipelines is a meaningful and relevant problem  using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea cons:  the main result of the paper concerning sufficient conditions for optimality of the method seems dubious  it is not obvious why this method would outperform simple baselines, and baselines for joint training were tried  the notation seems unnecessarily bloated and overly formal  the exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing the submission describes a method for smoothing a nondifferentiable machine learning pipeline (such as the fasterrcnn detector), so that gradientbased methods may be applied to jointly train all the parameters of the pipeline.	2
the authors mention how we should interpret them, but not enough information about the structure of the network is given to satisfy this question.	0
the model in this paper computes attention values which is interpreted by the authors as corresponding to the structure of the sentence but there are equivalent means to trace back feature computation in other network topologies as well.	0
it lacks a bit in structure (e.g., i would introduce the problem and dataset before introducing the model) and sometimes fails to explain what insights the authors draw from certain results, or why certain results are reported.	0
however the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper.	0
importantly, modern random ksat instances are not only characterized by their number of variables, and their ratio #clauses / #variables, but with an additional “structure” which mimics realworld, industrial instances (see e.g. [4]).	0
some questions and concerns: 1. the paper spends too much space introducing the bottleneck structures and a whole lot of the details on the optimization of nc and cd are put in the appendix.	0
the paper is wellwritten but the structure is a bit disconnected; most notably, i didn't see clearly how section 2 and 3 fit together.	0
i understand how some of the other gcnnbased models like khalil et al.'s is 4localgather (assuming 4 embedding iterations of structure2vec), but how is graph2seq infinitylocalgather?	0
the strategy this paper use for indexed data is to encode all data in a blackbox, which can be inefficient since the order of temporal data or the geometric structure of spatial data is not handled in the model.	0
pros:  overall the paper is well written and has a transparent and meaningful structure.	2
the evaluations uses lstm but will the structure of g() influence the rate of learning?	0
pros:  an interesting approach for representing tree structure encoding using a series of transformation.	2
i felt that the study into the covariance structure of adversarial perturbations was interesting but as it stands was not complete enough to be informative in general.	0
(in the example case of a 1×1 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)  how is the structure regulariser backpropagated into the parameters of each layer?	0
it is claimed that the capsg model can preserve relative positions of nodes, but i believe it is straightforward to obtain the relative positions of nodes from just observing the graph structure.	0
many works reported that lack of degree corrections would result in bad estimates of community structures [1,2,3].	0
similarly, in section 3.2 you liken layca to optimization on a manifold but i am not convinced that this makes sense for matrices which inherently have some structure (e.g, perhaps the stiefel manifold would be more meaningful).	0
but the rnn structure here is not applied to the input sequence, but to the sequence of blocks inside the transformer encoder/decoder.	0
the setup and formulation (using pg to metaoptimize a hyper parameter controller) is not extremely novel (there have been similar work learning hyper parameter controllers), but the structure, the problem domain, and applications are.	0
strong points:  the paper is well written, has clear structure and is technically easy to understand.	2
the approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack appletoapple comparisons.	0
"the authors open the optimization black box of the inference process by adding a few very clever tricks that facilitate convergence:  intermediate rewards based on the gain on f1 score  self critical training approach  ""clamped"" pretraining enabled by the use of state embeddings that are multiplied my a transition to any state in the free mode, and just the next states in the hierarchy in the clamped mode  addition of a flat loss to improve the quality of the document representation while those tricks may have been used for other applications, they seem new in the context of hierarchical/multilabel/structured classification."	0
aren’t they actual diffraction images but in a controlled known and controlled setting: set of parameters (beam, structure to analyze)?	0
o it’s stated in figure 2 and table 6 that 2 classes are either blank or nocrystal but is that a known fact (purposely chosen) or no pattern images for crystalline structures due to inadequate experimental settings to uncover the crystalline nature of the analyzed structure?	0
1. as a general method, the proposed neuron search not only can work with structures found by enas, but also other network structures, for example resnet.	0
i think the contributions of this work is incremental compared with [luo et al (2018)] in which the major difference is the partial zero sum reward structure is used and the observations and actions information from the tracker are incorporated into the target network, while the network architecture is quite similar to [luo et al (2018)].	0
the logical inference results were promising – they seem to suggest that you capture some but not all of the value of explicit tree structure (a treelstm) on a task like this.	0
the proposed method looks more like a binary classifier with a betterregularized structure, but still, it is unclear why an energybased autoencoder is a good choice.	0
i agree that independence assumption makes the model and computation easier, but the prior itself should reflect the possible dependence structure of the channels.	0
i didn’t check all details of the proof, but the structure of the proof and several key constructions seem correct to me.	0
"that is reasonable but it does not necessarily demonstrate longterm structure, anymore than learning that ""q"" is often followed by ""u"" demonstrates longterm structure."	0
pros: the flexibility of the network structure is an interesting point.	2
also these variants may not yield enough performance compared with lstm, but 1d convolution and/or deep structure helps to avoid the degradation.	0
pros:  core ideas seem promising  leveraging mathematical structure is a great strategy for constructing algorithms with desirable properties cons:  very limited experimental results  not clear if improvements are significant  hierarchy construction seems to be too limited to work in any reasonably sized problem  no evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method	2
"other factorization techniques exist, e.g., ""incremental stochastic factorization for online reinforcement learning"", barreto et al (aaai' 16), to uncover an mdp structure."	0
but if that’s the case, that’s a very interesting data point that the additional structure actually helps considerably beyond the incremental advantage exemplified here.	0
strengths: the idea of leveraging feature groups in a neural network structure; the novelty of the rese model; weaknesses: the main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use gbdt.	2
more specifically, you can optimize a kl divergence with a gan, as shown in the fgan paper (1606.00709) but this requires attention be paid to the functional form of the loss and structure of the discriminator.	0
"the necessity for latent spaces to ""respect the highdimensional structure of the [data] distribution"" is stated as a fact but not welljustified."	0
the structurepreserving regularization term compares distances in x and z space, but i doubt that pixelwise euclidian distances are good at capturing an intuitive notion of distance: for example, if we translate an image by a few pixels the result is perceptually very similar but its euclidian distance to the original image is likely to be high.	0
a vae is a generative model which an autoencoderlike structure, but this does not make all autoencoders generative models.	0
the paper says using the semantic structure and the diverse contexts are weaknesses of approaches using the contexts, but i don't see any method that uses the context in an embedding manner  say the cove context vectors.	0
it mentions arbitrary metric spaces several times, as well as the notion of dimension induced purely by metric structure, but the equations only really address the euclidean vector space.	0
q2: using the bilstm and the transformer as baselines seems reasonable but there are other existing models such as tree lstms, graph convolutional neural networks [a] and tbcnns [b] that could also be strong baselines which take tree structure into account.	0
an example on mnist data illustrate these properties, but it is still not clear what are the visual clues as the criterion to select a good dr method and what are the global structures.	0
authors discussed the results in figure 4 for six realworld datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.	0
the method does not make major changes to the network structure, but by modifying the calculations in the network.	0
in fact, this paper reads a lot like a more elaborated (or incremental) version of the preliminary paper of sogaard et al. (acl 2018), with the paper structure and some descriptions borrowed from the previous work (e.g., the focus on the muse algorithm, the description of kisospectrality), with two main enhancements compared to the prior work: 1) the introduction of procrustes fit as a means to diagnose and anticipate (im)possibility of unsupervised embedding learning.	0
clarity the text is well written in general, but the structure could be improved.	0
"the authors acknowledge this difference between their work and the stateoftheart (e.g., ""modified images are sometimes visible but still can keep original structures and information, this shows that adversarial images can be in more forms than small perturbations"", section 4.1), but it remains unclear why generating such images would be interesting in practice."	0
again, this is a shortcoming that existing approaches do not suffer from (given appropriate assumptions about the structure of goal space), so the lack of comparisons is problematic.	0
"it's unclear what is meant by ""structured knowledge"", but to my best understanding this sentence is misleading or wrong."	0
the effect of using different norms seems only dependent on the properties of the norms and the matrices, but not anything related to the structure of a lstm.	0
pros: while i am not familiar enough with the background literature on model compression in neural networks, i thought the augmented optimization problem used to induce lowrank structure on the space of activations was interesting and worthy of investigation.	2
wording and minor comments:  the abstract is rather lengthy, but should probably contain somewhere a spellingout of rbfi, since it informs the reader that the radial basis function (with infinitynorm) is the structure of the new network unit.	0
my main concern is that the authors only show single examples, without quantification, for some main results (rf structure).	0
weaknesses: ' the discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused.	0
the authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the lstm is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences. '	0
pixelrnn samples are funky (colorful but no global structure).	0
cons: the paper should be checked for sentence structure, typos, etc. there are a number of places with linguistic errors.	0
the ltvae not only learns the location of each cluster to best represent the data, but also their number and the hierarchical structure of the underlying tree.	0
the idea of using a structure on the latent prior of a vae to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.	0
overall the paper proposes a bnstyle structure on vae latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.	0
as such it not compatible with quantizing the weights of a given network structure, which is the more common scenario, but rather with choosing the network structure under a given level of quantization.	0
the proposed network structure is simple, but it surprisingly works well in general.	0
descriptions of training details are reasonable, and the experimental results across several datasets are extensive cons  the network structure may not be novel, though the performance is very nice.	2
pros: ' the problem is interesting and important ' the combination of compressed sensing and gans for image reconstruction is novel cons: ' the model structure is unclear: for example, what is the role of the variable .theta?	2
3. the cifar10 results are bit concerning, and one can't help but wondering if the technique really only helps when the data has simpler shared structure.	0
pros :  good numerical results  nice incorporation of structure via wavelets cons :  sometimes the paper is not really clear i have severals comments: 1/ (1) « some subsequent works devote to making spectral methods spectrumfree… avoiding high computational cost ».	2
[weaknesses] 1. the main contribution of this paper fall to the proposed method for modeling the relational structure for multiple objects in the images.	0
the proposed framework is formulated as a minmax game between two players: a mask m corresponding to the saliency mapping, and a function f sampled from a set of classifiers with the same structure but different parameters.	0
in this context it would also be interesting to see how the encoderdecoder structure performs without .deltaconstraint, but with regularization as in .betavae.	0
it would be good if the authors could address the following suggestions or concerns: 1) the proposed attack model assumes the only the graph structure are accessiable to the attackers, which might limit the proposed model in real applications.	0
pros: 1. the best of both worlds from parallelizable transformer and recurrent structure for repeated selfattention mechanism.	2
the performances of this work heavily rely on the taxonomy of labels, while in some cases the taxonomy of labels is not tree structure but a graph, i.e. a label may belong to multiple hyperlabels.	0
cons: the major flaw is the lack of comparison with ``any'' of the related work on interpretability or the prior work on imposing structure upon hidden representations.	0
also, the manuscripts lacks a clear discussion of where does this work stands in the literature like structured vaes, graphical models, sum product nets  factor graphs.	0
however, the paper spends lots of effort explaining representations, but only a few sentences explaining about how the proposed representations/data structures can help find a somehow generic value iteration solution, which allows to efficiently compute/retrieve a particular solution once a .delta vector is specified.	0
i like the idea of learning a latent structure dag for vaes, but this paper introduces a rather weak way to try to achieve this, and the experimental results are not convincing.	0
%%% encoder dependency structure does not match the generative model %%% my second major concern is that the dependency structure used for the encoder is incorrect from the point of view of the generative model.	0
should it not, it would provide an empirical justification for what is, in essence, a different restriction to that of the learned prior structure: it is conceivably actually the case that these encoder restrictions induce the desired decoder behavior, but this is distinct to learning a particular dependency structure in the generative model.	0
"this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
it's unclear to what extent the theory would generalize not only to deep, nonlinear networks (which the paper addresses empirically) but also different structures in the task that are not well approximated by the svd.	0
this contains analogous dual attention structure as the manuscript describes, but without reinforcement learning component.	0
pros:  the motivations for the work are clearly explained and highly relevant  comprehensive overview of related work  clear and consistent structure and notations throughout  convergence proof is provided under certain assumptions cons:  no intuition is given as to why rcpo isn't able to outperform reward shaping in the walker2dv2 domain minor remarks:  in table 2, it would be good if the torque threshold value appeared somewhere  in figure 3, the variable of the xaxis should appear either in the plots or in the legend  in appendix b, r_s should be r_step and r_t should be r_goal to stay consistent with notation in 5.1.1	2
among other, those have the peculiarity that they are not independent but with a dependency structure, which can be encoded as a graph or a network.	0
the result this paper suggests an alternative, which is finding good fixed treeshaped structures but continuing to do soft parameterization like n2nmn.	0
the tsne plots show some preliminary interesting structure for the simple regression and rl tasks, but not for the classification task.	0
a fundamental distinction between parametric and nonparametric tests for cpd in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structuree.g. a first order autoregressive model introduces a simple markov structurewhereas nonparametric kernel tests typically imagine samples to be iid (before and after the changepoint).	0
if the learned q is better than the variance network  it means the network structure is better for optimization, but the structure itself might not be so special.	0
the work here tries to not learn a lot of these structures but impose them.	0
i understand there are differences in the details, but i would like to see confirmation of whether the claims about usfa’s superior ability to exploit structure is supported by results.	0
specific comments  in the inverse fourier transformation, a voxellike grid structure is used, however  how to control the size of this volume?	0
one aim of the present work, that appears to be a unique contribution above the prior work is to focus on the role played by task structure, suggesting that certain notions of task structure may play a more significant role than architecture and that any bounds which consider architecture but not task structure are doomed to be excessively loose.	0
my understanding is that this alignment doesn't come from the s1 manifold but comes from some additional structure in the image signal.	0
overall, to use the underlying manifold structure to align data batches is an interesting and straightforward proposal, but i hope the authors can address these question carefully and make the argument stronger.	0
“… a lack of certain constraints on the generative capacity of current neuralnetwork based generative models make it challenging to infer structure from their latent generative representations.” what does “a lack of certain constraints” mean?	0
learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting.	0
this inductive bias comes with a strong benefit when the assumption is true  as demonstrated in the toy dataset experiment  but when it is not true, the visualization would strongly distort the underlying structure of the model.	0
specific points on positives and negatives of the work follow: positives:  the paper shows a solid understanding of the literature in this domain and presents a strong motivation  the problem itself is addressed at a deep level with many nuanced (but important) considerations discussed  ultimately the results of the model seem convincing in particular with the accompanying psychophysical experiments negatives:  (maybe not a negative, but a question) at the extreme tradeoff between intrinsic structure and texture, the notion of a metamer seems somewhat obscured.	0
a major claim (in table 4) is that lstms are timeorder 2 whereas qunns are timeorder l and the implication is that this means lstms are worse at modeling long term structure than qunns ; but how does that actually relate to practical ability to model longterm dependencies?	0
it's not obvious that sparse local motion maps mean a heirarchical tree structure, but i see how it could help.	0
"i suggest that without evidence for this loss you soften the claim to ""this is intended to help encourage the model to learn a heirarchical tree structure"" figure 4: it appears that each row indicates a different video in the dataset, but then in 4f you still have two rows but they appear to correspond to different algorithms... a vertical separator here might help show that the rows in 4f do not correspond to the rows in 4ae."	0
the authors show that the rfm increases learning speed in several games  the authors show that the rfm does somewhat better at forward action prediction than a naïve lstmmlp setup and other competing models weak point  the rfm is compared to other models in predicting forwards actions but is not compared to other models in figure 5, so it is not clear that the graphical structure is actually required to speed up learning.	0
strengths: attempts to solve a longstanding problem in modelfree rl (effective exploration in sparse reward environments) clear writing and structure, easy to understand (except for some minor details) novel, intuitive, and simple method building on ideas from previous works good empirical results (better than state of the art, in terms of performance) on some challenging tasks weaknesses: not very clear why (and when) the method works  more insight from experiments in less complex environments or some theoretical analysis would be helpful it would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) not clear how stable (to train) and robust (to different environment dynamics) the method is main comments / questions: the paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis.	2
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
my main concerns with the paper are the insufficient comparison with prior work, its lack of clarity and organization in certain places, and the limited amount of work.	0
"pros:  proposed regularizer is wellexplained and seems to work well, ablation study is helpful cons:  the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text  some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work specific comments / nits: (in reading order) 1. i think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative rl; i think ""forethinking"" or something like that that is an actual word would be better, but i can't think of a good word... maybe frugal?"	2
however the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper.	0
the paper is generally easy to follow but the organization needs to be improved.	0
cons: 1) the organization and clarity could be improved.	0
"for example, it is unclear to me what ""weight space traversal"" means, ""training size"" is mixed with ""dataset size"", and ""we will show that convergence ... to final weights"" seems to be a trivial comment (unless there is some special meaning of ""convergence rate""), etc. it also lacks some clarity and organization in the results  some more summarizing comments and sections (and in particular, a separate and clearer conclusion section), as well as less repetitions of the qualitative comments, should largely improve the readability of the paper."	0
pros:  overall the paper is wellwritten and the organization is easy to follow.	2
detailed comments are the following: pros: 1. the paper is clearly written with a good organization to fully explain their model.	2
the paper is mostly clear but the high level organization could be improved.	0
cons () :  the paper would benefit more motivation and organization.	2
i appreciate the extensive evaluation, but its organization can also be improved, considering that some important information are, again, in the appendix.	0
strengths:  this work is probably the first to distill the structured manifold knowledge from a teacher network to a student network.	2
next, considering additive demixing, the authors assume that the corruption/structured signal is unknown but it can be modelled using a convolutional network (using the architecture of dcgan).	0
reasons to accept the paper  novelty  works in an unsupervised setting  well written/structured reasons to reject  doubts concerning the experimental setting  (minor) related work is not complete  (minor) not all common performance measures are reported  [1] wang, peifeng, shuangyin li, and rong pan.	0
thorough experiments studying the effect of sparsity on the representation weaknesses  no discussion/comparison to other vae approaches that incorporate sparsity into the latents: eptimoic vaes (2017), discrete vaes with binary or categorical latents are sparse (see: discrete vaes, concrete/gumbelsoftmax, vqvae, outputinterpretable vaes), stick breaking vaes, structured vaes for the betabernoulli process (singh, ling, et al., 2017).	0
2) the authors refer to g as a structured prediction model but starting from section 4, they have switched to call it a classifier, which is confusing.	0
cons: 1) the paper is structured nicely, but the central part of the paper, section 3, is written poorly; many necessary details are omitted.	0
pros:  improving joint training of nondifferentiable pipelines is a meaningful and relevant problem  using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea cons:  the main result of the paper concerning sufficient conditions for optimality of the method seems dubious  it is not obvious why this method would outperform simple baselines, and baselines for joint training were tried  the notation seems unnecessarily bloated and overly formal  the exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing the submission describes a method for smoothing a nondifferentiable machine learning pipeline (such as the fasterrcnn detector), so that gradientbased methods may be applied to jointly train all the parameters of the pipeline.	2
however this has been done by a number of authors:  vlachos and clark (2014): http://www.aclweb.org/anthology/q141042  berant and liang (2015): https://nlp.stanford.edu/pubs/berantliangtacl2015.pdf while the idea of using such oracles for structured prediction tasks in nlp was first proposed by daume iii et al. 2009: https://arxiv.org/abs/0907.0786 furthermore, it has been applied for rnn decoding in nlp, see: https://arxiv.org/abs/1511.06732 apart from this, some further comments:  the subset of sql tackled in this paper is less expressive than what has been done in previous work on atis and geoquery datasets.	0
the experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine.	0
in terms of model presentation, the authors can compare the model with a large set of deep recurrent models that have recently been proposed for modeling time series with nonlinear latent dynamics (e.g. variational sequential monte carlo, structured inference networks for nonlinear state space models, black box variational inference for state space models, composing graphical models with neural networks for structured representations and fast inference, etc.).	0
pros:  wellmotivated intuition treating language as structured.	2
noise from sampling tends to be a structured noise that aids exploration/convergence over batch gradient descent, but it is not immediately clear to me why the choice between with and withoutreplacement should imply exploration.	0
the approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack appletoapple comparisons.	0
"the authors open the optimization black box of the inference process by adding a few very clever tricks that facilitate convergence:  intermediate rewards based on the gain on f1 score  self critical training approach  ""clamped"" pretraining enabled by the use of state embeddings that are multiplied my a transition to any state in the free mode, and just the next states in the hierarchy in the clamped mode  addition of a flat loss to improve the quality of the document representation while those tricks may have been used for other applications, they seem new in the context of hierarchical/multilabel/structured classification."	0
pros:  well written, well structured, an overall enjoyable read.	2
pros:  interesting comparison of unsupervised feature learning techniques  interesting topic of bioplausible deep learning cons:  only mnist, no indications if method will scale  results are not better than stateoftheart minor comments: the paper is generally wellwritten and structured, although some of the design choices could have been explained in more detail.	2
on the positive side, the paper is well written and structured.	2
the authors propose a structured variational approximation, but the factorization assumptions are not clear from the notation (i had to rely on figure 2a to fully understand them).	0
pro: ' interesting algorithm for structured prediction (base on reward) ' interesting results on some (toy) usecases cons: ' lack of discussion on the positive/negative point of the approach w.r.t sota, and on the influence of the reward function ' lack of experimental comparisons ' only toy (but complicated) problems with limited training sets	0
given that the paper is well structured into 9 different observations, i will next comment them one by one:  observation 1: impossible cases are not solely the results of linguistic differences, but also of corpus characteristics.	0
the authors show the result that tree structured neural module networks generalize very well, but other strong visual reasoning approaches do not.	0
significance: the paper acknowledges that methods do indeed exist for the various structured data types, but claim that they are complex, and that the proposed method is a simple general alternative.	0
the proposed method is simple and general, but it does require that the information lost when converting the structured data to the set is encoded as features in the set elements, e.g. the sequence index is added.	0
generally, the paper is quite poorly structured and there were several grammatical errors which made the paper quite hard to follow.	0
"it's unclear what is meant by ""structured knowledge"", but to my best understanding this sentence is misleading or wrong."	0
5. the paper pursues the familiar path of a treestructured network isomorphic to the parse tree of a propositionalcalculus formula, but with the original twist of passing information topdown rather than bottomup.	0
beyond the method’s explanation, i found the experiment section to be poorly structured.	0
the idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide.	0
also, the manuscripts lacks a clear discussion of where does this work stands in the literature like structured vaes, graphical models, sum product nets  factor graphs.	0
clarity: 7/10 results are clearly presented, although more intuition and context would be helpful, and some sections are not well explained or contextualized originality: 2/10 the methods are not novel, to my knowledge, and there is little exploration or insight given for the extensive experimental results significance: 7/10 measuring and reporting results is valuable, and the reported results are interesting pros:  interesting results, good plots  overall well structured and explained cons:  plots could use more explanation and interpretation in the captions, and more investigation and insight from the experiments  some sections are not clearly worded and i think the objective/interpretation of the experiments would not be clear to someone even a little unfamiliar with the approaches cited specific comments/nits (in order reading through paper): 1. first paragraph of intro is kind of fluff/unnecessary.	2
the paper ‘learning discrete wasserstein embeddings' describes a new embedding method that, contrary to usual embedding approaches, does not try to embed (complex, structured) data into an hilbertian space where euclidean distance is used, but rather to the space of probability measures endowed with the wasserstein distance.	0
2) pros:  novel recurrent neural network architecture to model structured dynamics of agents in an environment.	2
the authors are also to be commended for the mathematical elegance although the paper is very well written and extremely well structured, i struggled with the lack of experiments available in the paper.	0
although the reviewer likes the idea of heating up softmax, this paper can be judged as a borderline slightly leaning toward reject, due to the above weak points, the details of which are explained as follows.	0
(3) maybe i miss it, but has table 2 been referenced and explained in the paper?	0
i have multiple concerns with the papers: (i) the writing is informal and the ideas are not well explained.	0
"specifically, the term ""metalearning"" appears in the title and throughout the paper, but is never defined or explained."	0
for example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here.	0
pros  the results are convincing  the approach is clearly explained cons  english must be checked	2
contribution 2 seems interesting, but is poorly explained.	0
i expect you have different configurations to their paper, but it would be good for this to be explained.	0
technical quality overall, the experiments are wellthought, but the following questions need to be explained: 1. in the introduction, the authors claim three contributions they made in this paper.	0
2. the major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding wes was neither introduced in the related work section nor was it clearly explained in the text.	0
such a perspective is interesting, but needs to be further developed and explained.	0
overall, i think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.	0
the strengths of the paper are not clear and need to be explained and evaluated well.	2
it is wellwritten from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.	0
the proposed approach consists of four parts: initial feature transformation local features aggregation graph pooling final aggregator unfortunately, each of the part is poorly explained and/or a method that has already been used before.	0
i do have concerns regarding the experimental evaluation:  the “demos” baseline approach should be explained in the main text!	0
it is commendable that 20 repetitions of each experiment were performed, but i am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean.	0
i agree with the other two reviewers that the work is somewhat incremental, but the differences are well explained, the experimental results are interesting (particularly the differences of parameter vs representationbased sparsity, and the plots in appendix showing neuron importance over tasks), and the progression from sni to slnid is wellpresented.	0
"pros:  proposed regularizer is wellexplained and seems to work well, ablation study is helpful cons:  the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text  some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work specific comments / nits: (in reading order) 1. i think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative rl; i think ""forethinking"" or something like that that is an actual word would be better, but i can't think of a good word... maybe frugal?"	2
in conclusion, the paper has a novel idea i like, it is explained clearly, but the work has to mature a bit more in terms of empirical work and interpretation of results.	0
unsurprisingly, the vanilla cnn is less robust than the densitybased architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla cnn are.	0
weaknesses  the statement in theorem 1 regarding the converse case is unclear, because it says that the limit of .delta as .epsilon approaches zero is zero, but it is not explained what .epsilon is or how it changes.	0
cons: first, an inherent limitation is that this approach is not applicable to oneshot learning, and i have doubts in its merit for very low shot learning (explained below).	0
figure 2 is very badly explained (i believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but i had to figure it out on my own).	0
perhaps this is all explained clearly in the literature introducing dro, but this introduction leaves a lot to be desired.	0
even the ‘focused experiments’ can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the roboschool examples are a bit more interesting, but also less conclusive).	0
figure 3(b) remains strange, because at some batch number the unmasked version simply ceases to work, but this could have been explained without a graph.	0
i feel that the cons and pros of the model and its design decisions are not fully analyzed or explained in the paper; when reading this paper, i wanted to learn a rule of thumb for deciding when (and why if so) a topdown model of logical formulae works better than a bottomup model.	2
regarding the clarity, beside the use of the gallery, several small issues should be improved: ' the paper use the term deformed images (until sect ~4) and then synthetic images for the generated images; ' figure 2 is introduced early but explained late in the paper and it is not clear how the second and the third images should be interpreted; ' in the related work section, wang et al. 2018 is said to generate imaginary images in contrast to realistic images of the proposed method not synthetic, however the images are clearly not real and are synthetically generated from the heuristic and the weight of the network.	0
the paper is poorly written: the concept of watermaking is only explained on page 2, and the description given refers to a more general use than the one facilitated by the methods in this paper (watermarking is defined to embed a userdefined signal s into a sample whereas the proposed technique can only transmit one bit  the sample has been watermarked or not).	0
minor points/typos: 1) there are several places where confusing concepts are introduced in one paragraph but explained several paragraphs later.	0
in particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though they’re introduced on page 3.	0
whitebox attack and blackbox attack the paper is well written, both theory and experiments are well explained.	0
clarity should be improved, for example it is mentioned that the structuring element is learned but never clearly explained how and what difficulties it poses.	0
in the main text it is written that alpha is {1, 1}, which would result in a combinatorial search, but never explained how it is learned in practice.	0
the connection with generalization is claimed but never quite explained (there are no generalization bounds in the paper).	0
2) it is not explained why each factor can be modeled as a product of gaussians 3) at nearly the top of page 3, a product over n is substituted by a product over t, with x_n replaced by x_t and so forth, but the total number of products goes up from n to txn.	0
it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)	0
clarity: fair there were some experimental details that were poorly explained but in general the paper was readable.	0
figure 3 left and middle show some weird dots and patterns, but they are not explained.	0
also, the one on the right tries to show “ghosting”, but colours and their meaning are not explained.	0
the authors end up with many inconclusive observations and doubts (“perhaps”) about small changes, at the end of section 5. other things such as the “spawn cap” and the “server merge” are poorly explained, with clear definitions and proper justification of their role.	0
the first one is wellexplained  easy to read cons:  ideas are not very surprising; and just tested on a few data sets; things could be more robust.	0
the way it is currently explained, it seems like the added variable introduces lack of 'overlap', but not strictly confounding.	0
[14] discusses that to some extent, but this should be explained here as well ' q_0 is never defined ' it's good practice to add numbers to all equations ' i believe the claim in the appendix of [14] was meant to be conditionally independent (see also the reviews of [14]).	0
cons:  in the experimental section, the methods used to learn the policies, dqn and ddpg, should be briefly explained or at least referenced.	0
the motivation for the work is not entirely clear: it is true that gans and vaes have their issues, but in my view it is not really explained / argued why the proposed method would solve them.	0
the impact of loss hyperparameters (r, .gamma,.mu) should be discussed thoughtfully; at some points in the paper, r and .gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained.	0
pros:  interesting comparison of unsupervised feature learning techniques  interesting topic of bioplausible deep learning cons:  only mnist, no indications if method will scale  results are not better than stateoftheart minor comments: the paper is generally wellwritten and structured, although some of the design choices could have been explained in more detail.	2
in section 2, the proposed model is explained with emphasizing the differences from existing models, but there needs a careful clarification.	0
something is missing here: maybe a (toy) dataset where modecollapse arises when training baselines, but not when the present modifications are introduced (as another comment points out), maybe training curves showing that the modifications provide a mode stable training process, etc. clarity: the paper is generally well written, with the proposed modifications being clearly explained.	0
on the negative side, i think the relevance and novelty of the results should be explained better.	0
if the subpixel layer were explained in detail, and with justification, then i would potentially be ok with the negative results, but in this case it’s not clear why spend this time on it here.	0
pros:  the paper is well written, very easy to read, well explained (and better formalized than [xiao and al.]);  the idea of deforming images is new (if we forget about [xiao and al.]) and simple;  experiments show what such a technique can achieve on mnist and imagenet.	2
the observations can be formalized and the curve fitting should be explained in further detail, the appendix touches upon simple cases but there is a strong literature behind those simple cases that could be quite useful for the purposes of the paper.	0
so i suspect something else is going on, but this is completely unclear and should be explained more transparently.	0
the collected dataset, which is a contribution of the paper is also poorly explained.	0
there may very well be a good reason why this is very dangerous, but that is not explained in the paper.	0
as pointed out by r2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (i am not suggesting that it is, but that possibility should be considered and explained in the paper).	0
it makes use of some existing results from other literature but it is not clearly explained how and why the results are being used.	0
i am not sure about why should it work (i explained in detail later), but it does work well empirically.	0
however the description in the table or in the text is not sufficient to convey the point that is supposed to be explained by the example.	0
the colearning strategy idea is interesting, but the details of the method are not well explained and the experiment results seem not convincing.	0
strengths of the paper:  the motivation to extend compression beyond the weights to activations in order to support the dnn accelerator designs and the technical details are clearly explained.	2
my guess is it's a standard pianoroll with 0s for inactive notes and 1s for active notes, where onsets and offsets contain linear fades, but this could be explained more clearly.	0
pros:  the background, model and experiments are clearly explained.	2
"the most expected contribution of this work is barely explained: how the haptic sensors' values/object labels vectors were built and fed to the predictor network, what their values looked like for the various objects, how these vectors clustered for the various objects etc. among the many evident weaknesses:  domain specific concepts and procedures of most importance to this work are not explained: ""... measure various physical properties of objects using the biotac sensor using five different exploration procedures (ep)""."	0
there are only a few minor things that can be improved:  'covariant' and 'equivariant', while common in (graph) signal processing, could be briefly explained to increase accessibility and impact  'order' and 'layer' are not used consistently: in the caption of figure 2a, the term 'order' is used, but for eq. 4 and 5, for example, the term 'layer' is employed.	0
cons: the idea is not clearly explained.	0
[1] multimodal unsupervised imagetoimage translation [2] diverse imagetoimage translation via disentangled representations [3] exemplar guided unsupervised imagetoimage translation [4] stargan: unified generative adversarial networks for multidomain imagetoimage translation overall, i think the proposed method is welldesigned but the comparison and experiment setting are not explained well.	0
2. detailed experimental analysis along with some user studies cons: 1. an important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained.	0
the authors explained that this is due to the simplicity of the player behavior (not much stochastic), but the result in table 2 shows good performance for graphvrnn for future prediction task.	0
there are a few issues that are listed below: 1in section 3 the reason that dimensionality of estimator can reduce to d from d^2 can be explained more clearly 2 figure 1 is located on first page of the paper but it has never been referred in main paper, just it is mentioned once in appendix , it can be moved to appendix.	0
i understand that later on this is explained, but for clarity it would be good to have a short explanation during the first mentioning of this term as well.	0
the term clamping is only explained on page 4 but used since the abstract.	0
6. the rating classifier (clf) is intriguing, but it's not clearly explained and its effect on the evaluation of the performance is not clear: one of the key metrics used in the evaluation relies on the output rating of a classifier, clf, that predicts reader ratings on reviews (eg on yelp).	0
uncertainty could have been better explained, clarity, the main methodological contribution (hierarchical cd) is well motivated but only provided in the form of an algorithm.	0
strengths: ' the paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly. '	2
the term h seems to be entropy, but its not explained.	0
the paper proposes two new modules to overcome some limitations of vin, but the additional or alternative hypotheses used compared to vin are not clearly stated and explained in my opinion.	0
pros :  experiments are numerous and advanced  transition probabilities are not transitioninvariant compared to vin  do not need pretraining trajectories cons :  limitation and hypotheses are not very explicit questions/remarks :  d_{rew} is not defined  the shared weights should be explained in more details  sometimes .psi(s) is written as parametrized by .theta, sometime not  is it normal that the .gamma never appears in your formula to update the .theta and w?	2
it has many symbols, but some of them are not explained.	0
i think this is explained in the second paragraph of the paper, but that paragraph is really not clear.	0
in all settings, it is assumed that the ground truth foreground masks of the inputs and the target composite images are available; these segmentation masks are used throughout the learning and the backgrounds are removed for simplicity (explained in paragraph 2 of section 3.3, paragraph 1 of section 3.4, etc) pros being able to generate compositional images is an ambitious goal and interesting application.	0
pros:  extension of airl which utilizes empowerment to advance the soe in reward learning  well written, related previous work well explained.	2
having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions.	0
# positive aspects of this submission  the intuition and motivation behind the proposed model are well explained.	2
it must be remarked that d_cnn is not even properly defined (for example, there is a function .delta in its definition but it is never explained what this function is).	0
if this is the case, the motivation to go for rl would be more understandable but this part is not explained at all.	0
yet, the other thing that has annoyed me and is causing me to only moderately champion the paper so far is that i found the notation heavy, not always well introduced nor explained, and while i believe that the authors have a clear understanding of things, it appears to me that the the opening sections 1 and 2 lack notation and/or conceptual clarity, making the paper hard to accept without additional care.	0
10. the theoretical benefit of proxquant is only intuitively explained, it looks to me there lacks a rigorous proof to show proxquant will converge to a solution of the original quantization constrained problem.	0
pros:  the motivations for the work are clearly explained and highly relevant  comprehensive overview of related work  clear and consistent structure and notations throughout  convergence proof is provided under certain assumptions cons:  no intuition is given as to why rcpo isn't able to outperform reward shaping in the walker2dv2 domain minor remarks:  in table 2, it would be good if the torque threshold value appeared somewhere  in figure 3, the variable of the xaxis should appear either in the plots or in the legend  in appendix b, r_s should be r_step and r_t should be r_goal to stay consistent with notation in 5.1.1	2
pros:  the paper is well written, mostly selfcontained, and easy to read (for someone familiar with information theory);  all mathematical points are detailed and well explained, with sufficient introduction;  the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;)  information bottleneck is a topic of prime interest in the community these days;  the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning;  the solution brought to the ib lagrangian issues is simplistic though efficient (squaring i(x,t) so that it's not linear in i(x,t) anymore).	2
1.1 possibly this has something to do with the episodic training, but this is exactly the kind of thing that should be analysed and explained, but is not discussed at all.	0
2. the paper is sold as a metalearning paper, but it’s not clearly explained what is the “meta” part of the algorithm.	0
clarity: 7/10 results are clearly presented, although more intuition and context would be helpful, and some sections are not well explained or contextualized originality: 2/10 the methods are not novel, to my knowledge, and there is little exploration or insight given for the extensive experimental results significance: 7/10 measuring and reporting results is valuable, and the reported results are interesting pros:  interesting results, good plots  overall well structured and explained cons:  plots could use more explanation and interpretation in the captions, and more investigation and insight from the experiments  some sections are not clearly worded and i think the objective/interpretation of the experiments would not be clear to someone even a little unfamiliar with the approaches cited specific comments/nits (in order reading through paper): 1. first paragraph of intro is kind of fluff/unnecessary.	2
concerning the experimental part of the paper, sections 4 & 5 are wellexplained but, in section 6, the solution decoding method, inspired from pca is a bit confusing.	0
"the breiman nonnegative garotte (https://www.jstor.org/stable/1269730) is a similar wellknown technique in statistics finally, i liked the paper and wanted to give it a higher score, but reduced it because of the occurrence of many broad claims made in the paper, such as: 1) method works on mnist => abstract claims it generally works on vision datasets 2) paper states ""typically used is fixed variance init"", but the popular toolkits (pytorch, keras) actually use the variance scaling one by default 3) the badly explained distinction between connection and weight and the relation that it implies to prior work."	0
# strengths: the method is well developed and explained.	2
"pros: ' the problem is interesting and well explained ' the proposed method is clearly motivated ' the proposal looks theoretically solid cons: ' it is unclear to me whether the ""efficient method for sn in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as miyato et al. 2018, which also used sn in conv nets with different strides."	2
i am overall positive about the paper: the proposed idea is simple, but is wellexplained and backed by rigorous evaluation.	0
however the novelty is limited and not well explained.	0
"1) in the title the word ""online"" is mentioned but never explained in the main text."	0
"because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to hartford et al. the tasks are explicitly designed to exploit these additional parameters  so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect hartford et al. to fail (because it's designed for a different setting)."	0
pros: ' the paper was wellwritten and explained the method and the experiments well cons: ' the problem seems illposed to me.	2
the main gist is fine, but important details are not explained so i could not get the entire argument stepbystep.	0
strengths while not really explained in the paper, this work connects gradientbased to embeddingbased metalearning approaches.	2
furthermore, i find the method for guessing the gradients to be rather arbitrary and poorly explainedat least i'm not sure if anyone unfamiliar with the mentioned gradient extrapolation methods would find this approach to be sensible at all.	0
overall, the evaluation is heavy on qualitative results (many of them on simple gridworld tasks) and light on quantitative results (the only quantitative result being on a simple gridworld which is poorly explained).	0
pros:  the paper is wellwritten and clearly explains the technique, and figure 1 nicely summarizes the weakness of static channel pruning  the technique itself is simple and memoryefficient  the performance decrease is small cons:  there is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x)  in contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, realtime)  the experiments are limited to classification and fairly dated architectures (vgg16, resnet18) overall, the method is nicely explained but the motivation is not clear.	2
the fact that the forward dynamics does worse than vanilla ppo (and the previous results in ostrovski et al and bellemare et al) on montezuma's revenge brings the strength of the used baseline into question overall, the experimental details are greatly lacking:  the way that the value function is trained (i.e. the objective function) is never explained in the paper.	0
overall, i think the empirical results appear very strong, but think this paper is below the acceptance threshold due to three factors, in ranked order:  (1) the theoretical guarantee, which is positioned as a core contribution of the paper (and in fact claims it as 'the first to correct labels with theoretical guarantees', which is not true), is based on assumptions that seem overly strong; these are somewhat relaxed in a 'remark', but this seems unproven and is a confusing presentation regardless.	0
weaknesses of the paper:  much of the presentation is vague or opaque.	0
review:###strengths: 1. this paper presents a unified framework cade for unsupervised graph representation learning, which combining the local neighbors and global information.	2
this paper should be rejected because (1) the paper lacks important latest references on domain adaptation, (2) the paper misuses the notations that makes the paper is not easy to follow, (3) the algorithm is not well justified either by theory or experiments, and (4) the presentation should be further polished.	0
# weaknesses i have some issues with the (a) evaluation, and (b) presentation of the work in its current state.	0
i like the idea and the overall direction, but the current paper looks a bit preliminary both in evaluation as well as presentation.	0
one straightforward baseline is that use the blackbox model for classification, and pick the top 'prototypes' with the highest similarity in the latent representation.	0
the presentation does a good job in giving a very brief description of the model to readers familiar with nonautoregressive generation; but for those who are not (like myself), much content needs to be clarified.	0
however, the current draft has fundamental experimental flaws in its evaluation/presentation and lacks comparison against relevant cheap channelwise attention mechanisms (such as squeezeandexcitation).	0
the main weakness is however the technical presentation which is painful to follow.	0
the presentation of this section is also not very clear, e.g. the equation on page 6. the citation style look odd to me, often you use either something like '[2,3]', or something like 'dhillon et al. (2018)', but not '(2,3)'.	0
presentation concerns  ## p1: need to simplify notation i found the descriptions of the neural net architecture throughout sec.	0
pros: 1. well organized paper with a clean presentation.	2
i have some concerns with the paper's claimed novelties, its empirical evaluation, and its overall presentation, and thus am initially recommending a weak reject.	0
as such, it is possible to get the overall message (and didn't strongly impact my score), but it does detract from the paper's overall presentation and quality.	0
(1) learning hierarchical representation using gan has been explored in kaneko et al. 2018 but not even mentioned in the paper.	0
i liked the motivation and presentation of the paper but see some critical shortcomings:  in theorem 1, shouldn't there be a term that accounts for the complexity of the hypothesis class (vcdim, rademacher complexity etc.).	0
while i did not have the luxury of time to parse the appendix to validate the legitimacy of the proof, i think the overall shortcomings of the paper (highly nonreadable, bad presentation and perhaps a fair attempt at masking the lack of contribution) warrants a clear reject from me.	0
weaknesses  1. limited methodological novelty: the methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications.	0
additional comments  1. lack of novelty: the data representation, which is presented as one of the core contributions of the paper, is not new.	0
p9: as shown in fig 7(a) > as shown in fig 7(b) p10: rout constrain > route constraint, gravity constrain > gravity constraint in sum, i think the paper is at the borderline but it could be improved and better by having more through experimental validation and more detailed presentation.	0
while the stabilizing offpolicy reinforcement learning algorithms would be a significant problem, i have some concerns regarding the presentation and the limitation of the proposed method.	0
the revised version has improved presentation, but the proposed method is still introduced as a method with stability guarantees while the proposition in the paper cannot serve as a stability guarantee, and can only provide intuition on the asymptotic performance.	0
i think this paper contains a number of unusual and interesting ideas but is let down by its presentation.	0
however, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas.	0
summary: the paper introduces some interesting ideas about dropout but suffers from bad writing and presentation of results.	0
weaknesses:  most of the weaknesses that i can find are in terms of the presentation and writing which could be improved.	0
moreover, the main contribution is a better denoising autoencoder, but in the grand scheme of representation learning, it is unclear how broad of a contribution this is.	0
minor issues: 1. i agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim “majority of the nodes are of very small degrees” is not justified and only apply to the internet topology in faloutsos et al. (1999).	0
however, i found the presentation of material very confusing and poorly organized, some claims misleading, and some experiments missing.	0
i found the paper hard to follow (presentation needs improvement) and the experimental results are lacking.	0
i think that these contributions are of interest of iclr community, however, i have some concerns regarding the presentations of the results.	0
but the work in the paper is not in an acceptable form due to the following key reasons: (1) the presentation of the ideas and the algorithm, 'despite the lack of theoretical guarantees' is hard to understand, (2) the dqn baseline compared to (definitely in the augmented chain environment, and possibly in the atari games), are based on a fixed epsilon exploration strategy whereas dqn as proposed uses a epsilonannealing strategy for exploration, (3) the baselines compared to are not comprehensive, (4) generally the paper is rather unpolished.	0
in terms of presentation, the paper repeatedly makes strong, but unsupported claims about the special properties of remote sensing images.	0
i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the combination of different assumptions used in the theory.	0
cons: 1. i think the main drawbacks of this paper is that the authors make a poor presentation.	0
perhaps 'fine tuning' is indeed 'super standard in the object recognition literature', but as all three reviews here indicate, the presentation of fine tuning is unclear in this paper.	0
taking in into account the issues with the method (or its presentation) and the experimental weaknesses i recommend reject for now.	0
in this sense, i have the impression that the presentation is not very clear (ep can learn something, but what it is?).	0
unfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking.	0
while this is definitely a contribution, the unclear presentation and lacking experimental evaluation combine to decrease the value of the paper.	0
however, i am not convinced by the presentation of the paper and by the lack of multitask learning baselines in the experiments.	0
concerning the presentation and the review of other works, i am a bit surprise that the 'teacherstudent' setting appears out of the blue, without any references to any paper.	0
[overall] pros: 1. the idea of avoiding the logsumexp computation in the dv representation of kl is good.	2
the approach looks interesting but the motivation of using an might not be well justified in the current presentation.	0
shortcomings: 1. the presentation is somewhat convoluted.	0
the experimental results look reasonable and thorough, however the methods are sold on the idea of better representations for data missing from the training set, whereas the results are focused on anomaly detection.	0
overall, the paper presents a promising idea but it needs a more clear and rigorous presentation.	0
overall, i agree with the main point of the paper but vote for the rejection for now, and the paper needs a major revision on its experimental validation and presentation.	0
though this paper provides some contributions to the field of molecular graph representation, it contains flaws in presentation, lacking details of technical contents; thus, the paper is regarded as 'borderline'.	0
i lean towards rejection for this paper but would consider changing my score if the presentation was improved.	0
overall the presentation is clean, but the results presented have the following issues.	0
finally, i am concerned by the presentation of theorem 1 and corollary 1.1 as it has been shown by reddi et al. that adam does not have bounded regret in general without algorithmic changes.	0
cons: the approach is incremental or notsonovel in terms of metarepresentation for neural networks.	0
i find that the presentation of this work to be lacking a balance.	0
assessment: overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.	0
the loss sensitive optimal decision method is interesting, but a lot of details are missing: 1. the presentation in section 3.3 is unclear, e.g. minimising eq. (4) w.r.t.	0
# minor presentation weaknesses some parts of the notation/explanation don't make sense to me: ' 'let lambda=... be the number of object classes in the dataset.'	0
this does not negate the potential of gtns which i feel are an interesting approach, but i believe the paper should be more straightforward with the presentation of these results.	0
pros: 1. this paper is well written and the presentation is clear.	2
learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.	0
here are my main concerns of the current paper: 1. the presentation is a little bit confusing in the motivation section.	0
all that said, i'm comfortable marking the paper as a 'weak accept' since i think there is a real scientific contribution here, but i encourage the authors to make a serious effort to improve their presentation and tightenup their experiments.	0
however, i have concerns with the presentation quality of the paper.	0
overall, i have the impression that this paper contains interesting ideas, but the presentation is very poor.	0
first, the paper mentions that the learned options/representation will help in planning, but planning is not studied in the paper.	0
the paper has interesting analyses, but i think the main drawback is that the clarity and presentation could be improved.	0
presentation weaknesses: ' in the synthetic dataset the model cannot tell the difference between correct and incorrect signals at train time.	0
cons: 1. the presentation of this paper is not clear.	0
overall, this is an okay paper with incremental technical novelty and clear presentation.	0
pros: 1. generally, the presentation is clear and easy to follow.	2
cons: my concerns are related to counterintuitive experimental results and lack of clarity in parts of the presentation.	0
the major concern is the presentation of this paper.	0
in terms of presentation, reminders on hmm are welcome, but unfortunately the authors have not kept the same standard for clarity of notation in section 3, which makes reading and understanding what the authors are doing quite difficult.	0
cons: however, there are several issues that should be addressed including the presentation of the paper: • the algorithm seeks to combine a' search with mcts (combined with policy and value networks), and is shown to outperform the baseline mcts method.	0
the motivation and the theoretical arguments are interesting, but the paper lacks in presentation and sufficient empirical evidence is also lacking to get fully convinced by the claims.	0
i think the paper would benefit from a good proofread not just from the grammar/spelling perspective (which there are multiple instances which could be improved) but also from the overall presentation and legibility perspective.	0
overall, i have concerns with some of the contributions, experiments, and presentation, which leaves me at a weak reject.	0
i may jump my score if the authors address all the aforementioned questions and concerns convincingly, and work on the presentation.	0
some of my primary concerns were regarding the presentation, and i feel they have been mostly addressed with the changes to the introduction and abstract (i'd still recommend using 'layerwise latent code' instead of 'layerwise representation' everywhere in the text).	0
3) in sec 4, this paper only shows some sample results other models e.g. biggan, but no ’semantic hierarchy in deep generative representation’ is shown (not surprising given only a global latent code).	0
however, i still have concerns about the presentation of the paper, and i keep my score.	0
your motivation (helping people with dyslexia in ethiopia) is certainly laudable, but neither the work carried out nor its presentation meets the standards of our research community.	0
however, i have a litany of concerns with the paper itself, concerning its high similarity with a paper published in may, its motivation, its presentation, its empirical evaluation, and the analysis presented within.	0
while my concerns and suggestions are extensive, this paper is perhaps unusual in that all of the issues i have are quite fixable; the core idea is good, but its realization and presentation in the paper need a substantial amount of revision.	0
the authors argue for the need of an inference network, but they explicitly make clear that their goal is to train this network to enable reconstruction of an x given a z, rather than e.g. to learn a “good representation” (bearing in mind that what constitutes a good representation is strongly subject to debate).	0
one might argue that choosing reconstruction quality is as arbitrary as pursuing improved sample quality (as is in vogue in gan papers) but there is substantial evidence that improved sample quality correlates with improved representation learning (mode dropping in gans notwithstanding); the case is more complex for highquality reconstructions.	0
the paper is poorly written, with lots of issues in math notation and poor motivation and explication of what the sections are introducing, and what parts of the presentation is novel and what is already known.	0
overall, i think the presentation is on a good way but needs some more work.	0
however sometimes the presentation rushes a bit towards results, in the sense that it is not easy to figure out over which ensemble the mean/median/max/min has been computed.	0
cons: 1) the presentation is quite confusing.	0
i have the following major concerns with the paper: 1. presentation: the paper's presentation is very weak.	0
the paper has two fundamental weaknesses: i) the paper misses a key reference, which addresses the same representation disentanglement problem using the same informationtheoretic approach, only with some minor technical divergences: alemi et al., fixing a broken elbo, icml, 2018. this paper is a mustcite, plus a key baseline.	0
my biggest concern about the paper is the presentation of the results.	0
my main concern about the paper is that i find it very difficult to appreciate the efficacy of the method given the current presentation of the results.	0
this stretches the usual definition of what a “disentangled representation” means, as this disentanglement is usually assumed to be globally consistent, but this is a fair extension.	0
?when i first read the paper, and looking at the losses in equations 911, i thought that this wasn’t the case (also considering this paper is about unsupervised representation learning), but some sentences and figures make this quite unclear: a. in section 3.2, you say “we train the encoder so that c_i = 1 and c_j = 0 if the input data has ifeature and no jfeature”.	0
review:###i have two general concerns, the first is related to the presentation and the second to the relevance of the results.	0
decision: weak reject 1. interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a lowlevel feature representation, but i believe the experiments could be improved.	0
the rebuttal addresses my questions and concerns about clarity of presentation.	0
could be really good work, but the presentation doesn't quite come across.	0
the proposed solution is to use a ffnn, but provide a 'history representation', which is a set of feature vectors for previous timesteps that is extracted from a second network.	0
the proposed algorithm for generating features seems relevant and correct, but there are shortcomings in the presentation and the experiments are not entirely convincing.	0
in particular, the paper begins by introducing weighted multiset automata quite clearly, but it fails to explain how exactly these automata would be used to generate set representations.	0
my only real concern is with the presentation.	0
improvements that would make future revision strong but has not impacted current assessment: overall, the presentation of the paper is very unpolished.	0
negative aspects: my main concern about this work is that the presentation is not didactic enough.	0
the paper contains some potentially interesting ideas, but the presentation quality is not sufficient for publication.	0
pros: 1. causal discovery is, in general, an interesting problem and causal discovery based on representation learning are of great importance.	2
this is propositional to gaussian likelihood which may work fine in practice but the math presentation is not rigorous.	0
“…a very important factor that (influence) the choice of normalization strategy”, “when powerlaw coefficient (decrease)”, “when the (sizes) of each class (become) more imbalanced”, “this is because column normalization better (leverage) …” , “in a similar manner (with) label ratio”, “when the density or density gap (increase)”, “highorder filters can help gather… and thus (makes) the representations…”, “when the density gap (increase)” in page 6. these can be continued but it is obvious that this paper needs proofreading.	0
(more on this in con2) cons (i put the ones that weighed the most on my decision first): 1. the presentation is confusing, and at times selfcontradictory.	2
overall, i think the paper presents an interesting application and parts of it are well written, however i have concerns with the technical presentation in parts of the paper and some of the methodology.	0
i have three major concerns, including a lack of novelty, unconvincing experiments, and poor presentation of the work.	0
the distinction has more to do with the ability (or lack of ability) to learn representations.	0
the proposed idea is interesting, but its presentation and evaluation could be significantly improved.	0
feedback: (1) the contrast between learning playable representations vs. planning via representation learning (as proposed) is poorly motivated.	0
my main concerns are as follows: the presentation of the paper could be improved.	0
pros: 1. the presentation is very clear and easy to read.	2
strengths: · clearlooking overall presentation.	2
weaknesses: · while the overall presentation looks clean, the paper does not talk about the setting (one parameter server and many workers, only workers, etc).	0
weaknesses: some presentation issues.	0
my concern is that it is impossible to compare the costs of these techniques given the current presentation of data.	0
eg h_i_j is defined to be the taskspecific representation of the source task i but is indexed over both tasks i and j where j is the target, or there is a e_j(x_j) where both indexes are j while e's index is over tasks and x's index is over datapoints.	0
the idea seems quite novel, but the presentation seems to be more complicated than it needs to be for the idea.	0
nevertheless, i am not convinced that there is enough novelty and substance on this, i have some concerns on the evaluation, and i think that the overall presentation should also be improved:  i am not convinced by the 'knowledge distillation' approach.	0
the presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance).	0
along these lines, qualitative results in the form of trajectory classification (say following the presentation conventions in bolanos, 15, https://arxiv.org/pdf/1505.01130.pdf ranchod, 15, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7353414&tag=1 would also help to address these concerns.	0
my main concerns have to do with presentation, but i think they are relatively significant concerns.	0
the idea of decomposing a probabilistic model hierarchically is potentially interesting, but this paper has drawbacks in terms of experimental quality, significance, and presentation.	0
the paper has both technical shortcomings as well as shortcomings in presentation.	0
presentation shortcomings: the paper does not do a good job at communicating its ideas.	0
while the few figures presented in the paper look plausible, the paper is unfortunately lacking both in evaluation and in presentation of the method.	0
for example, making two objectives (i.e., multitask learning) for both the press release and paper abstract but sharing the internal representations of encoder and decoders might be one way.	0
based on these concerns, i am currently leaning toward ‘weak reject’ as i think the technical contribution is solid, but the presentation is weak to the point where i still have some remaining questions and don’t think i will really understand what is happening in some sections until i look at the code.	0
the study shows that the regularizer has three effects: (i) it promote the emergence of cell assemblies with similar connectivity and firing patterns (ii) it promotes targetclass selectivity of individual neurons (iii) it substantially improves performance of fewshot learning on mnist and fashionmnist compared to vanilla networks and an alternative fewshot learning method (imitation networks, kimura et al. 2018) although this study is intriguing (especially (iii)), i would tend not to recommend acceptance at this time because the presentation has issues, and the test cases of the method are too limited (mnist and fashionmnist) pros:  interesting result on fewshot learning.	2
the presentation is clear in the sense that the reader can understand the design of the technique and of the experiments; but less clear in the sense that the term bias is overloaded: early on (introduction) the authors state that removing the gender dimension is equivalent to removing gender bias (this is attributed to bolukbasi et al. 2016 and prost et al. 2019).	0
strengths: clear presentation.	2
the presentation of the proposed methodology seems to be at some rather high level and, as a consequence, i got the idea but the technical side is not totally clear to me.	0
however, the paper suffers from lack of clarity and in presentation, in exposition and in the experimental section.	0
i have a number of concerns with the paper: 'the presentation: the paper seems to come out in a rush, with many critical details missing.	0
the theoretical analysis is not at all new, but does provide a nice way to understand the empirical results, and in its presentation here is more understandable to nonstatistical physicists than in the original formulation.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
due to optimisation similarities to maml, it can suffer of similar issues  gradients vanishing as number of steps increases, paired with lack of representational power (in nonlinear case) when the number is small.	0
some material/presentation concerns: # overall  considering a 'fixed initialisation' scenario is arguably not important, as if one could store the initial network weights, one could also just store  heta' and have an empty 'synthetic dataset' to train on.	0
overall, while the proposed approach is an interesting one, i have some serious concerns regarding the presentation and experiments, as listed below.	0
join image and perpixel annotation can be a good idea however the paper needs significant improvement on literature review, model presentation and experiment result to be able to publish in iclr.	0
however i find it a bit hard to advocate acceptance just yet, mainly because of two concerns that a) this paper is largely about stitching multiple existing techniques (e.g. neural processes, influence functions, monte carlo dropout, etc.) together and may hence be a little bit lacking in its own originality, and b) both the presentation quality and the empirical studies still leave quite a bit to be desired (details to follow).	0
this paper may not be fully relevant to this conference because most of the main methodology does not rely on any representation learning criteria, but is almost rulebased.	0
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
however, based on the current presentation, i have the following concerns.	0
maybe the authors have derived the useful equation, but unfortunately, the paper did not reach the threshold because of the presentation of results.	0
generally, i have a major concern about the presentation of this paper.	0
in general the paper is written well and the presentation is clear, but the problem doesn’t seem important to me.	0
this paper achieves slightly better performances on some stateoftheart baselines; but the overall presentation and writing of the paper is very confusing, and more importantly, it is very unclear to the reader (a) what exactly is done, (b) why, and (c) which parts are actually important.	0
the submission claims to accommodate pure fixedpoint models, but the soft quantization of biases is swept aside in the presentation: “since additions play a subordinate role for complexity, we focus on weight multiplications.” can the authors clarify whether and how soft quantization is applied to biases?	0
my concerns related to the presentation of the model have been addressed, and i find the architecture much easier to understand.	0
however to this reader's knowledge, this is a new variant that is both creative and motivated by an actual realworld study, which is exciting and alone warrants presentation at the conference in my opinion.	0
strengths: a) the proposed concept, extracting the 'consistency' among representation of different networks, is interested and using that towards understanding what's going on underthehood of neural networks makes sense.	2
the presentation seems to be improved, and the additional ablation study seems to address my concerns.	0
however, there are some concerns about the presentation and the main methodology: 1. can the paper give more explanation on the purpose of inserting the feature transfer term in the objective function?	0
the only concern i have is the presentation of the paper.	0
pros  the presentation is relatively clear.	0
while this submission has the elements of a good paper and the presentation is great, certain concerns make me hesitant to recommend acceptance.	0
i consider any meaningful direction for understanding the convergence of this method a nice fit to iclr, however i find the presentation of this paper somehow confusing, especially section 3 which is supposed to be the main contribution of the paper.	0
### strengths:  the paper explores an interesting direction of learning generic feature representations for visuallinguistic tasks for downstreaming tasks.	2
the two major concerns i have with this paper are with its presentation and with the quality of the baselines.	0
2. opinion and rationales i’m leaning towards “accept” for this paper since it presents two interesting contributions (albeit of incremental novelty) to the approximate inference area, it has clear execution and super clean presentation, and the experiments clearly demonstrate the values of the proposed approaches.	0
overall assessment: i think this paper presents interesting and novel ideas, but it has shortcomings in the presentation.	0
model presentation and notation:  you mention negative sampling earlier in the text but then don’t mention how you do it  notation below (6) is utterly confusing and lacking: what is s_{l,i}^(t), is there a softmax somewhere, what are numbers below ‘score’  what is the meaning of e_ and e_ given that you omit details of negative sampling?	0
review:###============================== update after rebuttal ======================================================= i did not have any major concerns about the paper in my initial review, only some suggestions for improving the presentation.	0
the empirical comparison with tbptt is substantial, but the waters are muddied a bit by imprecise presentation of baselines.	0
i suspect that the authors assume a canonical ordering of the graph representations, which is a strong assumption, but does not seem to be mentioned in the paper.	0
the paper, however is well written and the presentation is very nice in my opinion.	0
overall, i am currently voting for weak accept, for solid presentation and content, but with with the following problems: it seems that neural odes are most beneficial, over the alternatives, when used with irregular data times and or sparse number of time points.	0
## major comments/concerns  i think the presentation the pretraining objective (eq 3) could be clearer.	0
minor comments: typo: 'upto a constant' postrebuttal update: i increased my rating to weak accept because the authors addressed my main concerns by a) giving additional information on the update of the buffer, b) improving the presentation of the experiments, c) fixing the description of maxentirl and d) by showing empirically that both the lower bound increases / wassersteindistance decreases.	0
despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper.	0
minor comments: the main message of the paper is clear but some parts still confuse me: 1. i suggest the author to merge the figure 3 and data generation (page 4) part for a better presentation.	0
the paper’s contributions are useful, but do not reach a level of generality, originality, or depth justifying presentation at iclr.	0
thus, i increased my score to 6. i suggested to have a clearer presentation on the possible pros and cons of bo in attack generation, e.g., making a comparison between bo and other methods in both query efficiency and computation efficiency.	2
my major concern is that the proposed method is conceptually not novel enough compared to nguyen et al., 2017. my secondary concern is that the presentation is very much open to improvement in points hinted above.	0
to conclude, the direction this paper takes is certaily new and interesting, but the preliminary results in combination with the complexity and limitations introduced by the gridbased representation make me hesitant to recommend accepting this paper in its current form.	0
weaknesses  it's hard to know what the impact of this paper will be because 1) it's unclear whether this model can generalize to more useful domains and 2) the presentation may turn some readers away.	0
presentation weaknesses / points of confusion / missing details  to mimic a typical decoder rnn there should be another input which copies the word    ilde{w}_{t1} from the previous iteration as input, somehow fused with the attention feature    ilde{a}_t.	0
i'm sure this is a familiar tradeoff, but from my perspective the paper would probably be more impactful if the presentation leaned more on examples.	0
[pros]  clearly written  clear motivation  correct derivations  interesting algorithm [cons]  experiments are a little weak (and focus on a single domain)  would have liked to see an explicit algorithm for the optimization procedure  small lack of clarity in the presentation of section 4.1notation q_t is not introduced for example  more discussion about the evaluation metric  linking it more to prior work	0
the proposed model seems to compare well with different baselines, but the presentation of the experiments is not that clear.	0
though the proposed idea is interesting, the depth and breadth of authors' presentation are simply lacking.	0
3) the cifar10 experiments in table 1 are encouraging and the toy experiment in 2d is illustrates nicely the deficiencies of the current mi estimators cons 1) the experimental section is lacking many details to fully understand how and what experiments were performed and how comparable they are to prior work 2) the paper would benefit greatly from a thorough editing to clarify the presentation  there are many missing concepts and definitions that makes it hard to follow without intimate knowledge of related literature.	2
the applied method seems conceptually quite simple (as admitted by the authors in section 5), and the neural rendering approach seems quite neat, but both method presentation (section 3) and evaluation (section 4) seem incomplete and leave significant open questions.	0
that said, the presentation lacks a certain amount of polish in its present form, which makes me lean towards the reject side.	0
pros: 1) relatively clear presentation.	2
overall it lacks clarity in the presentation of the results, the assumptions made are not always clearly stated and the split between previous work and original derivations should be improved.	0
i find my concerns on motivation and presentation properly addressed in the feedback and the revised paper as well.	0
i understand that the authors are guiding the presentation towards a version of sumandsample they can compare with in theory and in experiments, but in the process they're doing a disservice to the reader.	0
i think the paper definitely has some merit but the presentation makes it hard to assess it.	0
i have a few reservations about the clarity of presentation, but i think those are easily addressed.	0
lack of clarity and rigor for the eigenvalue approximation: in appendix b the authors discuss approximating the eigenvalues of k and h, i think it is okay to develop numerically efficient approximations to the quantities in question but the presentation is not very intuitive nor rigorous.	0
having said this, i have the following concerns which are mostly related to the presentation of the paper.	0
overall, i like the basic idea of the paper but i found the presentation lacking.	0
another concern i still have is the claim of 'reasoning', and i'd suggest to narrow down the claim to be only on pattern demixing, since the reasoning part seems to be writing down continuous constraints from the discrete constraints (same as the concern in review #3).	0
experiment results also suggest that pros: the writing of this paper is very well.	2
during writing this, i read the bias correction section again and had another concern, the bias correction effect is only justified for functions with small mixed degree terms: 'for functions with small mixed degree terms, this can lead to bias reduction, at the cost of an increased variance because of sampling an auxiliary variable' for general multivariate functions, it is even not clear whether the proposed estimator has a smaller bias than the straightthrough one.	0
unfortunately, the lack of clarity in the paper and poor writing prevents me from writing a thorough review.	0
pros:  the writing is good  satisfactory empirical results cons:  the proposed method is very similar to certain methods in the literature detail comments: (1) the proposed loss function eq.(8) is very similar to the contrastive loss proposed by hadsell et al. (2006, eq.(4)), which is used in siamese gan variants (juefeixu et al. 2018, hsu et al. 2019).	2
writing:  sections 3.3 up to section 5 need lots of rewriting; i suggest some changes in 'minor' below but please make a full pass as the paper is difficult to read at the moment.	0
however only 4 epochs were run, so i believe the xaxis should be 'iterations'' besides improving the quality of writing in the paper, i would strongly suggest that the authors improve their empirical evaluation.	0
overall, i think there are significant concerns with the paper, both in terms of writing and the soundness/novelty of the technical results and experiments.	0
by the way, the writing in section 4.1 is not clear:  the overall misclassification probability is presented first, but after that k only represents the misclassification probability into a specific class.	0
cons:  writing of the paper could be significantly improved.	0
in summary, it is good that a theoretical bound can be derived from the paper, but this paper's quality may need more enhancement particularly on its writing and experimental parts.	0
overall, i don’t have much concerns, but here are some more specific comment/questions (most relates to writing) in the intro, it would be great to mention some past success on using md, as opposed to just saying it’s wellknown.	0
the paper is well structured, but the writing can be improved, and some parts are hard to read and follow (see minor comments below).	0
last but not least, the writing of the paper is a bit clumsy, and i was having a hard time to figure out what exactly is the proposed method.	0
this paper should be rejected because (1) this method only combines existing techs, such as stochastic generative hashing (eq.1 and eq. 6), and lacks novelty; (2) lack of introduction to related work and baselines, (3) the experiments results can not support the claim, i.e. the effectiveness of cgh in marketing area, and (4) paper writing is awful and very hard to follow.	0
feedback/suggestions/nits (not necessarily part of decision assessment): 1. cite the definition of continual learning (e.g. with a reference to a textbook or review) 2. a lot of the writing is unclear, wordy, and/or grammatically incorrect  inconsistent verb tense  e.g. 'if one would need .... it is required' should be 'if one would need .... it would require' i'd suggest rewording this sentence entirely, because it's misleading  it says 'retrain on this new dataset (which sounds like train just on the new data), but i guess you mean retrain on all data including the new data.	0
in the following i will do my best to make concrete suggestions for improvement, but i would strongly encourage the authors to revise the paper thoroughly and have it checked for grammar and writing quality.	0
i will keep my score, but do believe that further experiments and small adjustments to the writing will see a future version of this accepted.	0
the writing is good, but provides very little intuition for why we should expect this approach to work (aside from its connection to equation 1)  i discuss this in more depth below.	0
however, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas.	0
weaknesses: the writing is often not clear, ambiguous or misleading and needs improvement.	0
summary: the paper introduces some interesting ideas about dropout but suffers from bad writing and presentation of results.	0
the authors suggest learning a trajectory embedding by using a triplet loss, justified by a sufficient condition for learning an embedding which corresponds to the true task id. overall i felt the paper fell a bit short of the standards for iclr, based on subpar writing, lack of comparison to convincing baselines, and unclear applicability to more complex environments.	0
weaknesses:  most of the weaknesses that i can find are in terms of the presentation and writing which could be improved.	0
overall the quality of the writing is probably a bit below what's acceptable for publication at iclr, but nothing that could not be fixed on subsequent revisions.	0
weaknesses: 1. the writing of this paper contains many mistakes, especially for the issue of using singular and plural.	0
making the writing more concise would, in my opinion, not only shorten the paper but also make it easier to read.	0
overall, given the quality of writing and lack of methodological novelty this paper is not ready for publication.	0
the paper is well written overall, although i found section 3 hard to read and i think the writing can be a bit improved by writing explicitly the algorithm or at least the mdp (see misc comments) while the reduction in number of branching decisions is satisfactory, this does not necessarily translates to actual speedups (since the vsids branching heuristic is faster in practice) and there are some concerns about applying gqsat to larger instances.	0
the writing is clear to understand the main idea but can be improved in various places.	0
on a personal note, i like informal writing and paper honesty, but i would not always recommend it for submission.	0
the overall writing style is quite unusual, but it is not a negative point from my perspective.	0
cons: 1. the paper could be improved in writing, especially on justification why this kind of dropout combination gives better performance.	0
there are some confusions in the writing: p4 first paragraph says that 'in principle one can learn using all pairs of nodes, however that is not scalable, and hence we restrict learning between pairs in e.'	0
most of the writing of the paper is relatively clear, but there are a couple of assumptions that should be discussed more clearly.	0
overall, although the paper does well in motivating the problem, the lack of rigorous experiments and poorly structured writing advocate for a weak rejection.	0
pros: 1. the writing is pretty clear.	2
i have some concerns on the writing and experiments of this paper.	0
regarding the quality of the writing, it’s in general ok but there are lots of grammatical mistakes, the authors should pay more attention to this.	0
of mixed strategy on continuous game other:  the paper alternates between his and it when referring to players overall, i think this work is an outline for a nice paper but would like to see 1) clarification regarding its relationship to existing work 2) experimental baselines in which sga and stable opponent shaping are parameterized to produce nondeterministic strategies 3) cleaner writing	0
more minor, but i found it hard to follow the writing in the paper (this is related to the motivation being unclear).	0
in my opinion, the motivation of the paper is clear and the writing is easy to follow, but one potential limitation is the lack of comparison with recent related work, e.g., generate to adapt: aligning domains using generative adversarial networks swami sankaranarayanan, yogesh balaji, carlos d. castillo, rama chellappa https://arxiv.org/abs/1704.01705 deep transfer learning with joint adaptation networks mingsheng long, han zhu, jianmin wang, michael i. jordan https://arxiv.org/abs/1605.06636	0
though my concern on writing has been resolved to some extent, i'm still unsatisfied with the empirical experiments.	0
i liked the idea quite a lot and would not mind seeing it in the conference, but given the number of issues raised by myself and others it seems that the best route forward is rewriting the paper given the inputs by the reviewers and submitting to a future venue.	0
i have three major reasons for this: (1) quality of writing and clarity is lacking: citations are all done inline, symbols are being used without proper explanation, and many sentences are just purely broken and impossible to understand.	0
despite the concerns regarding the clarity in writing and the rigor in the theory of the paper, i think that the algorithmic idea proposed in this paper is interesting, novel, and practical.	0
4. the authors claim their implementation is 'approximately x times faster' but there is no quantitative proof of it, which seems to be one of the sellingpoint of the paper (or at least one of the best results) 2. writing those typos do not impact the review score, i hope it can help the authors to gain more clarity in their writing.	0
after the rebuttal phase, i now have an additional new concern about the clarity of the writing and the completeness of the empirical details.	0
pros  the writing is great and easy to follow  the method is theoretically motivated  experiments prove effective cons  the proposed method may not work well for complicated environments (1) in the discussion after def.4, given an alignment task set d_{x,y}, how do we know whether a common (w.r.t.	2
i thought the overall clarity of the writing was somewhat lacking with many grammatical mistakes throughout, and the necessity to refer repeatedly to the appendices in order to understand the basic functioning of the rl algorithms and reward learning (7.4).	0
i encourage the authors to keep improving their writing of section 3.4. the description of the dataset is much clearer than the previous version, but i still think it is worthwhile to use more space for a better description.	0
one of the concern regards the writing of the paper.	0
last but not least, the paper can be improved a lot if the authors can thoroughly polish the writing.	0
3. the paper writing is ok, but some explanation and organization should be improved as mention in cons.	0
cons: 1. the writing needs to be improved.	0
however, i found the writing / notation imprecise at times and the experimental section too small (lacking an extensive set of baselines, and only on two datasets).	0
my current rating is weak reject based on the weakness of the writing and the lack of strong empirical evidence in support of the effectiveness of the proposed contributions.	0
independent of any other concerns, i would be hesitant to accept the paper with the current writing given the very general nature of assertions made despite experiments in far more specific settings.	0
however, the paper is let down by poor writing and a lack of detail.	0
as the paper is not really in my research area, i would have liked the paper to be a bit more selfcontained, but the writing of the paper is generally clear.	0
2. clear writing, with sufficient but not redundant introduction of background knowledge and explanation of both the advantages and drawbacks of existing models (too large computational complexity on highdimensional data).	0
i dislike writing short reviews, but i fear this paper falls too far short of iclr standard.	0
although  at least in my case  this might be partly due to limited indepth expertise, this seems to indicates some deeper shortcomings in the writing, as well as the insights and intuitions offered by the paper, in order to be accessible to a larger audience.	0
furthermore, the grammar in places is a bit tenuous, but on the whole the writing is understandable.	0
moreover, for my question number 1 about the optimization problem, the authors referred me to corollary 1 from the paper, but that didn't really help me because, as the other reviewers also point out, the writing is quite hard to follow.	0
cons 1. the writing of this paper is difficult and in many of the core parts of the paper, the important definitions are not clear.	2
this important missing information alone makes it difficult to assess the paper, but the rest of the writing is similarly confusing.	0
cons 1. the writing of this paper is not well organised.	2
i have some concerns about this paper: (1) the analysis lacks theoretical insights and does not seem to be very useful in practice; (2) the proposed method for graph sparsification lacks novelty and the experiments are not thorough to validate its usefulness; (3) the writing of this paper is messy, missing many details.	0
i think the writing has improved significantly, but could still be further improved and clarified.	0
the paper seems novel and sensible and has some experimental results that are not trivial but the writing is so difficult to follow that it makes it impossible for me to assess the contributions and even check correctness.	0
the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.	0
my second argument is that the paper lacks clarity in writing (for detailed suggestions please refer to comments and feedbacks).	0
1. the paper is trying to develop rigorous results, but its writing is arguably not rigorous.	0
in writing, the paper also lacks the necessary references in many places.	0
overall i lean towards accepting the paper, but i encourage the authors to revise the writing and to add a few plots explicitly showing the time vs quality tradeoff both in likelihood (wrt the full model) and in downstream metrics like fid.	0
4/5 [strengths] 'clarity' the overall writing is clear.	2
cons: 1) the writing could be significantly improved.	0
3.3] t=0,...,  au1 since the writing significantly affects the paper readability, and the core contribution of the paper seems incremental, i will vote for a reject to this paper.	0
another concern i have is the clarity of the writing.	0
strengths: 1. the writing logic ascends step by step.	2
in general the writing is clear, it gets however quite unclear for the main part of the algorithm (after eq. 7).	0
cons: the writing is not clear.	0
evaluation: while the general flavor of question the paper studies is undoubtedly interesting, i found the paper severely lacking both in terms of the quality of writing (in particular, i was at confused about the goal of various sections/experiments), as well as the significance of the results the authors observe (and how they are reported  i found them to be oversold).	0
the paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. the writing is also quite unclear, to the point that i often didn't understand exactly what argument was being made.	0
given the apparent lack of any technical contribution to machine learning theory or practice, the inconclusive empirical results, and the generally unpolished writing (e.g., long runon sentence in the conclusion, vague problem definition), i do not believe this paper is suitable for publication.	0
pros:  the writing is mostly great.	2
strengths: 1. clear writing logic.	2
writing quality: the paper is generally easy to read, but there is considerable room for improvement.	0
lastly, the writing of the paper doesn’t interfere with understanding, but can definitely use more work.	0
however, from a writing perspective, it can be hard to follow as the paper lacks storytelling as to why such or such methods were chosen/implemented.	0
not by default, but by the style of writing and lack of effort to make the studied problem accessible to nonexpert audience.	0
============= final decision ============= while the paper addresses an important problem and reports improvements, there are many concerns with it including the writing, method, and experimental setup.	0
3) i am concerned about the writing style of this paper.	0
cons:  the writing of the manuscript could be significantly improved.	0
after reading their rebuttal, i still have main concerns about the novelty of the problem and the writing quality.	0
i trust that the writing issues will be addressed in due course, but i am also concerned about the fact that evaluations are qualitative.	0
paper contains some interesting ideas to integrate causality into an autoencoder (but see weaknesses below)  paper proposes a new dataset for evaluating causal mechanisms (but the approach is not evaluated) weaknesses:  the quality of the writing is inappropriate for a scientific venue.	0
i am voting to reject this submission, but i am willing to reevaluate this paper if the author(s) significantly improves their writing.	0
unfortunately, the writing is still confusing, some of the claims in the introduction and rebuttal are inexact ([5] does not embed the observations and does work with partially observed environments), and the method lacks originality compared to existing work.	0
less technical comments: the paper writing is fine to me, but i don't like the typesetting.	0
although the submission had a few typos (i am not a native english speaker, but i'd encourage the authors to polish the writing of the paper), it's a very wellwritten paper overall.	0
(1) problem formulation is far from clear, perhaps because of the lack of clarity in writing.	0
to obtain a better score, i suggest the authors to modify this paper in these ways: first, the introduction section needs to provide more details, including the pros and cons of previous related works on the research problem of this paper, the challenges you face when dealing with this issue and the contributions; second, there were more than a few spelling and grammatical errors, please proofread the work and improve the writing; third, the paper lacks logic in writing.	2
i suggest to move the details of experiments in section 4 to the appendix, but it may depend on the authors’ writing style.	0
overall, i am not an expert in the area but a lot of details from the writing (such as point 1 under weakness) and the theoretical justification of the regularizer are unclear to me.	0
on quality of writing, the paper is well written but it can use a figure that demonstrates proposed architecture.	0
significant work on writing and experimental side should be complete, but because this is novel and important work for classification, with some serious revisions, i would suggest accepting this paper.	0
cons: 1. the writing sometimes seems unnecessarily complicated.	0
but i do think it could use some further writing revisions to emphasize/clarify key points: a) the method is not new (it says e.g. in abstract “new model” which is misleading) but its application in cl is underexplored b) the experiments show poor performance on existing methods because most of those are not designed nor work well for the shared head “task agnostic” setting, while metric learning handles it gracefully.	0
for these reasons, i think the paper should be rejected for lacking novelty and writing quality.	0
i almost all the time do not end up rejecting a paper due to the lack of clear writing, but for this paper, unfortunately, despite spending time, i could not understand the paper.	0
post rebuttal: i think that this issue was addressed in a reasonable manner through the rebuttal, but will require a bit of rewriting.	0
the writing seemed relatively clear overall, however in section 2.3 the model architecture, and its motivation, are a bit hard to follow.	0
overall assessment: the paper presents an interesting idea, but it has shortcomings in technical writing and evaluation.	0
this is the commonly believed writing scheme in the press release, but how the equally divided pieces of text could be corresponding to each category?	0
i have a number of concerns regarding the motivation, experiments and overall writing quality, so i am currently leaning toward rejecting this paper: 1. the justification for paths in section 2.1 is extremely handwavy.	0
my main concern is that the writing in general is quite unclear in parts and there are a lot of typos and grammar errors throughout.	0
also, the introduction lacks examples to explain clearly to readers who aren’t familiar with these concepts (e.g., inform vs. request slots) — not just a writing criticism, but in my opinion, while the stated architecture ostensibly performs well, it really isn’t clear if it is because the conceptual explanation or just a lot of machinery minimizing the loss well and many other architectures would work.	0
the paper should be rejected because 1) its writing lacks clarity.	0
this is an interesting an novel idea, but unfortunately the paper is let down by being extremely unreadable: due to a combination of many grammatical errors, and what appears to be plain sloppy writing, it's virtually impossible to follow the main ideas.	0
for this reason, i feel like this paper needs one revision cycle before resubmission, but with tighter writing would be a good addition to the adversarial text literature.	0
we will start with the imprecise writing: this mainly concerns section 3: the whole of section 3.1 uses the word 'morphology' in a very loose sense.	0
indeed, i believe i understand the workings of the model, but there are certain aspects that would require me to investigate the code to fully understand, which indicates that the paper could substantially benefit from improved technical writing and more detailed descriptions of the methodology.	0
reasons to accept:  strong empirical results  thorough treatment of baselines and related work reasons to reject:  incremental contribution (especially compared to tsai et al., 2019)  writing lacks sufficient technical detail	0
strengths of the paper: 1. the research problem is of great value in applications, as users may not have expertise on writing a good sql queries.	2
i'd also be willing to be lenient on this point, given that it's still possible to understand the technical contribution, but i'd strongly recommend that they authors better proofread the writing.	0
pros: while a bit hard to follow at times, the writing is quite good and the topic is important and relevant to the field.	2
the problem is interesting and important, but the current version is not satisfactory, especially the writing, which can be greatly improved.	0
in conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.	0
concerns #3: writing is not wellorganized.	0
the proposed method is simple and effective and provides meaningful gain, but verification and explanation seem to be not enough and writing could be improved.	0
but i have concerns about the technical details and writing of the paper.	0
some of the observations are quite interesting, however the organization and the poor writing of the paper makes the contribution unclear.	0
============================================================================================== writing, soundness and organization of the paper the writing of this paper is acceptable, but the organization can be improved.	0
addendum: after writing this review the authors have responded to my concerns regarding the cl experiments and have begun to recompute some of the experiments using a more rigorous comparison with promising results on permuted mnist.	0
pros: the writing of this paper is clear.	2
= strong/weak points  the method seems to work in practice  the writing is reasonably clear, and it feels sufficiently precise to reproduce the results (given enough time)  little novelty on modeling side (straightforward extension of seq2seq) and data extraction (straightforward application of control flow graphs)  the central novelty in the analysis (extracting 'kinds') seems to contribute very little to results (cf. tab.	0
review:###the paper proposes to make two modifications in hard negative mining procedure for descriptor learning:  instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors  gradient wrt model parameters is weighted inverse proportionally to the distance the authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, ubc phototour 2011 and hpatches 2017. i propose reject mainly for the following reasons: 1) dataset selection and quality of experimental evaluation 2) the writing (presentation of the results) regarding (1), my main concern is on statistical significance of presented experiments.	0
4. the writing is not great, and i've find several typos, which a more thorough proofreading could have caught (i mention some i found at the end of this review, but i stopped taking such notes after section 3.2.1).	0
(5) individual sentences of the paper are adequately written, but the writing of the paper currently feels like a long list of facts and details, which makes extracting the deep takeaway much harder.	0
my main concern with the paper is the poor writing quality.	0
there are some other concerns about the writing clarity, technical material and inadequate discussions for the experimental results (see questions/ concerns below).	0
strengths: 1. the writing is easy to understand.	2
comments / questions the writing of most of the paper is clear, however i found section 4.1 confusing.	0
pros: 1. the writing is clear 2. the literature review seems thorough 3. segmentation results provide ample evidence cons: 1. the global classification results are not convincing enough to claim 'significant improvements in terms of imagelevel classification', as stated in the paper, according to the experiments.	2
this paper achieves slightly better performances on some stateoftheart baselines; but the overall presentation and writing of the paper is very confusing, and more importantly, it is very unclear to the reader (a) what exactly is done, (b) why, and (c) which parts are actually important.	0
as such, the results toward the end of the paper do not seem valid to me but aren't really overstated (because of the writing, its hard to take the result very seriously, and the authors seem to know).	0
i found the writing to be somewhat dense and lack in clarity.	0
strengths: 1. the writing of this paper is satisfactory.	2
writing: 'the model is expected to excel... but to fall short', 'we follow the human subject research protocols' (which ones?	0
on the whole it is understandable, but the writing should be improved.	0
the work here is complicated, but the writing and explanations are very clear.	0
pros: 1. clear writing, method is easy to understand.	2
the writing is clear in general, but misses some implementation details.	0
<conclusion> although this work shows a new application of nas for object detection, my initial decision is ‘weak reject’ mainly due to lack of technical novelty, limited experiments and poor writing.	0
the idea seems interesting, the writing is wellwritten, and the analysis seems correct (i did not fully check all steps, but the key steps seems ok to me).	0
the square bracket 'slicing' notation used for slicing and concatenating is neat, but even though i spend all day writing python it still took me a while to realise what this meant, as i haven't seen this used in typeset maths before.	0
i found the paper very interesting but the writing appeared somewhat unclear at times.	0
strengths: (1) writing & clarity: the proposed method is well motivated, the paper is carefully written, and clearly presented.	2
my main concern with this paper is clarity of writing: i have the feeling that important details are missing and some modeling decisions and formulas are difficult to understand.	0
in my opinion, this may be due to a difference in writing style, but the paper, in general, is slightly hard to read.	0
i suspect that the authors have concrete reasons for making these design decisions, but these do not come across in the paper in the writing, or by means of additional baselines.	0
the paper writing is okay, but there are serious issues.	0
pros:   good results  method appears largely novel (although bears some resemblance to https://arxiv.org/abs/1906.04309) cons:   badly written  the method is poorly explained  uncertainty re: madds as a primary comparator i propose a weak reject for this paper for two primary reasons: 1) the standard of writing, and the explanation of the allimportant method are not up to scratch for a top tier conference 2) i have concerns regarding the madd calculations.	2
novelty is a little on the lower side, but thorough writing, results, and insightful comparisons make up for this in my opinion.	0
strong points of this work include the writing and experiments.	2
personally, i don't think the leaderboard results are that critical, but just want to make sure the writing is accurate at the time of publishing.	0
preliminary evaluation  clarity: the writing is fairly clear, though some details are lacking.	0
that said, i feel the lack of clarity in the writing is actually the main drawback.	0
the writing has improved a lot and most of my concerns are addressed.	0
particularly with blackbox transfer, where baseline performance is so sensitive to small choices, it's important to ensure baselines are properly implemented, and the current writing of the paper makes it impossible for the reader to assess this unless they are very familiar with the blackbox transfer literature.	0
while reviewing, i’ve been assuming this was just a mistake in writing but please double check and clarify.	0
m_z seems to just be a bottleneck but the writing makes it seem like it is more.	0
lacking an explicit motivation, the exercise of writing the canonical parameters of a multivariate gaussian in (2)  (4) is not very informative.	0
pros:  the writing is easy to follow and concise, with contributions and place in the literature clearly stated.	2
the writing and experiments could use some improvement, but i believe that the majority of the iclr audience would enjoy seeing this result (even though it would have no impact on most people's research) = detailed comments  page 4, sect.	0
considering that the premise of the paper is that mlp’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” the mnist visualization of groupselect felt informative, but the xor example for grouping visualizations seemed too easy.	0
it seems that the contribution of the network structure is rather incremental.	0
in my understanding, the paper assumes that the source and target domain shared the same task (reward structure) but only differs in dynamics.	0
i realize nekfac was an attempt at providing such a comparison, but since it uses a different structure, it remains unclear whether it’s poor performance stems from the fundamental difficulty of learning posteriors over high dimensional weights or simply a suboptimal network structure.	0
all deterministic weights are learned, but the structure only for some parts of the model?	0
overall, this is a mediocre paper which directly use a similar structure introduced in infogan but absolutely lose beautiful insights in the construction of the loss function in infogan, i.e., the mutual information between the generated image and codes.	0
(for instance, all the n^d arms are present at the leaves of the tree) the authors only mention in their conclusion that they leverage a 'hierarchy dimension structure', but this claimed structure was is never defined nor formalized in the paper.	0
the structure of the paper is strange because it discusses attribution priors but then they are not used for the method.	0
i appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3d molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain.	0
for the manifold modeling, though the authors defined each part of the formulation in equations (1)(5), but it is not clear how to design corresponding efficient structures for different lowlevel problems.	0
weaknesses: 1 the proposed bon is built upon the existing splitlbi algorithm that can identify the sparse approximation of the weight structure.	0
due to the residual structure of the encoding model, the inversion (backward mapping) is a bit more expensive since it requires solving a fixed point problem in each layer, but in principle it can be done provided the layer weight matrices have small spectral norm.	0
cons:  the paper is not very clear, and the structure is somehow confusing.	0
major comments:  overall, i find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to vibnet.	0
(2) for the structure shown in fig.1, the decoding dictionaries are , but i am confused why the encoding dictionaries are reciprocal to encoding dictionaries.	0
it is not exactly clear to me why the anonymization of fragments is necessary, but the authors suggest this places a greater focus on the graph structure and minimizes the model acting differently with different constants.	0
this approach does not capture the special structure of tensor and seems incremental.	0
recent works have proposed parameter space called path space to leverage this insight for feedforward as well as cnns, but this is not really the case for recurrent neural networks (due to the recurrent structure and the parameter sharing).	0
the comparison between different topologies is nice, but implies that the structure of the graph has to be fixed manually.	0
cons:  while the replication study is well appreciated, the novelty contribution of the paper is marginally incremental as the model structure is largely unchanged from bert.	0
the dimensionality reduction step in the strategy space does recover some latent structure, but this is still with respect to a taskspecific space of strategy annotations, rather than the demonstrated behaviors themselves.	0
a broader ablation study in which certain components of the model are replaced by simpler network structures could be instructive; the current ablation study only investigates hyperparameters and a few design choices  but what happens for example if a feature transformer is simply replaced by fully connected layer instead of the used architecture, i.e. how general is the structure?	0
questions: ' the authors claim that constructing an explicit causal structure, instead of a latent feature encoding, leads to better generalization in “longhorizon” tasks  but they seem to only test on one task, fairly limited in complexity. '	0
it is different from other neural networks as it not only approximates a target function, but also constructs and reveals its structure.'                  	0
my primary concern here is that the prior ends up becoming kronecker structured (after eq. 7), so it isn’t clear to my why dense matrices and dense variational bounds have to be derived in this setting.	0
concerning the existing invertible flowbased models for graph structured data, madhawa’s work is one of the first attempts in the literature.	0
the authors further motivate their work by assuming that these phenomena are due to lack of prior structure that can be alleviated by further supervision during training.	0
rnn are renn but restrained to a linear chain structure  in introduction: ... in the domain extremely complex data that is language ... > i'm not sure the sentence is correct  in introduction: the last sentence should be shorten and rephrased.	0
# weaknesses i have a gripe with the authors' choice to ignore program structure (e.g. abstract syntax trees) or features (e.g. types) in their program representation.	0
====== strengths and weaknesses ======  the motivation to make random forests respect the input structure similar to convnets is wellgrounded and important since random forests are still being used in certain applications where computational complexity and/or interpretability are crucial factors.	2
this is a common problem with various 'structure discovery methods' such as clustering techniques, but an important one to address.	0
'global shape and structure the group' > 'global shape and structure of the group'  ''anchor' agent (mehrasa et al.)' > year for citation  post review i appreciate the author addressing my concerns and making clarifications here in discussion and in the text.	0
review:###there exists two papers: [1] multimodal learning with deep boltzmann machines, http://jmlr.org/papers/volume15/srivastava14b/srivastava14b.pdf [2] deep restricted kernel machines using conjugate feature duality, ftp://ftp.esat.kuleuven.ac.be/stadius/suykens/reports/deeprkm1.pdf in particular [2] considers a model, which is similar to a boltzmann machine, but at the same time it is based on kernel features, and uses structure of the corresponding optimization problem to obtain a solution in a semiexplicit way.	0
strengths:  the paper is clearly written, and the authors have taken great care to describe the unique structures of the data they are modeling.	2
this contrasts with much of the earlier work in survival analysis, which is mainly or only concerned with getting the best prediction without concern with the “thematic structure” of features.	0
such a thematic structure could be useful to clinicians, but analysis of the learned topic structure by clinical experts is apparently left for future work.	0
it lacks of some details for the model: (1) what is the rnn structure?	0
this paper is concerned with tractable (approximate) forms of natural gradient updates for neural networks, in particular with the recent kfac approximation, which applies a set of approximation (layerwise independence, kronecker structure for affine maps) in order to obtain a hessian that can be computed and inverted efficiently.	0
in my opinion, it may not only improve the scalability but also help the model learn representations which better capture the structure or speed up the learning process.	0
because for the subsequent eabased network structure search, the loss scale is not important, but the sorting (ranking) is important.	0
dip is a method to use untrained neural nets for compressed sensing, but uses the structure of the neural net to implicitly regularize and constrain the solution (though, as a major result of this paper shows, it doesn't regularize  it can learn noise).	0
however after reading the appendix, i found the imposed structure to be fairly arbitrary, and the usage of natural language overkill and not necessarily well justified.	0
the paper lacks this structure at present and for a conference submission, all main contributions should also be a part of the main text.	0
human listening studies compare longterm structure, but not shortscale fidelity.	0
one of the claims of the paper is that it is important to model the finescale structure of spectrograms, but it is not clear if that really is the case.	0
melnet can model the long range structure for unconditional generation of speech, but its audio fidelity is not as good as autoregressive or nonautoregressive models on raw waveforms.	0
# concern 1: technical correctness the paper claims at multiple places that the geometry of euclidean space is 'trivial' or 'too simplistic' to meaningfully reflect the structure of the data.	0
the proof structure is reasonably well written, however more context and examples could help a reader unfamiliar with the literature.	0
i weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below:  the paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ml techniques.	0
cvpr 2018. the paper contains mostly empirical evaluations however the provided experiments do not well support the claim that cnn works well with geometric structures in geolocalization.	0
several concerns are listed: 1. it will be better if the author can show a structure map for encoder, classification, class condition embedding, and mutual information evaluation networks, to clarify their relationships.	0
this is somehow predictable since the proposed method does not exploit any extra useful information (such as graph structure) but simply base the new parameters on the existing components of the gcn message passing procedure.	0
however, the roles or status of the nodes in a network are usually unavailable in practice; and if they can be ever be inferred, one has to use structural clues, unfortunately, the extra parameters added here, namely the matrix p, q, are dependent only on the features of the nodes (namely hw part), but not the graph structures, therefore i do not think such parametrization would lead to significant benefit in improving the diffusion process.	0
since the authors didn’t really add any priors in a bayesian settings but rather designed an architecture, i suggest to reword something like “how to explicitly encode hierarchies into the model structure”.	0
overall, this paper sheds light on how we can use the underlying structure to better be able to make predictions on labels even when we don't have original labels but proportional bags.	0
eqn 3 does not apparently have the same structure as eqn 2. in particular, the first term in (2) is a function of x, but for (3) it is not a function of x'.	0
it is awkward since the paper gives a stable hypothesis for deeper nonlocal structure, but experimentally the deeper structure doesn't work well.	0
strengths: this paper questions nearly every component in the training pipeline, including choices about the model structure, initialization strategies, and optimization procedures.	2
i do, however, have significant concerns about the novelty of the paper as well as its structure and clarity, as detailed below.	0
overall this paper is well structured/written and well placed in the literature, but i think it is not yet ready for acceptance due to the following key reasons: (i) i think dpvae, the currently better performing method, is illposed for ssad since it makes the assumption that anomalies are generated from one common latent prior and thus must be similar; (ii) i think the worse performance of mmlvae, which i find theoretically sound for ssad, is mainly due to optimization issues that should be investigated; (iii) the experiments do not show for the bulk of experiments how much of the improvement is due to metaalgorithms (ensemble and hyperparameter selection on a validation set with some labels).	0
the sensitivity of glow model to even 12 pixel translations (section 4.1) and exploiting multiscale structure of glow (zeroing latent variables in section 4.3) are interesting, but i believe, not enough for a full paper.	0
additional experiments can on different environments structure (city grids, metro network) of complexity of the environments (the width of the cell in the grid) to measure the performance of the algorithm based on hyperparameters (3) realworld data brings more applicationbased matter into the subject, but requires more thorough investigation on the optimality of the solutions, proposed by rl method.	0
(also: please include error bars in fig. 2(a1) and 2(a2)) the proper comparison would probably be to a lowrank parameter matrix, where the parameter count is similarly reduced, but in an unstructured way.	0
finally, i am concerned that the method may give a false sense of explainability to the model  why it is true that a highly structured, symbolic language is being used to craft an inductive bias, there is no 'symbol grounding'.	0
however, i think there are some weaknesses of the paper that would put the paper slightly below the acceptance threshold: empirically the environments are not diverse enough, and they have an implicit structure assumed and favorable for the proposed method  there remains a question whether the method is general and not taskspecific.	0
this paper shows that gcn applied on a graph of a predicted protein structure achieves higher accuracy than the previously proposed neural networks strengths:  the proposed gcn outperforms other neural network baselines.	2
weaknesses:  methodological novelty is low  this is a straightforward application of gcn  the objective of qa is a bit suspect for the sole reason that the training and testing is performed using experimentally resolved protein structures.	0
several computational methods have been proposed for protein structure prediction, but none of these models perform well in all circumstances.	0
3. section 3.3: i would not call this a crystal, but more something like ''random finite structure''.	0
note my point above about deterministic policies 'we perform a hyperparameter search to over both agents’' > spurious 'to' 'we investigate a similar scenario but concern ourselves with learning agents as opposed to fullyrational agents that have full knowledge of the structure of the game, and we do not assume that agents use an existing language, but train agents to emerge their own' .this would be interesting, if the game was complex.	0
however, motivation for css is that there is structure in the recovered signal — however no comparison of the recovered structure is made.	0
while it is true, that is the signal is perfectly recovered, it would follow the structure from this data was obtained, however no such guarantees can be made for nonzero errors.	0
strengths: 1?the paper proposes a learnable similarity metric function and a graph regularization for learning an optimal graph structure for prediction.	2
weaknesses: 1?compared with lds [1], this work seems to overlook the bilevel optimization problem for learning model parameters based on the optimal graph structure.	0
there is also the rich related literature on graph generation (e.g., as in drug design), graph transformation (e.g., as in chemical reaction), structure learning in classical probabilistic graphical models, graph pooling (which is essentially building new latent graphs from an original graph), knowledgegraph completion, etc. this is not to say that the problem is solved (it isn't), but it is fair to place this work in a broader context.	0
the paper lacks thorough comparison to (a) baselines with the same semiparametric structure that perform planning at test time (like the real method of [4]) and (b) methods that generate reactive policies without constructing a semiparametric memory (e.g. offpolicy rl).	0
related, the approach seems closest to planet in structure, but rather than being used for planning, is executed directly as an offpolicy actorcritic algorithm, generalizing sac.	0
in this section, the authors mention that 'a multilayer perceptron is used' but do not provide any training or structure details; these details should be included in an appendix.	0
the core claim being made as follows: 'the representations learned by the models are shaped enormously by the kinds of supervision the models get suggesting that much of the categorical structure is not present in the visual input, but requires topdown guidance in the form of category labels. '	0
strong points: 1. the authors propose a new refine structure that seems to have a longer 'memory'.	2
my original concerns are like the other reviewers: why bn, not other techniques such as structure of dnns mlp vs cnn vs resnet, activation functions, weight decay, learning rates, softmax etc. my initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out.	0
in particular it would be useful to explain in what sense the task considered is related to continual learning (switches between allocentric and egocentric tasks that are not informed by experimenter, but that the rodent has to figure out), because for now it is only briefly stated in the legend of figure 1. analysis of neural data: analysis of neural data using dpca reveals interesting results regarding the representations of correct/incorrect trials, allocentric/egocentric tasks, temporal structure.	0
pros: 1. the proposed method incorporates the clustering method into model training to get better representation to keep the original data structure in the target domain.	2
this seems reasonable as it used general wellknown insights about the structure of cnns, but again does not seem particularly novel or technically difficult.	0
language is a nice complement to structure our knowledge and to interpret our perception (cf work on embodiment or grounded cognition), but it is risky to solely rely on language too (cf grounding symbol problem [3]).	0
specific comments: 1. a major concern is that this paper misses an important reference: 'structured content preservation for unsupervised text style transfer', which first uses postag information for text style transfer.	0
the basic idea of leveraging hierarchical structure in language is of course sensible, but it's not clear to me how wellmotivated or original this particular instantiation of hierarchical representation is, or what we have learned based on the comparisons reported.	0
however, i have some following major concerns about the paper: (1) the idea is not novel, because the use of hierarchical structure to model long text is straightforward and has been widely studied in natural language processing tasks, such as dialogue [1,2] and document summarization [3,4].	0
strong points of the paper: 1) the wellorganized structure.	2
strengths: the paper proposed an autoencoder based framework, which can handle the information of network structure, node attributes and node labels simultaneously weaknesses: the novelty of this paper is very limited.	2
this is not a very flexible encoder distribution but still enjoys the same conditional independence structure, because parameter sharing and independence are different concepts.	0
strengths:  the paper studies an important problem  human study is interesting weaknesses:  the description of the gramnet is rather short  making it hard to asses the novelty of the method  the structure of the paper could be improved comments: although the study seem to be sound, the observations about texture importance are not surprising (e. g. see https://openreview.net/pdf?id=bygh9j09kx)  making the contribution rather incremental.	2
clarity:  the paper is lacking structure.	0
the paper claims to introduce 'a novel back reprojection reconstruction loss, that allows the network not only to learn the underlying 3d structure but also to maintain a natural appearance', however i found the loss is quite straightforward and only includes existing loss terms. '	0
pros: 1. the proposed method shows the ability to learn the nested distributions with the help of hierarchical structure information.	2
there is a nice motivation for using a hierarchical structure in the introduction, but it is unclear if there is real benefit to this structure and how it could be used to help answer scientific questions, as is ostensibly the goal of this manuscript.	0
suggestion: it would be great if the authors could come up with a toy dataset with ambiguous but known bounding boxes (i.e. overlapping objects with particular poses) to study how well the proposed model recovers those structures.	0
some major concerns: 1) the bidirectional gated gnn doesn’t seem novel enough in comparison to previous work 2) i believe rl to graph2seq is a minor extension from seq2seq, since rl mostly deals with the decoder part which is common in across both graph2seq and seq2seq arguments: 1) adding the structure information to the encoder via the gnns is an interesting angle for question generation.	0
summary: the paper observed one common pattern of searched cell by 5 nas algorithms, which is the cell found usually has large width but small depth structure, and claims the reason of such pattern is because architectures with shallow but wide structure converge fast during training, and thus are sampled by the nas policy.	0
the authors claim that 'the main purpose of explicitly injecting such a bias is examining what changes are made to the resulting tree structures' but the purposed changes are not discussed at length in the text.	0
my concern is that by scaling the reward in proportion to l_k redistributes the rewards in a way that is not reflective of the underlying reward structure of the mdp.	0
the authors note that: ' features reflecting the global structure are more robust wrt adversarial perturbations, but generalize less; ' features reflecting the local structure generalize well, but are less robust wrt adversarial perturbations.	0
also the relational induction algorithm used in this paper does not seem to be the same as the one used by zambaldi et al. second, im concerned about the explanation made where they try to justify that the use of the bridgeboxworld which is argued to provide a more complex logical structure because of the use of linear logicformulas using more connectives.	0
i think that paper is wellwritten in general, but the structure needs to be improved.	0
therefore the 'learning error at time t cannot be decomposed as errors from a set of consecutive time steps before t, but errors from a set of nonconsecutive time steps without any structure.'	0
this algorithm is reasonably well thought out, building on an established literature of exploration bonuses, but with a slightly different take on the structure of the bonus.	0
2) i think it would be good to contrast this approach with ones that also have latent mixing strategies such as [1, 2]  these do not uncover latent modular structure, but can also produce hybridized images via linear latent space mixing unlike counterfactual assignment of latent vectors like in your work.	0
strengths:  the justifications for the design choices in this paper, in particular the convolution structure and connection to image geometry, was quite strong compared to recent papers (although, many of the presented ideas are known in more classical, nonlearning, techniques).	2
this new attention mechanism takes into account not just their feature similarity, but also extra structure information, which also enables their method to attend not only over direct neighbors, but also up to khop neighbors.	0
in section 2.1., the authors did a good job explaining the limitations of current approaches on a generic graph structure, but this is only under the main assumption that a node should attend more to neighbors in its denser community than other neighbors that not connected so strongly (fig 1).	0
strengths:  interesting problem  good results areas of improvement:  structure of paper  main technical section (4.1) very hard to understand.	2
my concern about this paper in its current form is that the layout/structure of the paper needs to be improved, for example: considering putting some of the key results in the appendix section in the main text removal of repeating results from the main text by shortening the early sections	0
previously  rating:  weak reject overall it is a clear and wellstructured paper, with interesting biologically derived architectures (strength #1), but as it stands the experimental results either do not completely support the claims of the paper (weakness #2) or are limited in scope (weakness #1).	0
the authors also consider (empirically) applications of these algorithms to 'input obfuscation': namely generating samples which are effectively noise, but the network classifies them as 'structure' (e.g. digits on mnist).	0
but i think there are lots of useful things you can do without that capability, e.g. do 3d point cloud completion, go image > structure, etc. i think this function composition angle should be deemphasized in the title/abstract, but i think the paper stands reasonably on its own without that.	0
i like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; i would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more indepth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large?	0
on the other hand, i wish the paper provided a bit more intuition on when the rankone tensor approximation structure will give good results (e.g. section 5.3 gives an empirical measure; but what about some simple theoretical examples which give low values, to provide more intuitions?).	0
the authors contrast with (a) via 'our student does not learn based on examples provided by the teacher, but is trained to mimic the internal structure of the teacher'.	0
the mentioned paper does not seem to use deeper structure but uses other ways to increase capacity.	0
pros:  dynamic graphs are an important but challenging data structure for many problems.	2
the novelty and significance of the contributions, however is limited, as many recent works have explored using graphstructured representations and attention in multiagent domains (e.g. vain: hoshen (neurips 2017), zambaldi et al. (iclr 2019), tacchetti et al. (iclr 2019)).	0
the overall structure, the figures, and the experimental results are very nice, but there are two major issues that are holding it back.	0
in many graph problems where each node is endowed with a certain feature set, we often observe the case where two subsets of nodes are isomorphic in terms of link structure but with different edge of node features.	0
it is not straightforward to apply 3d convolutions on such a data structure, however this paper proposes a method to apply the same regulargrid kernels used in gridbased convolutions to the particle structure.	0
once you obtain a mapping, can you comment on any insights concerning the structure of the partition and schedule?	0
fakeenglish: english, but with the unicode codes of english characters all shifted by a large constant so that there is no overlap between fakeenglish characters and those of actual languages, but the languageinternal structure remains that of english.	0
related work is covered to a satisfactory degree, but a discussion of some of the following closely related papers could improve the paper: ' chang et al., a compositional objectbased approach to learning physical dynamics, iclr 2017 ' greff et al., neural expectation maximization, neurips 2017 ' kipf et al., neural relational inference for interacting systems, icml 2018 ' greff et al., multiobject representation learning with iterative variational inference, icml 2019 ' sun et al., actorcentric relation network, eccv 2018 ' sun et al., relational action forecasting, cvpr 2019 ' wang et al., nervenet: learning structured policy with graph neural networks, iclr 2018 ' xu et al., unsupervised discovery of parts, structure and dynamics, iclr 2019 ' erhardt et al., unsupervised intuitive physics from visual observations, accv 2018 in terms of clarity, the paper could be improved by making the used model architecture more explicit, e.g., by adding a model figure, and by providing an introduction to the supair model (stelzner et al., 2019) — the authors assume that the reader is more or less familiar with this particular model.	0
in sec 3.3, you say that 'perhaps surprisingly, we find that we can predict performance directly from program structure,' but you never provide any evidence of doing so.	0
cons: ' missing information, which can be critical for the success of the model: (a) the estimation of the keys, and (b) how does the convolutional layer in eq (2) work, given that the input for it is concatenation of matrices, which has no spatial structure? '	0
one would expect them to perform a bit better than lstms, but that might be contingent on the size of the dataset more than the structure of the inputs.	0
my main concern is the structure f(x) = g(p_a x)  h(x) looks somewhat limited.	0
the current paper demonstrates an interesting new approach for imc that does not require any metadata about new users/items, but rather, exploits the structure of the observed ratings themselves.	0
it's clear you should combine the textual and nontextual features, but is this 'exact' network structure needed?	0
detailed comments:  the proposed estimator is not using control variate but using dual structure between value function and stationary distribution ratio, which is a novel idea comparing with similar doubly robust estimators.	0
the model is reported to achieve a slightly worse but comparable performance to the rosetta energy function, the stateoftheart method widely used in protein structure prediction and design.	0
this work proposes geometric aggregation scheme for gcns, which aims to overcome the limitations in traditional gcns; those are lacking long distance dependencies and structure information in nodes.	0
they show empirical evidence of the lowrank structure in few classical control tasks (mountain car, inverted pendulum, cart pole), and provide an iterative procedure  structure valuebased planning (svp), that is similar to value iteration but is able to exploit the lowrank structure to reduce the computational time.	0
it is not strictly necessary, but i could see multiple uses of 'rank' appearing in the rl literature as a means of exploiting structure for faster learning being confusing.	0
it would be interesting to test the clustering procedure on a shuffled version of the readout weights (shuffle across features and v1 cells), so as to keep sparsity but not any other structure.	0
my concerns are as follows: ' the novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning. '	0
pros:  a wellwritten paper with a good organization; notations are clear.	2
as far as organization is concerned, a lot of real estate is spent on background material, and few experimental results are presented to support claims made in the introduction.	0
the organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable.	0
my main concern is about the paper organization.	0
paper organization the most interesting result of the paper is the one about the threelayer network but the entire analysis is relegated to the appendix.	0
3. the paper writing is ok, but some explanation and organization should be improved as mention in cons.	0
my main concern about the paper is on its organization.	0
furthermore, neural networks have randomly initialized parameters, whereas it seems unlikely that human infants' brains would lack existing organization to such a drastic extent.	0
overall this paper raises important issues, but its current organization makes it feel incomplete.	0
weaknesses: w1: i think the main weakness of the paper lies within the organization and description of the sec.	0
some of the observations are quite interesting, however the organization and the poor writing of the paper makes the contribution unclear.	0
============================================================================================== writing, soundness and organization of the paper the writing of this paper is acceptable, but the organization can be improved.	0
the method is mainly compared against retinanet (which is 'anchor' driven), and also compared against other more recent methods (extremenet, centernet, fcos) etc.  the paper is quite well written and structured, the illustrations are also clear;  i have also read its previous version, and the new version has added a significant amount of work improving it  e.g. added feature alignment and group norm;  i haven't fully checked the results section of other concurrent papers for full comparison, but the current results are among the stateoftheart for onestage detectors.	0
i also understand that hrl can reduce the number of parameters, but i don't see how structured exploration reduces the number of parameters.	0
the paper is well structured, but the writing can be improved, and some parts are hard to read and follow (see minor comments below).	0
i like the idea for this paper quite a bit, it really got me thinking, but i think it isn't quite structured how i would like.	0
pros: 1. this paper is clearly written and wellstructured in logic.	2
on the negative side:  in my opinion, the introduction messes unstructured and structured.	0
robustness to independent noise, as the authors have, is a good experiment to have  however typical adversarial examples may be quite structured and such a randomized strategy may not give an accurate indication about the nature of the representation.	0
strengths of the paper: 1. the paper is wellwritten and wellstructured.	2
strength • simple yet effective regularization technique to neural networks for graphstructured data weaknesses • weak technical contribution to the problem • the performance gain is from mostly the use of a semisupervised learning approach based on entropy minimization, which has been developed without consideration of graphstructured data.	0
overall, although the paper does well in motivating the problem, the lack of rigorous experiments and poorly structured writing advocate for a weak rejection.	0
my primary concern here is that the prior ends up becoming kronecker structured (after eq. 7), so it isn’t clear to my why dense matrices and dense variational bounds have to be derived in this setting.	0
concerning the existing invertible flowbased models for graph structured data, madhawa’s work is one of the first attempts in the literature.	0
the performance is below the upper bound from 2018, but better than a classifier system from 2017. overall, the paper is well structured and manages to show that relic is a versatile tool on which various tasks can be performed.	0
pros:  the core idea of adapting bayesian truth serum to ensemble prediction in machine learning seems sensible  there is some evidence that the methods have an advantage over other common ensemble approaches in practice  although there are quite a few small english mistakes, the paper is well structured and generally quite easy to follow cons & questions: 1. the theorem statements and proofs are underwhelming.	2
strengths interesting nonparametric approach to estimating uncertainty in the agent's forward dynamics model clearly written paper with sufficient technical depth well structured discussion of related work weaknesses my main problem with the paper is a missing fair comparison to prior work.	2
pros: 1. this paper is clearly written and wellstructured in logic.	2
overall this paper is well structured/written and well placed in the literature, but i think it is not yet ready for acceptance due to the following key reasons: (i) i think dpvae, the currently better performing method, is illposed for ssad since it makes the assumption that anomalies are generated from one common latent prior and thus must be similar; (ii) i think the worse performance of mmlvae, which i find theoretically sound for ssad, is mainly due to optimization issues that should be investigated; (iii) the experiments do not show for the bulk of experiments how much of the improvement is due to metaalgorithms (ensemble and hyperparameter selection on a validation set with some labels).	0
however, i still have several concerns: 1. the authors proposed to compose the freeform filters and structured filters with a twostep convolution.	0
overall, the paper manages in a very clear and structured manner, (1) to show the current approaches for stabilizing learning and their downsides, (2) to show why common normalization methods fail and (3) formulates a possible solution for this problem.	0
first, this paper lacks a structured literature review.	0
(also: please include error bars in fig. 2(a1) and 2(a2)) the proper comparison would probably be to a lowrank parameter matrix, where the parameter count is similarly reduced, but in an unstructured way.	0
finally, i am concerned that the method may give a false sense of explainability to the model  why it is true that a highly structured, symbolic language is being used to craft an inductive bias, there is no 'symbol grounding'.	0
there is some repetition in the early parts, so this could be restructured a bit, but these are minor points.	0
3. given points 1 and 2, the literature review is lacking  there's a lot of prior work done on macroactions in both rl and robotics (planning, hri, ...) that goes well beyond the few recent papers mentioned by the authors, and i think it might be necessary to mention work on options where the termination function is structured / biased in some way.	0
the treestructured lstm model is an existing work but applying it to generate adversarial text is new.	0
review:###the paper is a very incremental addition to tu and gimple, 2018, which originally talks about using an inference network to find the optimum of a structured prediction energy network.	0
much of the iclr community is very interested in structured prediction, but it appears that spens have not had much traction.	0
pros: they have experimentally shown some improvements in the different mltc metrics with (bertsgm) and mixed methods, compared to the vanilla bert, specially, for data sets with hierarchically structured classes.	2
certainly we want estimators that can handle highdimensional gaussians, but can this estimator deal with very highdimensional, very structured data that mi estimators are currently being asked to handle (and not doing a great job of)?	0
strengths: 1. the paper is very wellwritten and wellstructured.	2
the paper highlights how the existing factverification studies have been restricted to work with unstructured evidence, and hence lack generalization to usecases where the evidence is in a structured format (eg.	0
previously  rating:  weak reject overall it is a clear and wellstructured paper, with interesting biologically derived architectures (strength #1), but as it stands the experimental results either do not completely support the claims of the paper (weakness #2) or are limited in scope (weakness #1).	0
my primary concerns come in relation to other methods for structured smoothness such as crf/mrf, and the tradeoffs and drawbacks of imperfect or downright wrong a, and alternative methods for practically finding good a matrices (potentially with an eye to applications outside fmri).	0
the novelty and significance of the contributions, however is limited, as many recent works have explored using graphstructured representations and attention in multiagent domains (e.g. vain: hoshen (neurips 2017), zambaldi et al. (iclr 2019), tacchetti et al. (iclr 2019)).	0
learning generative model in online fashion may work well in simple structured data such as mnist, but i highly doubt that the generative model could be trained properly for cifar10 or cifar100, especially in online setting.	0
related work is covered to a satisfactory degree, but a discussion of some of the following closely related papers could improve the paper: ' chang et al., a compositional objectbased approach to learning physical dynamics, iclr 2017 ' greff et al., neural expectation maximization, neurips 2017 ' kipf et al., neural relational inference for interacting systems, icml 2018 ' greff et al., multiobject representation learning with iterative variational inference, icml 2019 ' sun et al., actorcentric relation network, eccv 2018 ' sun et al., relational action forecasting, cvpr 2019 ' wang et al., nervenet: learning structured policy with graph neural networks, iclr 2018 ' xu et al., unsupervised discovery of parts, structure and dynamics, iclr 2019 ' erhardt et al., unsupervised intuitive physics from visual observations, accv 2018 in terms of clarity, the paper could be improved by making the used model architecture more explicit, e.g., by adding a model figure, and by providing an introduction to the supair model (stelzner et al., 2019) — the authors assume that the reader is more or less familiar with this particular model.	0
this is the first time anyone has demonstrated such a semantic parser (siva reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to openie methods, but this is qualitatively different).	0
the meanfield theory analysis has the activation function after the reweighting by s but before multiplying by w, while the framework in section 2.2 has the activation after the reweighting by s and after the multiplying by w. i am not sure how much this difference makes, or if it is significant, but i think it should be explained by the authors.	0
while the presented work already goes into a good direction, i cannot give it my endorsement for acceptance because of the following reasons:  the paper is lacking clarity: concepts could be explained somewhat better, and the paper is suffering from language/grammar issues that make it harder to understand the contents.	0
cons: ' some details could be better demonstrated / explained (even if only in the appendix).	0
however, i also identified two major and a couple of minor shortcomings of the paper that are explained in detail below.	0
this problem is not explained or motivated well in the paper, but instead the readers are referred to the airl paper.	0
rigor: as explained above, many details are omitted in favor of strong assumptions, but there are also some technical details that i think may be wrong.	0
technical soundness: [a] optimization of the elbo: (1) the ordering of data (i.e., the directional information) was mentioned repeatedly in the paper but its importance to the fast approximation was neither explained nor discussed.	0
weaknesses ' it would be nice if the authors show the performance of the postprocessed word vectors on other nlp benchmark: text classification, ner, ... etc. question ' 'neural word embedding as implicit matrix factorization' and 'analogies explained: towards understanding word embeddings' show that pmi is the global optimum point of the previous word embedding model's problem space and prove word analogy can be explained from the pmi characteristics of word embedding models.	0
( your methods rely on q(s, a) which is actionvalue function, or the constraint in equation 3 is integral of q over all actions which would be valuefunction in traditional definition ) note: this is explained very well towards the end in the appendix b, but this is a review of the paper and not appendix b or c.	0
at some level, it seems like the theoretical results have come along for the ride but do not clearly demonstrate that there is a problem with postln and that this problem is fixed by switching to preln, or at least the relationship is not clearly explained.	0
there is an example in figure 2, but it isn't really explained and i didn't find it helpful.	0
another issue is that there is no comparison to other representation learning techniques (like those mentioned in the related work section, or the recent 'unsupervised state representation learning in atari'), nor to a natural and more straightforward variant of the proposed method where z would simply be sampled from a (learned) gaussian distribution z ~ n(mu(x), var(x)), which at first sight seems like an easiertooptimize objective (using the reparameterization trick)… i may be wrong, but then this should probably be explained in the paper (i realize that the proposed approach is more general, but then it should be shown how this extra flexibility can lead to improved results).	0
here are the main ones i noted: • the discount factor is not accounted for in the derivation of the objectives in eq. 12 (i know this is often the case in practice, but the reason for dropping it should at least be mentioned) • the jump from v^pi(x) to v^pi(z) at the end of section 3 is explained too succintly.	0
2 the projection operator in equation (4) is a key step of the boosting procedure, but is not clearly defined and explained.	0
generally, the architecture of the model is poorly explained.	0
the model is explained very poorly; if i weren't already familiar with iresnet i don't think i would be able to understand the model and be able to reproduce the results by reading the paper.	0
developing a new strategy based on the proposed metric to quantize and prune a network in a pareto optimal sense: this is briefly and not very well explained in section 2.7, which sends back to 2.3, but it is hard to understand how it is exactly done.	0
cons ' the method itself is not explained very well.	2
cons: the paper is not wellwritten and the experiments and discussions are not support the ideas, more specifically i can mention several concerns: 1 motivation: the flaw of the previously proposed calibration metric is not explained clearly.	0
the present paper builds on the ptc method of schonsheck et al., but this method is not properly explained in the paper.	0
p4:  'embed in r^3' > embedded  in 2.1 it is explained that t_x m is the tangent plane, but you've already discussed t_x m several times before.	0
pros: 1. the paper is wellwritten and the methodology is explained very clearly.	2
regarding the second phase of the pipeline, it is briefly mentioned that “p has low capacity” (bottom of p.4), but this is not explained further.	0
pros: the paper is written clearly and the motivation of designed loss functions are explained well.	2
the reviewer also disagrees with using only 1k samples for selecting best dataset to transfer from  explained in minor weaknesses.	0
the proposed method works with a variety of quantization approaches, such as binary, simple pq or lsq (even if the authors aren't able to report results for this last method due to technical issues as explained in appendix 7.7) weaknesses of the paper:  the related work could be more detailed, see for example: 'spreading vectors for similarity search', sablayrolles et al. ; 'pairwise quantization', babenko et al ; 'unsupervised neural quantization for compresseddomain similarity search', morozov et al. justification of rating: the paper proposes a new loss function that weights the scalar products differently according to their importance than can be applied to a wide range of existing quantization methods.	0
the training procedure which ends up being used is td(1) but that procedure (in the context of the setup of the paper) has not been explained at all.	0
i’m recommending a weak reject, for the following reasons, in order of importance (explained in more detail later):  section 4.2 assumes that you must deploy a single policy across all timesteps of the metapomdp, but it should be possible to update your policy across timesteps.	0
the author claims that it is fine since we don't need sound bounds thanks to their 'theory', however the 'theory' itself is implausible, as will explained below.	0
i don't doubt the experiments results, but if one discovers something like that, it needs to be explained.	0
moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with onehot vectors), but the basic technical foundations of transformers were not adequately explained.	0
for different environments, different abstract representation dimensions are chosen, but the reason is not explained.	0
i understand the gist of figure 4 but it's not well explained and i do not see why these dimensions were picked.	0
i hope the authors will address my concerns 1)the derivation of their probabilistic method is not satisfactorily explained from a statistical perspective.	0
overall this paper makes a potentially interesting contribution, but it is not well situated within the drug discovery literature, and the results are not explained in enough detail to be understandable by experts in the drug discovery field.	0
the abstract and intro summary of contributions didn't do a very good job of conveying what the methods do, although they are clearly explained elsewhere (see suggestions below)  a lot of time is spent on detail of experiments and long, clear explanations (this is great), but makes it read a bit like a lab report.	0
the size of text is mostly good, although the legend could also be decreased in size) 5. incremental labels are wellexplained in the dedicated section, but not until then.	0
6. adaptive compensation is well explained in the abstract, but very confusingly in the intro summary of contributions.	0
it is explained in the appendix that one maps into 4x4 feature maps, but that might be too large for the current environments?	0
5. the paper keeps mentioning that it “implicitly imposes transitions to be sparse”, however it is never explained how that would come about?	0
?i was expecting more discussion of the results in figure 2, for example at the end of section 4.1. b. as explained above, figure 3 only shows that the joint training can perform a 1step prediction, which is ok but is the bare minimum.	0
the result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution.	0
>  the result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution.	0
exp3.p algorithm this is one of the key components of the algorithm presented by the authors but it is not properly explained in the paper.	0
(b) is explained in a generic way but the authors do not give an example of the inter/intra feature map comparisons.	0
(2) some of the technical details necessary for understanding the soundness of the techniques are either missing or are poorly explained.	0
comments i appreciate the clarification of the notation at section 2.1, however many exotic notation for machine learning researchers are presented before this section without any reference to 2.1. the operation `propto` (latex) is never explained.	0
the 2nd idea is interesting but not very well explained to some extent: 1. the goal of the modified objective function is to encourage the model to use the discrete states (instead of pushing all useful information to the continuous states).	0
however, the related work is not always well described, the experiments lack important comparisons, and the practical effects of pcgrad should be explained in more details instead of focusing on a proof for a convex case.	0
the use of an unfolding algorithm similar to lista is not clearly motivated and explained, especially concerning the training procedure when working in the sr context.	0
i get that it is akin to selecting a point on the 'elbow' of a curve, but this should be briefly explained.	0
the difference between the two sasa(nag) in the bottom row is well explained, but is there any difference between sasa(nag) in the top row and the ones in the bottom row?	0
figure 1 seems important for understanding the ideas in the paper, but is not explained in much detail.	0
clearly this change seems to speed up the process, and in the example explained in the paper it makes sense, but could there be examples where lack of bias might lead to worse results?	0
the paper is mostly well written (it could use some proofreading, but the main ideas are explained well).	0
similarly, the ablation study evaluates the use of l1 or l2 noise instead of the cyclic shift likelihood, but does not say what variance has been used for these, which would (as explained above) be an important parameter to take into account.	0
the results were averaged but were not clearly explained.	0
this is not experimentally explained, but i suspect there are optimization benefits that are hard to pin down exactly.	0
the authors also provide experiments that includes the usual loss v. epoch, but also loss v. wallclock time (which is nice when proposing secondorderlike methods where extra computations are necessary), and a test error v. epoch (which is again nice for secondorder methods as explained below).	0
further suggestions to improve clarity that did not influence the decision: ' the acronym vic is used frequently throughout the introduction, but is not explained until section 2. please introduce variational intrinsic control in the introduction. '	0
this seems an important part of evaluation but not explained at all.	0
the method is wellmotivated and explained, but the experiment section is not very clearly written and i’m not confident whether the technique represents an advancement in the stateoftheart or not.	0
the description of how to classify new points after equation 6 is poorly explained.	0
pros: 1: the paper is well written and motivations are clearly explained.	2
fscore is harmonic mean of them, which is 0.86. rmse and mae of the zero prediction is shown, but the more standard baseline of the error would be a constant prediction (e.g., the average of test points is often used, which can evaluate how much variance can be explained by the model).	0
the reporting of the verification ratio as a function of the perturbation radius is an interesting measure that i think is very benificial to making the point but i think it should be better explained as it took me a long time to get the point.	0
the idea itself is intriguing but the derivation and some design choice are not very wellexplained.	0
cons & questions: 1, the overall paper seems lack of focus in a sense that section 3 and 4 discuss too much on general universality whereas the main contribution, i.e., section 5 is not explained clearly.	2
i have a few concerns about some technical details of the paper, as explained below: 1) the paper motivated the new model with difficulty in detecting change points in seasonal time series.	0
the theory in support of the method seems reasonablish, but key definitions and steps in the proof are not explained in detail, referring instead of an earlier paper.	0
the authors mention in several places in the paper that their assumption is very weak, but looking at lemma 1, lemma 2 and lemma 3: lemma 1 and lemma 3 are the already known cases where signsgd works, and lemma 2 is a new case where signsgd works but as i explained before, it is not clear to me how restrictive this assumption is.	0
table 1 mentioned classification but the task is never clearly explained.	0
you explained in the paper that they could 'come from other source domain', but in the specific case of characterlevel language modeling (in which you are just using a decoder transformer without encoderdecoder attention), i don't think this is a problem.	0
strengths: the paper is wellwritten and the proofs are clearly explained.	2
## minor edit suggestions  fig 2 seems to define the blue square as the target, the text next to it describes the blue square as the agent, please make coherent  for fig 7: the numbers contained in the figure are not explained in the caption, especially the numbers below the images are cryptic, please explain or omit [novelty]: minor [technical novelty]: minor [experimental design]: okay [potential impact]: minor ################ [overall recommendation]: weakreject  the exposition of the problem and treatment of related work are not sufficient, the actual novelty of the proposed paper is low and the lack of comparison to strong baselines push this paper below the bar for acceptance.	0
the losses are explained but not within a particular probabilistic or stochastic framework.	0
summary of negatives:  the method is quite complex and explained, in my view poorly (although i'm open to the other reviewers' opinion on the matter).	0
the authors proposed a parametrization in equation3 for state transition and claimed that this parametrization yields ``an efficient estimation of mias as explained in the next section’’, but did not give any explanation throughout the paper.	0
this paper could be improved in the following aspects: 1. the novelty of the proposed model is somewhat incremental, which combines some existing methods, especially the unsupervised learning for action representation part that just combines methods such as vae, temporal skip connections… 2. some components of the proposed methods are ad hoc, and are not explained why using this design, such as why bilstm for encoder and why lstm for decoder.	0
i vote for rejection for four major weaknesses explained as follows.	0
strong experiments and benchmarks cons:  some sections are not explained well and unclear as mentioned earlier.	0
the effect from these word combinations should have strong effect on the model, but the effect may not be explained as the argument.	0
the defender's objective is stated in eq (2) and (3) but not explained clearly.	0
the claim is then discussed in section 3.1 and 3.2 but is not clearly explained.	0
the previous proxsvrg method using a proximal operator is explained to converge fast but leads to inaccurate final solutions, while the katyusha method is an algorithm based on momentum acceleration.	0
4. in section 3, the term of privacy budget is also mentioned but not explained.	0
the captions of the tables could explain the tables better (some of the abbreviations are not explained  informed helps but make it more explicit.)	0
table 3 is particularly poorly explained: what ssl ('semisupervised learning' i presume?)	0
i don't believe this paper should be accepted to iclr based off these observations:  the paper claims on l120 the headmaster learns how to group agents, but it is not explained in the paper how it learns it, it seems just randomly formed according to the description.	0
the main reasons for this are: 1) the main contribution of the paper (page 4) is poorly explained and lacks motivation.	0
i assume this just means 'evaluate the term inside the brackets in the context of an rnn' but this is not clearly explained.	0
the initial ideas and approximations are well explained, but applying it all to neural nets is not that clear i did find it hard understanding how it all works together, and an algorithm for computing all the metrics would in a table would be much easier to follow.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
section 3.4 mentions that one of the weaknesses of snapshot distillation is that it relies on a cyclic learning rate, but it is not explained how using dups can address this problem.	0
extra robustness and redundancy could both be explained by a structural failure of dnns to take advantage of having more parameters (i don't believe so, but it would be nice to [dis]prove!)	0
pros: 1. the proposed method is clearly introduced 2. the differences between the proposed method and related work are clearly explained and demonstrated through ablation study.	2
cons  the choices made when designing the method are poorly explained and strong baselines are missing.	2
there are some minor typos, then some key terms of are not sufficiently explained/defined (e.g. g_ heta in sec.2.2 is said to project “the latent data to unimportant features”, but then there is discussion of “in settings where does matter” – i’m confused.)	0
the proposed method is simple, but well motivated, sound, and well explained.	0
the paper presents math, which is nice, but without much intuition explained.	0
for the experiments, there are some missing detail and concerns: 1. the finetuning using ulseq (eq 7) procedure is not well explained.	0
some minor points:  many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved  some explanation about the choice of auc as a metric would be informative and could help connect to the initial motivation of the method  experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a 'neural network' is trained, the architecture should be specifically given alongside the discussion of the experiment	0
3 – related to weaknesses #1 and #2, did the authors perform any experiment on other contextual illusions like the ones explained in (mély et al., 2018), namely “color induction” or “enhanced color shifts”?	0
the only concern that i see in the paper is that the deterministic function module is not explained very well.	0
comments:  the authors argue selftraining enhances smoothness, but i would like to see this explained mathematically/conceptually in greater detail.	0
there's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in nonconvolutional quantum neural networks?	0
i have some concerns about many of the assumptions made across the paper that are not explained or verified.	0
names such as 'views' are used without explanation, and it's not explained how a device is an input to a program (yes, i get what this means, but it makes in unnecessarily hard to follow the paper) = recommendation i would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider iclr audience.	0
in the main text, please make sure to mention, however briefly, every result that appears in the appendix (something along the lines of 'this result could not be explained by confound x or y (appendix z)' would suffice).	0
'for fair comparisons' > 'for a fair comparison' the method is poorly explained; i have read section 3.2 several times, and i'm still not entirely certain of what's going on.	0
pros:   good results  method appears largely novel (although bears some resemblance to https://arxiv.org/abs/1906.04309) cons:   badly written  the method is poorly explained  uncertainty re: madds as a primary comparator i propose a weak reject for this paper for two primary reasons: 1) the standard of writing, and the explanation of the allimportant method are not up to scratch for a top tier conference 2) i have concerns regarding the madd calculations.	2
i feel the additions to existing pipelines are well motivated but insufficiently explained.	0
phrases like 'a 1d cnn ... with 4 default setting residual blocks' is to me insufficient  residual networks have many details such as resnet v1 or v2 style (ie is there a path right through the network which doesn't hit any activation functions), what kind of normalization is applied, number of channels in each block, how to do skips between different spatial resolutions, etc. the upsampling step for the descriptor head, which is claimed as a novel contribution, is not fully explained  'fast upsampling' implies (correctly) there are many variants of upsampling with different tradeoffs, but from the text i am unsure whether this is nearest neighbour upsampling, a convtranspose, etc. similarly, 'vgg style block' leaves some details unclear  whether the resolution downsampling is with a strided convolution / pooling / etc. lots of the details are implied to be in other previous papers, but i feel that the paper would be hugely improved by exact architectural details.	0
pros: 1. the margin maximization idea has been wellexplained, both intuitively and theoretically.	2
the discussion in 2.3 should be expanded and made more concrete (some of these you can write out the expressions for), and more pros and cons explained, e.g., which theoretical properties are lost for the wrapped distributions?	2
the reasons for the weak acceptance instead of a strong acceptance are explained by the following weaknesses.	0
(i see it is explained in algorithm box, but that's much later)	0
the maths is poorly explained and the technical jargon is quite confusing.	0
if i understand correctly they have a matrix called w_m^(k) which is multiplied onto the previous iterate—but for each layer / time unit (k), the matrix may vary by learning (note that in the main body of their paper they don’t explicitly use ^(k) notation, but it is explained to be the same as the other lista parameter matrices which vary with time, and is made explicit in the appendix).	0
this seems very critical to the performance of the method as shown in table 2 but explained nowhere in the paper.	0
pros: 1. the proposed approach is easytofollow and explained clearly, and the paper is overall wellwritten.	2
but the way it is presented and explained lacks of clarity: for instance in section 2, some notations are not well defined (e.g what is f?)	0
pros  the idea is fresh, well explained and experiments are sufficiently thorough.	0
this was not extremely clear from reading the main text (and confused me when i first saw figure 1 and 3a), but as explained in the appendix, the cnn is forced to output a sigmoid logit between [0, 1] per object channel.?	0
since molecules are structured objects that can be represented as graphs of connected atoms， care has to be taken to design suitable generative models for them， and this paper is a step in this direction.	1
separately， training sets are enriched with nonequilibrium structures， so as to confront the nn with more diverse data.	1
review:###tldr: split node embeddings into medatadata and graph structure， force them to be orthogonal.	1
review:###this paper study the structured filter pruning of depthwise separable convolution weights.	1
review:###this paper leverages metric learning to learn graph structure jointly with the learning of graph embedding.	1
review:###this paper employs markov random fields to exploit the input data structure and further model the covariance structure of the gradients.	1
review:###the paper studies extracting events from unstructured text， specifically on the low resource language amharic.	1
review:###summary: this paper develops a method for taking advantage of structure in the value function to facilitate faster planning and learning.	1
review:###[summary] the paper presents a video classification framework that employs 4d convolution to capture longer term temporal structure than the popular 3d convolution schemes.	1
references: [1] metaexploration of structured exploration strategies， gupta et.al. neurips 2018	1
quantitative analysis together with ablation studies on the structure are insightful.	1
overall， the analysis and the empirical evaluations suggest that kmatrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters， inference speed and accuracy， and ability to learn complex structures (e.g. permutations).	1
once one understands this conceptual modification of modifying the nat string encoderdecoder to a more structured nat encoderdecoder (which in dst is more of a normalized string)， they apply all of the stateoftheart techniques to build a dst system: gating of all potential (domain， slot) pairs as an auxiliary prediction (e.g.， [wu et al.， acl19])， delexicalizing defined value types [lei， et al.， acl18]， content encoder， and domainslot encoder (with pretty standard hyperparameters， etc.).	1
most of the 'strong generalization' is coming from the fact that the iterative structure is fixed.	1
most of the section focuses on analyzing weight magnitude， though i was hoping for something more about the actual structure of the sparse subnetworkespecially given the title of the paper.	1
it showed how to use the 2d spatial structures of natural images by constructing structured atoms.	1
it seems as though， in the application of advil to the dbm， the authors are exploiting the structure of the model in how they define their sampling procedure.	1
it has the analogous two model structure like the one in the manuscript.	1
information in appendix also enable reproducibility by providing parameters and architecture structure.	1
in this paper， the authors develop a highly structured model to predict motions of objects defined by segmentation masks and depths.	1
in this case， any benefit appears to be ill explained by the underlying motivation.	1
in the experiment part， the author compares a related work fbnet， which uses the structure parameter heta and temperature parameter au to control the sampling.	1
in particular， they use groupwise regularizing to accompany the particular structure of the model to aid interpretability.	1
in particular， protein structure prediction is a very active area of research witnessing steady progress during the previous decade or so.	1
in my understanding， the variable dependency structure studied in this paper (fig. 1) is studied by [ryu19].	1
in my understanding， the paper assumes that the source and target domain shared the same task (reward structure) but only differs in dynamics.	1
in gcns， the convergence of the embeddings is better explained by the mixing of a random walk (theorem 1 of (xu et al.， 2018))， which， in the special case of a gcn， converges to 1/sqrt(degree of node)， as shown by (li et al.， 2018) in their theorem 1 for the symmetric laplacian.	1
in fact， i feel this work complements that one in a number of ways， including the presentation of the temporal difference method for learning the action probabilities.	1
however， i am more skeptical about using this in a recurrent architecture and claiming that this is structure learning.	1
grass: generative recursive autoencoders for shape structures.	1
furthermore， the architecture blends individualspecific induced causal structures and crosspopulation prototypical causal structures.	1
for example: i) 'benefit from longrange temporal structure' in this section the authors train the model with temporally clipped videos by dividing the videos into n bins.	1
finally， the authors state that during testing they ignore the covariance structures， which also seems illadvised as the model is reduced to a deterministic one in that case.	1
figure 2 suggests that the learned weights jump around quite a bit from layer to layer without much structure (likewise with figure 5).	1
figure 17， figure 18， indicates the author used the same structure as used by [1].	1
especially， the response generator is strongly dependent on that structure.	1
each protein folds to a 3d structure.	1
e.g.， the lottery tickets drawn at epoch 120 and 200 have a mask distance of 0.082 in fig. 3， which is much larger than the mask distance between epoch 180 and 200. whereas， all three tickets achieve comparably high accuracy as shown in fig. 1， implying a shallow correlation between the accuracy and the mask distance'' as far as i know， this observation only tells us that the structure of the lottery tickets changes， which are drawn from 120 epochs to 180 epochs (although the maintain a similar accuracy).	1
does it rely on shared 'decision boundary structure' between classes (eg.	1
dnns can operate on very highdimensional， structured inputs.	1
dip is a method to use untrained neural nets for compressed sensing， but uses the structure of the neural net to implicitly regularize and constrain the solution (though， as a major result of this paper shows， it doesn't regularizeit can learn noise).	1
'deep graph convolutional encoders for structured data to text generation.'	1
dai et al. iclr 2018， syntaxdirected generative model for structured data.	1
cons: 1) the presentation is quite confusing.	1
c) the instability of adversarial perturbations can be explained by a larger gradient norm and hessian spectrum (figure 2， 3).	1
because devlin’s presentation only shows the result on the test set from the official leaderboard.)	1
as explained by the authors， the comparison between signgd and gd will depend on the trajectory being followed.	1
anyway， the manifold structure is nowhere exploited in the paper， so i suggest to call it 'transformed latent distribution'.	1
an architecture is then presented to alleviate posterior collapse by coupling the vae and dae， with structure of the encoder and decoder in both networks being shared.	1
also， the third goal 'independence from prior knowledge' is quite vaguely explained， and hard to understand.	1
after training the supernet， the network uses the genetic algorithm to search the structure.	1
a reduction of imitation learning and structured prediction to noregret online learning.	1
4: structured ... rl]'previously， we start by'::'previously， we started by''which in deep scenarios'::'which in scenarios with large state spaces'， or perhaps: 'which in deep scenarios'::'which in scenarios where value function approximation is used' references: calandriello， daniele， alessandro lazaric， and marcello restelli.	1
4. presentation of quantitative results: the quantitative results are scattered in text throughout the results section without being summarized in a table.	1
3. investigated the robustness of graph neural networks on classification as the structure of graphs become more random (e.g.， by rewiring edges while maintaining degree distribution).	1
3. a section about ablation studies will help the reader better understanding each component of the network structure.	1
what’s more， the approach uses 'the same' point estimates for different structures.	1
this paper presents a semantic parser that operates over passages of text instead of a structured data source.	1
there seems to be some blurring between the meaning of structure (used to motivate k matrices in the introduction) and sparsity (used to analyze k matrices).	1
discussing how the model (especially the generative part) translate to other structured latent variables， e.g.， syntactic trees and semantic graphs.	1
bayesian reasoning on the structure.	1
'assumes the underlying structure of the sentence to be a sequence， while allowing for long term dependencies': if anything， the treelstm more easily allows for longterm dependencies than the linear lstm.	1
overall, despite the good presentation, i am skeptical about the contribution and impact of this paper.	0
"i suggest that the authors try to simplify the presentation for the considered case of r^n and avoid unnecessarily complicated (""mathy"") definitions as they distract from the actual results."	0
yet there are issues with clarity of presentation that made it somewhat difficult to fully understand the exact procedures that were implemented.	0
the paper is easy to read and the presentation is clear, and i really appreciate this.	2
the authors address the very important topic of feature extraction and state representation learning.	0
1. the authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking.	0
"then i can simply use a pca as my ""auto encoder"" and happily learn a high quality state representation close to the ground truth."	0
2. section 3 states some desirable characteristics in constructing a state representation.	0
does this work tell me something that i don't already know in terms of learning new feature representation that is highly suitable for my task?	0
while the presentation is clear and the evaluation of the model is thorough, i am unsure of the significance of the proposed method.	0
there has been a growing number of works that aim to find learning algorithms that learn to discover and disentangle objectlike representations without having so much prior put into the model, but rather through some general purpose objective.	0
overall, this paper tackles its tasks in an interesting but maybe too specific way; in addition, it could be improved in a variety of ways, both in terms of presentation and content.	2
however, i found the presentation a little difficult.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
overall the generic idea of “quantile regression rl” from section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multiobjective statealigned rewards) makes it more a distraction than an asset.	1
the written focus on novelty detracts from the presentation, and a discussion of neural ode methods (whether acting as activations, or solvers) would serve as good background material.	0
this is not good technical presentation.	0
the vi presentation is cluttered and the justification of using the pseudoreward for reinforce is not clear.	0
this paper makes a good presentation in explaining coveragedriven training and qualitydriven training.	0
specifically, the process of metalearning is divided into a slow and a fast learning modules, where the slowlylearnt component accounts for lowlevel representation that is progressively optimized over all data seen so far to achieve generalization power, and the fastlylearnt component is supposed to pick up the target in a new task for quick adaptation.	0
pros novel treatment and formulation of metalearning from the perspective of fast and slow learning process cons some interesting cases not tested presentation could be improved [originality] the paper approaches the recently popular metalearning from a novel perspective by decomposing the learning process into slow and fast ones.	0
the paper itself is very bad in its presentation.	0
in terms of technical presentation, it is missing a lot of details, which makes reading and understanding the paper very hard.	0
2. the presentation of the methodology is also missing a lot of the explanation for many of the details used in the method.	0
3. the theoretical results come in plain words without proper mathematical presentation and the proofs for the statements are not well organized.	0
the lack of details especially for the technical presentation part make it very hard to read.	0
and the presentation of the results seem to be short of clarity and organization.	0
in addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established.	0
this representation does not require precomputation of flow, and thus is more computationally efficient compared to approaches that fuse two streams (e.g. 2scnn).	0
on the other end, i have several reservations/criticisms to the presentation and conclusions of the paper: 1. the paper dismisses other efforts that make similar assumptions as irrelevant in the related work.	0
sun et al (2018) also only use rgb input, propose a feature representation layer inspired by flow calculations and perform impressively on hmdb outperforming this paper’s results.	0
it is not clear how the network is trained, is the rgb network first trained and then the flow representation layer?	0
the paper is generally easy to follow but the presentation could be improved.	0
i am not stating that the proposed model for the first step is not achieving the intended purpose, but that from the current presentation in the text i was not able to understand that.	0
in general, i found presentation to assume specific domain background knowledge that probably is not known by most of the iclr attendees.	0
in addition to the presentation issue, i think that the experimental part needs to be improved by enlarging the set of used data.	0
cons:  the presentation of the paper is sometimes confusing.	0
overall evaluation: the proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand.	0
i am willing to increase my score if the authors will be able to improve the quality of the presentation of the text and the experiments.	0
presentation  i found the presentation to be somewhat wanting.	0
however, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology.	0
only the presentation of the standard vae could be more clear.	0
further comments page 3 the presentation of the vae objective is a bit oblique.	0
more specifically, the paper lacks in clarity and the presentation of the main methodology needs improvement.	0
apart from the general equation of the elbo the rest of the methodology looks more like a guide to follow rather than a scientific presentation of a novel idea.	0
international conference on learning representations.	2
'presentation can be improved'' the left and right plots of the figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings.                 	0
strengths: 1. nice presentation of the model.	2
presentation improvements, typos, edits, style, missing references: none	0
pros:  interesting idea, nice results, mostly readable presentation.	2
the paper delivers a superficial message on the representation: i do not consider that nice having modes near physical locations (paris, france) is wrong.	0
my main concern with the paper is that the presentation is very hard to follow.	0
instead, i found the presentation in scardapane et al much clearer (that paper has a clear separation of background, related work, and their contributions).	0
"although i am 'not' claiming this is the intention of the authors, and can very well believe that they found it interesting that so many successive simplifications would yield such a simple modification, i believe that a large pool of readers at iclr will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the wasserstein distance, nor with a ""wasserstein natural gradient""."	0
it is not clear to me how those challenges are solved in section 2.1. i think that the presentation in section 2.1 needs to provide more details.	0
an unusual presentation could be justified if the connection to rd models were exploited in some useful way, but that does not seem to be the case here.	0
and therefore i strongly encourage the authors to revise the presentation of their work and solidify their methodology further.	0
a major flaw in the present paper is in its omission of several lines of research that are focusing on representations of hidden layers, see for instance http://papers.nips.cc/paper/5347howtransferablearefeaturesindeepneuralnetworks.pdf there is a deep literature on representations of hidden layers that is relevant when one is interested in transferring the knowledge of the hidden layers.	0
are they generic representation, do they depend on the task or dataset?	0
how is it related to the hidden layer representations?	0
the presentation though is a bit convoluted and unclear and i would strongly encourage the writers to break it down and make it more readable the method is based on looking at the midsection rules of rk as a feed forward process and mapping that as information propagation in the network.	0
although this idea is interesting, i trend to reject this submission as i think its presentation is unclear and the technical detail is a little difficult to follow.	0
however, the presentation is unclear and i suggest the authors to revise their draft by providing more technical details.	0
if the users need to generate good representations such that classification becomes linearly separable, then why don’t they just train the last linear layer themselves?	0
to do this the users need to map raw input to good representations.	0
it seems that there are two ways this can work: users and service providers agree on publicly available representations, or the service provider is willing to share everything except the top layer.	0
nonetheless even with practically useful scenarios, it is standard practice to use good features/representations whenever available, so not really an academic contribution.	0
the presentation of lola can still be improved, but i see the main ideas.	1
in general the presentation of concepts and results is a bit problematic and unclear.	0
improving these could make the paper candidate for a poster presentation.	0
finally, the authors provide a somewhat odd presentation of ucbstyle algorithms.	0
in the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.	0
on the other hand, i have several concerns about the presentation of the results:  the abstract and the introduction sets up a misleading narrative around the results: the authors seem to suggest that their work somehow explains why certain learningrate schedules work better than others for deep learning applications / nonconvex optimization, although the actual results exclusively concern the classical problem of linear leastsquares regression.	0
this presentation is completely uncalled for as the authors themselves admit that it is unclear how the results would generalize to other convex optimization settings, let alone nonconvex optimization.	0
also, i think that this presentation style is rather harmful as it suggests that learningtheory results concerning classical setups are somehow embarrassing, so they need to be sold through some madeup connections to trendy topics in deep learning.	0
"i would suggest that the authors completely ""rethink"" the presentation of the paper and write it in a style that is consistent with the actual results: as a learning theory paper, without the irrelevant deep learning experiments (that only show wellknown phenomena anyway)."	0
while the idea of incorporating chemical domain knowledge in the interactions of an atomistic neural network is interesting in principle, this paper has severe issues ranging from presentation over the proposed approach to the results.	0
"the authors use a variety of heuristics and approximations such as a ""charge transfer ability"", bond strength, exponential decay of distances and overlaps of atomic orbitals which are multiplied ""to mix all things up"", to arrive at the pixelchem representation which is then fed into an atomistic neural network (pcnet)."	0
beyond that their definition of periodic pixelchem does only include adjacent cells, while for a unique representation more cells might be required.	0
a comparison of the pixelchem representation to the coulomb matrix is not sufficient here.	0
clarity:' the goal is well stated but the presentation of the method is confusing.                 	0
the presentation of the result is weird.	0
however, i think it needs to provide more solid evaluations of the proposed algorithm, and presentation also need to be improved a bit.	0
yee whye teh's nips 2017 presentation.	1
the motivation in his presentation is very clear.	2
emnlp 2018. the presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and english usage issues as well as other unclear statements.	0
the authors should work on their presentation and this could be a very nice paper.	0
the presentation of the paper can be improved a lot.	0
"[4] liu, hanxiao, et al. ""hierarchical representations for efficient architecture search."""	0
then the authors propose to use hmc on this lower dimensional representation.	0
while the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance in the current form.	0
to justify this approach, the authors should show how the posterior approximation quality varies as a function of the size of the low dimensional approximation, d. i reiterate that the idea of approximating the posterior distribution over neural network weights with a posterior distribution over a lower dimensional representation of weights is interesting.	0
unfortunately, the abundance of errors in presentation cloud the positive contributions of this paper.	0
summary: this paper proposes a novel bilinear representation based on a codebook model.	0
the proposed method is build upon the form of hadamard product for efficient representation of a bilinear model.	0
through introducing the framework of codebook, it is naturally extended into a multiplerank representation while the efficient pooling scheme is also derived from the (sparse) codeword assignment.	0
comparison eventually, the proposed method is closely related to the multiplerank representation of a bilinear model; z_i = x^t w_i x (eq.5) ~ x^t u_i v_i^t x (onerank, eq.6) ~ x^t u_i v_i^t x (multiplerank), ... eq.(a) which is a straightforward extension from the onerank model.	0
presentation in section 4.2, the performance results of the factorization model in eq.(13) are merely shown without deep discussion nor analysis on them.	0
the authors claim contributions in three areas: 1) learning representations on physiological signals.	0
2) they use the hidden state of the lstms as a representation of the inputs signals.	0
from this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation.	0
however, i think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments.	0
# clarity the paper is in general wellwritten, although some elements could be removed to actually help with the presentation.	0
i’m also confused by the presentation of the results.	0
the presentation also needs to be revised to find the simplest expression of the method and to focus on the interesting parts.	0
this paper proposes a novel crosslingual multitasking framework based on a dualencoder model that can learn crosslingual sentence representations which are useful in monolingual tasks and crosslingual tasks for both languages involved in the training, as observed on the experiments for three language pairs.	0
the main idea of the approach is to model all tasks as inputresponse ranking tasks and introduce crosslingual representation tying through the translation ranking task, introduced by guo et al. (2018).	0
i have several comments on how to further strengthen the paper and improve the presentation of the main findings.	0
how does the monolingual english sts model trained with the crosslingual multitask framework compare to the work of conneau et al. (emnlp 2017) which also used snli as the task on which to learn universal sentence representations.                 	0
1) the presentation of this work should further be improved.	0
the presentation is very easy to follow.	0
the presentation of the approach makes sense, and experimental results using several different gans methods and competing regularization methods are extensive and good in general cons  i didn't find major issues of the paper.	2
adding a sentence explaining the intuition behind using satlu in equation (1) might be helpful to summarize my feedback: i think experimental results and analysis are strong, but the presentation is strongly lacking!	0
i would recommend the manuscript for acceptance, if the presentation is significantly improved!	0
clarity and presentation: the result tables should be merged and formatted better.	0
cons:  paper presentation needs some work on clarity.	1
the main idea behind the paper is to use random projections as the initial word representations, rather than the vocabsize 1hot representations, as is usually done in language modeling.	0
just before equation 6 it says that the resulting vector representation is the 'sum' of all the nonzero entries.	0
the overall presentation could be better, and i would encourage the authors to tidy the paper up in any subsequent submission.	0
i miss more formality in the presentation of the methodology.	0
the presentation is overall clear, although i think there are a few typos and confusing equations: 1. there should be a negative sign on the lhs of equation 2.	0
the quality of presentation is good.	2
just judging the presentation of the paper, it looks sound.	0
to me it makes a lot of sense, but in the experimental part i could not clearly see if the improvement in performance is due to this representation of the binarized bn.	0
the presentation of the results seems biased.	2
global evaluation: i thinks that a representation of a binary net based on a continuation approach is quite interesting.	0
the presentation of the method in section 2 is rather clear and convincing.	0
from the current presentation, i find it difficult to understand what is the motivation of adopting the proposed relaxation of the optimization method, and how is the neural networkbased estimation strategy connected to the logistic regression model.	0
3. the presentation of the current paper needs to be improved.	0
strong aspects: abstract makes the reader curious and excited about the content interesting and important topic for representation learning.	2
a clearer presentation of the three different exploration types for instance would help the reader.	0
i am not seeing why a representation of space should emerge without any pressure for minimality or the like.	0
the idea and presentation are clear and straightforward.	2
such embeddings are the output of a bidirectional lstm taking as input the “raw” (discrete) nn representations (when regarded as a covariance function of the “raw” (discrete) nn representations, the kernel is a deep kernel).	0
for this reason, i believe the presentation and motivation of this work is not presented clearly.	0
writing and presentation suggestions/questions: 1. if the proposed method is a breakthrough, i am fine with the title but i think the experiment results tell us that wasserstein is not all you need.	0
my main criticism is with respect to the quality and clarity of the presentation.	0
comments: i think the ideas presented in this paper are interesting, but i think their presentation could be a bit clearer.	0
the presentation of the paper is pretty good.	2
"a few things about presentation: 1. the first sentence of section 4: ""the our proposed framework"" > ""the proposed framework of"" 2. figure 3: all lines are overlapping."	0
the state representation, transition, and observation models are learned jointly by backpropagation.	0
however, i believe the authors can improve the presentation of the model and empirical evaluation.	0
in terms of model presentation, the authors can compare the model with a large set of deep recurrent models that have recently been proposed for modeling time series with nonlinear latent dynamics (e.g. variational sequential monte carlo, structured inference networks for nonlinear state space models, black box variational inference for state space models, composing graphical models with neural networks for structured representations and fast inference, etc.).	0
the model’s overall distribution can then be defined in a somewhat different way than the authors’ presentation, which i think makes more clear what the model is doing: p(y | x) = .sum_a .prod_{t=1}^n p(y_t | y_{<t}, x, a_t) p(a_t | y_{<t}, x_ a_{t1}).	0
more details in terms of presentation:  section 2.1 and algorithm 1 reviewed gradient dropping which is the main related work to this paper but it is in the middle of the related work section.	0
overview and contributions: the authors present a newly collected dataset and evaluation framework for learning representations for landing an airplane.	0
the answers to these questions are important towards whether this new dataset will be a strong benchmark for representation learning.	0
4. why do you think disentangled representations will help?	0
sure, they have been generally shown to help learn more interpretable representations, and help in flexible generation from disentangled factors.	0
but in terms of discriminative or generative performance on your newly proposed dataset, does learning disentangled representations help?	0
what are some models that can learn effectively learn such disentangled representations?	0
"presentation improvements, typos, edits, style, missing references: section 3, line 7, 'with with a frequency' > 'with' i would suggest referring to some recent work on multimodal temporal fusion, such as ""memory fusion network for multiview sequential learning."	0
1. poor presentation  this paper is written in poor english, containing considerable amount of grammatical errors.	0
"as to mathematical presentation, the notations are conflicted and/or confusing; especially, in eq.(1), the notations of i and j are conflicted and thus it is difficult to understand its mathematical meaning at first glance, though it actually shows a simple ""3d"" conv."	0
2. the presentation of the result and proofs is clear and easy to follow.	2
3. is the assumption on the orthogonal and normalized filters in cnn a must thing for the argument or just for convenience of the presentation?	0
the presentation is clear the the approach intuitive.	2
the presentation should be selfcontained.	0
while this is primarily an empirical study, one aspect considered was the observation representation (raw pixels, random features, vae, and inverse dynamics features [pathak, et al., icml17]).	0
in examining reward curves (generally extrinsic during testing), ‘curiositybased’ reward generally works with the representation effectiveness varying across different testbeds.	0
they also conduct more indepth experiments on specific testbeds to study the dynamics (e.g., super mario, juggling, ant robot, multiagent pong) — perhaps most interestingly showing representationbased transfer of different embeddings across levels in super mario.	0
the comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly noncontroversial (and not really conclusively supported).	0
beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scalingup of [pathak, et al., icml17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).	0
the submitted code and videos result in a highquality presentation and trustworthiness of the results.	0
(5/10) === pros ===  demonstrates that curiositybased reward works in simpler game environments  (implicitly) calls into question the value of these testbed environments  well written, with a large set of experiments and some interesting observations/discussions === cons ===  little methodological innovation or analytical explanations  offers minimal (but some) evidence that curiositybased reward works in more realistic settings  doesn’t answer the one question regarding observation representation that it set out to evaluate  the more interesting problem, rl  auxiliary loss isn’t evaluated in detail  presumably, the sample complexity is ridiculous overall, i am ambivalent.	2
the ostensible goal of learning more about observation representations is mostly preliminary — and this direction holds promise of for a stronger set of findings.	0
it's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.	0
in general, i think this paper would benefit from a much more expanded and clear presentation of the contents of section 3. at the moment, the algorithm is only understandable from a close read of the algorithm block.	0
different from typical multimodal work, the proposed model does not project text and vision derived representations into a single space.	0
instead the proposed model uses a joint loss function that allows textderived representations and the visionderived representations to influence each other, while remaining in their separate representation spaces.	0
in particular the loss functions ensure the following: sentences that describe the same image should have similar textderived representations.	0
the similarity of textderived representations of a pair of sentences should correlate with similarity of the representations of their corresponding images.	0
the paper then describes multiple experiments that test whether the visionbased loss functions lead to improvements over textonly representations and if this method of incorporation vision information is better than projection based methods for multimodal representations.	0
2. the main idea can be implemented in a simple fashion using loss functions, without requiring any changes to the original models that compute the representations.	0
in general i found the presentation hard to follow and the results don’t look convincing for an end application.	0
there appears to be an attention model that somehow picks out relevant frames using the word embeddings and the frame representation.	0
minor comments:  it is unclear from the presentation whether both proposed pruning methods may be trained in tandem or in series.	0
the presentation is clear and understandable.	2
understanding sec 3 takes some efforts and i think the presentation could be much improved.	0
the presentation of the paper is clear and in very details, and also provides intuitive illustrative examples, and appendix details on data, implementations, and related knowledge.	0
writing and general presentation can be improved especially regarding bayesian neural networks, where some clarity issues almost become quality issues.	0
"in details: 1. the organisation of section 1.1 can be improved: a general concept ""attack"" and specific example ""pgd attack"" are on the same level of representation, while it seems more logical that ""pgd attack"" should be a subsection of ""attack""."	0
"and it seems to be a typo that it is bayes by backprop rather than bayes by prop 5. there are missing citations in the text: a) no models from nips 2017 adversarial attack and defence competition (kurakin et al. 2018) are mentioned b) citation to justify the claim “c&w attack and pgd attack (mentioned below) have been recognized as two stateoftheart whitebox attacks for image classification task” c) “we can approximate the true posterior p(w|x, y) by a parametric distribution q_θ(w), where the unknown parameter θ is estimated by minimizing kl(q_θ(w) || p(w|x, y)) over θ”  there are a variety of works in approximate inference in bnn, it would be better to cite some of them here d) citation to justify the claim ""although in these cases the kldivergence of prior and posterior is hard to compute and practically we replace it with the montecarlo estimator, which has higher variance, resulting in slower convergence rate.” 6. the goal and result interpretation of the correlation experiment is not very clear 7. from the presentation of figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior."	0
the authors evaluated different generative models including vae and various variants of the gans on the benchmark, but the current presentation leaves the details in the dark.	0
but the presentation of the work, especially the experiment section, only gives abundant number of results without detailed explanation regarding the pros and cons of the existing models, the efficacy of the proposed metrics, or the reason behind some nice generative properties of gans that are not able to learn the distribution well.	0
this work falls short of the standards of iclr in a few ways: 1. the presentation is unclear.	0
smaller notes: i found the presentation of the stride 1 resampling approach a little confusing.	0
the analysis on the network's representation and convergence is nice but it does not bring much insights.	0
the presentation of the paper needs some polishing.	0
unfortunately, i think the work is slightly let down by the presentation (there are many typos, and the first couple of sections could do with a rewrite), as well as a lack of rigorous experimentation.	0
finally, the presentation of the paper should also be carefully proofread and revised.	0
in terms of presentation quality, the paper is clearly written, the proposed methods are well explained, and the notation is consistent.	2
i think the clearest presentation of the paper is to think about the algorithm in terms of metalearning, so i will call this part the 'inner loop' from now on.	0
to summarize my feedback, i like most of the presentation and it is good to see effort towards reducing training times by selecting good training samples, but i think the manuscript requires significant effort to justify acceptance.	0
presentation =========== in the experiment part, the authors need to describe their model configuration.	0
the presentation of datasets consumes a lot of space and can be reduced (e.g., using a table).	0
furthermore, the experiments seem pretty rudimentary and the presentation can be significantly improved.	0
furthermore, any other synthetic data experiments which could demonstrate the various facets of accuracyinterpretability tradeoffs are missing 3. the presentation of the paper is quite unclear.	0
moreover, the intermediate representations are always complete graphs, so there is no graph to graph transformation here.	0
the decoding part starts from the assumption of independence among the output labels and then, via interaction with the encoded representation of the input, eventually turns to an output where relevant statistical dependences among output labels emerge with decoding.	0
presentation of the proposal is generally good, although there are some issues that are not clear.	0
in addition, the sentence about model parameters at page 5 is a bit ambiguous and it is not sufficient to resolve the presentation problem.	0
the discussion at the end of page 4 on the fact that a sequential representation for the input components is not natural is actually out of place for the specific application task selected for presentation.	0
the fact that such order is lost with the bagofword representation is a problem of preprocessing, not of the nature of the data.	0
i think the paper is relevant for iclr (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.	0
i guess this is a presentation mistake, otherwise there are relevant issues that are completely not addressed by the presentation.	0
the paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase.	0
moreover, for the presentation of the algorithm in section 2, an algorithm box summarizing the method would be of great help.	0
authors need to polish the presentation and term (e.g., sd), and fix typos (e.g., stateoftheart) throughout the paper.	0
nevertheless the manuscript has a couple of weaknesses, one concerned with the presentation and another related to the design of the study.	0
clarity: the presentation for this paper is relatively clear, but it is quite technical, so some parts are hard to follow, without knowing the prior work in detail.	0
the writing and presentation of the paper are clear.	2
deep contextualized word representations.	0
concatenated pmean word embeddings as universal crosslingual sentence representations.	0
increasing the network's depth increases the efficiency of the representation (i.e. fewer total parameters needed to have the same representational power as a shallow network).	0
this paper is a slightly difficult read  not because of the language or the presentation of the material but more because there is not one main coherent argument or goal for the paper.	0
the authors state multiple times that “all our results apply to the multivariate case” but that they restrict themselves to the univariate case for simplicity of presentation.	0
the latter helps the encoder to produce a domainagnostic intermediate representation of the audio.	0
"the formulation on p.7, 2nd paragraph is a bit confusing: ""amt freelancers tended to choose the same domain as the source, regardless of the real source and the presentation order."""	0
i understand the nested optimization problem in section 6, but the presentation is somewhat unclearly written.	0
it is very likely that there is some merit to the proposed methods that introduce non linearity, but these points simply get lost in the mediocre presentation.	0
although the problem of generating knockoffs based on vae is novel, the paper presentation is not easy to follow and the notation seems confusing.	0
detailed comments: 1. the presentation of the main results is a bit short.	0
the presentation is in general clear.	2
with the current presentation it is very difficult to understand what is being proposed.	0
pros:  it is an interesting result that adding a weak visual imitation loss dramatically improves rl with sparse rewards  the idea of a visual imitation signal is wellmotivated and could be used to solve practical problems  the method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up rl in practice cons:  it seems possible that imitation only helps rl where imitation alone works pretty well already  some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal gail  the presentation borders on handwavy at parts and may benefit from a clean, formal description the submission tackles a real, wellmotivated problem that would appeal to many in the iclr community.	1
on presentation: i think the presentation is a bit overly handwavy in parts.	0
"but then the confusion matrix does 'not' take into account the state, which is justified by ""to let our presentation stay focused (...)""."	0
hard to parse “different and more difficult than graph generation designed only for learning the distribution of graph representations, for graph translation one needs to learn not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously.“  hard to parse “graph translation requires to learn”  hard to parse “in most of them the input signal is given over node with a static set of edge and their weights fixed for all samples”  “we propose an graph” > “we propose a graph”  hard to parse “the two components of the formula refers to direction filters as talked above”  hard to parse “next, graph translator requires to”  “as shown in equations equation 7 and equations equation 6,” > “as shown in equation 6 and equation 7”  hard to parse “the challenge is that we need not only to learn the”  figure 2 would seem to need more explanation.	0
they have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the appendix section), and committed to opensourcing the platform post publication.	0
this paper describes a framework  tree reconstruction error (tre)  for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation.	0
the paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning, compares the correspondence of tre with human judgments of compositionality of bigrams, provides an explanation of the relationship of the metric to topographic similarity, and uses the framework to draw conclusions about the role of compositionality in model generalization.	0
the reported experiments cover reasonable ground in terms of questions relevant to compositionality (relationship to representation compression, generalization), and i appreciate the comparison to human judgments, which lends credibility to applicability of the framework.	0
"minor comment: p8 typo: ""training and accuracies""  reviewer 2 makes a good point that the presentation of the framework could be much clearer, currently obscuring the central role of learning the primitive representations."	0
reviewer 2's comments also remind me that, from a perspective of learning compositionready primitives, fyshe et al. (2015) is a relevant reference here, as it similarly learns primitive (word) representations to be compatible with a chosen composition function.	0
beyond issues of presentation, it seems that we are all in agreement that the paper's takeaways would also benefit from an increase in the scope of the experiments.	0
the presentation could be improved by clearly motivating this approximation and its role in the model at this point.	0
the presentation needs to be improved.	0
however, in its current form, there is a large scope for improvement in the presentation of the paper.	0
given that the iclr has a wide audience, it would have been nice if the authors have made the presentation of the paper selfcontained.	0
######################### post rebuttal: the authors have addressed most of my concerns regarding the poor presentation of the earlier version.	0
the motivation of the paper is clear and the presentation is easy to follow.	2
in this paper the authors propose dl2 a system for training and querying neural networks with logical constraints the proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved.	0
when it comes to the algorithm itself, the presentation is vague, introduces unnecessary complexity, and fails to reference significant related work.	0
this paper present an adversarialbased approach for private and fair representations.	2
however, the approach itself is similar to edwards & storkey 2015, and i find the presentation of this paper confusing at a few points.	0
first, while both the title and abstract suggest it is about learning representation, the approach might be better considered as dataaugmentation.	0
this contradiction with more commonly accepted meaning of representation learning (learning abstract/high level representation of data) is confusing.	0
however, the novelty is not enough for acceptance at iclr, and the presentation can be improved.	0
overview and contributions: the authors introduce a model that mimics expectation learning (i.e. learning multisensory representations by training to predict the other modalities from a given modality, for example, image to audio, audio to image).	0
the proposed model is based on an autoencoder structure with a recurrent selforganizing network for multisensory binding of latent representations.	2
2. i liked the recurrent selforganizing network presentation and usage.	2
presentation improvements, typos, edits, style, missing references: 1. page 2: our hybrid approach allowed > our hybrid approach allows 1. page 7: audiotry recognition > auditory recognition 2. page 7: while improved the visual stimuli in 3% > and improving ... by 3% 3. multiple other typos and awkward phrasing, i would suggest the authors spend more time proofreading their paper before submission.	0
even though the need of handling arbitrary input size is an interesting problem, i have several major concerns about this paper:  one of the main problems of this paper is its presentation, both the writing and methodology.	0
presentation improvements, typos, edits, style, missing references: 1. general poor presentation of experimental results.	0
the paper extends to 9 pages when a lot of space could be saved by making the presentation of experimental results more compact.	0
the presentation of this paper is very poor.	0
this paper proposes a combination of evolutionary methods and variational representation learning to improve the sample efficiency of rl methods.	0
they train a vae on environment frames, as well as an actionconditioned dynamics model to predict the next frames, and these form the representations fed into a policy network which is trained through es.	0
the use of cmaes instead of nes is a good improvement, and the way they motivate using vae representations to obtain manageable representation sizes is well put forward.	0
[edit: as mentioned by the other reviewers, this extension isn't as novel, given ha et al's work, hence that reduces my confidence about accepting this paper further...] however, this paper suffers from several issues in its current state: 1. its presentation is overly detailed about known literature.	0
2. the actual model presentation is too succinct and split into section 2.2 and section 3 (network architectures and parameters).	0
3. there is no clear evaluation of the performance of the vae representations and of the rnn dynamics model.	2
compare this to higgins et al. 2017, which used vaes which represented enough information to perform at the same performance as nonvariational representations.	0
(seeing learning curves might help) the fact that no gradients are passed back from the policy to the vae/rnn clearly emphasises that issue (the policy only affect the data on which the representations are periodically retrained on).	0
# summary this paper proposes a simple regularizer for rl which encourages the state representations learned by neural networks to be more discriminative across different observations.	0
the experimental results on atari games show that this regularizer improves a3c on most of the games and show that the learned representations with the proposed regularizer has a high rank compared to the baseline.	0
[pros]  makes an interesting point about the correlation between rl performance and state representations.	2
3) learning state representations just from the reconstruction task (without rl) with/without the proposed regularizer and separately learning policies on top of that (with fixed representations).	0
it would be much more convincing if the regularizer helps even in this setup, because this would show the general effect of the expressiveness term (by removing the effect of rl algorithm on the representation).	0
# clarity and presentation  figure 1a is not much insightful.	0
it is not surprising that the representations that led to a poor policy (which achieves 0 reward) are much less discriminative given five situations with five distinct optimal actions, because the the policy has no idea which action is better than the other in such situations.	0
please consider an alternative presentation where you first discuss the natural gradient and its various related works and algorithms, then present your algorithm and then demonstrate your empirical observations.	0
this presentation might contradict the timeline of the development of your approach but it might help to better connect your work to other works on the same topic.	1
clarity: the presentation is clear enough, and the motivation seems reasonable, though the assertion that all ar models are slow seems a bit belied by the recent wavernn work, which gets a wavenet like model running in realtime on a phone.	2
i think this work is promising and interesting to the probabilistic modeling community, but needs some cleanup and some more compelling presentation (non image data?	0
unfortunately, the presentation of this paper makes it hard to follow.	0
3. in the presentation of the methods, it is not clear what is novel and what was already done by abadi et al., 2016 4. the theoretical analysis of the algorithm is only implied and not stated clearly 5. in reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible.	0
22. in the bibliography you have brendan mcmahan appearing both as brendan mcmahan and h. brendan mcmahan it is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find.	0
the paper's presentation suffers, and it fails to communicate essential details clearly.	0
the primary difficulty in reviewing this paper is the poor presentation of the paper.	0
due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.	0
it also seems that the paper could use some further polishing in both writing and presentation.	0
i think the paper should be accepted for presentation at the conference.	0
however, the presentation of the technical results suffers from a real dilemma: on one hand, the authors completely ignored the technical difference between infinite dimensional banach spaces and finite dimensional spaces.	0
although the presentation of the hypervolume maximization method (section 3.2) is not clear, the resulting loss function (equation 10) is simple, and shares the same form with other previous methods.	0
clarity: overall, the presentation of the paper can be significantly improved.	0
the presentation is also clear.	2
major points: overall, i think this paper is not good enough for an iclr paper and the presentation is confusing in both its contributions and its technical novelty.	0
this paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.	0
3. in figure 6, the authors claim that more layers lead to “better” representations.	0
4. in figure 9, the presentation is highly confusing.	0
minor comments: 1. i don’t understand the “tension” between hierarchical feature representations and residual representations brought up in section 2. do the prednet and predrnn not contain a hierarchy of representations?	0
also, i still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering.	0
the paper proposes an objective function for learning representations, termed the conditional entropy bottleneck (ceb).	0
for details of such problem associated with ib type terms see: [1] r. a. amjad and b. c. geiger 'learning representations for neural networkbased classification using the information bottleneck principle', 2018 (https://arxiv.org/abs/1802.09766).	0
a more thorough analysis of the robustness of the algorithm is necessary, in particular a detailed presentation of failure cases.	0
the presentation can be improved by having a formal definition of linear quantized networks and nonlinear quantized networks, functionindependent structure and functiondependent structure in section 3 to make the discussion mathematically rigorous.	0
however, the paper seems to be still work in progress and could be improved on the theoretical side, the presentation, and especially the experimental evaluation.	0
regarding the presentation, i found many typos which i don't consider in my evaluation.	0
the paper proposes the factorized action variational autoencoder (favae) as a new model for learning disentangled representations in highdimensional sequences, such as videos.	0
as a result, the authors claim that their approach leads to useful disentangled representation learning in toy video data, and in data taken from a robotic task.	0
but more importantly, the reasons why i don’t recommend accepting the paper are the following ones: lack of clarity: i found that the paper lacks clarity in its presentation.	0
also, the paper lacks comparisons to the recently proposed disentangled representation learning models (fhvae and disentangled sequential autoencoder).	0
results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain.	0
i did not found the paper super clear in the presentation and it is difficult to really get all useful information for it.	0
review summary  while the focus on variadic learning is interesting, i think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for iclr.	0
presentation concerns  i have serious concerns about the presentation quality of this paper.	0
i would be more than happy to give this paper a very strong recommendation, if not for numerous flaws in presentation.	0
deepström networks quality: good originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: a number of open issues in the presentation  see detailed comments the paper is based on some concepts to link nonlinear data representations by means of nonlinear kernel mappings, with the deep learning approach.	0
i think the main idea of the paper is interesting but the presentation lacks a number of details in the description/parametrization and the experimental design.	0
basically one motivation in your approach is to go beyond the classical deep learning data representation  this may not be so interesting for image recognition problems  i may suggest to spend a small subsection to a complexity analysis (including parameter optimization) of the various methods  figure 2  what do you mean by number of parameters?	0
2) secondorder attack introduced in the paper i think they are a number of important details that are ignored in the presentation.	0
presentation issues:  while i understand the point figures 1, 2 and 4b are trying to make, i don't understand what those figures actually depict.	0
declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation.	0
other comments:  i am not sure of the value of section 3. in particular, it seems like the presentation of the paper would be as effective if this section was summarized in a short paragraph (and perhaps detailed in an appendix).	0
writing and presentation: the quality of writing should be improved.	0
the results do not try to claim state of the art on all benchmarks, which i find refreshing and i appreciate the authors candor in giving an honest presentation of their results.	0
there are some other problems with the presentation, including the fact that contrary to what is suggested in the introduction, the model seems to have access to the ground truth size of the blank (since positional encodings are given), making it all but useless in a real world application setting, but it is really difficult to evaluate the proposed task and the authors' conclusions without a much more detailed description of the experimental setting.	0
they suggest that the mutual information describes how distinguishable the hidden representation values are after a gaussian perturbation (which is equivalent to estimating the means of a mixture of gaussians).	0
also, in my opinion, more clustered representations seem to indicate that the mutual information with the output increases.	0
do more clustered representations lead to increased mutual information of the layer with the output?	0
i think the presentation and summary after each experiment could be improved and made more reader friendly.	0
my main problem with this paper is its presentation.	0
post rebuttal: i lowered my rating from 5 to 4. while the paper's presentation improved during the rebuttal, i lost confidence in the method's novelty, thanks to prior work brought up by anonreviewer3.	0
2. the clarity of the presentation is also not great.	0
the quality of presentation is also a problem, and both the organization of the main matter as well as of the figures can use some polishing.	0
# presentation issues clarity of the proofs can be improved.	0
other minor presentation issues include: ' page 1: why lbfgs and not lbfgs?	0
clarity: while the paper studies an important problem  it's important to move out of the norm ball based attack models and consider different attacks like spatial transformations, in the current version, the presentation lacks clarity in both the formulation of the attack model, attacks, defenses and explanation of the results.	0
however, the current presentation makes it hard to understand the main results provided and hence i would rate that the contribution is not very significant.	0
overall: i highly recommend the authors to revise the presentation and clarify a) the main conceptual differences of the new attack model (matrix m of proposition 1) b) formalize the impossibility and possibility results carefully with concrete theoretical/empirical results to back the claims	0
the presentation of the paper is not clear enough.	0
moreover, the presentation of the paper needs to be improved.	0
a network is trained to transform an input face image to simultaneously stay close to the low and midlevel representations while maximising the distance of the highlevel representation of the input image in a face recognition network.	0
while i have some questions about the evaluation and minor concerns about the presentation i would overall recommend acceptance of the submission.	0
i like the presentation of the work, especially answering some of my obvious questions like what happens if the vae is initialized with wasserstein objective.	0
here are three key equations from section 2. the notations are simplified for textual presentation: d – p_data; d(y|x) – p_d(y|x); m(y|x) – p_theta(y|x) max e_x~d e_y~d(y|x) [ log m (y|x) ] (1) max e_x~d { e_y~d(y|x) ) [ log d(y|x) ]}  e_y~d(y|x) [ log m (y|x) ]} (2) max { e_y~d [ log (y) ]  e_y~d log e_x~d(x|y) [ m (y|x) ]} (3) first error is that the “max” in (2) and (3) should be “min”.	0
unclear statements and presentation:  it is mentioned in the paper: “while the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and nonlinearity of the manifold.” is it necessary for the relative rotation to be formulated to classification task?                  	0
the previous presentation suggests tt and tr should have same number of cores.	0
"however, there are two major problems with the papers as it stands: 1) it completely ignores the decades of nlg literature on this topic before the ""neural revolution"" in nlp; 2) the presentation of the paper is confusing, in a number of respects (some details below)."	0
a good place to start looking at the extensive literature on this topic is the following paper: modeling local coherence: an entitybased approach, barzilay and lapata (2007) one aspect in which the presentation is muddled is the order of the results tables.	0
"minor presentation points  ""followed by a logically sound sentence""  might want to rephrase this, since you don't mean logical soundness in a technical sense here (i don't think)."	0
i have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.	0
the authors apply an existing method (mainly 2 vs 2 test) to explore the representations learned by cnns both during/after training.	0
## weakness 1. it seems to me that the methodological novelty is limited, which mainly follows [the emergence of semantics in neural network representations of visual information](http://aclweb.org/anthology/n182122).	0
for example in section 5, the claim of “cnns learn semantics from images” is mainly proposed in a previous work, but the way of presentation sounds like this is a contribution of this work.	0
it could be lack of background on my part, but i find the presentation extremely confusing, and unnecessarily verbose/florid.	0
an extensive analysis of a pathological counter example, introduced in reddi et al. 2018 is analysed, before the algorithm presentation and experimental validation.	0
regarding presentation, the reviewer's opinion is that the paper is too long.	0
this should be summarized (and its interesting argument stated more concisely), to the benefit of the actual algorithm presentation, that should appear in the main text (algorithm 3).	0
finally, the presentation of section 3 could be significantly improved.	0
it seems that the current presentation does not take into account the diagonal core tensor that is present in cp.	0
the first proposed estimator is unbiased, as shown by a proof, and accepts arbitrary losses, an improvement over prior approaches  the overall presentation is clear and clean cons:  one of the main claims of the paper is the proposal of an unbiased estimator.	0
the presentation is easy to follow and technically sounds good.	2
in terms of presentation, the grammar needs more careful checking.	0
there are several things about the presentation i find confusing.	0
if what the method does is an incremental version of the lowrank factorization proposed in earle at al (2018), i think the presentation can be better described in those terms.	0
i am still trying to evaluate the paper, but for now, my rating for this is low given that the main novelty in the paper: the environments, the evaluations, the tasks are so unclear because of the verbose presentation style on trying to tell us what we already know, such as goalconditioned learning, offpolicy learning, impala, etc.	0
this task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to make the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between r(x1, x2).	0
the minimum requirements laid out can be met easily by existing methods for sentence representations.	0
2) another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes.	0
there are several sentencelevel representation functions such as esim [chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences.	0
a similar representation is computed in bidaf [seo et al., 2017] in the context of matching question representation with sentence representations.	0
to summarize, i really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.	0
this paper proposes a new representation learning method for graphs.	0
quality: the quality of the paper is not high due to vague presentation of the proposed method (see clarity).	0
cons:  presentation is not good.	0
in this paper, an unsupervised representation learning method for visual inputs is proposed.	0
it would had been interesting to see how representation learning can be seamlessly incorporated for robotic grasping which involves control as a tangible contribution however, the presented scenario does is not contributing to that problem and is only doing visual matching.	0
the presentation of the paper can be improved.	0
in my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for iclr.	0
clarity: the presentation should be improved, especially in the descriptions for experiments.	0
the paper is generally well written, although the initial presentation of the model could be made a little clearer (it is not obvious from the text that the decoder takes the text as input  figure 2 helps, but comes a couple pages later).	0
the paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.	0
regarding the presentation, in section 3, i suggest to move the explanation of mentornet and backwardcorrection before their upgrade by pumpout.	0
minor points for improving presentation:  section 3 can be made more concise and to the point.	0
i have a few comments about the presentation (for which i would be willing to change my score to a 6):  when possible, please use the numbers reported by mena et al and consider reporting error (instead of accuracy) as they do to ease comparison.	0
########################################### updated review: the authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time.	0
after a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work.	0
for references, please first check this seminal work and then follow the line of research: ng, andrew y and harada, daishi and russell, stuart, icml 1999, policy invariance under reward transformations: theory and application to reward shaping the method proposed in this paper seems a way of automatically shaping the reward, but loses the optimal policy invariance (for how this invariance is ensured in reward shaping, please check out this tutorial: http://wwwusers.cs.york.ac.uk/~devlin/presentations/pbrstut.pdf).	0
however, the presentation and experiments leave me unconvinced that the presented approaches are a significant step ahead in attack generation (particularly in ways to generate attacks that can efficiently be incorporated into adversarial training of rl agents).	0
cons 1. unclear presentation of technical contributions, experimental results do not support the key contributions of faster attack generation 2. i am also unconvinced of the relevance of blackbox attack algorithms given the nascent stage of deeprl  since these agents are just being developed and their abilities need to improve significantly before they become deployable (and blackbox adversarial attacks are a real concern), i feel this work is premature and will need to be redone once more capable/robust agents can be trained for practical rl settings ### in light of the revision, i have revised my score given the rewriting of section 3 that addresses the second con i raised above.	0
however, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, i feel that the paper still require improvement before it can be accepted.	0
while the introduction claimed to make a systematic evaluation of attacks against rl, the presentation of the experimental section can be improved to ensure the analysis points out the relevant takeaways.	0
[a] learning discriminative video representations using adversarial perturbations, wang and cherian, eccv 2018 [b] sparse adversarial perturbations for videos, wei et al., arxiv, 2018 3) it is unclear why the paper chose to consider flow produced by a flownet model as their inputs for the attack?	0
overall, i think the paper has some observations that may be slightly interesting; however, it lacks novelty and the analysis or presentation are unconvincing.	0
the presentation is clear enough.	2
in order to resolve the above issues, i suggest that the authors limit their presentation to policy evaluation problems where at least some of these technical problems are not present.	0
general evaluation ( pro/  con, more specific comments/questions below):  the paper is very wellwritten  the bbp presentation is light but very accessible.	0
separately, the presentation of the paper could be significantly improved, for instance by introducing relevant notions more clearly in the introduction and related work sections, and by providing more insight and discussion of the obtained results in the main paper.	0
however, despite my favorable opinion, i consider that the paper presentation lacks rigor at many levels.	0
"review summary  the paper presents a combination of rule lists, prototypes, and deep representation learning to fit classifiers that are said to be simultaneously ""accurate"" and ""interpretable""."	0
the paper suggests learning a representation for each subject's data by feeding the ehr time series into a recurrent convolutional nn.	0
comments on presentation  overall i think every section of the paper needs significant revision to improve a reader's ability to understand main ideas.	0
what size is the learned representation h(x)?	0
comments about presentation: the writing of this paper needs to be improved.	0
(iv) the paper has problems with presentation.	0
while the focus in [13] was on stationary kernels, their theoretical derivations and presentation was designed for general basis functions.	2
"the presentation of the new forget gate in ""system 2"" is clear in terms of being able to implement it, but it's not intuitive to me what this actually looks like."	0
however, the presentation of algorithm 3.2 was confusing.	0
== quality & presentation: overall, this paper provides solid comparisons with previous work in terms of generation quality and representation power.	0
as a contribution to iclr, the paper should also try to better characterize the quality of the learned representations, against alternatives.	0
this is the transpose of the usual data presentation for doing, e.g. pca, wherein the days are along rows, not columns. '	0
the presentation form of the paper is unsatisfactory.	0
while the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and i initially struggled to properly understand what’s being proposed.	0
the sloppiness in the presentation is also manifested in other ways.	0
pros/cons summary:  the proposal yields good results in the provided experiments  minor contributions that are not convincing enough  muddled presentation of ideas  dubious or weakly motivated design choices  poorly written with plenty of typos  difficult to follow	2
regarding the presentation, i found odd having some experimental results (page 5) before the section on experience even have started.	0
the paper is generally well written with a clear presentation of the proposed model.	2
without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.	0
overall, i would recommend the authors to improve the presentation by providing more context for the use of double sampling method and other relevant works in this area (at least showing the impact of using double sampling on training).	0
2. the presentation of the experiment part is also wired, to report compression rates, speedup rates, and accuracy might have a more explicit demonstration.	0
overall, this paper is well written with clearly presentation.	2
there are experiments to show that meshadv can attack several vision systems for various tasks, and learned representation can transfer to new renderers and settings.	0
cons: the biggest issue is that the overall presentation of the paper is weak, with several noticeable grammar mistakes and obscure sentences that are hard to follow.	0
the presentation of the experiment section needs to be improved.	0
in general, the paper presentation is easy to follow.	0
the presentation of experimental results stops at the summary metrics, leaving many doubts on why they are as such.	0
in summary, the paper's quality has significantly improved, but some presentation issues keep it from being a great paper.	0
to improve this paper and make it acceptable, i recommend the following: 1. improve the presentation so that it's very clear what's being contributed.	0
in terms of presentation, there are some details that are omitted which would have made understanding easier and the work more reproducible. '	0
moreover, the presentation of the method is not very well done (missing details), especially since the authors used the upper limit of 10 pages. '	0
2. aside from evaluation, there are some other details missing from the presentation.	0
however, i believe the paper needs more work on presentation and evaluation, especially since the authors opted for 10 pages and according to iclr guidelines “reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.”.	0
the review of literature in the introduction is relatively complete, though again, the presentation makes this section difficult to understand.	0
unfortunately, the paper is clearly not ready for publication in its current form, given the many presentation issues.	0
regarding more specific details, there are too many small presentation issues for me to list them all, but here are a few representative samples:  please use “tile” or “cell” to denote individual elements in the grid (not “grid” itself)  it is unclear how states are represented as inputs to the model for generals.io  the 77% win rate against flobot seems to be an important achievement of the paper since it is mentioned in the abstract and introduction... but not in experiments (!)	0
the paper is technically sound and the presentation is generally clear.	2
this motivates the use of pca at each layer to identify a novel lowdimensional representation of the filters.	0
i found the paper presentation not optimal.	0
clarity research is well motivated, and paper presentation shows a nice, coherent story.	2
to make it acceptable, the clarity of presentation, especially of the results, must be improved, but more importantly, more work seems necessary to reduce the currently significant accuracy hit from the method, and the tradeoff of quantization level vs. robustness should be addressed.	0
pros and cons:  sound approach  good theoretical support of the approach (biasvariance analysis)  great results reported  of importance for optimizing without gradients  presentation of the method lacking many details and not very clear  overall quality of the paper is subpar, tend to be very textual and hard to follow in several parts  experiments are not exhaustive and detailed.	0
i think that if the paper can be rewritten to be more tight, clearer in its presentation, with figures and pseudocode to illustrate the method better, with more exhaustive testing, it can be really great.	0
given the technical nature of the paper, the authors have done a good job with the presentation.	0
the presentation of the paper is excellent  clear, wellmotivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.	0
i would recommend the authors to improve the presentation by providing more context for the use of double integral or sampling method and other mostly relevant works in this area.	0
the presentation is pretty good as far as the english goes but suffers from serious problems on the level of formulating concepts.	0
this paper presents a multiview framework for sentence representation in nlp tasks.	0
the paper is globally well written and has a clear presentation.	2
it sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation.. my main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation).	0
is it really to take advantages of decoder information on the encoder/representation part?	0
however, there are presentation and clarity issues in the technical development, and the comparative analysis is lacking broader comparisons with the stateoftheart (to be fair, the authors recognize that layerwise max as a baseline is particularly susceptible to outliers).	0
this combined with the clear presentation and thorough analysis in my opinion warrants publication and will certainly prove useful to the community.	0
the arguably independent contributions are: 1. the use of the faster sael 3.2.1 over cryptonet — not really an innovation per se 2. flexible representation of message space — the main contribution 3. a much more complex network than cryptonet for an experiment on cifar10 — minor contribution/application of 2 4. using pretrained model for embedding images before encryption — minor contribution i think the authors should refocus this work on point 2 and 3 only.	0
i will mainly focus on the central contribution of the paper — the data representation — which in my opinion has the potential to progress this area further.	0
unfortunately, the current quality of presentation is suboptimal.	0
the choices on architecture and intermediate representations of lola in section 4 are hard to follow, and it is not clear to me when the authors are making a choice or satisfying a constraint.	0
that is, for example when we aim to vs. we have to use one representation in place of another?	0
since this is the main contribution, i suggest the author to help the presentation with diagrams and formulae in this section.	0
e.g. responding to answers such as: what are the most efficient data representation in input and output if we want to compute a convolutional layer?	0
this paper presents an instructionfollowing model consisting of two modules: a goalprediction model that maps commands to goal representations, and an execution model that maps goal representations to policies.	0
however, the paper also contains a number of serious evaluation and presentation issues.	0
even if we set aside the distinctions between humangenerated instructions and synthetic command languages like used in hermann hill & al., the goal > policy module is defined by a buffer of cached trajectories and goal representations.	0
the model is trained to optimize the combination of 1. loss between reconstructions of the original reviews (from the encoded reviews) and original the reviews, 2. and the average similarity of the encoded version of the docs with the encoded representation of the summary, generated from the mean representation of the given documents.	0
comments / issues [ issue 6 is most important ] 1. problem presentation.	0
similar problem with presentations of the models, parameters, and hyperparameters.	0
it is difficult to get a sense of how these metrics corresponds to the actual perceived quality of the summary from the presentation.	0
summary: this paper proposes to extend the pretraining used for word representations in qa (e.g., elmo) in the following sense: instead of just predicting next/previous words in a sentence/paragraph, performing a hierarchical prediction over the whole document, by having a local lstm and a global lstm as presented in fig. 1  the idea of masked language model.	0
pros:  good presentation and clear explanations.	2
the results are stated clearly and the presentation of different metrics is a nice addition to properly compare the results.	0
however, i had a few concerns about the presentation which affected my understanding of the methodology.	0
in summary, even though i liked the idea and the approach to the problem, the presentation is quite lacking, and in its current form, the paper might lose impact just by virtue of being hard to understand and subsequently being hard to reproduce.	0
i would recommend improving the presentation before publication.	0
another minor weakness is the clarity and presentation of results: 1. the proof outline of the main result is hard to follow.	0
the presentation is in a quite poor quality, including many typos/grammatical errors.	0
i think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.	0
more generally, a simpler presentation of the key results would be useful, so as to allow the reader to better appreciate what are the main claims and if they are as substantial as claimed.	0
the problem is very relevant, and the approach is interesting, but unfortunately, the presentation is very misleading (see details below).	0
therefore, i recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).	0
i therefore vote and argue for accepting the paper for presentation at the conference.	0
the presentation of the paper could be significantly improved.	0
however, as this is to me mostly a detailed empirical investigation and presentation of highperformance gans on large scales, i would be likely to share this with colleagues who want to tackle similar problems.	0
overall, i think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as biggan, probably more, given that it is doing unconditional generation.	0
"the main contributions are 1) extending the adversarial discriminator in adda to be multiclass (distinguishing between k source classes and 1 ""nonsource"" class), and 2) developing a new, mmdbased loss to optimize a representation against the multiclass discriminator."	0
the presentation of the method section is a bit confusing or sloppy sometimes.	0
although the authors have demonstrated that their method is effective on standard domain adaptation tasks, the presentation of the method is a bit unclear in a number of ways, and analysis of the components of the proposed method is limited (only one baseline instantiation is explored, as opposed to a more thorough ablation), making it hard to gain a lot of insight from the proposed method.	0
it seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ml venue. ''''''	0
these two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space.	0
"here is a list of remarks meant to help the authors write a clearer presentation of their method:  the ""problem formulation"" section contains various things."	0
learning to reach goals and learning goal representations are two extremely active domains at the moment and the authors should position themselves with respect to more of these works.	0
about state representation learning, if you consider the topic as relevant for your work, you might have a look at the recent survey from lesort et al. (2018).	0
in its current form, i do not recommend accepting this paper but i do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via rl experiments.	0
the presentation needs work in several areas, and the experimental results require more explanation, but otherwise this seems like a solid paper.	2
the overall presentation of the method is direct but slightly confusing.	0
the section 4.3. shows that the proposed method does find better representations of the mnist than vae, but does not mention that there are numerous extended vae methods (and others) that would perform better than the ltvae here.	0
overall the paper proposes a bnstyle structure on vae latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.	0
originality 2/5 more straightforward extension of previous work based on current presentation.	0
the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention  presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks.	0
the choice to place the related work at the end also reduces presentation clarity.	0
the presentation is rather sloppy overall, with agrammatical sentences (noungerundinfinitive confusions, mainly) which repeatedly get in the way of understanding, sloppy character spacing and punctuation, references from the main text to the supplementary material.	0
the related work section is uncomfortably similar to godard et al 2017, from which it borrows the presentation structure, and mostly the order of introduction of individual references.	0
the presentation of this method is clear to a level that should allows the reader to implement this him/herself.	0
it would be great if the code associated to this could be released but the presentation allows for reproducibility.	0
however, i have some concerns about presentation and a number of specific questions about model implementation and evaluation.	0
"presentation and naming first off: i implore the authors to find some descriptor other than ""selfaware"" for the proposed model."	0
representations of language don't even interact with the visual attention mechanism except by way of the hidden state, and the salient new feature of the visual attention is the fact that it considers the full panoramic context before choosing a direction.	0
although the format of the paper is seemingly unusual (it may feel like reading a survey at first), the authors propose a concise and pedagogical presentation of jordan networks, lstm, neural stacks and neural rams while drawing connections between these different model families.	0
the paper is interesting and topical: robustness to adversarial input presentation (or shifts in training data itself, even those of the nature described by the authors 'semanticlossless' shifts).	0
the presentation is not too hard to follow.	0
they first learn vector representations of both the question and context, and then iteratively change the vector representation of the question to improve results.	0
the presentation of the results left a few open questions for me:  it is not clear to me which retrieval method was used for each of the baselines in table 2.	0
the presentation should include a table with number of steps and f1 for different step numbers they tried.	0
nevertheless, i feel the paper is quite weak from the perspective of presentation.	0
there are a couple of aspects the presentation can be improved from.	0
the authors propose a measure of compositionality in representations.	0
given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation z of the input x. the authors find that this measure correlates with the mutual information between the input x and z, approximates the human judges of compositionality on a language dataset and finally presents a study on the relation between the proposed measure and generalizalization performance, concluding that their measure correlates with generalization error as well as absolute test accuracy.	0
this in an interesting study and attacks a very fundamental question; tracking compositionality in representations could pave the way towards representations that facilitate transfer learning and better generalization.	0
while the paper is very clear with respects to results, i found the presentation of the proposed measure overly confusing (and somewhat more exaggerated that what is really going on).	0
it needs several reads to really distill the idea that essentially the authors are simply learning vectors of primitives that when added should resemble the representation of the input.	0
i like the idea of learning basis vectors from the representations and constraining to follow the primitive semantics.	0
moreover, given the simplicity of the datasets in the current study, wouldn't a reasonable baseline be to obtain the basis vector of blue by averaging all the latent representations of blue?	0
i suspect that low mi means that input and latent representation are somewhat independent but i don't see the connection to compositional components.	0
the authors mention that they learn word and phrase representations.	0
where are the word representations used?	0
my understanding is that you derive basis word representations by using sgd and the phrase vectors and compute tre with these.	0
section 8 presents results on discrete representations.	0
the presentation of paper could be improved, in particular the first paragraph of page 2 where the representation in complex domain is introduced is hard to follow and could be improved by inserting formulations instead of merely text.	0
in experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number?	0
the authors may want to correct typos and improve the presentation.	0
the presentation of the paper is intuitive and easy to follow.	2
as it stands, i believe some of the claims are insufficiently supported and the overall presentation of the results overreaches.	0
pros  the presentation of the approach is clean and easytofollow.	2
i think the paper shows good results, but it could very much benefit from improved presentation and evaluation.	0
2) the presentation is quite good, although some details are missing.	0
gordon et al. (2018) proposed a more general presentation, unifying the above works (and others) in a bayesian framework that allows for different functional forms of posterior inference (both point estimates and distributional) of the taskspecific parameters, including gradient based procedures.	0
the presentation is crisp, especially the pseudocode in figure 5. in addition, the paper includes several wellexecuted experiments assessing the contributions of different design choices on different metrics and making careful comparisons with several recent methods addressing the same problem.	0
however, the model’s presentation is confusing, and many important details of the experiments are missing.	0
while i have a few concerns about presentation and experimentation, these are issues that can easily be remedied and i recommend acceptance.	0
accordingly, my suggestions mainly relate to the presentation and related work:  it seems a bit strange to argue that the proposed approach doesn't increase the time complexity over mle.	0
i would, however, prefer more clarity in the presentation of the approach.	0
this especially applies to the presentation of the approach around eq. (6).	0
presentation: the presentation is mostly good.                  	2
that said, those complaints are just about presentation and not about the method, which seems quite good once you take the time to dig it out of the appendix.	0
i strongly suggest improving the the presentation of the paper.	0
as the presentation is now much more sound, i slightly increased my rating.	0
strengths:  even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to mt is novel  the presentation is very clear  the choice of methods is well argued and justified  the experiments are well executed and analysed  thorough and varied analysis of the experimental findings i recommend this paper for the best paper award.	2
this seems to be bad presentation.	0
some minor comments on the presentation.	0
the authors suggest a method to create combined lowdimensional representations for combinations of pairs of words which have a specific syntactic relationship (e.g. adjective  noun).	0
although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.	0
general presentation fairly clear and easy to read # cons ' would have been more impactful to focus experiments on realworld scenarios in which bandwidth is constrained and naturally contentious # other comments ' pg.	2
the central idea underlying ratedistortion, i.e. lossy compression by discarding irrelevant information, seems very suitable as a guiding principle for representation learning.	0
in particular, learning representations that generalize well is essentially another instance of a lossy compression problem.	0
the paper thus addresses an important and timely topic which should be of broad interest to the representation learning community.	0
i vote and argue for accepting the paper for presentation at the conference.	0
presentation of this paper: this paper contains numerous typos (for example a) m_' versus m^', b) what is mdpg exactly in the experiment section, c) grammatical errors etc.).	0
while the presentation of this paper is technical, it lacks intuition on the assumptions made as well as the conditions posed.	0
unfortunately given the current presentation i find it rather difficult to grasp the general ideas presented in this paper.	0
conclusion: despite my concerns on the first part of this paper, i think the very thorough experiments, clear presentation and the interesting results on learning rate warmups and model distillation merit its acceptance.	2
overall i found the presentation somewhat confusing, and not focused enough from the algorithmic learning perspective.	0
it uses three losses: sample reconstructon, adversarialy loss and feature matching on a representation learned on an unsupervised way.	0
the presentation of the paper is correct.	0
there are many such representations for audio signals.	0
specifically the magnitude of timefrequency representations (like spectrograms) or more sophisticated features such as scattering coefficients.	2
neural discrete representation learning.	0
this is a paper that communicates a large scale experiment on human object/semantic representations and a model of such representations.	0
but we do not know how robust this is to the many heuristics clarity, the presentation of the inference process is clear.	0
at the same time, i have some concerns about the novelty and the presentation.	0
perhaps a more concise presentation of the argument is needed.	0
while this paper presents an interesting approach to the above two problems, its presentation and overall contribution was pretty unclear to me.	0
further, i believe that the current presentation of the empirical results does not support the nature of the claims being made by the authors.	0
the presentation of the work however lacks sufficient details and motivations, which makes it difficult to judge the proposed model.	0
issues/concerns  i assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which i discuss below.	0
it would be very important to explain the intuition of this model and make the presentation clear and understandable.	0
the main idea of the approach is to first learn a joint representation of the specification and grammar using a graph neural network model, and then train a policy using reinforcement learning to guide the search with a grammar adaptive policy network that is conditioned on the joint representation.	0
the overall approach of using graph neural networks to learn a joint representation of grammars with the corresponding logical specifications, and then using reinforcement learning to learn a search policy over the grammar is quite interesting and novel.	0
however, there were some details in the model description and evaluation that were not very clear in the current presentation.	0
first, the paper mentions that it uses the idea of static single assignment (ssa) form for the graph representation.	0
the algorithm is mainly based on a special design of representation/data structure of pwc function, which can be used to store value functions and allows to efficiently compute several relevant operations in bellman equation.	0
however, the paper spends lots of effort explaining representations, but only a few sentences explaining about how the proposed representations/data structures can help find a somehow generic value iteration solution, which allows to efficiently compute/retrieve a particular solution once a .delta vector is specified.	0
furthermore, it is also unclear why the representation of pwc in discrete case is novel.	0
i would think the really interesting things are the operations based on those representations.	0
for example, the part of computing summation of two pwc function representation is not justified.	0
i think a better way to explain those things is to redefine a new bellman operator, which can operate on those defined representations of pwc function.	0
i think it could be a nice work if the author can improve the motivation and presentation.	0
the reasons are the following: first, the authors claim that dian is an attentionbased mechanism spanning over both time and graph structure, and also based on different edge representation than hohsen (2017) and velickovic (2017).	0
apart from these, the presentation of the paper is too poor.	0
in summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.	0
overall presentation is good, although some details/explanations/motivations are missing.	0
i think the paper is relevant for iclr (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.	0
the paper's presentation also wavers between being strong (there were some sections i read and immediately understood) and impenetrable (there were other sections which i had to read 510 times just to try and grip what was going on).	0
with another editing pass to improve language and presentation this would be a strong, relevant paper worthy of the attention of the iclr community.	0
the paper can be improved in presentation.	0
can you clarify and/or change the presentation regarding to this part?	0
good results and good analysis of the model ' mostly clear writing and presentation (few typos etc. nothing too serious).	2
summary: this paper proposes a method to learn a state representation for rl using the laplacian.	0
one usecase of the learnt state representation is for rewardshaping that is said to accelerate the training of standard goaldriven rl algorithms.	0
i have some following major concerns regarding to the quality and presentation of the paper.	0
though the idea of learning a state representation seems interesting and might be of interest within the rl research, the authors have not yet articulated the usefulness of this learnt representation.	0
for larger domains, learning such a representation using a random policy might not be ideal because the random policy can not explore the whole state space efficiently.	0
i wish to see more discussions on this, e.g. transfer learning, multitask learning etc.  in terms of an application of the learnt representation, rewardshaping looks interesting and promising.	0
on the other hand, the learnt representation for rewardshaping is fixed to one goal, can one do transfer learning/multitask learning to gain the benefit of such an expensive step of representation learning with a random policy.	0
unfortunately, i have found the presentation rushed.	0
the quality of the presentation is high and accessible.	0
the clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.	0
the presentation is mostly clear, and there is an improvement in benchmark scores compared to the baseline jtvae.	2
however, the implementation and presentation of the paper could be improved.	0
the presentation of the paper is clear and easy to follow.	2
this paper proposes a deep learning method for representation learning in time series data.	0
the goal is to learn a discrete twodimensional representation of the time series data in an interpretable manner.	0
in order to address the nondifferentiability in the discrete representation assignment, the authors propose to include an extra reconstruction loss term w.r.t.	0
the discrete representation.	0
this paper deals with an interesting problem as learning an interpretable representation in time series data is important in areas such as health care and business.	0
however, i am afraid the presentation of this paper is a bit difficult to follow.	0
the paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data.	0
on the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology.	0
detailed comments:  as also stated by the authors, the use of a 2d latent representation is completely arbitrary.	0
it may be true that a 2d embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …).	0
therefore the feeling is that the proposed structure may be quite adhoc, and one may wonder whether the algorithm would still generalise to more complex latent representations.	0
however, given this setting, there are several major issues with the presentation and evaluation, which make the paper unsuitable in its current from.	0
any standard automatic differentiation toolkit will be able to compute all the necessary derivatives, and for a paper such as this the authors can simply specify the architecture of the system (that they use a cholesky factorization representation of h, with diagonals required to be strictly positive) and let everything else be handled by tensorflow, or pytorch, etc. the derivations in sections 4.2 and 4.3 aren't needed.	0
despite these drawbacks, i really do like the overall idea of the approach presented here, it's just that the authors would need to _substantially_ revise the presentation and experiments in order to make this a compelling paper.	0
some of the presentation of the material is somewhat vague, in particular section 5. in this section a joint embedding model is described that helps facilitate zeroshot learning.	0
"the presentation is lacking in some regard and would benefit from some reworking i.e. section 5. unclear in the paper: section 5 describing the ""joint embedding model using compositional task descriptors"" is very sparse on detail."	0
'summary' a clear an interresting presentation on learning sequences distributions.	2
evaluation: the paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing.	0
strengths  thorough analysis with a good set of questions weaknesses  some peculiar evaluation and presentation decisions  introduces 'yet another' synthetic visual reasoning dataset rather than reusing existing ones i think this paper would have been stronger if it investigated a slightly broader notion of generalization and had some additional modeling comparisons.	0
presentation  basically all of the tables in this paper are in the wrong place.	0
the enorm presentation is 4 pages long (which is quite a lot).	0
the presentation of the algorithms for i) the nonoverlapping and ii) overlapping groups as well as iii) the proposed refinement are clear.	0
the following comments could help authors to improve the quality of presentation of their paper:  section 3.1 (score functions and feature tensor) is a bit short and difficult to read.	0
the paper lacks a lot in clarity and quality of presentation.	0
important issues: one of the biggest concerns is the presentation of the planning algorithm, and more importantly, a proper formalization of what is calculated, and thus a proper justification of this part.	0
9. the presentation of the paper is very dense, i would advise the authors to move certain parts to the appendix and remove the inlining of important equations to improve readability.	0
concerns regarding the clarity of presentation and interpretation of the results.	0
overall the paper suffers from a lack of clarity in the presentation, especially in algorithm 1, and does not communicate well why the assumption of different dynamical processes should be important in practice.	0
the authors claim to learn functions to compute node representations, however the representations z^u seem to be direct embeddings of the nodes.	0
if the representations are computed as functions it should be clear what is the input and which functional form is assumed.	0
"in terms of presentation, the motivation in introduction is fine, but the following section named ""notations and pseudocodes"" is confusing and has many undefined notations which makes the paper very hard to read."	0
in the presentation of rmsprop what the means?	0
while in the formal presentation in the update rule there is .nabla f(x) which is the full gradient of the objective function of the original problem.	0
if the above statements in terms of presentation, are ignored the convergence results and numerical experiments are interesting.	0
statements as 'although this work is superficially similar to ours, it differs a lot in the details' makes clear that this work is only of potential interest for a rather small audience, a tenet also supported by the density of presentation.	0
"the title of the paper in my opinion undersells the result which is not only that ""deep skinny neural networks"" are not universal approximators, but that the class of functions which cannot be approximated includes a set of practically relevant classifiers as illustrated by the figure on page 8. the presentation is extremely clear with helpful illustrations and toy but insightful experiments."	0
the presentation needs to be improved for me to find it acceptable.	0
i would therefore suggest adding experiments where authors pick the mode of the distribution and estimate an error metric such as root mean square error (rmse or psnr https://en.wikipedia.org/wiki/peak_signaltonoise_ratio ) i also found the 'marketing'/presentation of the algorithm little misleading, especially in the introduction, given that there exists another gan based imputation algorithm.	0
the focus on novelty (mentioned in both the abstract, and conclusion as a direct claim) in the presentation hurts the paper overall.	0
variational bilstm https://arxiv.org/abs/1711.05717 stochastic wavenet https://arxiv.org/abs/1806.06116 pixelvae https://arxiv.org/abs/1611.05013 filtering variational objectives https://github.com/tensorflow/models/tree/master/research/fivo improved variational autoencoders for text modeling using dilated convolutions https://arxiv.org/abs/1702.08139 temporal sigmoid belief networks for sequential modeling http://papers.nips.cc/paper/5655deeptemporalsigmoidbeliefnetworksforsequencemodeling neural discrete representation learning (vqvae) https://arxiv.org/abs/1711.00937 the challenge of realistic music generation: modelling raw audio at scale (ada) https://arxiv.org/abs/1806.10474 learning hierarchical features from generative models https://arxiv.org/abs/1702.08396 avoiding latent variable collapse with generative skip models https://arxiv.org/abs/1807.04863 edit: updated score after second revisions and author responses	0
paper summary  the primary contribution of this paper is the presentation of a novel elbo objective for training bnns which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors featured in the literature.	0
presentation is quite rough around the edges.	0
in this paper, the authors propose a new approach to representation learning in the context of reinforcement learning.	0
the method of the authors assumes that a goalconditioned policy is already learned, and they use a kullbackleiblerbased distance between policies conditioned by these two states as the loss that the representation learning algorithm should minimize.	0
"the first weakness of the approach is that it assumes that a learned goalconditioned policy is already available, and that the representation extracted from it can only be useful for learning ""downstream tasks"" in a second step."	0
but learning the goalconditioned policy from the raw input representation in the first place might be the most difficult task.	0
in that respect, wouldn't it be possible to 'simultaneously' learn a goalconditioned policy and the representation it is based on?	0
this is partly suggested when the authors mention that the representation could be learned from only a partial goalconditioned policy, but this idea definitely needs to be investigated further.	0
but thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would endup with a different intuitive presentation of their framework.	0
"from actionable representation""."	0
though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.	0
the authors should also read laversannefinot et al. (2018, corl) who learn goal space representations and show an ability to extract independently controllable features from that.	0
a positive side of the experimental study is that the 6 simulated environments are wellchosen, as they illustrate various aspects of what it means to learn an adequate representation.	0
globally, although it is interesting, fig.6 only conveys a quite indirect message about the quality of the learned representation.	0
"the authors should describe the oracle in more details and discuss why it does not provide a ""perfect"" representation."	0
much of this has to do with the presentation, which reads to me more like a summary blog post rather than a technical paper.	0
a reference would be helpful and improve the presentation quality of the paper.	0
this paper provides a good presentation of a serious problem in evaluating (as well as training!)	2
the methodology is presented in section 3. even though the outline does not seem to be complicated, the presentation in section 3 leaves me puzzled.	0
the paper introduces an adaptation of the scattering transform to signals defined on graphs by relying on multiscale diffusion wavelets, and studies a notion of stability of this representation with respect to changes in the graph structure with an appropriate diffusion metric.	0
the proposed representation seems to provide benefits compared to the previous work of zou & lerman, particularly regarding computational efficiency, as well as stability with respect to a metric that is perhaps more useful, though there is a dependence on the graph topology through the spectral gaps.	0
in addition, the experiments on author attribution and source localization suggest that the resulting representation remains discriminative, in addition to providing stability to changes in graph structure.	0
i find that these contributions provide an interesting advance in theoretical understanding of graph convolutional networks from a stability perspective, in addition to introducing a useful nonlearned representation, and am thus in favor of acceptance.	0
yet, in euclidian scattering, the same representation is applied to a deformed signal and the original signal, and stability is measured with the euclidian metric.	0
the paper is heavy on terminology from wavelets and harmonic analysis, a more detailed presentation of diffusion wavelets and related concepts such as localization would be beneficial.	0
overall, this is an original work with clear presentation.	2
"the first paragraph in section 4 (""the presentation flows ..."") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of section 4 or to section 5."	0
comments: i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the numerical evaluation.	0
3) section 2 is devoted mostly on the formal presentation of algorithm amb.	0
the only question i have is how does this hold on a real quantum computer such as ibmq/rigetti quantum computing etc.. or even under a noisy simulator although the paper is sounds and it is a good idea, the presentation is a bit lacking.	0
this paper proposes an unsupervised approach to learning node representations.	0
1. presentation of section 3.2 can be improved.	0
6. the learned node representations possess a clear clustering structure (figure 3).	0
while the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is selfcontained.	0
clarity: the paper is mostly clear, though the presentation could be improved in various parts, as i mention below.	0
overall, i would recommend accepting the paper for presentation in the conference.	0
second, instead of using a single cnn as discriminator, the authors use multiple embedded representations.	0
(2) presentation: this paper is carefully written and easy to follow.	2
the general construction and presentation of the algorithms are generally clear, and pretty complete.	2
if not, the authors should be more explicit in their presentation  what is the complexity of the proposed methods?	0
these two distinct subjects are discussed in a mixed way, which reduces clarity of the presentation.	0
comment:  the presentation would be too plain to find what parts are novel contributions.	0
every part of presentations seems originated from some past studies at the first glance.	0
this submission presents a simple model for sentence representation based on maxpooling of word vectors.	0
when compared to standard sentence representation models, the proposed approach has very good performance, while being very efficient.	0
moreover, if u is the identity, i don't think that the reader really need this eq... i have several questions and remarks that, if answered would make the quality of the presentation better: ' in infersent, the authors reported the performance of a randomlyinitialized and maxpooled bilstm with fasttext vectors as the input lookup.	0
strengths:  good coverage of related work  clear presentation of the methods  evaluation using established semeval datasets weaknesses: 1. it is not entirely clear what is the connection between fuzzy bag of words and dynamax.	2
4. some relevant work that is missing:  de boom, c., van canneyt, s., demeester, t., dhoedt, b.: representation learning for very short texts using weighted word embedding aggregation.	0
“… a lack of certain constraints on the generative capacity of current neuralnetwork based generative models make it challenging to infer structure from their latent generative representations.” what does “a lack of certain constraints” mean?	0
to conclude, the paper presents quite good qualitative results on the celebahq dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation.	0
this is a very opaque presentation that we cannot see how you solved the integral.	0
clarity: i thought the presentation was tolerable.	0
the presentation of their methodology is clear, so are the numerical studies.	0
i think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired.	0
https://arxiv.org/abs/1810.01392 pros:  interesting observation of density modelling shortcoming  clear presentation cons:  lack of a strong explanation for the results or a solution to the problem  lack of an extensive exploration of datasets	2
the presentation is clear and easy to follow.	2
my only concern is the incremental nature of the method, which is only partly offset by the good presentation.	0
'pros:'  easily accessible paper with good illustrations and a mostly fair presentation of the results (see suggestions below).	0
on top of that the presentation could also use some improvements.	0
the presentation of the method itself is not selfcontained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper.	0
the presentation could be cleaner and clearer, the paper contains solid work and contributes to an interesting perspective/interpretation of deep networks.	0
the presentation is reasonably clear, although somewhat cluttered by a large number of subscripts and superscripts, which could be avoided by using a more modular formulation; e.g., in equation (1), when referring to a specific layer l, the superscript l can be dropped as it adds no useful information.	0
nevertheless, i think the theoretical presentation is good and i believe the manuscript has very good potential.	0
supervision is provided in the form of class probabilities and the learning algorithm learns lowdimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities.	0
the representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lowerdimensional space.	0
the idea is wellmotivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (i will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials).	0
i strongly recommend acceptance and have only a couple of comments on presentation.	0
in conclusion, it is the reviewer's opinion that significant rework in term of presentation and strong improvement of the experiment section to make the case for the game optimizer.	0
the paper is well written and the presentation is clear.	2
2. the presentation is sometimes confusing.	0
overall, i think explicitly taking into account different feature types in the lstm cell update rules is interesting, but the contributions of this paper compared to tlstm are not significant enough for acceptance, and the presentation can be made more clear.	0
then again, perhaps this is more of a presentation issue, as it may be the case that the authors did not manage to highlight clearly enough the technical challenges they needed to overcome to prove their theoretical results.	0
at the same time, i found the presentation of this work to be a little misleading.	0
i suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologicallyinspired learning models.	0
overall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.	0
the technical presentation of the work by the authors starts only at page 5 (taking less than a full page), after several pages of imprecise presentation of previous and related work.	0
i don't think this is a necessity of the material, as the urnn paper (http://proceedings.mlr.press/v48/arjovsky16.pdf) describes very similar concepts with a much clearer presentation and background.	0
the problem is of great importance but the theoretical results and presentation contain many issues that make the paper unfit for publication.	0
the current presentation angle seems a bit off though.	0
for the better presentation of this paper, it will be great if a background section (with math) on shaley value is included.	0
general comment  in general, i believe more experiments and better presentation is needed for this paper.	0
strong aspects:  combination of evolutionary search over morphology and hyper parameters with rl for controller  introducing analysis tools from game theory to the community  entertaining video  good presentation/clear writing weak aspects:  authors do not mention that they will make the code public  presentation is based on the best of 50 runs!	0
"there are several issues with the presentation of this work, that make it incredibly difficult to identify a technical contribution: 1. overreaching statements without details to backup: you are writing the paper as if you are learning a ""rl algorithm"" that can be used to quickly learn new tasks."	0
finally, the presentation of the paper is marginal.	0
the presentation quality is not satisfactory.	0
result presentation (figure 3/5 & table 1) is very different for different attack scenarios, which makes them hard to compare.	0
the contribution could have been stronger with a more detailed evaluation and better presentation.	0
the paper still has issues with presentation, and weak comparisons to earlier methods.	0
this paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture.	0
i also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way.	0
vision research, 2015. secondly, the presentation is not perfect.	0
a symbolic representation is mentioned in the introduction section.	0
=========== update: authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.	0
[i would have given the paper a rating of 7 were it not for these issues] regarding the presentation of the content, i found this paper generally easy to follow and the arguments sound.	0
i also realized that i misunderstood one of the sections, and i encourage the authors to improve the presentation to (1) present the significance of the experiments more clear, (2) not overstate the results, and (3) emphasize the contribution more clearly.	0
pros: presentation of new application of representation learning models construction of a new dataset to the community for binary software vulnerability detection the proposed model shows a good performance cons: the presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.	2
however, while the method itself is promising, the weak baselines (in particular, the lack of evidence disentangling the benefits of larger scale / more frames vs the benefits of the proposed method) and unclear presentation make me unable to yet recommend the paper for acceptance.	0
the main problem which i see is the presentation of learning curves as a function of training steps rather than acting steps.	0
evaluation  the writing and presentation of the paper are in general well carried, except some part seems a little unclear, taking me quite a while to understand.	0
the technical presentation could use some improvement.	0
there are basic concerns about how implicit and explicit rewards can be combined, and the technical presentation needs some improvement.	0
the presentation of the core idea is clear but imo there are some key missing details and experiments. '	0
while it has technical merit, the presentation of this paper make it unready for publication.	0
section 5: the presentation of results into table 1 and table 2 is quite odd.	0
pros:  a simple idea with good empirical results that would be of interest to the community cons:  (extremely) unclear presentation which hinders the message of the paper.	2
in particular, the presentation of the model would be more clear if the graphical model can be specified.	0
a major weakness of this paper is the presentation.	0
the quality of writing of the related work section is worse that the rest of the paper.	0
5. overall, i think this is a good paper, gives a good overview of an important problem; the matching networks idea is nice and simple; but the paper could be more broader in terms of writing than trying to portray the success of discern specifically.	0
quality of writing.	0
partial conclusion: the description of the method contains relevant information and is functional, but the writing could be improved. '	0
i generally found the writing to be clear and the idea to be elegant; i think readers will value this paper.	0
the writing of this paper makes it a bit hard to understand what the novel contributions of this paper are, and how the proposed method should go beyond the two problems that it solves.	0
cons:  writing could be improved, as the methodology currently reads as a summation of facts, and some parts are written out of order, resulting in various forward references to components that only become clear later.	0
while the writing is good i think that some light restructuring would improve the flow.	0
cons: 1. the paper in its current form isn't very polished yet and clarity can still be improved in several ways: ' there were more than a few spelling and grammatical errors, please proofread the work and improve the writing.	0
the writing quality is rather good.	0
cons:  the writing, in my opinion, needs to be improved [see my comments below].	0
the writing is not very clear at places, and it does not help that the references being merged with the text.	0
other: the writing and grammar of the paper needs serious revision.	0
i have multiple concerns with the papers: (i) the writing is informal and the ideas are not well explained.	0
they are many editing problems and the english is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.	0
besides, i found the writing needs to be improved.	0
another issue i suggest revising pertains to the writing.	0
the writing of the paper needs a major improvement.	0
a few points regarding the writing: 1) figure 1 is impossible to understand, especially that zero explanation is given in the caption.	0
nits/writing feedback:  there is no need for such repetitive citing (esp paragraph 2 on page 2).	0
cons 5. the paper wastes valuable space writing out in detail the equations for backpropagation in a standard feedforward mlp.	0
specifically, such impression mainly comes from the writing in sec 3.2. when discussing details of the proposed method, authors keep referring to [a], indicating heavily that authors are applying an existing method, i.e. yet another application of [a].	0
it reads as if it was written in one pass with no editing or rewriting to clarify contributions or key points, or ensure consistency.	0
there is also a statement in the paper that is problematic but can be fixed by rewriting.	1
the writing is very clear and experiments are quite thorough.	2
it is hard to understand why the column size is w .times h .times w .times h only writing it concatenation.	0
in section 6, the paper could give more details about random walk concepts such as 'stochastic', 'stationary distribution' etc., as it would make the paper more accessible (i am familiar with these concepts but since the writing of the paper is of high quality in the other sections, i am convinced this would improve its impact, and attract more readers).	2
figures 2 and 3 are the central architectural choices, but the writing around them does not clarify why all the choices have been made, and what the implications are.	0
the biggest problem with the paper is that it is hard to read and has several writing issues: 1. it is not self contained.	0
response to rebuttals the writing of the introduction has been greatly improved.	0
"a lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me:  ""... most of these sucesses have been demonstrated in either video games or simulation environments."	0
the quality of the writing is good, apart from some odd expressions on page 7. the figures are of good quality and sufficiently legible.	0
there is no clear and proved statement despite the suggestive mathematical nature of the writing (conjecture, proposition).	2
the paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.	0
"2. it try to link it to information theory but most of study is just empirical (which is fine, but avoid it can simplify the writing and make it more readable), e.g. "" according to information theory and the attention mechanism (bahdanau et al., 2014), it is clear that we.."" i agree with the intuition but how it can be ""if and only if""?"	0
"distributions of tensors > ""distribution of tensor elements"" this comment also applies in a number of other places, where the writing refers to the marginal distribution of values taken on by entries in a tensor as the distribution over the tensor."	0
the paper might contain some interesting ideas, however, i am quite confused about the paper due to lack of clarity in writing.	0
there are some typos and errors in the writing.	0
the writing of the paper was not very clear and pretty hard to follow.	0
this is straightforward by writing down the two transformations.	0
the reviewer suggests taking a more humble and scientific writing style and instead highlighting all sources of uncertainty, assumptions and missing knowledge, etc., in this setting.	0
this is one of the best papers i reviewed so far this year (iclr, nips, icml, aistats), in terms of both the writing and technical novelty.	0
writing: the author provided sufficient context and did comprehensive literature survey, which made the paper easily accessible to a larger audience.	0
2) the writing needs to be significantly polished.	0
first, please simply the writing both in terms of general logic and language.	0
what is the benefit of explicitly talking about gaussian complexities and delineating theorem 4.2 when the same can be achieved by writing a much simpler form?	0
i found the writing mostly clear, except for the following issues:  the introduction has not made it crystal clear that the considered paradigm is different from e.g. dagger and gail in that expert demonstrations are used at the inference time.	0
as to the writing style, there are many places where the writing is not quite clear.	0
both the writing and the experiments could be strengthened, per the suggestions above.	0
other comments: 1) though the writing is generally very good, there are a few exceptions:  the second paragraph in the intro becomes a list of related work from the point where dec is introduced.	0
the paragraph after eq 3 needs some rewriting  the explanations around and including equations 5 and 6 were quite poor: .pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in eq 6 to avoid ambiguity 2) the reference formatting is wrong (i.e. cite is used everywhere instead of citep) 3) i thought the motivation for the approach in the intro was very good 4) as the seemingly most related work, it would be good to elaborate more on the goyal et al paper and the differences of your approach to theirs.	0
the writing in this paper is somewhat problematic.	0
to conclude, because of the writing, limited novelty, and limited experiments, i think this paper currently does not pass the bar for iclr.	0
the idea is interesting and the writing is clear.	2
detecting adversarial examples is not the same problem to detecting outofdistribution samples, and the writing of the paper should be changed to reflect this more.	0
# clarity the writing in this paper needs significant improvement.	0
# summary pros: ' useful dynamic batching trick that can lead to speedups ' empirical evaluation compares to two existing techniques and breaks down individual components of runtime cons: ' no critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage ' some questionable statements and assumptions ' lack of formalization and clear definitions ' paper reads longdrawnout, subpar writing hurts readability	0
if the paper is reorganized and the writing cleaned up i would be happy to increase my rating because i think the work is good.	0
"pros:  proposed regularizer is wellexplained and seems to work well, ablation study is helpful cons:  the intro section is almost completely repetitive of section 3 and could be significantly shortened, and make more room for some of the experimental results to be moved from the appendix to main text  some wording choices and wordiness make some sentences unclear, and overall the organization and writing could use some work specific comments / nits: (in reading order) 1. i think the name ""selfless sequential learning"" is a bit misleading and sounds like something to do with multiagent cooperative rl; i think ""forethinking"" or something like that that is an actual word would be better, but i can't think of a good word... maybe frugal?"	2
cons: while i appreciated the writing clarity of the paper, the paper misses the whole point of defensive ml research: in the model poisoning case, a minimal requirement for a defense mechanism is to be formally proven 'whatever is the behavior of the attacker' (within the threat model).	0
additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in section 3; (2) omitted details, e.g., how much overlap exists between kernels (figs.	0
weaknesses:  paper writing is good until section 3.1. this section is very confusing.	0
however, the writing of the paper at the current stage is rather subpar, to the extend that makes me decide to vote for rejection.	0
2. the rigor in the writing can be improved.	0
overall, i found the writing very clear, the main idea sound, and paper generally well executed, but i have serious concerns about the significance of the contributions that lead me to recommend rejection.	0
most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions.	0
the writing is still confusing.	0
# clarity the writing in the paper is passable.	0
page1par2: writing specifications for programs is never harder than writing the program  a program is a specification, after all.	0
what you mean is the hardness of writing a /correct/ and exact spec, which can be substantially harder.	0
overall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).	0
another point about this section is that poor writing does not help understanding some points.	0
"typos:  p4: we can easily counting (include borders) => including is provide => provided are less lessvisited states: quite inelegant  p7: in montezuma's revenge, comparing => comparing where they encourage => remove ""where""  p8: recallcan => recall can the problem of reach goals => reaching it succesfully reproduce => reproduces the last paragraph of section 4.2 needs a careful rewriting, as long sentences with parenthese in the middle appear to be some draft version."	0
e.g. the writing makes it seem like proposition 2 and theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size.	0
overall: pros: 1. clear writing 2. good motivation description.	2
"then p(z|xy), which is partial scoring, corresponds to p(xyz)/p(xy), which can be viewed as ""discounting"" or ""normalizing for"" the probability of putting y in place of the pronoun given the context x. for wsc, i think one of the goals in writing the instances is to make the ""true"" p(xy) approximately equal for both values of y. the language model will not naturally have this be the case (i.e., that p(xy) is the same for both antecedents), so dividing by p(xy) causes the resulting partial score to account for the natural differences in p(xy) for different antecedents."	0
pros:  the writing is clear.	2
here are some examples of the writing that feel awkward:  consequently, people start to develop nonautoregressive neural machine translation (nart) models to speed up the inference process (gu et al., 2017; kaiser et al., 2018; lee et al., 2018).	0
writing and presentation suggestions/questions: 1. if the proposed method is a breakthrough, i am fine with the title but i think the experiment results tell us that wasserstein is not all you need.	0
minor writing suggestions: 1. in section 3, present the full name of cite 2. if you put some important equations to the appendix (e.g., the definition of sppmi_{alpha,gamma}), remember to point readers to the appendix.	0
the writing and structure of the paper can be improved.	0
overall this paper gives a useful but incremental improvement over the deep value network proposed by gygli et al. 2017. however, the writing should be substantially improved to make the paper more selfcontained and to include missing experiment details.	0
2) the writing is poor and hard to follow.	0
i am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but i am not overly concerned about this since proper due credit is assigned in the paper.	0
perhaps a bit more discussion about the advantages of the proposed writing scheme could go a long way, since as it stands now, the paper simply claims that this past work “only considers the pairwise relationships between the current data instance and each individual memory”, and i’m not sure how much substance actually underlies this difference.	0
this places particularly tough demands on the rlbased writing mechanism, which will undoubtedly face problems with temporal credit assignment if: (a) the time between encoding and retrieval is long, and (b) there is high reward noise in the intermediate time.	0
the writing must be improved.	0
in addition, the writing need improvements.	0
but in the writing, the paper describes the setting as a potentially different environment at every timestep.	0
the authors are suggested to improve the english writing.	0
for example, what do you mean by writing {.theta^{'r}_k  .theta^{swr}_k}?	0
what do you mean by writing the equation (4)?	0
"i would have preferred something that makes it clearer that updates are constrained in some way as ""writing"" implies that the location is constrained, rather than the update minimizing a loss."	0
the writing is clear and concise, with all the math properly introduced.	0
the writing is generally clear but i have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).	0
but i think the writing needs significant work for clarity and completeness, and there needs to be many more baseline comparisons.	0
4. the writing of this paper needs improvement.	0
both theory and writing need a major revision.	0
for writing quality, the authors' explanation of their environment is quite clear.	0
one of the problems of using replay buffers in onpolicy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue.	0
both the organization and the technical writing of the paper are poor.	0
if you claim that the proposed framework can generate better images, at least the framework should be compared to the latest stateoftheart gans (e.g. spectral gans, etc.) ' the writing is not polished.	0
there are some detailed questions that are unaddressed or unclear from the writing.	0
figure 2 is nice as a sales pitch, but it doesn't replace actually writing down the objective functions used for training!	0
comments:  the paper is overloaded with notations and the writing is not very smooth.	0
clarity: the paper writing is mostly clear.	2
"also please check some writing problems: > introduction: ""to acquire a generative function mapping a latent space (such as rn)"" > difficult to read, rephrase."	0
clarity: some sections of the paper were harder to digest, but overall the quality of the writing is good and the authors have made efforts to present examples and diagrams where appropriate.	0
also, the paper writing needs to much improved.	0
"the writing is mainly fine, though some sentences are written poorly and would benefit from a revision, e.g., 2nd paragraph, ""but none of them is suitable to train deep neural nets (dnns)."""	0
the analysis can be more intuitively related to practice by avoiding the rescaling of the loss into the interval [0,1] and writing expressions in terms of a maximum bound on the loss l_max.	0
however, the writing still can be improved for rigor, consistency, typos, completeness, and readability.	0
predict'ive' mean square error (page 2) introduction can use some rewriting.	0
the authors also made little to no effort in writing to ensure the clarity of their paper.	0
in terms of writing, the first few pages tend to be repetitive yet vague about what exactly will be done (generally ok for introduction, but a bit too vague).	0
further, writing can be improved, and i would encourage a thorough revision since there are typos making the paper a bit hard to follow.	0
cons:  writing can be improved.	0
in general authors explain details of the previous techniques as they use them too which is a good writing technique and improves the readability.	0
i would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.	0
the overall writing of the paper could be improved.	0
clarity  my understanding is that the writing of section 3 and 4 can be (and should be) further polished.	0
"the writing could also use some improvement for a future iteration, i've listed a few points below: pg.1, neither brown & sandholm nor moravcik et al use ""rl algorithms"" pg.1, finn et al unmatched ) pg.1, ""a scalar reward despite observed or not"" > ""a scalar reward whether observed or not"" pg.2, ""either the range of"" > ""both the range of"" (and this sentence needs further cleanup) pg.2, ""which records the pathing of players"" ??"	0
the writing needs to be significantly improved.	0
"also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.)."	0
con  the writing is a bit dense in places, e.g., the discussion of baselines is a bit hard to read.	0
45. and they both require a lot better writing.	2
the writing is also clean and clear.	2
review: — the writing is not sufficiently clear and a lot of the ideas are hard to follow (the sections 3.2 and 3.3 which should cover proposed methods are only a paragraph long each, have no loss functions and no architecture descriptions/diagrams) — the ideas presented are only derivative and are not sufficiently novel for the venue — the experimental section is incomplete having results on one dataset and not enough stateofthe art baselines.	0
a general recommendation on writing: try to limit the content to relevant details.	0
i strongly encourage a rewriting to emphasize that this is a repetition of previous knowledge.	0
the authors also took great care in writing details for important parts of the experiments in the appendix section, and open sourced the implementation to reproduce all their experiments.	0
given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks.	0
even without the code, i felt i might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.	0
# criticism  the proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks.	0
beyond the highlevel lack of clarity about the contribution of the paper, the writing lacks precision and rigor, and many things are undefined (though one can figure them out after reading many times back and forth).	0
in short, a promising direction, but the contribution of the paper appears to be over claimed and the writing of the paper needs significant improvement before the paper can be accepted for publication.	0
quality: the introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing. '	0
• typos and writing style: 1. the citation style is most often used incorrectly.	0
the writing is very poor.	0
not only is the writing hard to understand (some sentences lack a verb!	0
additionally, due to the lack of clarity in the writing and lack of mathematical rigor, theorem 1 does not seem to be true as stated.	0
3. the writing is not clear.	0
with respect to different sections: • [gvf for pg]: rewriting the policygradient theorem with 2 gvfs, the authors propose to improve on the baseline algorithm with bootstrapping on both the critic and the actorgradients.	0
therefore, i’m unable to appreciate the importance of writing optionvalue in gvfs and/or deriving an algorithmic improvement out of that.	0
the writing is clear and the idea is original.	2
the writing and presentation of the paper are clear.	2
"minor writing suggestions: 1. in the 4th paragraph of section 3, ""shown in equation equation 2"", and bitwise should be elementwise 2. in the 4th paragraph of section 4, i think the citation after alternating minimization should be arora et al. 2016b and aharon et al. 2006 rather than arora et al., 2016a 3. in the 2nd paragraph of section 6.1, (jeffrey pennington, 2014) should be (pennington et al., 2014)."	0
third, the writing in the paper has some significant lapses in clarity.	0
the writing is not very clear.	0
the writing is confusing, the approach is not very novel, and the experiments are fairly minimal.	0
the writing was clear.	2
the paper also has the following weak points: 1. the writing is a bit rough throughout, though not to extreme distraction.	0
# weaknesses ## writing i have to start with the most obvious one.	0
in addition to the lack of novelty or new insights, the writing needs serious attention.	0
great work on explaining the motivation and the model  the writing is clear and explains background knowledge extremely well.	2
the manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.	0
moreover, there are some issues in the writing, e.g.,  classically, as far as i know, rl is not considered to be a metaheuristic, although i understand that someone could make the case for it.	0
using a generative model as the surrogate distribution for kernel twosample test is novel  an important and new application of deep generative models  strong experiments on synthetic and realworld time series data sets  very clear writing and explanation of the idea  reply sample segments from both directions (past and future) while in the practical setting, cpd is usually sequential and in one directional  lack theoretical understanding of the limit of the neuralgenerator in the kernel twosample test	0
the exposition is not at all clear and needs substantial rewriting.	0
a few writing suggestions for the authors, for next time: this paper does not follow the typical flow of an academic paper.	0
comment on writing:  in section 3.1: the dimension of the tensor should reflect the meaning (vocab size, embedding size or the number of documents) rather than numbers.	0
clarity: while the ideas in the paper were easy to follow, there are a number of problems with the writing.	0
but the execution is limited and the writing poor with critical details lacking.	0
3) weak points: the writing is sloppy.	0
writing would benefit from laying out intuition beyond both the model and the experimental results.	0
if the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.	0
the writing and organization of the paper are clear.	2
please consider this rubric when writing your review: 1. briefly establish your personal expertise in the field of the paper.	0
the writing in the paper needs improving.	0
the writing is relatively easy to follow.	0
the writing is in general clear.	2
the writing of the manuscript needs significant improvement.	0
as i mentioned, the paper has novel and interesting ideas, but it would be greatly improved with some important rewriting.	0
minor comments there are some places where the writing could be cleaned up.	0
"and just stylistically, i would avoid writing ""we could obtain"" and simply write ""we obtain""."	0
the writing can be sometime a bit imprecise.	0
writing: the paper does not seem to be polished.	0
the organization could be better; some parts are vague and difficult to understand; the writing could be improved to be more clearly demonstrate the contributions of this paper.	0
they have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the appendix section), and committed to opensourcing the platform post publication.	0
my recommendations to improve the article: (1) writing  i really enjoyed this work, but frankly, the writing is horrible.	0
in contrast, older works related to populationbased rl training like [2], or rl in games like [3] are examples of clear and understandable writing.	0
i highly recommend you give the draft to someone outside of your team, who is sufficiently isolated from this project (or perhaps to a professional writer if your lab has one), to go over each paragraph, and make the writing more clear.	0
currently it pains me that i can only assign a score of 5 of this work (note: this has been since revised upwards to 7 upon reading revision after rebuttal period), since i don't think the current writing is up to standards.	0
in my opinion, it deserves a score of 78. if you work on points (1) and (2) and submit a revised draft with much better writing, visualization, figures to explain the work, i'll happily revise my score and improve it by 13 points depending on how much improvement is made.	0
"just writing ""j >i"" is enough here."	0
there are a few areas where the writing could be more clear: .omega_t is introduced right after equation 5, but it is unclear what is that parameter.	0
review: the writing could be clearer.	0
i would suggest making this section more rigorous and writing out everything carefully.	0
pros: 1. the defense technique does not require knowledge of the attack method cons: 1. the paper is incredibly difficult to understand due to the writing.	0
while the writing overall is clear and the motivation wellwritten, there are many issues with the modeling and experimental work.	0
clarity: the writing is clear in general.	2
even though the need of handling arbitrary input size is an interesting problem, i have several major concerns about this paper:  one of the main problems of this paper is its presentation, both the writing and methodology.	0
the writing is very poor, with continuous errors and many wrong definitions and concepts.	0
auhtors definitely need to improve their writing and the layout of the paper.	0
but there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc. the authors also rely mostly on the fid metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.) the writing is understandable for the most part, but the paper seems to lack focus  there is no clear take home message.	1
while the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to iclr and hard to judge/interpret the extensive simulation results.	0
"it is comparable to and to my ears not better than existing polyphonic systems such as the ones below (links to sample audio are provided here): bachbot  https://soundcloud.com/bachbot (liang et al 2017)  tied parallel nets  http://www.hexahedria.com/2015/08/03/composingmusicwithrecurrentneuralnetworks/ (johnson 2015, ref below) performancernn  https://magenta.tensorflow.org/performancernn  (simon & oore 2017) ..others as well.. clarity  some of the writing is ""locally"" clear, but one large, poorlyorganized section makes the whole thing confusing (details below)."	1
in terms of writing, i have difficulty understanding some details about the method.	0
but i do not suggest acceptance, unless the author can improve the writing and include more experimental results.	0
this should be stated clearly and even better, i'd recommend writing down the gradient update rules for the interpolator parameters (you can put them in the appendix).	0
quality and clarity: the writing is good and easy to read, and the idea is clearly demonstrated.	0
1) writing: needs major work.	0
again this could just be lazy writing.	0
even ignoring the holes in writing, a single task and empirical evaluation on one dataset with precision/recall/f1 scores at different k is not evidence that the proposed method is verified.	0
it also seems that the paper could use some further polishing in both writing and presentation.	0
also the writing itself needs a thorough revision.	0
clarity: the writing of this work is mostly easily to follow.	0
pros:  the model is interesting and the motivation is quite clear  analysis is quite nice  writing is quite clear and decent cons:  extremely lacking experimental validation  there are literally no baseline models, no numbers or any kind of quantitative analysis.	2
strengths:  clear writing.	2
in general the paper writing and reporting on the experiments sounds adhoc and not well thoughtout. '	0
the writing is clear and the model is easy to understand.	2
thank you for a pleasurable and informative read, i consider the writing and structure of the paper to be coherent and well written.	0
the writing is not very clear, especially around equations.	0
furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so i am still confused about the exact implementation of the attribute features .phi(x, m) for example.	0
also, it will be better if the authors can improve the paper a little bit with the writing.	0
the writing is a bit repetitive at times and i do believe the algorithm can be more tersely summarized earlier in the paper.	0
in addition, the writing of section 4 is not very clear and easy to follow.	0
if the author can improve the writing in experiments and answer the above questions, i would support for the acceptance.	0
3) some writing issues: it would be better to 'clearly' demonstrate the final accuracy of different models (i.e. resnet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text.	0
the reason that i give such a rating is that of the confusing writing. '	0
the writing is clear and accessible, except possibly for the architectural details described in section 2.1.2, which do not seem very important.	0
the paper would be in a better shape if more time is spend to improve the writing, provide more details on the method, and extend the experiments.	0
the writing is generally clear, but the paper should be checked for typos.	0
the overall writing style is perfectible.	0
the composition of the paper and its writing makes it an easy read.	0
while overall the writing quality of the paper is high, the paper itself is a strong rejection.	0
writing and presentation: the quality of writing should be improved.	0
### comments about writing ### the findings are in general interesting and inspiring, but the explanations need some further improvement.	0
in particular, the writing lacks some consistency and clarity in the wordings.	0
however, in my opinion, writing and organization of this paper should be much improved as a conference paper.	0
overall, this work is presented in a fairly clear and logical manner, and the writing is easy to follow.	2
clarity:  overall, the writing can be improved via proofreading and polishing the sentences.	0
the submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.	0
the writing of this paper also needs to be improved significantly.	0
the quality of the paper writing could be improved.	0
the writing of the paper is not clear.	0
the writing is quite clear.	2
i found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.	0
"(5) several small comments regarding writing: (a) is the final classifier layer denoted as or in the third paragraph of ""definitions and notations""?"	0
writing.	2
2. lots of writing issues: 2.1 many things are not explained transparently enough at best (or major overclaim at worst).	0
paired with the apparent disinterest of the authors to cite recent and older literature executing strongly related underlying ideas combining neural networks with auxiliary latent variables, i can only recommend that the authors significantly change the writing and the attribution of ideas in this paper for a potential next submission focusing on multitask learning and clarify and align the core ideas in the theory sections and the experiment sections.	0
# clarity although the writing is grammatically well done, i found it difficult to follow the explanation of the proposed method.	0
clarity and quality of writing.	0
my major concern is the writing.	0
if authors can significantly improve the writing, i am happy to reevaluate my comments and increase my rating.	0
correction and suggestions writing: ' it is better to introduce an additional , and for the vanilla translation accuracy (e.g., ) so that most formulations in section 3 can be largely simplified. '	0
preliminary evaluation there are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization’’ capabilities of the classifier trained to remember train vs val images is not clear in general.	0
it might be a very good paper if the writing could be improved.	0
the contribution of this paper is almost trivial which is worsen by truly bad writing and questionable experiments.	0
it is not hinder to understand the content; however, the writing can be improved by proofreading and correcting a few grammatical errors.	0
to do this well the writing of the paper must proceed very carefully, and this submission falls a little flat here.	0
this could all be addressed with a change in the pitch and tone of the paper, but the authors should ask themselves “what are we adding beyond what can be found in the work of sutton et al (2011), modayil et al (2013), white (2015), and sutton’s numerous writings on predictive approaches to ai?” the connection between nexting (and thus gvfs) and successor features is well known, while theorem 1 follows directly from the original policy gradient theorem.	0
of course, an exhaustive discussion is not expected, but the writing makes it sound like there hasn't been much work at all.	0
as i was asked for an emergency reviewer for this paper rather late, i refrain myself from reading at other reviewers comment when writing this review for an unbiased judgment.	0
evaluation  the writing of the paper is in general ok, but reading the introduction that categorizes the ssl by three streams seem somehow unnatural to me.	0
the writing is organized poorly and the formulas are sloppy and scattered.	0
i found the paper somewhat hard to parse owing to excessive use of notation and verbose writing.	0
overall, i find the writing really a pain to parse.	0
clarity: the writing is good.	0
2. the writing of the paper needs to be improved.	0
weaknesses: the writing quality is somewhat weak.	0
overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.	0
> section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.	0
3. some writing mistakes:  table 1: the deep'' autoencodr mixture'' clustering (damic) algorithm.	0
the writing of the paper should also improve to make the paper more understandable and easier to follow.	0
i would encourage the authors to significantly enrich the content of this writing before considering resubmitting to another venue.	0
this is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like minibatches sizes , value of .lambda in the h derivation, how one calculates the .sigma^2_g within the algorithm presented in the appendix.	0
[summary] i think this is good workneat idea, nice results and clear writing.	2
i appreciated the openness of the authors to discuss the limitations of their result in theorem 4.3. the writing is generally good, up to some minor typos.	0
right now, this aspect remains hidden due to the writingthe authors should discuss this assumption more honestly in the paper.	0
i believe that a paper focusing only on policy evaluation should have sufficient merit for publicationbut then again, rewriting the paper accordingly should be a bit too much to ask for in a conference revision.	0
please fix or clarify eq. 1. i think you might clarify notation by just writing yhat(h(x)) if you mean the predicted label of some example as done by your nns.	0
comments about presentation: the writing of this paper needs to be improved.	0
also, explicitly writing the highlevel chain of computations from o_t and z_{t1}^ to o_t^ and s_t^ would be extremely useful.	0
my biggest complaint about the paper is the writing, which does not introduce and present ideas in a clear sequential manner, making the paper hard to read.	0
the model then trains on both model parameters theta and on the noise itself in an effort to achieve adversarial robustness quality  the empirical results appear sound, which suggest that multiplicative trained noise is a sensible approach  the writing that leads up to these results is in my opinion problematically vague, as it makes a variety of loose claims and theoretical connections and uses imprecise language.	0
minor points: the paper is at 10 pages, and while it is wellwritten, the writing is verbose and could be use tightening.	0
the writing and organization can also be improved.	0
the technical approach (combining vae vectors to make new shapes) is not particularly novel[ overall: the paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.	0
to be honest, i don’t like your writing style.	0
clarity: ' the writing of the paper was clear for the most part, however the experimental section could have been clearer.	0
moreover, in spite of the authors writing that their goal is “completely different” from [lee at al 18a, ma et al 18a], i found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.	0
pros:  the writing is mostly clear and easy to follow.	2
in general i find the writing poor, as it is not exactly clear what the focus of the paper is  the evaluation or the new attack?	0
third, writing q_z .sim q(z | x) seems strange to me  q_z is a distribution, and i don't believe that q(z | x) is a distribution over distributions, so how are you sampling a distribution (q_z) from q(z | x) as suggested by the use of the .sim notation?	0
the parer is in general interesting, however the clarity of the paper is hindered by the existence of several typos, and the writing in certain passages can be improved.	0
the writing isn't clear, especially in the introduction.	0
however, paper writing seems to be done in a rush, and i wish the description of the approach can be significantly improved.	0
while the proposed game is interesting and the algorithm is reasonable, the writing of the paper can be improved.	0
the writing is not organized enough and it takes many backandforth rounds of checking during reading to find out about certain details that are given long after their first references in other contexts.	0
the writing has improved, but still has stylistic and grammatical issues.	0
i encourage the authors to iterate more on the writing, and get the paper proofread by more people.	0
the writing is also very unclear.	0
instead of writing the chronological story of what you did, instead you should explain the problem, explain why current solutions are lacking, and then present your own solutions, and then quantify the improvements from your solution.	0
however, the authors way of writing makes it appear as if their work only differs in some details.	0
high level comments: ' clarity: in terms of language and writing style, the paper is written very clearly and easy to follow.	0
current, the method appears to be great, but the writing quality of the paper is not yet there.	0
while the paper does a thorough job in explaining the metrics used and in detailing the procedure followed, i would have liked to see the following the details in the paper, 1)number of unique word tokens in bengali, cebuano etc. while clicking on the reference link to muse provides these details, including the same in the paper, would make the writing wholesome.	0
the writing is clear and and focused, and the experiments are careful and rigorous.	2
the main weaknesses are poor writing, and that some details of the implementation required to reproduce the results are missing.	0
as above, my other main issue is with the writing, there are many examples where i would suggest improvements: this work could be improved greatly by copy editing for english grammar.	0
overall i find the idea to be interesting and fairly novel, and commend the authors for the fluid writing style.	0
5. the writing of this paper needs further improvement and parts of this paper is not clearly written which makes it challenging for readers to follow the authors' ideas.	0
this function might seem a little ridiculous, but continuous or even lipschitz version of this counterexample could be constructed by smoothing things out around , and a function such as this could be obfuscated by writing it down with a long, complicated expression making it hard to identify by inspection.	0
i found the paper very difficult to read; not so much because of the topic, but due the writing.	0
doc inputs could be covering different aspects of the review subject (heterogeneity among the input docs, including topics, sentiment etc), or they could have very different writing styles or length compared to a summary.	0
in the writing of section 3.2, the authors should clearly cite the previous works on hierarchical lstm and acknowledge that this is not the contributions of this paper.	0
under the current writing, for unfamiliarized readers, it sounds like this is proposed by the authors of this paper, which is not the case.	0
2) presentation: the writing of this paper is a little hard to follow, for example, it presents the two discriminators after the objective function (equation 2) and does not explain the intuition behind each model.	0
2. the writing and organization of the paper need some improvement, especially the experiments section.	0
the writing was a little awkward to follow at times, and i'm still not sure what ι am suppose to take away from the figures plotting the latent representation.	0
while the paper is well structured, the writing at the mathematical level is careless, which leads to ambiguities and mistakes (though one might be able to work out the right formula after going through the details of the entire paper).	0
the writing could be tremendously improved if some background of the capsule networks is included.	0
some parts also require better writing.	0
with respect to the writing, i’m a bit uncertain as to the primary message of the paper.	0
overall the writing needs significant polishing, though this is only at a local level, i.e, it doesn’t obscure the flow of the paper.	0
taking beta = 1/2, multiplying through by 2, and writing i(x;z)  i(y z) = i(x;z|y), we find cib.	0
"the authors first derive ""uniform writing"" (uw) which updates the memory at regular intervals instead of every timestep."	0
current manns only support dense writing  presumably this means dense as in 'every timestep', but this terminology is overloaded  you could consider ntm / dnc as doing dense writing, and then work of rae et al 2016 doing sparse writing.	0
to sum it up, i feel that the paper needs to be clearer in writing and in experimental structure.	0
one flaw of this paper is that the writing might be clearer.	0
nevertheless, i have several small issues:  i like the writing of this paper, fluent description and clear topic.	0
the experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree.	2
writing: overall the writing is fairly good though i felt that the model description could be made more clear by some streamlining  with a single pass through the generative model, inference model and learning.	0
while interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.	2
clarity: excellent writing until it comes to the experiments.	0
the writing is clear and sharp, and the reading experience is quite enjoyable (the witty first paragraph sets the tone for what is to follow), even if the text is at times a bit verbose.	0
pros  highquality writing  very clear  complete experiments on a variety of tasks, some of which do not have optimal solvers  honest assessment of the model cons  the theoretical contributions are not groundbreaking (either the the tweak on reinforce or the model architecture)  the model is still far from obtaining meaningful results on tsp (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...) details  dai et al has been published at nips and is no longer an arxiv preprint  the comparison to alphago should either be expanded upon or scratched.	2
after reading the author response and the updated paper, i am satisfied on several of my concerns, many of which were due to the writing in the earlier submission.	0
"quality: the writing of the paper needs more polishing; i saw grammatical errors here and there: for example, at the first paragraph of page 2, ""alternating"" should be alternative and ""synthetical"" should be synthetic... clarity: i have not been able to fully understand why the proposed (uniform) sampling variant of bn is better than previous effort at making bn less computationally expensive in a gpubased training environment by reading the paper: 1. the authors argue that the ""summation"" operation is the one that makes bn expensive; however, the authors have not demonstrated enough evidence of this argument 2. if ""summation"" operation is what makes bn expensive, then in a gpubased environment, can we simply divide the data into smaller batch, and train on each gpu using a smaller batch (this is way, each gpu is essentially calculating the statistics based on a subsample) 3. the authors discussed microbn, which ""alleviate the diminishing of bn's effectiveness when the amount of data in each gpu node is too small..."" this seems to show that in practice, training with bn does not suffer from having a large batch, but instead suffers from having too small batch size on each gpu node."	0
the writing is clear and comprehensive.	2
the writing is verbose and informal in many places.	0
some areas of writing could be improved, either too casual, or sloppy.	0
"examples of imprecise / casual writing: ""good performance without discounting, but training was less stable."""	0
in its current form, i do not recommend accepting this paper but i do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via rl experiments.	0
the authors are suggested to improve the english writing.	0
3) the writing is fairly dense for what is a fairly straightforward idea.	0
i'm guessing it could), better baselines, better scholarship, and condensing the writing, i think this paper can be an important step forward.	0
these concern principally notation issues and some potential improvements in the writing.	0
"besides this, writing ""the last samples"" requires disambiguation (using ""respective""?"	0
if the author fixes the writing to include detailed discussion with video prediction literature, with good quantitative and qualitative comparison to existing methods, that is worth 1 extra point.	0
even though such connection was not made explicitly clear in the writing, its heavy reliance on a recent (not yet peerreviewed) paper suggests such.	0
cons: 1. the writing is imprecise and often hard to follow.	0
i sincerely hope that the authors will be more precise in their future writing and focus on articulating and testing their key hypotheses.	0
the minor issue of this paper is that the writing should be polished.	0
overall the writing is good... but i did find the main statement of the algorithm confusing!	0
c. there are several technical issues with the writing (and potentially with the claims/conclusions), most of which are potentially flexible with some corrections and more exposition by the authors: is l_{raml} to be maximized or minimized?	0
first, the writing can be better.	0
i have other concerns which mainly stem from lack of clarity in writing: 1. in the line right above remark 1, it is not clear what “assumption” refer to.	0
overall i think the paper is good 'if' section 4.1 is sorted out and writing (especially in section 4.2) is improved.	0
the writing of the paper is good, but the writing of the captions could be improved.	0
there are many formulations which are taken from prior gan works, there is no need to repeat writing these formulations.	0
i managed not to look at existing comments/ratings for this paper before writing my review.	0
"i would include all test set accuracy values instead of writing ""its almost as high""."	0
"a few minor comments about writing:  in table 1, please put the highest numbers in bold to improve readability  on page 7, the word ""summaries"" is missing in ""the model produces naturallooking with no noticeable negative impact""  on page 9, ""cove content"" should be ""core content"""	0
(4/10) = clarity: the writing is fairly clear and wellcontextualized wrt existing work.	2
some other minor issues in the writing includes: 1) the introduction makes it seem the generative replay is new, without citing approaches such as dgr (which are cited in the related work).	0
the proof of correctness while correct is a bit unclear, can perhaps be simplified if you view the mcmc algorithm as operating on an augmented space (x,a,x') with stationary distribution p(x)q(a|x)q(x'|a) (writing writing q for .tilde(q)).	0
the writing needs quite a bit oof polish for the motivations to clearly stand out.	0
i have no doubt to the novelty, but the writing could definitely be improved.	0
this paper looks at ways to improve memorywriting in memory augmented neural networks.	0
"authors proposed two methods to compare against ""regular writing"" method as well as compare against each other, namely ""uniform writing"" and ""cached uniform writing""."	0
"latter one attempts to utilize a small size memory efficiently by introducing memory overwriting in other words ""forgetting""."	0
"other refs to mention: randomized relu (randomly set the leak threshold) https://arxiv.org/pdf/1505.00853.pdf noisy activation functions quality: decent (6.5/10), but i think the experiments, level of analysis, and quality of writing are more like a very good blog post than an academic paper (although i think that that line is becoming very blurry) clarity: decent (7/10), but there are many small grammar errors, especially to do with pluralization (e.g. ""activation functions .... its importance"" should be ""their importance"") and incorrect verb tense (e.g. ""was used"" should be ""is used"" or ""has been used""; past tense implies it no longer is used)."	0
"i pointed out the ones that were particularly confusing in the ""specific comments"" section, but the paper should be thoroughly reviewed for writing quality."	0
pros:  marginal improvement across the board  partial replacement strategy might be useful for other methods cons:  writing contains grammatical errors / change of tense / confusing sentence construction which make the paper somewhat hard to get through  not much exploration or insight about the learned functions or the activation function performs well  numbers seem quite incremental to me, and the method is not tested on larger/deeper networks where the gains might be mode significant.	2
while this could be an interesting result, i have several concerns regarding the assumptions, correctness, and writing.	0
the writing needs to be significantly improved to reach the level of a top conference.	0
the writing is clear and fluent.	2
from reading the paper, my guess is the authors came from a background that is not pure ml research (for instance, they are experts in javascript, webgl, and their writing style is easy to read), and it's great to see new ideas into our field.	0
while i'm happy about the writing style of this paper, maybe some reviewers who are more academic types might not like it and have a negative bias against this work.	0
comments: the writing of this paper is excellent, and contributions are well presented and demonstrated.	0
2. this is an interesting paper that combines a novel technique for writing to external memory based on surprisal and using it for more difficult tasks such as deductive reasoning.	0
writing: the writing is quite confusing at places and is the biggest problem with this paper.	0
suggestions and clarification requests:  the structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works.	0
clarity the writing/exposition is in general extremely clear.	0
minor remarks: the writing of this paper is somewhat fuzzy, using nonstandard technical language, which i could not decipher and which seems to me somewhat misleading.	0
typos and writing suggestions above eq 8: masked such that > masked so that eq 8: dimensions of o and h^t are incompatible: d'v, m'd; to evacuate the notation issue for transposition, cf footnote 1, here and elsewhere, you could use either or or .	0
pros:  natural, novel extension to gradientbased metalearning  state of the art results on two competitive fewshot benchmarks  good analysis  clear writing cons:  realistic, highdim data is only from the image domain minor questions for the authors:  relation networks are computationally intensive, although in fewshot learning the sets encoded are fairly small.	2
issues/concerns  i assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which i discuss below.	0
"the writing in this paper is a little awkward at times (many omitted articles such as ""the"" or ""a'), but, with a few exceptions, it is generally easy to understand what the authors are saying."	0
review versions of papers often lack polished writing.	0
i am truly impressed by the writing style of the authors.	0
the writing is very clear and direct.	2
clarity and writing: the skeleton of the paper is well written and easy to follow.	2
quality: ' the quality of the writing was overall high, with a few exceptions, including the related work and the experimental section. '	0
good results and good analysis of the model ' mostly clear writing and presentation (few typos etc. nothing too serious).	2
the authors do a good job of writing the paper and the paper is clear which is appreciatable.	0
a favorite of theirs is something that they call lempelziv complexity, which is measured by choosing an arbitrarily ordering of the domain, writing the outputs of the function in that ordering, and looking at how well the lempelziv algorithm compresses this sequence.	0
the writing is difficult to follow, unclear in several places, and has grammatical mistakes.	0
based on the method, this paper could go either way, but given my concerns on its novelty and writing quality/clarity, i lean slightly towards reject.	0
"this indicates that the attacks were not tuned properly, as you should always have pgd as a stronger attacker than fgsm  the method does not perform as well as adversarial training in standard defense tasks  several writing/clarity errors (detailed below) smaller edits: page 2: paragraph 2: second last line: ""feed"" instead of ""fed"" page 2: bullet 1: under our contribution: line 3: ""which are unchanged"" instead of ""which is unchanged"" page 3: paragraph 3: second last line: ""two distribution"" missing an s (plural) page 3: section 2.2: paragraph 2: line 2: ""here are two most famous attacks"" missing ""the"" before ""two most famous"" page 4: section 3.2: first paragraph: line 4: ""the latent codes is decomposed"" should be ""are"" instead of ""is"" page 5: paragraph 1: line 9: ""e are trained"" should be ""e is trained"" page 5: section 4: paragraph 1: last line: ""are those have access "" should be ""are those which have access"" missing which/that page 6: last paragraph: line 1: ""the attacker can only access to the classifier"" there is no need for ""to"""	0
pros:  the paper is well written, mostly selfcontained, and easy to read (for someone familiar with information theory);  all mathematical points are detailed and well explained, with sufficient introduction;  the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;)  information bottleneck is a topic of prime interest in the community these days;  the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning;  the solution brought to the ib lagrangian issues is simplistic though efficient (squaring i(x,t) so that it's not linear in i(x,t) anymore).	2
my main comment is that this work requires (a) more substantiation of the claim that attention shift is the phenomenon at play when it comes to lack of transferability, (b) improvement to the writing, and (c) more motivation behind the choice of mitigation mechanism.	0
smaller comments:  page 2 paragraph 3 line 3 “composed of an legitimate”: a instead of an  page 3 paragraph 1 line 3 “another line of works that perform attacks” work instead of works and performs instead of perform  page 3 paragraph 1 line 6 missing reference  page 3 paragraph 2 line 3,4 missing reference  page 4 paragraph 2 in section 3.2, and last sentence of pg 4: meaning is very unclear, needs rewriting  page 7 paragraph 3 in section 4.3 line 2 “it should by noted“: be rather than by [1] https://arxiv.org/pdf/1707.07397.pdf [2] https://arxiv.org/pdf/1711.09115.pdf [3] https://arxiv.org/pdf/1712.02779.pdf	0
the english, grammar and writing style is very good, as are the citations.	0
the mathematical writing in the paper, especially describing the proof of theorem 1, is very nice  they expose the main ideas effectively.	0
the writing is also very rushed.	0
8. the writing is very rushed.	0
there are lots of writing and editorial errors.	0
the technical writing is unclear and jargon is often used without definition.	0
more careful writing would disentangle these two components.	0
"since the computation of .tau is purely feedforward, i recommend writing ""compute."""	0
quality: 6/10 the writing is mostly clear to follow, although paragraphs don't always flow well into each other and some sections assume a lot of knowledge.	0
"2. it is usually advised in scientific writing not to use phrases like ""it is known""."	0
overall, the writing is quite clear, the problem is interesting and important, and the results are promising.	2
however i do have a few concerns: 1. the writing can certainly be improved.	0
the paper presents a new model for reading and writing memory in the context of taskoriented dialogue.	0
however, there is still substantial room for improvement in writing clarity.	0
overall the clarity of the writing is sufficient.	0
first, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in bartunov et al. the scientific question in bartunov et al. is not a matter of whether bp algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns.	0
the tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms.	0
the paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.	0
writingwise the paper is hard to read on the technical part with many unclear details and this portion needs a good amount of extra explanations.	0
the writing is good and easy to follow.	2
yet, it is better scientific writing practice to provide languageindependent algorithmic findings as pseudocode instead of native python.	0
however, the writing can be largely improved.	0
the writing is also easy to follow.	0
the writing is welldone overall, and the presented method and diagrams are clear.	2
nevetheless, i think stronger experiments are required, as well as improvements in terms of clarity in the writing of the paper, and stronger support for the motivation.	0
however, due to the writing style, it is hard to analyze which part in section 4 is novel and which part is already known.	0
please consider this rubric when writing your review: 1. briefly establish your personal expertise in the field of the paper.	0
[clarity] the writing is basically clear.	2
cons:  clarity of writing: lots of typos and bits of math that could be more clear (see detailed comments below) [fixed]  the plots in section 4 are all extremely jagged.	0
my concerns/questions are as following: (a) firstly, the writing is too verbose and vague without clarifying the details and there are too many references to laflaquiere et al. i would recommend the authors to be more precise, i.e. define topological/metric invariants, clarify how inconsistent sensory/motor pairs are sampled in mtm condition and how environment is perturbed in mmt condition.	0
[unresolved] fig 4 embeddings should be compared against (trivedi et al. 2017) [after rebuttal: author revision does not make qualitative comparison against trivedi et al. (2017)] besides the limited innovation, the writing needs work.	0
my main concerns about this paper are its related work and its writing.	0
quality the paper is very verbose, the figures and captions are tedious to read, the mathematical notation seems strange as well, making the writing more concise is highly encouraged.	0
clear, pleasant writing and good communication of a complicated algorithm.	0
structurewise, i would say that the choice of writing the paper in the form of a q&a, with very brief explanations and details was more distracting and at times unnecessary than i liked (e.g. question 7 could move to appendix as it is quite trivial).	0
the clarity and style of the writing should be improved to better stress the significance of the underlying motivation.	0
this level of writing is not professional.	0
what appears as a circular definition is merely the effect of loose writing yet i am afraid it would confuse readers.	0
however, it feels quite adhoc and the writing of the paper is very obscure at various places, which leaves room for improvement.	0
at various places the writing is somewhat sloppy (missing words, commas, broken sentences), which could have been avoided by carefully proofreading the paper.	0
the writing is very technical and looks solid.	0
fully predicting the code text writing process as opposed to the code itself is an interesting task with possible big impact, if the accuracy of this edit model manages to significantly outperform simple lefttoright code text prediction techniques.	0
pro:  first fine grained text (code) evolution evaluation and formulation of the challenge to provide labels for predicting the process of code writing.	0
the writing is generally quite clear.	0
writing: i like the writing.	0
yes, except the experiments confidence: 2/5 seen submission posted elsewhere: no (but i did find it on arxiv after writing the review) detailed comments: in this work, the authors propose a new type of memory cell for rnns which account for multiple types of time series.	0
writing: the writing was mostly ok, though there were some issues early in section 2. the authors rather awkwardly transition from a mathematical formalism that included the two halves of the dialogue as x (call) and y (response), to a formalism that only considers a single sequence x. novelty and impact: the proposed approach explicitly combines an established model with two components that are themselves wellestablished.	0
the paper is interesting however the benefit over the traditional maximum likelihood estimator is small and the writing needs a bunch of work.	0
"2) binary classifier citation on page 1 (avati, rajkomar) should also cite the plethora of recent machine learning for healthcare results in this field 3) likelihoods are calibrated (as is any error measured by a proper scoring loss) 4) there are other methods to fit survival functions such as ""adversarial timetoevent modeling"" by chapfuwa in icml 2018. there are probably also moment methods 5) i think the evaluation might also want utilty because sharpness is a utility claim 6) some of the statements in the writing are funny like probability distributions are uniquely identified by parameters."	0
overall, much of the writing seems quite rushed with many typos and grammatical errors throughout.	0
conclusion: while definitely a promising direction, the paper requires significant further work, writing improvement, and polishing.	0
the writing is mostly ok, although there is room for improvement in terms of english use (especially on the front of articles which seem to be off in almost every sentence).	0
the idea paper is nice however, this draft needs more writing work to bring to a conference standard in my opinion.	0
my objection to the writing begins with the definition and example of a “close set” which i details below.	0
in summary this is potentially interesting work but the writing should be sharper, their should be less ambiguity of interpretation of close set.	0
overall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.	0
the writing of the paper is clear and easy to understand.	2
this paper could benefit from better writing.	0
strong aspects:  combination of evolutionary search over morphology and hyper parameters with rl for controller  introducing analysis tools from game theory to the community  entertaining video  good presentation/clear writing weak aspects:  authors do not mention that they will make the code public  presentation is based on the best of 50 runs!	0
"there are several issues with the presentation of this work, that make it incredibly difficult to identify a technical contribution: 1. overreaching statements without details to backup: you are writing the paper as if you are learning a ""rl algorithm"" that can be used to quickly learn new tasks."	0
pros: 1. very clear writing.	2
the results are presented in a confusing manor, and the paper is perhaps done a disservice by the inclusion of a very large number of tables both in the main text and the appendix without the necessary writing required to situate the reader.	0
the writing is sometimes hard to follow.	0
2. the writing and organization of this paper need to be significantly improved.	0
minor remarks: the writing seems like it could be improved in multiple places and the main thing that makes some sections of the paper hard to follow is that the concepts often get mentioned and discussed before they are formally defined/introduced.	0
the writing is clear and the idea is an original refinement of earlier work, justified by its exceeding stateoftheart approaches.	0
the overall clarity of the paper's writing could be improved.	0
first, although the paper is well written, the writing can be improved.	0
there are several things to like about this paper:  the writing is clear and well thought out.	0
the writing is good, the results are good, the algorithm is good and i think it will have impact.	0
evaluation  the writing and presentation of the paper are in general well carried, except some part seems a little unclear, taking me quite a while to understand.	0
the writing was clear and easy to follow.	2
3. about writing: g(z,.eta) and h(z,.eta) appear in section 1 and section 2, and they are used to define generalized sosp.	0
the writing is clear and makes most arguments easy to follow.	2
strengths: attempts to solve a longstanding problem in modelfree rl (effective exploration in sparse reward environments) clear writing and structure, easy to understand (except for some minor details) novel, intuitive, and simple method building on ideas from previous works good empirical results (better than state of the art, in terms of performance) on some challenging tasks weaknesses: not very clear why (and when) the method works  more insight from experiments in less complex environments or some theoretical analysis would be helpful it would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) not clear how stable (to train) and robust (to different environment dynamics) the method is main comments / questions: the paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis.	2
the writing is also extremely poor.	0
the writing needs to improve quite a bit to get accepted at a conference like iclr.	0
however, there are several places where this paper falls down:  the writing/introduction is extremely loose... terms are used and introduced without proper definition for many pages.	0
in particular, the writing needs to be reworked, the experiments to be consolidated, and the link to neuronal modulation to be further investigated.	0
strengths:  this work is probably the first to distill the structured manifold knowledge from a teacher network to a student network.	2
even the fsp method mentioned by the authors in the related work should be compared, which transfers the gram matrix modelling the structure information either.	0
due to the properties of manifold learning, the updated weights in each iteration are contracted with a lowrank structure, such that the number of the parameters of tt can be automatically decreased during the training procedure.	0
i like the topic chosen by the authors, using tt to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the riemannian manifold structure to speed up the optimization.	0
this approach is motivated by the results of nickel & kiela (2017), who showed tha hyperbolic space can provide important advantages for embedding graphs with hierarchical structure.	0
the advantage of hyperbolic embeddings is that they impose a hierarchical structure in the latent space.	0
they reparametrize the discriminator to be an explicit function of two densities: the generator probability density function q and a structured gibbs distribution v. comments: 1: this paper focuses on mode coverage problems, where spurious modes of learned model(q) not supported by target model(p) are pruned off.	0
4: the explanation of imposing structure on the model distribution is not clear.	0
in the introduction they first claim “we cannot impose structure directly on the joint distribution of a gan’s outputs.” but after they claim “we submit that regularizing the structure of a gan’s generator and discriminator is generally more difficult than imposing meaningful structure directly on the model distribution, which we will refer to as q. these two statements conflict because the model distribution is a joint distribution of gan’s outputs.	0
strengths: the idea of using the hierarchical structure of the labels is innovative and wellmotivated.	0
next, considering additive demixing, the authors assume that the corruption/structured signal is unknown but it can be modelled using a convolutional network (using the architecture of dcgan).	0
my main questions revolve around under what conditions the column spaces of the two generators are mutually independent and what is the type of components structure that the proposed model can recover.	0
however, the main technical contribution of this paper is otherwise not clear  the methodology section covers a very broad set of techniques but doesn't provide a clear picture of what is novel; furthermore, it makes a strong assumption about linear structure in the embedding space that may not hold.	0
this will also clearly identify the benefits of the wavelet structure to the filters and multiresolution analysis approaches.	0
3. for an iclr submission, there should be additional investigations as to the structure of the learned representations, at least by the dqn and the ddpg models — is the state embedding learned by the networks somewhat suggestive of economically meaningful properties of the items or the sales environment?	0
this would be heavily dependent on the model structure (with more complex decision boundaries likely being harder to optimize) but they show empirically in 4 models that this method works well.	0
senses follows the structure of skipgram model.	2
there existing several approaches that learn node embedding vectors from attributed graph (considering both the node content text and graph topology structure), such as tadw [1], hsca [2], plane [3],gae[4], aane[5], anrl [6].	0
ijcai, 2015 [2] d. zhang, j. yin, x. zhu, c. zhang, homophily, structure, and content augmented network representation learning.	0
this paper introduces a domain adaptation approach for structured output data, with a focus here on semantic segmentation.	0
the idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence.	0
strengths:  modeling the structure via patches is an interesting idea.	2
weaknesses: method:  the idea of relying on patches to model the structure is not new.	0
i expect that if the paper would be better structured, it would be easier to understanding how all the parts fit together.	0
methods seems particularly designed for 'video games', where the object and background structures have well defined sizes, appearance, etc. how will the moap fair in more realistic situations with noisy observations, occluded objects, changing appearances and lighting conditions, etc.? '	0
to show the detailed structure of the effect net module.	0
thus it would be appropriate to consider models that specifically account for spatial structure, e.g. gaussian processes.	0
more importantly, it seems that all experiments are performed on tasks where the underlying structure is known, however this is almost never the case in practice.	0
for the topological inference experiment it is assumed that one knows the structure, but how to address the more general problem?	0
more importantly the selection of w, crucial for defining structure, is not discussed at all in the paper.	0
the parameter generator network has a special nested structure to regularize parameters within layers and filters.	2
the ablation study also verified the need for the proposed nested parameter structure.	0
given that the proposed algorithms do not leverage the underlying model structure very much, why the proposed algorithms are special to the graphical neural network is not very clear.	0
3. i found the structure of the ablation study to be very difficult to follow.	0
the novelty of the paper is rather limited, both in terms of the convergence analysis and exploiting the lowrank structure in tensor trains.	0
"the ""pyramid"" recurrent neural network seems to be a extension of rnn using the idea of multiscale structure."	2
then a hierarchical clustering method is applied to the generated feature vectors, such that treestructured relationships among hidden layer units are revealed.	0
given the tree structure of lnn for the mnist data set, i am still not able to understand how this lnn distinguishes the digit 0 from other digits.	0
the proposed method provides a tree structure to describe the relationships between the hidden layer units.	0
the authors also do not illustrate why learning the tree structure is particularly important.	0
the authors do not explain why the treestructured clustering results are more superior than the kmeans clustering results.	0
i'd been interested in having an artificial task where to proposed algorithm does not succeed (an ideally some discussion on what make the structure recoverable or not).	0
the difficulty with shapely values is they are extremely (exponentially) expensive to compute, and the contribution of this paper is to provide two efficient methods of computing approximate shapely values when there is a known structure (a graph) relating the features to each other.	0
pros: 1. introducing the tree structure is a neat way of extending the existing rslds model to multiscale scenarios.	2
it is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.	0
11. is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations' and 'maintaining a global structure to minimize a common cost function is important' ?	0
the paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership.	0
experiments show that, when there is structure in the data set, the neural bloom filter can achieve the same false positive rate with less space.	0
it seems like this behavior is supported directly in the structure of the model, which is great — but i don’t think it can be called “zeroshot” inference.	0
but 0.1/255 will be much smaller than 0.031. another factor might be the model structure.	0
if alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (madry et al and athalye et al) to make the conclusion more clear.	0
"introduction, first paragraph, claims that ""such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability""  what do you mean by these concepts and how exactly is the current approach limited with respect to them?"	0
while the novelty and good structure of the paper are reasons to accept it, i have doubts concerning the soundness of the results due to the experimental setup.	0
reasons to accept the paper  novelty  works in an unsupervised setting  well written/structured reasons to reject  doubts concerning the experimental setting  (minor) related work is not complete  (minor) not all common performance measures are reported  [1] wang, peifeng, shuangyin li, and rong pan.	0
how would this method generalise to objects with less/no structure?	0
for the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a siamese convolutional neural network, noted as sasnet, and to use the contact map of two binding proteins’ native structure.	0
moreover, it is the prediction performance that matters to such task, but the authors remove the nonstructure features from the compared methods.	0
this paper proposes graphconvolutional gans for irregular 3d point clouds that learn domain (the graph structure) and features at the same time.	0
the paper is very well written, addresses a relevant problem (classification of 3d point clouds with arbitrary, a priori unknown graph structure) in an original way, and supports the presented ideas with convincing experiments.	2
"the comparison with one shot and darts seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose ""to save time""."	0
moreover, lstm is not just a neuron nonlinearity, but a recurrent network with a particular structure.	0
by artifically alterning the training images by changing color, structure and frequency components in individual examples, it is shown that the training process targets different characteristics that is dataset dependent.	0
section 6 proposes to test a model on data with a different structure from data provided in training.	0
to perform traceable inference, a recognition model with the same mixture structure as the spike and slap prior is used to produce the approximate posterior.	0
"technicalwise, the paper is relatively incremental  all of the building blocks for performing tractable inference are standard: since the posterior is intractable for nonlinear sparse coding, a recognition network is used; the prior is spike and slap, thus the recognition network will output parameters in a similar mixture structure with both a spike and a slap component; to apply reparametrization trick on the nondifferentiable latent code, a continuous relaxation, similar to the one used in concrete distribution/gamble trick, is applied to approximate the step selection function with a controllable ""temperature"" parameter."	0
the paper proposes a graph convolutional networkbased encoderdecoder model with sequential attention for goaloriented dialogue systems, with the purpose of exploiting the graph structures in kb and sentences in conversation.	0
() without dependency parser when it is not possible () limited novelty () limited convincing the advantage of gcn itself detailed comments the paper incorporates the graph structures in sentences and kb to make richer representations of conversation and achieves a stateoftheart performance on the dstc2 dataset.	0
the random graph experiments (table 3) show the effect of good structure in gcn, but i felt that it is not enough to demonstrate an improvement by gcns.	0
maybe the authors should look into models that output a set instead of a sequence since neighbors are more like a set in their structure.	0
this is achieved by introducing several directed graphs that have orientation built into the graph structure.	0
the results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest.	0
they also structure the target domain encoder such that at each time step, it has a bias toward attending to the hidden state in the source encoder at the same position.	0
there are helpful figures 12 which explain the network structure and the quantum circuit.	0
a better description of the results, e.g., in table 3 should be presented, as well a better linking with the findings; a better structure of the latter would also be required to improve consistency of them.	0
4. what is the nmt network structure?	0
the authors bridge two components (density of cnns and sparsity structures) by proposing a new network structure with locally dense yet externally sparse connections.	0
"there is an interesting work: ""using deep learning to model the hierarchical structure and function of a cell"" https://www.nature.com/articles/nmeth.4627"	2
this paper presents an approach to multimodal imitation learning by using a variational autoencoder to embed demonstrated trajectories into a structured latent space that captures the multimodal structure.	0
the method is tested on three lowdimensional continuous control tasks and is able to learn structured latent spaces capturing the modes in the training data as well as generating good trajectory reconstructions.	0
a lot of the ideas in this paper are similar to those proposed in prior work  the network for embedding the trajectory is similar to the ones from wang et al & coreyes et al with the major difference being in the structure of the action decoder (and what inputs to encoder).	0
multimodal imitation learning from unstructured demonstrations using generative adversarial nets.	0
i recommend shedding some light on the structure of this model that generates these shapley values.	0
van der maaten (2009) proposes the use of aes to learn a parametric mapping between highdimensional datapoints and lowdimensional representations by enforcing structure obtained via studentt stochastic neighborhood embedding (tsne): this is not a correct description, van der maaten (2009) optimizes the ae using tsne cost function (instead of running some separate tsne step to yield structural constraints as the description seems to say).	0
inducing applicationspecific structure while training autoencoders allows to learn embeddings with better neighborhood preserving properties.	0
pros:  improving joint training of nondifferentiable pipelines is a meaningful and relevant problem  using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea cons:  the main result of the paper concerning sufficient conditions for optimality of the method seems dubious  it is not obvious why this method would outperform simple baselines, and baselines for joint training were tried  the notation seems unnecessarily bloated and overly formal  the exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing the submission describes a method for smoothing a nondifferentiable machine learning pipeline (such as the fasterrcnn detector), so that gradientbased methods may be applied to jointly train all the parameters of the pipeline.	2
if we want to really smooth the pipeline itself, then it is also easy to do this by devising a suitable mdp and then applying reinforce with the usual mdp structure.	0
2. it is unclear to me how the model works at test time; as the model is essentially building a relational structure in the data, does the user have to provide multiple eeg trials at time of prediction?	0
overall i think there is limited novelty in the approach; the idea to learn the structure of the data relationally instead of absolutely is pretty straightforward, and is a standard practice for example in nonparametric statistical modeling.	0
