While generative neural networks can learn to transform a specific input dataset into a specific target dataset, they require having just such a paired set of input output datasets. For instance, to fool the discriminator, a generative adversarial network (GAN) exclusively trained to transform images of black-haired  men  to blond-haired  men  would need to change gender-related characteristics as well as hair color when given images of black-haired  women  as input. This is problematic, as often it is possible to obtain  a  pair of (source, target) distributions but then have a second source distribution where the target distribution is unknown. The computational challenge is that generative models are good at generation within the manifold of the data that they are trained on. However, generating new samples outside of the manifold or extrapolating  out-of-sample  is a much harder problem that has been less well studied. To address this, we introduce a technique called  neuron editing  that learns how neurons encode an edit for a particular transformation in a latent space. We use an autoencoder to decompose the variation within the dataset into activations of different neurons and generate transformed data by defining an editing transformation on those neurons. By performing the transformation in a latent trained space, we encode fairly complex and non-linear transformations to the data with much simpler distribution shifts to the neuron s activations. Our technique is general and works on a wide variety of data domains and applications. We first demonstrate it on image transformations and then move to our two main biological applications  removal of batch artifacts representing unwanted noise and modeling the effect of drug treatments to predict synergy between drugs. GAN, domain transfer, computational biology, latent space manipulations