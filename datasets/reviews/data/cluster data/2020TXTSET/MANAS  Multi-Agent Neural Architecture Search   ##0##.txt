The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximize a graph-level global objective. Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use. In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures. We provide two distinct lightweight implementations, with reduced memory requirements ( th of state-of-the-art), and performances above those of much more computationally expensive methods. Theoretically, we demonstrate vanishing regrets of the form , with being the total number of rounds. Finally, aware that random search is an (often ignored) effective baseline we perform additional experiments on alternative datasets and network configurations, and achieve favorable results in comparison with this baseline and other competing methods. Neural Architecture Search, NAS, AutoML, Computer Vision