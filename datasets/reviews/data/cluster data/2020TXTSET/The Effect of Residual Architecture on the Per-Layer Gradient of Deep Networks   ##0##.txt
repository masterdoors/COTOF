A critical part of the training process of neural networks takes place in the very first gradient steps post initialization. In this work, we study the connection between the network s architecture and initialization parameters, to the statistical properties of the gradient in random fully connected ReLU networks, through the study of the the Jacobian. We compare three types of architectures  vanilla networks, ResNets and DenseNets. The later two, as we show, preserve the variance of the gradient norm through arbitrary depths when initialized properly, which prevents exploding or decaying gradients at deeper layers. In addition, we show that the statistics of the per layer gradient norm is a function of the architecture and the layer s size, but surprisingly not the layer s depth. This depth invariant result is surprising in light of the literature results that state that the norm of the layer s activations grows exponentially with the specific layer s depth. Experimental support is given in order to validate our theoretical results and to reintroduce concatenated ReLU blocks, which, as we show, present better initialization properties than ReLU blocks in the case of fully connected networks. 