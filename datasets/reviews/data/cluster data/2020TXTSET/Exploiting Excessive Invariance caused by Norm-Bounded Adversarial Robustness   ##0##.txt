Adversarial examples are malicious inputs crafted to cause a model to misclassify them. In their most common instantiation,  perturbation-based  adversarial examples introduce changes to the input that leave its true label unchanged, yet result in a different model prediction. Conversely,  invariance-based  adversarial examples insert changes to the input that leave the model s prediction unaffected despite the underlying input s label having changed. So far, the relationship between these two notions of adversarial examples has not been studied, we close this gap. We demonstrate that solely achieving perturbation-based robustness is insufficient for complete adversarial robustness. Worse, we find that classifiers trained to be Lp-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts. We construct theoretical arguments and analytical examples to justify why this is the case. We then illustrate empirically that the consequences of excessive perturbation-robustness can be exploited to craft new attacks. Finally, we show how to attack a provably robust defense --- certified on the MNIST test set to have at least 87% accuracy (with respect to the original test labels) under perturbations of Linfinity-norm below epsilon=0.4 --- and reduce its accuracy (under this threat model with respect to an ensemble of human labelers) to 60% with an automated attack, or just 12% with human-crafted adversarial examples. Invariance, Robustness, Adversarial Examples