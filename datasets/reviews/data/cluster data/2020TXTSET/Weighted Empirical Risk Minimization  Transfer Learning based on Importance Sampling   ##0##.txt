We consider statistical learning problems, when the distribution of the training observations differs from the distribution involved in the risk one seeks to minimize (referred to as the 	extit{test distribution}) but is still defined on the same measurable space as and dominates it. In the unrealistic case where the likelihood ratio is known, one may straightforwardly extends the Empirical Risk Minimization (ERM) approach to this specific 	extit{transfer learning} setup using the same idea as that behind Importance Sampling, by minimizing a weighted version of the empirical risk functional computed from the  biased  training data with weights . Although the 	extit{importance function} is generally unknown in practice, we show that, in various situations frequently encountered in practice, it takes a simple form and can be directly estimated from the  s and some auxiliary information on the statistical population . By means of linearization techniques, we then prove that the generalization capacity of the approach aforementioned is preserved when plugging the resulting estimates of the  s into the weighted empirical risk. Beyond these theoretical guarantees, numerical results provide strong empirical evidence of the relevance of the approach promoted in this article. statistical learning theory, importance sampling, positive unlabeled (PU) learning, selection bias