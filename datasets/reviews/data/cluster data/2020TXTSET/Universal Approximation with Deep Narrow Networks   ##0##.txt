The classical Universal Approximation Theorem certifies that the universal approximation property holds for the class of neural networks of arbitrary width. Here we consider the natural `dual  theorem for width-bounded networks of arbitrary depth. Precisely, let be the number of inputs neurons, be the number of output neurons, and let be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then we show that the class of neural networks of arbitrary depth, width , and activation function , exhibits the universal approximation property with respect to the uniform norm on compact subsets of . This covers every activation function possible to use in practice; in particular this includes polynomial activation functions, making this genuinely different to the classical case. We go on to consider extensions of this result. First we show an analogous result for a certain class of nowhere differentiable activation functions. Second we establish an analogous result for noncompact domains, by showing that deep narrow networks with the ReLU activation function exhibit the universal approximation property with respect to the -norm on . Finally we show that width of only suffices for `most  activation functions. deep learning, universal approximation, deep narrow networks