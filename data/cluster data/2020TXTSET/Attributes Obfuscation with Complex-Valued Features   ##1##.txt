The paper studies the possibility of hiding sensitive input information from the intermediate-layer features without too much accuracy degradation. We propose a generic method to revise a conventional neural network to boost the challenge of adversarially inferring about the input but still yields useful outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input information is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the prediction from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method significantly diminishes the adversary s ability in inferring about the input while largely preserves the accuracy of the predicted outcome. Deep Learning, Privacy Protection, Complex-Valued Neural Networks