The tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks, but many of the theoretical results proposed have only been for shallow networks. In this paper, we study an important primitive for understanding the meaningful input space of a deep network  	extit{span recovery}. For , let be the innermost weight matrix of an arbitrary feed forward neural network , so can be written as , for some network . The goal is then to recover the row span of given only oracle access to the value of . We show that if is a multi-layered network with ReLU activation functions, then partial recovery is possible  namely, we can provably recover linearly independent vectors in the row span of using non-adaptive queries to . Furthermore, if has differentiable activation functions, we demonstrate that 	extit{full} span recovery is possible even when the output is first passed through a sign or thresholding function; in this case our algorithm is adaptive. Empirically, we confirm that full span recovery is not always possible, but only for unrealistically thin layers. For reasonably wide networks, we obtain full span recovery on both random networks and networks trained on MNIST data. Furthermore, we demonstrate the utility of span recovery as an attack by inducing neural networks to misclassify data obfuscated by controlled random noise as sensical inputs. Span recovery, low rank neural networks, adversarial attack