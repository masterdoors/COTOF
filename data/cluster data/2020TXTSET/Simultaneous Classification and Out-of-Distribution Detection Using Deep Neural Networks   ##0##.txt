Deep neural networks have achieved great success in classi cation tasks during the last years. However, one major problem to the path towards arti cial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classi cation algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to ef ciently detect out-of-distribution (OOD) examples without compromising much of its classi cation accuracy on the test examples from known classes. Based on the Outlier Exposure (OE) technique, we propose a novel loss function that achieves state-of-the-art results in out-of-distribution detection with OE both on image and text classi cation tasks. Additionally, the way this method was constructed makes it suitable for training any classi cation algorithm that is based on Maximum Likelihood methods. Out-of-Distribution Detection, OOD detection, Outlier Exposure, Classification, Open-World Classification, Anomaly Detection, Novelty Detection, Calibration, Neural Networks