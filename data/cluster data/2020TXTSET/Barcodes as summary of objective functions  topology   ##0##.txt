We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces. We present an algorithm for calculations of the objective function s barcodes of minima. Our experiments confirm two principal observations  (1) the barcodes of minima are located in a small lower part of the range of values of objective function and (2) increase of the neural network s depth brings down the minima s barcodes. This has natural implications for the neural network learning and the ability to generalize. Barcodes, canonical form invariants, loss surface, gradient complexes