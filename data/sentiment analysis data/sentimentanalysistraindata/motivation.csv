text	label
on the overall, while the idea may be of interested, the paper lacks in motivations in connecting to relevant previous works and in providing insights on why it works.	0
for the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow.	0
fourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc.	0
the numbers in the tables are good but i have several comments on the motivation, originality and experiments.	0
overall i found the paper interesting but also not very clear at pointing out the major contribution and the motivation behind it.	0
the paper was mostly clear in its exposition, however some additional information of the motivation for why the said reduction is better than simpler alternatives would help.	0
i feel the motivation for this is lacking.	0
2. lack of motivation for ie or uie.	0
weaknesses: 1. the motivation behind the proposed task needs to be better elaborated.	0
the paper contains some interesting experimental results, but unfortunately lacks concise motivation and description of the method and quality of writing.	0
to push this motivation into the learning process, the master policy is updated always but the subpolicies are only updated after an extended warmup period (called the jointupdate or training period).	0
"finally, the motivation behind the ""reverse ratio trick"" was unclear to me (the trick is clear, but i didn't fully understand why it's needed)."	0
my main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty adhoc to me and there is a need for more experimental study to show how the methodology work.	0
it would be nice to have a little bit of discussion about the motivation for these particular modeling implementationssome are basically the same as in hu et al. (2017), but obviously the type system here is richer and it might be helpful to highlight some of the extra things it can do.	0
cons: 1) the motivation of the paper is not convincing.	0
however, i found a lack of motivation for the specific design choices made to obtain equations 9 and 10. what is a_t in equation 9?	0
i find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below.	0
pros:  the motivation for the work and its connections to the cognitive/philosophical models of concepts and predictive coding is very interesting  the iterative improvements in the celeba samples in fig. 3 and the corresponding improvements in the loglikelihood in tbl.	2
the motivation and attempt is quite good, but the paper is written quite poorly without any flow, it is very difficult for the reader to understand the novelty or significance of the work, no intuition is given and descriptions and so short and cryptic.	0
i guess the motivation is the mixture of pyramiddrop and shakeshake motivations, but the main surprising part (forward weight can be negative) is not motivated at all.	0
many ideas are presented with some abstract motivation, but there is no comparative evaluation to demonstrate what happens when any one piece is removed from the system.	0
together with the above concerns, it makes me doubt the motivation of this work on reading comprehension.	0
comments: the word analogy task was developed as an interesting way to analyse and understand word embedding spaces, but motivation for learning word embeddings was as generalpurpose representations for language processing tasks (as in collobert et al, 2011), not as a way of resolving analogy questions.	0
the central motivation of the method proposed in the paper, is a conjecture that the lack of global consistency in gangenerated samples is due to the binary classification formulation of the discriminator.	0
the main concern is the motivation of the twopass decomposition.	0
cons: 1) lacking motivation/intuition the main motivation for the approach, as far as i understand, is to learn cluster boundaries for nonlinear data  where kmeans fails.	0
the motivation behind it is that in gans interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions.	0
 the example in fig. 2 is extremely important to understand the motivation behind the hyperprior but i think it needs a little more explanation.	0
basically the motivation and method is good, the drawback of this paper is its narrow scope and lack of necessary explanations.	0
i do have some serious questions/concerns about this method: part of the motivation for this paper is the goal of scaling to very large sets of examples.	0
despite the strong motivation, the article raises some concerns regarding the method.	0
it’s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.	0
 the proposed idea (treeqn) and underlying motivation are almost same as those of vpn [oh et al.], but there is no indepth discussion that shows why treeqn is potentially better than vpn.	0
the motivation has certainly been clarified, but in my opinion it is still hazy.	0
however, my main concerns with this paper are related to motivation and experiments.	0
for mnist and mnistfashion experiments, the motivation is mentioned to be similar to srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized.	0
other than that, i have the following questions and remarks:  i might have misunderstood the motivation but the gan objective for the `g` network is a bit weird; why is it a good idea to push the counterfactual outcomes close to the factual outcomes (which is what the gan objective is aiming for)?	0
 cons: the authors depart from the original motivation when the central limit theorem is invoked.	0
main comments the motivation to consider algebraic topology and dataset difficulty is interesting, but i think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings.	0
re: motivation the response didn't address my comments about the lack motivation for the proposed method.	0
 in figure 1, the “g1” on the right should be “g2”;  section 2.2.1, “x_f” should be “x_f”;  the motivation of having “z_v” should be introduced earlier;  section 2.2.4, please use either “alpha” or “.alpha” but not both;  section 3.3, the dataset information is incorrect: “20599 images” should be “202599 images”; missing reference:  neural face editing with intrinsic image disentangling, shu et al. in cvpr 2017.	0
pros:  qualitatively the proposed method has good results in several tasks cons:  writing needs to be improved  lack of motivation  not easy to follow technique details the motivation part is missing.	2
additional comments:  the assumption of the availability of a target score goes against the motivation that one need not learn individual networks .. authors say instead one can use 'published' scores, but that only assumes someone else has done the work (and furthermore, published it!).	0
in principle, the idea seems to be clear, but then the description and motivation of the model remains very vague.	0
let's take them one by one: (a) as pointed out by authors, this is in the line of work being done in speech recognition, selfdriving cars, ocr etc. and is a good motivation for the work but not a contribution.	0
e.g. there is no results of algorithm 1. i know it serves as a motivation, but it would be nice to see how it works.	0
quality i appreciate this line of research in general, but there are some flaws in its motivation and in the design of the experiments.	0
 since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on cl conclusion:  i feel that the motivation is good, but the proposed model is too handcrafted.	0
strengths  the proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.	2
 the experiment/motivation i found most compelling was 4.1 (since it clearly matches the issue of partial observability) but we only see results compared to dqn... it feels like you don't put a compelling case for the nonmarkovian benefits of arm vs other policy gradient methods.	0
 the experiments are not constructed to support the motivation / claim, but just to show that model performance improves.	0
there is some text about a variant of ctc, but it does not explain very clearly what was done or what the motivation was.	0
cons: the motivation (section 2) needs to be improved.	0
i think the authors have the right intuition, but no evidence or citation is presented to motivate result 5. indeed, dcgans are known to have extremely sharp interpolations, suggesting that small jumps in z lead to large jumps in images, thus having the potential to assign low probability to tunnels.	0
"the method is specifically about determining the rank of a matrix, but the authors motivate it with way too general and vague relationships, such as ""determining the number of nodes in neural networks""."	0
however, the lack of sufficient theoretical justification is now well complemented by extensive experiments, and it will motivate more theoretical work.	0
this expression is used to develop a new layer called a “warp layer” which essentially tries to compute several layers of the residual network using the taylor expansion expression — however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (resnet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with.	0
overall it's not clear what this paper adds to existing body of work: 1. axiomatic treatment takes a bulk of the paper, but does not motivate any significantly new method 2. from experimental evaluation it's not clear the results are better than existing work, ie yosinsky http://yosinski.com/deepvis	0
however the thesis of this work should be better motivated and explained.	0
in section 1, overfitting concerns seem like a strange way to motivate the desire for smoothness.	0
it appears to be motivated by improving training dynamics, which is understandably a significant concern.	0
i do not think the method is theoretically wellmotivated as presented, but the empirical results seem solid.	0
pros:  the proposed modification to vin is simple, wellmotivated, and addresses the nondifferentiability of vin  experiments on synthetic data demonstrate a significant improvement over the standard vin method cons:  some important references are missing (e.g., maxent ioc with deeplearned features)  although intuitive, more detailed justification could be provided for replacing the maxoveractions with an exponentiallyweighted average  no baselines are provided for the experiments with real data  all the experimental scenarios are fairly simple (2d gridworlds with discrete actions, 1channel input features) the proposed method is simple, wellmotivated, and addresses a real concern in vins, which is their nondifferentiability.	2
pros: ' interesting energy formulation and variation over began cons: ' not a clear paper ' results are only partially motivated and analyzed	2
clarity, significance and correctness  clarity: the main idea is clearly motivated and presented, but the experiment section failed to convince me (see details below).	0
unrolled gan looks at historical d's through unrolled optimization, but not the history of g. so this lack of significant difference in results raise the question of whether any improvement of chekhov gan is coming from the online learning perspective for d and g, or simply due to the fact that it considers historical d models (which could be motivated by sth other than the online learning theory).	0
overall: the proposed dataset seems reasonable but neither the dataset seems properly motivated (something where analysts actually struggle and models can help) nor it is clear if it will actually be useful for the research community (models performing well on this dataset will need to focus on specific abilities which have not been studied in the research community).	0
the architecture was described but not really motivated.	0
then again, the major contribution of this work is not advancing the stateoftheart on many benchmark tasks, but constructing a solid framework that will enable stable and solid application and research of these wellmotivated models.	0
strengths: the model and the mixed objective is wellmotivated and clearly explained.	2
pros: 1. the paper is well written, and wellmotivated.	2
figure 1 is interesting but it could use better labelling (words instead of letters) overall: pros: wellwritten, good empirical results, wellmotivated and intuitively explained cons: not particularly novel, a modification of an existing idea, more sensitivity results would be nice	2
the other is to prune, but with a very specialized schedule as to the pruning and pruning weight, motivated by the work of narang et al 2017 for nongroup sparsity.	0
pros: this discriminator architecture is well motivated, intuitive and novel.	2
the regularization is motivated from the point of view of sampling the hidden states to be from the exponential family, but all the experiments provided seem to use a gaussian distribution.	0
finally, much of the body of the paper is focused on topic (1) from above, but i did not feel as though this part of the paper was well motivated, and it was not made clear what insights arose from this generalization.	0
pros:  wellmotivated and ambitious goals  human evaluation conducted on the outputs.	2
 pros: ' asynchronous modelparallel training of deep learning models would potentially help in further scaling of deep learning models ' the paper is clearly written and easy to understand cons: ' weak experiments: performance of algorithms are not analyzed in terms of wallclock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm ' weak theory: although the algorithm is claimed to be motivated by continuoustime formulation of gradient descent, neither convergence proof nor algorithm design really use the continuoustime formulation and discretetime formulation seems to suffice; the proof is straightforward corollary of lin et al. summary: this paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously.	2
authors argue their algorithm is motivated by continuoustime formulation of stochastic gradient descent, but it is unclear to me whether the continuoustime formulation was really necessary to derive the proposed algorithm.	0
"the work focuses on tftlike policies, motivated by ""if one can commit to them, create incentives for a partner to behave cooperatively"" however it seems that, as made clear below definition 4, we can only create such incentives for sufficiently powerful agents, that remember and learn from their failures to cooperate in the past?"	0
if convenient, could the authors comment on a similarly motivated paper under review at iclr 2018: variancebased gradient compression for efficient distributed deep learning pros:  good use of intuition to guide algorithm choices  good compression with little loss of accuracy on best strategy  good problem for fa algorithm / well motivated  cons:  some experiment choices do not appear well motivated / inclusion is not best choice  explanations of algos / lack of 'algorithms' adds to confusion a useful reference: strom, nikko.	2
in summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated.	0
 typo, find the “most orthogonal” representation if the inputs > of the inputs overall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental.	0
this paper has many strengths: 1) the writing is clear, and the paper is wellmotivated 2) the proposed algorithm is described in excellent detail, which is essential to reproducibility 3) as stated previously, the approach is validated with a large number of real android projects 4) the fact that the language generated is nontrivial (javalike) is a substantial plus 5) good discussion of limitations overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.	2
this is motivated by use cases in which multiclass classification problems are learned during training, but where binary or reduced multiclass classifications is performed at test time.	0
on the negative side, the paper could be better written and motivated.	0
 pros:  new module  good performances (not stateoftheart) cons:  additional experiments the paper is well motivated, and is purely experimental and proposes a new architecture.	2
a much more important concern i have is that the proposed input invariance property is not well motivated.	0
however, i am concerned about multiple aspects in the current version:  what is the motivation for using this particular form of translation?	0
 the sota result miniimagenet is the result of a bagoftricks approach that is not well motivated by the main methodology of the paper in section 2. major points:  the motivation for and derivation of the approach in section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data s. moreover, the approach is not fully bayesian as a point estimate of the hyperparameter .alpha is computed; instead, the approach is more similar to empirical bayes.	0
" the second motivation is not clear to me, and the claim that ""the training text collection should include many instances of sentences that have only minor lexical differences but found in completely different contexts"" needs more support, either theoretical or empirical."	0
i have some concerns about the motivation of this method:  what are the motivations to use frankwolfe ?	0
in overall, i think the proposed idea is interesting, but the authors’ motivation from dpp is arguable.	0
 in terms of quality: clear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered.	0
the authors state that they apply evolution rater than handcrafting intrinsic motivation, but it appears that intrinsic motivation is handcrafted in the paper.	0
 it would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5. conclusion: the experimental results seem promising however the motivation for the approach is not clear.	0
 the paragraph after eq 3 needs some rewriting  the explanations around and including equations 5 and 6 were quite poor: .pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in eq 6 to avoid ambiguity 2) the reference formatting is wrong (i.e. cite is used everywhere instead of citep) 3) i thought the motivation for the approach in the intro was very good 4) as the seemingly most related work, it would be good to elaborate more on the goyal et al paper and the differences of your approach to theirs.	0
the general idea and motivation are generally appealing but the experimental validation is a mess.	0
my concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of guo etal 2018, which this paper says is the motivation behind their work.	0
pros:  originality of the approach cons:  experiments could have been more convincing:  should compare against at least one other stateoftheart domain adaptation method  results on dependency parsing (the most challenging task they consider) were mostly negative  evaluation on other more recent multidomain nlp tasks would have been nice (e.g. multinli)  abstract and intro could provide better description of the conceptual contribution, as well as motivation	2
 below is based on the original paper this paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentationtaking actions mainly for the purpose of learning an improved dynamics model.	0
justification of the criterion is relegated to earlier work in li (https://arxiv.org/pdf/1512.04202.pdf), but i failed to fully grasp the motivation.	0
"the main concerning to me is that the motivation of using shifting operation to solve the attention ""shift"" is entirely unclear."	0
motivation: the introduction begins by motivating the interpretability story of deep learning, but i don’t see gaining any more interpretability by reading the rest of the paper.	0
using wasserstein to detect hypernym seems to also be novel, but the motivation is also not clear.	0
in sum, while the proposed model seems novel, its motivation is unclear and it is difficult to assess the effectiveness of the proposed method due to lack of experimental validation.	0
comments: there are multiple shortcomings in the motivation of the approach.	0
there are some nontrivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.	0
[cons in summary] 1. the motivations/claimed contributions are not well supported/illustrated by the proposed algorithm or experiments.	2
it is wellknown that larger batch sizes can be used for later stages of optimization (ie, https://arxiv.org/abs/1711.00489, https://arxiv.org/abs/1706.05699), but they are missing motivation as to why use hessian information for this.	0
overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.	0
also authors discuss this work again very briefly in page 8 due to the high similarity in methods and motivation with the proposed method, but the authors don’t show any quantitative comparison.	0
overall the math seems solid but i have a few questions about the motivation and assumption.	0
"i can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides ""to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors""."	0
 pros:  introducing softlabels to selftraining has a good common sense motivation.	2
here 'intrinsic motivation' is used but the 'intrinsic motivation' proposed depends on already having an externally defined reward.	0
detailed comments: • major issues: 1. part of your motivation in the intro states that your algorithm is not replacing sgd, but “complementing” it.	0
the contrast of voxel versus mesh looks like a motivation but it only a speculation.	0
a major motivation for unifying ideas under gvfs is to facilitate development of new algorithms, but i find the paper slightly less convincing on this front.	0
quick summary: while i liked aspects of this  including the motivation of having a lightweight way of understanding how well representations transfer across tasks, overall my concerns surrounding the methodology and some missing analysis leads me to believe this needs more work before it is ready for publication.	0
coupled with the fact that there is no motivation to explain results or future work, this makes for a very poorly written paper that is very challenging to read.	0
i detail some of my primary concerns below:  the entire motivation of the paper is predicated on the hypothesis that more randomness is better for training.	0
pros:  the introduction and motivation is really well written and figure 3 provides a clear visualisation main max pooling operator issues.	2
 the deformation considered is proposed in terms of the graph metric perturbation, which appears to lack sufficient motivation.	0
first it is hindered by a confusing motivation, and the lack of clarity on the real purpose of the work is a problem throughout.	0
these questions point back to the concern regarding what is the real motivation for this work.	0
major points:  organisationally, i thought that the authors could have gotten to the loss function sooner, as much of the development of the theory is lacking in motivation until specific tasks are defined.	0
" ""investigations, we "": don't capitalize  ""the averaging 'has' a smoothing effect""  ""our motivation are""  ""contributed it to""  ""available 'to the' adversary""  ""crafting adversarial perturbations""  ""directly evaluation""  ""be fixed 100"" pros:  transferability and robustness of adversarial examples is a very important problem  interesting insights, esp."	2
ultimately this paper is interesting but falls well below the standards of exposition that i expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances.	0
comments: %%%%%%%%%%%%%%% i found the motivation of the approach to be very lacking.	0
"strengths:  practical proposal to use graph regularization for neural network regularization  the proposal to construct graphs based on the current batch makes sense from an algorithmic point of view cons: experimental results are a bit weak  the most significant results seem to be obtained for ""implementation robustness"", but it is unclear why the proposed approach should be particularly good for this setting since the theoretical motivation is to prevent overfitting."	2
 the motivation for the work is not entirely clear: it is true that gans and vaes have their issues, but in my view it is not really explained / argued why the proposed method would solve them.	0
provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and has rhythmic and polyphonic complexity and sounds quite good (better, in my opinion) some unclear sections (fixable, especially with an appendix; more detail below) despite the extensive architectural comparisons, i was not always clear about rationale behind certain choices, eg.	0
the motivation is not 100% clear because the first experiment uses spikes, and shows a nonnegligible difference to rate models (the authors claim it's almost the same, but for mnist differences of 0.5% are significant).	0
in my opinion, this paper lacks a discussion of that motivation with respect to the proposed approach.	0
however, the paper is lacking clarity and motivation which makes it almost impossible to understand at the first pass.	0
i find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking. '	0
" the technical contribution of this paper is reduced to adding a ""personality embedding"" to approach the image captioning problem (authors also propose a new task and data set, but motivation is not convincing)."	0
[] ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed [] it seems to me the motivation is similar to that of sparselygated moe (shazeer et al. 2017), but it is not clear how the proposed twohierarchy method is superior to the sparselygated moe.	0
this is a small point, but the treatment of intrinsic motivation (i.e. changing the reward function) for exploration seems overly harsh.	0
weakness: my main concern is the lack of motivation for embedding words on the hyperboloid and the choice of evaluation metrics.	0
i thought the motivation for choosing this regularizer was lacking.	0
their motivation clearly outlines that reducing memory can allow for larger batch sizes and larger networks that can improve the performance of training, but the authors do not demonstrate an example of this principle.	0
moderate there is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.	0
strengths:  the paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical bayes setup.	2
due to the lack of discussion, motivation, justification and details of the proposed approach, i recommend this paper to be rejected and resubmitted when all these concerns will be addressed.	0
pros: the motivation of this paper is clear.	2
the extension of the previous work is trivial and the combination of the two ideas lack of any motivation.	0
algorithmically, controlling the lipschitz constant is achieved by using the orthogonal regulariser presented in the paper cisse et.al which has the same motivation for this work but for standard neural network training but not quantising.	0
the motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimumtime optimal control problems, or control schemes such as sliding mode control).	0
you have orthogonal motivation but it might be that in practice your technique works for a reason similar to the reason other techniques work.	0
strengths of the paper:  the motivation to extend compression beyond the weights to activations in order to support the dnn accelerator designs and the technical details are clearly explained.	2
my concern with this paper however, is that i feel the paper lacks a motivation, was it derive an online similarity metric that outperforms sif(without pca)?	0
 derived similarity metric doesn't require knowledge of the entire dataset (in comparison to sif  pca) cons  performance seems to be slightly better than sif, wmd, and averaging word embeddings, but below that of sif  pca  unclear motivation for the model, was it derive an online similarity metric that outperforms sif(without pca)?	2
clarity: the paper is generally wellwritten, but i would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results.	0
"i like to see theoretical work in a field that desperately needs some, but overall i feel the paper could do a much better job at explaining the motivation behind the work, which is limited to ""cosine similarity [...] is not backed by a solid theoretical foundation""."	0
however, in the approach proposed in this paper, a is a simple prior over the weights in a linearintheparameters model, and from my perspective, having a prior conditioned on the inducing variables lacks any theoretical motivation.	0
in summary, it is unclear to me if there is any novelty in the approach (missing references, lack of motivation of the algorithm) and if the results show any improvement over previous works (only one previous work has been compared and the qualitative examples do not show anything particularly interesting).	0
(this motivation wasn't clear in the initial version, but is clearer now).	0
the motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to?	0
the jump from the dp formulation to the learning case is rather abrupt, and lacks sufficient motivation.	0
it sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation.. my main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation).	0
i think there are some interesting ideas in the paper but the motivation is too handwavey and the method exposition is insufficient.	0
over all the paper is well written and easy to follow but is limited by its lack of well detailed motivation and insufficient baselines and applied tasks.	0
later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier.	0
while the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited.	0
since the main motivation of the algorithm is finding flat minima, the lack of convergence proof for nonconvex setting concerns me.	0
"''detailed comments'' _paper strengths_  the idea to use a negative video example for unsupervised detection learning seems novel  the proposed method is simple and the needed data can be collected with widely available equipments  the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example  the authors collected realworld data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)  the authors compare to a number of nonlearning approaches from opensource implementations (the reviewer cannot judge whether any relevant technique is missing)  the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes _paper weaknesses_  the authors clearly reduced the horizontal margins of the standard iclr style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard iclr style template upon update of their manuscript  the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. makerbased approaches could be employed for detecting the object  the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multiobject examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is selfoccluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)  other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging nontraining scenes with heavier clutter, nonseen lighting changes and occlusions to support their robustness claim  the proposed method cannot operate inthewild (e.g. youtube data) as it makes very strong assumptions about the required input data  the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene  the authors make no comparison to other unsupervised detection approaches (e.g. to the selfcited jonschkowski et al. (2017)) to prove shortcomings of other methods on the newly generated dataset  as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for egomotion estimation) makes applications to nonvideo data impossible, also none of the experiments exploit the nontemporal property of the approach to show single frame detection on a more varied set of scenes  the experiment showcasing the proposed application to ""learning from demonstration"" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task  the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)  the reviewer cannot follow the references to objectcentric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an objectcentric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion  the ""random search optimization"" discussed in section 3.3 is not a valid method as it ""solves"" this problem of instable training by picking the best of n runs with varying random seeds  figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach _reproducibility_  given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work."	2
_conclusion_  overall, the reviewer appreciates the effort that went into the work but sees considerable need for improvement concerning the motivation and possible applications given the strong assumptions that are made about the input data.	0
the proposed method does not provide considerable technical novelty or insight to compensate for the lack in motivation.	0
the motivation for the proposed method is to solve the mode collapse problem of mcdropout, but using ensemble loses the bayesian support benefit of mcdropout.	0
weaknesses: ' the discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused.	0
note that the original motivation for signsgd is not for faster convergence but less communication.	0
pros: the introduction and related work section is very well written, and motivation of why one should try adversarial loss for forward models is clear.	2
the ideas in the paper are somewhat interesting, but i have major concerns with the motivation (it is unclear) and the experiments (not convincing): motivation: the authors motivate their inclusion of a hebbian working memory from the perspective of trying to mimic the human visual system.	0
"a similar motivation here is lacking, with the main justification seeming to be to ""move towards a biologically motivated model of vision""."	0
pros:  nice motivation, and the connection with industrial recommendation systems where candidate nomination and ranker is being used is engaging  it provides a conditional generative modeling framework for slate recommendation  the simulation experiments very clearly show that the expected number of clicks as obtained by the proposed listcvae is much higher compared to the chosen baselines.	2
the motivation is to build a network that does not heavily rely on pretrained discriminative networks such as vgg or resnet, but to build one that is fully targeting the segmentation problem.	0
strengths:  motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step  cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation  maze navigation shows incremental benefits over nonmodulated plasticity  thorough experimentation  clippingtrick is a neat observation weaknesses:  evaluation: only on toy tasks (which includes ptb), no real world tasks  very incremental improvements on ptb over a very simple baseline (far from sota)  evaluated models (feedforward nns and lstms) are very basic and far from current sota architectures  no qualitative analysis on how modulation is actually use by the systems.	2
# positive aspects of this submission  the intuition and motivation behind the proposed model are well explained.	2
the general architecture seems to be a reasonable application of the capsule principle in the graph domain, following the proof of concept mnist architecture proposed by sabour et al. my main concern is that i have problems grasping the motivation behind using capsules in the given scenario.	0
pros: 1. the technique has a nice intuitive (but not particularly novel) motivation which is kindasorta theoretically motivated if you squint at it hard enough.	2
i can see the motivation of trying to use smaller augmented memory, however experimentation around slightly larger augmented memories will be useful for the audience to draw some conclusions.	0
pros:  neat motivation;  extensive experiments;  clear illustration; cons  there are still some experiment results missing, as the authors themselves mentioned in the kinetics section (but the reviewer thinks it would be ready);  in page 3 the training section and page 4, the first paragraph, it mentioned θ and φ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative metalearning style method.	2
motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).	0
cons () :  the paper would benefit more motivation and organization.	2
further details below ( for pros / ~ for suggestions /  for cons): the paper could benefit a little more motivation:  mentioning a few tasks in the introduction may not be enough.	0
i understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it.	0
all in all i think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification.	0
i summarized the weaknesses as follows: # lack of motivation and its validation the paper should have more motivational questions at the beginning of why such flow information is necessary for the task.	0
if this is the case, the motivation to go for rl would be more understandable but this part is not explained at all.	0
i understand the motivation from the previous paragraph but felt that the formal result added little.	0
in summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.	0
i would like the authors to carefully explain the motivation, and also provide visual results like using the same random noise as input, but only changing the class conditions.	0
simple idea and relatively easy to implement cons: ' clarity could be improved, especially in the experimental section ' the motivation for the svhn 04 to mnist 59 is not clear.	0
on the negative side, this reviewer would argue the paper is a bit incremental, as it appears to be “just” a combination of two models without a clear motivation from a molecular design perspective – why is the model e.g. better (motivated) than the model by you et al, the molgan by decao and kipf, or the cgvae by liu et al?	0
however, i found the paper lack of motivation about the designs of the coarse and fine scoring models.	0
my main comment is that this work requires (a) more substantiation of the claim that attention shift is the phenomenon at play when it comes to lack of transferability, (b) improvement to the writing, and (c) more motivation behind the choice of mitigation mechanism.	0
weaknesses:  the motivation of uncovering latent modes of a task distribution does not align with the proposed method.	0
this change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.	0
one understands the motivation and the main approach but lacks a detailed understanding.	0
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
however, i am concerned with two issues here, which are related to the motivation and performance evaluation, respectively.	0
that doesn't mean the proposed approach is useless, but the paper needs to give a much stronger motivation for when and why measuring distances between words may be helpful.	0
the proposed clipping does not account for classification accuracy (on training set), but i understand the motivation being that the training set is not available.	0
i believe that the proposed approach (or similar one) might be useful for practice of natural language processing, but to asses that one would need to base on clear motivation and support this motivation with some examples showing that hyperbolicity indeed helps to capture semantics better (like famous world analogy examples for word2vec).	0
i know that motivation is a bit different for stl and proposed method but some comparisons are needed.	0
strengths: ' nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers ' each choice in the model definition are motivated, although some clarity is still missing (see below) weaknesses: ' novelty: the proposed model is a small extension of a previous work (wang et al., 2017) # 2. clarity and motivation in general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: a) page 2 “unlike the conventional memory transition function, it learns the size of temporal interactions.	2
this weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation  one of the mentioned motivations for unbiased estimates  being able to perform model selection on complementary labeled validation set  is not illustrated in the experiments questions:  i believe 1/(k1) normalisation factor in (5) is not needed  there seems to be a mistake in (9) (and its modifications later on)  i would expect either the subscript of the probability distribution in the last summand to be exchanged with in the loss, or a factor added  also, i think there are some mistakes in subscripts in (11)  what loss is the method from [ishida'17] optimising in the experiments?	0
however, the reviewer has concerns about the motivation of the presented analysis and insufficient empirical results.	0
specific points on positives and negatives of the work follow: positives:  the paper shows a solid understanding of the literature in this domain and presents a strong motivation  the problem itself is addressed at a deep level with many nuanced (but important) considerations discussed  ultimately the results of the model seem convincing in particular with the accompanying psychophysical experiments negatives:  (maybe not a negative, but a question) at the extreme tradeoff between intrinsic structure and texture, the notion of a metamer seems somewhat obscured.	0
overall, the approach falls into the field of intrinsic motivation / curiositylike reward generation procedures but with respect to target agent behaviour instead of the agent’s environment.	0
authors provide nice theoretical motivation, yet empirical results seem incremental and do not fully support the effectiveness of this approach.	0
===================================== pros: introduction and preliminaries section give useful background context and motivation.	2
the proposed method seems to at least superficially share motivation with this work (and uses the same atari benchmark, as far as i can tell) but it is not discussed or compared.	0
 this paper attempts to do three things: 1) introduce a generalization / formalism for describing rnn architectures 2) demonstrate how various popular rnn architectures fit within the proposed framework 3) propose a new rnn architecture within the proposed framework that overcomes some limitations in lstms the ultimate goal of this work is to develop an architecture that: 1) is better able to model longterm dependencies 2) is stable and efficient to train some strengths, concerns, and questions loosely ordered by section: stable rnns  it's not clear to me where equations (2) and (3) come from; what is the motivation?	2
weakness: 1. the paper is lack of novelty and the motivation is weak.	0
cons: 1. poor motivation for the particular algorithm implementationfeatures used in the actions, parameter values chosen.	0
pros:  the paper is wellwritten and clearly explains the technique, and figure 1 nicely summarizes the weakness of static channel pruning  the technique itself is simple and memoryefficient  the performance decrease is small cons:  there is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x)  in contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, realtime)  the experiments are limited to classification and fairly dated architectures (vgg16, resnet18) overall, the method is nicely explained but the motivation is not clear.	2
cons:  motivation is not clear.	0
the proposed method is made up of several changes compared to the baselines (e.g. using qlearning without irl instead of irl, using offpolicy learning, using conditioning to obtain a stochastic policy) but motivation for each component is presented late within the paper.	0
on the positive side, the authors managed to nicely motivate their work and tried to make a direct connection to popular approaches like the vae and the sleep and wake algorithm.	2
this type of effort is needed to motivate an extension of maml which makes everything quite a bit more expensive, and lacks behind the stateofart, which uses amortized inference networks (versa, neural processes) rather than gradientbased.	0
1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract).	0
conclusion: the paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate.	0
my main concern is that the various choices are not motivated well, e.g. with examples or detailed descriptions of the issues addressed and that the resulting implications are not discussed in detail (see detailed comments below).	0
in the summary video, authors motivate the case by learning from sources like internet videos, but that setting is also very far away from the case here, because such video collections are much larger but more importantly the main problem is dealing with the third person perspective.	0
the fact that it is being motivated for the specific problem of reducing catastrophic forgetting during lifelong learning is the main novelty here, but the relative amount of novelty might to be somewhat limited when viewed from this perspective.	0
overall, the paper brings up some interesting ideas, but it doesn't motivate all its design choices, and doesn't make a clear argument about the settings in which the proposed method would provide an actual advantage.	0
the authors motivate and demonstrate these constraints on 3 atari games, but it is clear that the constraints they come up with negatively affect performance on most of the games, so they are not improving performance or safety of the agent.	0
the paper however does not motivate its contributions sufficiently, and does not provide enough experimental results to justify their method.	0
the ideas in the paper are somewhat interesting, but i have major concerns with the motivation (it is unclear) and the experiments (not convincing): motivation: the authors motivate their inclusion of a hebbian working memory from the perspective of trying to mimic the human visual system.	0
"a similar motivation here is lacking, with the main justification seeming to be to ""move towards a biologically motivated model of vision""."	0
section 2 discusses, at a high level, broad concepts from the visual neuroscience literature, but this also does not clearly motivate why the authors are interested in this particular instantiation of these ideas, indeed, their model is only weakly related to many of the neuroscience ideas discussed.	0
i do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper).	0
in section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.	0
it doesn't appear to be, but it is unclear what else could motivate it.	0
negative: we are not sure how significant these results are for the following reasons:  ms coco image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on longterm dependencies.	0
pros and cons:  clearly written and motivated  try to address bn’s weakness, which is an important direction in deep learning  i found similar papier in the literature  the proposed method aims to make bn perform better, but pushes it toward small batch settings, which is where bn performs poorly.	0
the authors are suggested to better motivate this paper from the angle of studying the learning capacity of input perturbation induced multitasking learning of a welltrained and fixed neural network model, and compare the pros and cons with transfer learning based on finetuning and joint multitask learning / metalearning on the same network architecture.	2
the authors want to tell a more motivated storyline from nestrovedualaverage, but that does not contribute to the novelty of this paper.	0
in the related work the authors try to motivate their approach but i am afraid in a too handwavy manner.	0
though fundamental understanding can happen asynchronously, i reserve my concern that such empirical method is not substantial enough to motivate acceptance in iclr, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved.	0
strengths: ' nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers ' each choice in the model definition are motivated, although some clarity is still missing (see below) weaknesses: ' novelty: the proposed model is a small extension of a previous work (wang et al., 2017) # 2. clarity and motivation in general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: a) page 2 “unlike the conventional memory transition function, it learns the size of temporal interactions.	2
also variability of concerns raised by other reviewers does not motivate acceptance.	0
al, 2017), but the authors somewhat motivate this approach by showing that it results in a stable and convergent algorithm.	0
the authors motivate this work by bringing an example where the state observation might be novel but important.	0
pros:  since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a wellmotivated problem  it is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision  the proposed penalized approximation of the problem is simple and straightforward to try  experiments indicate that the method works as advertised cons:  there are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines  experimental gains are very modest compared to an independentlytrained crf  novelty is a bit low, since the trick of differentiating through meanfield updates is wellknown at this point  efficiency: the number of variables in the crf scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems differentiating the minimumcost multicut problem is wellmotivated, and i think the multiplehuman pose estimation problem is a nice application that seems to fit this abstraction well.	2
"i am not convinced by the choice of tasks, which is not motivated in the paper (the paper only says ""we don't do sentiment analysis and question answering"", but why?)."	0
strengths: the idea of using the hierarchical structure of the labels is innovative and wellmotivated.	2
writing is very clear and method is well motivated cons: ' i found the experimental validation a bit limited  the presented results are nice and for the problem quite comprehensive but i would have wanted something a bit more complicated than change point detection.	0
better results are obtained on high dimensional toy problems but (a) in these problems the relationship between actions and reward components is somewhat trivial, and (b) since the paper was originally motivated by spatial positioning tasks, it is not clear how often in practice one will need to go beyond 2 or 3 dimensions... finally, since there is no theoretical analysis either (except for some intuition as to what happens in a limit case at the bottom of p.	0
weaknesses:  the paper is motivated in a confusing manner and neglects to thoroughly review the literature on weight uncertainty in neural networks.	0
 the sota result miniimagenet is the result of a bagoftricks approach that is not well motivated by the main methodology of the paper in section 2. major points:  the motivation for and derivation of the approach in section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data s. moreover, the approach is not fully bayesian as a point estimate of the hyperparameter .alpha is computed; instead, the approach is more similar to empirical bayes.	0
pros: ' overall, the calibration approach is well motivated, and likely to be of some benefit. '	2
3. motivation and advantages of the approach:  the approach is motivated by shortcomings of sentence encodings based on language modeling, such as skipthought, which are computationally intensive due to the large output space and the complicated decoding process.	0
bottomline: the paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated.	0
pros: paper was easy to follow and well motivated design choices were extensively tested via ablation results demonstrate successful transfer between sgn, eqa, and the auxiliary detection task cons: with the exception of the 2nd round of feature gating in equation (3), i fail to see how the proposed gating > spatial attention scheme is any different from the common innerproduct based spatial attention used in a large number of prior works, including [1], [2], and [3] and many more.	2
the authors claim that the gdpp loss (eq (5)) is motivated by the dpp, but i think it does not utilize dpp characteristics at all.	0
nevertheless, the paper is not well motivated, and the key procedures, such as “finetuning” lack detail, and comparison with other options.	0
pros:  the paper is wellmotivated, wellorganized and clearly written.	2
the idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [a]) as well as the simple baselines i have mentioned.	0
 paper summary: this paper introduces a biologically motivated blackbox attack algorithm.	0
2. the idea of enforcing a content loss is well motivated but the approach considered seems rather heuristic.	0
i do, however, still have some concerns about how well the privacy guarantees of the proposed algorithm would hold up in practice against a motivated adversary (since formal privacy guarantees appear to be relatively weak right now).	0
cons: the methods are either not very novel or not very wellmotivated.	0
if the doubts about the experiments are clarified and the methods are motivated better (or the strengths/weaknesses are better analyzed), i will vote for acceptance.	2
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
i think that this approach might be motivated in the very early layers of a vision network, but can hardly be a good idea in higher layers, since they require increasingly specific features for a small subset of the overall imagespace to be learned.	0
however, on the same time, i agree with the points raised by the other two reviewers which are all wellmotivated and relevant concerns.	0
strengths: 1. the problem is well motivated and the approach is interesting in that it tries to avoid a difficult problem of projecting text and visual features into a single space.	2
"strengths:  proposed techniques are intuitive and very well motivated  one of the big pluses of this work is that authors try to ""quantify"" each proposed technique with training speed and/or performance improvement."	2
my main concern is that the ideas, while interesting, are not novel, the method not clearly motivated, and the paper fails to convince.	0
"intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled."	0
 “despite the architecture being explicitly designed to provide such invariances” i agree that this has motivated the use and design of cnns in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.	0
conclusion: the paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate.	0
the model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.	0
my main concern is that the various choices are not motivated well, e.g. with examples or detailed descriptions of the issues addressed and that the resulting implications are not discussed in detail (see detailed comments below).	0
the approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated.	0
paper strengths  comparing finegrained vs. coarsegrained action categories for transfer learning is well motivated.	2
" typos in section 3 (""trailanderror""), section 4 (""autonmous"", ""knowledge to"") # recommendation although the theoretical benefits of the method are wellmotivated and clear (offpolicy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple carla test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work."	0
strengths: the challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.	2
the three modifications seem independently motivated and aim to overcome separate potential shortcomings of the current dropout approach.	0
more precisely, since the authors introduce a reference measure lambda on a space z (these objects are not motivated anywhere, but i guess are used to allow for multivalued transport maps?	0
strengths: the nhn method is wellmotivated.	2
how are the representations to be used and what type of users is it intended to serve (expert/patients etc) pros and cons  interesting problem modeling could be better motivated experimental platform is limited for interpretability studies == hsu, w.n., zhang, y. and glass, j., 2017. unsupervised learning of disentangled and interpretable representations from sequential data.	2
 the proposed approach is poorly motivated and presented.	0
first and foremost, the ideas are vague and poorly motivated.	0
strengths: 1. the paper is well motivated by the point of view of human learning.	2
weaknesses: 1. while the paper is well motivated, i believe that the presence of multisensory expectation learning depends heavily on the type of multisensory data.	0
pros:  intuitive idea for a common problem  solution elegantly has the form of a modified policy gradient  convincing experimental results  selfcritique of core idea, and extension to address its main weakness  nicely written text, does not leave a lot of questions cons:  while the core idea is nicely motivated and described and good to follow, section 2.3 feels very dense and too short.	2
1. the paper is poorly motivated and does not make an attempt to relate its results to observations in practice or the design of new techniques.	0
how does this compare with the flag method of chen et al from aistats, which is motivated by similar issues and addresses similar concerns, obtaining stronger results as far as i can tell?	0
 more broadly, but following from the above: the paper does not provide any real world examples, real or hypothetical, to give the reader an idea of whether the above uniformity assumptionor really any of these assumptionsare wellmotivated or empirically justified.	0
cons  the problem is not very well motivated and the novelty is limited.	2
the loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?	0
paper strengths  the problem of reducing computational requirements when using cnns for video analysis is well motivated.	2
the fact that it is being motivated for the specific problem of reducing catastrophic forgetting during lifelong learning is the main novelty here, but the relative amount of novelty might to be somewhat limited when viewed from this perspective.	0
in terms of strengths, the ganbased approach is wellmotivated and it appears that the authors were thorough in their experiments on cora/citseer (e.g., with a number of ablation/sensitivity studies).	2
5) the gan method is interesting and wellmotivated, but it is not clear if this method offers any utility beyond the “distribution matching” approach of zugner et al (section 4.1 of their paper).	0
concerns:  the problem doesn't seem to be wellmotivated.	0
the incremental technical innovation is not wellmotivated or justified, either: the definitions of new concepts such as ‘functional essence’ and ‘importance vector’ are adhoc.	0
the overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout.	0
pros/cons summary:  the proposal yields good results in the provided experiments  minor contributions that are not convincing enough  muddled presentation of ideas  dubious or weakly motivated design choices  poorly written with plenty of typos  difficult to follow	2
cons:  the paper seems to have a flaw which calls into question whether it is well motivated (see main text).	0
the presentation of the paper is excellent  clear, wellmotivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.	0
more specifically, i have understood that your approach is motivated by theorem 1 (nice result, by the way) but i am not sure it is better than just applying the kl divergence with respect to p(x) and q(x), directly.	0
ultimately, the benefit of using gapnet over the other architectures is not strongly motivated, as training time is less of a concern in this application than predictive power.	0
the paper is not very well motivated, and therefore lacks clarity and leaves the reader with a lot of unanswered questions.	0
uncertainty could have been better explained, clarity, the main methodological contribution (hierarchical cd) is well motivated but only provided in the form of an algorithm.	0
uncertainty estimates could have improved the significance of the usability study pros and cons  interesting problem  wellmotivated algorithmic extension of cd  uncertainty of usability experiment?	2
importantly, the authors are not seeking to establish new results for unsolved problems, but instead they are motivated by analyzing and comparing the quality of solutions predicted by reinforcement learners with respect to the wellknown nearoptimal strategies for some oco tasks.	0
 pros: the paper is wellwritten and wellmotivated.	2
in summary, the paper makes a number of observations that have been motivated in a number of prior works, but the contributions of this paper is not highlighted (e.g., over neural scene derendering).	0
pros:  a wellmotivated and interesting method.	2
"a similar motivation here is lacking, with the main justification seeming to be to ""move towards a biologically motivated model of vision""."	0
"""lotionscale"" > ""locationscale"" pros: ' an interesting and wellmotivated approach to an important problem ' interesting connections to gps in mdps cons: ' experimental domain does not ""exercise"" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known ' work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper)parameter settings"	2
 motivated by the observation that most of previous dimensionality reduction methods focus on preserving local pairwise neighboring probabilities and lack in preserving global properties, this paper proposes a method called trimap to optimize a loss function preserving similarities among triplets of data points.	0
the idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide.	0
pros: 1. the technique has a nice intuitive (but not particularly novel) motivation which is kindasorta theoretically motivated if you squint at it hard enough.	2
pros:  problem is wellmotivated with a reasonably good overview of this research area.	2
the strengths of this paper are:  this work addresses an important problem and is well motivated  experiments on both simulated and on a real system are performed the weaknesses:  the related work section is biased towards the ml community.	2
i do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper).	0
first, this paper is not highly motivated and lacks of intuition.	0
===================================== overall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and wellmotivated approach with strong empirical results.	0
pros and cons:  clearly written and motivated  try to address bn’s weakness, which is an important direction in deep learning  i found similar papier in the literature  the proposed method aims to make bn perform better, but pushes it toward small batch settings, which is where bn performs poorly.	0
originality: this paper presents original findings but occasionally relies on work motivated by itself to convince the reader of its importance.	0
as the argument is that the proposed loss is better than the reconstruction one and that of hoffman et al. 2018 for lowresource supervised adaptation, it would be worth demonstrating this empirically in table 2. summary: the proposed objective functions are well motivated, but i feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at iclr.	0
i admit that the paper is interesting and well motivated, but it is still far from being publishable.	0
my enthouasiasm was somehow tempered when discovering that the maso modelling here was in fact a special case of balestriero and baraniuk (icml 2018), but it seems that despite this the specific contribution is well motivated and justified, especially regarding application results.	0
the authors want to tell a more motivated storyline from nestrovedualaverage, but that does not contribute to the novelty of this paper.	0
the experimental validation is not extensive, but the proposed method is well motivated and as far as i can tell original.	0
on the negative side, this reviewer would argue the paper is a bit incremental, as it appears to be “just” a combination of two models without a clear motivation from a molecular design perspective – why is the model e.g. better (motivated) than the model by you et al, the molgan by decao and kipf, or the cgvae by liu et al?	0
the methodological novelties seem moreorless limited, but the theoretical analysis and the intuitive (and wellmotivated) modification over cgans add merits to the paper.	0
pros and cons:  the paper is well motivated, not only through the text but also with empirical evidence (section 2).	0
pros and cons:  clearly written  clearly motivated  nice review of literature  quite incremental (close to pathsgd / weight normalization), and missing actual comparison with weight normalization, which seems to be the direct competitor of enorm (see detailed comments)  some flaws in the experimental setup (see detailed comments), particularly in the fullyconnected experiment.	0
while the presented ideas are well motivated and it is certainly a good idea to combine deep rl and evoluationary search, novelty of the approach is limited as the setup is quite similar to the erl algorithm (which is still on archive and not published, but still...).	0
of course the specific choice of lsif e.g. can be motivated since it has a closedform solution, but the basic point is that the two approaches really boil down to changing the underlying loss function.	0
"pros: ' the problem is interesting and well explained ' the proposed method is clearly motivated ' the proposal looks theoretically solid cons: ' it is unclear to me whether the ""efficient method for sn in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as miyato et al. 2018, which also used sn in conv nets with different strides."	2
such concerns have motivated many of the recent developments in approximate posterior distributions.	0
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
the following questions could be addressed by the authors in a revised manuscript: ' the upsampling operation is not well motivated, e.g., neighboring node features are weighted independently, but root node features are not.	0
pros:  the concept of certified costsensitive robustness is well motivated and clearly presented.	2
pros:  the proposed method is well motivated from empirical studies by visualizing parameters of three tasks, and the analysis on rare words are convincing.	2
pros:  the problem tackled by this paper is interesting and well motivated.	2
pros/cons:  properlymotivated contributions and wellwritten paper.	0
pros:  appealing, wellmotivated idea for training policies via language.	2
strengths: ' nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers ' each choice in the model definition are motivated, although some clarity is still missing (see below) weaknesses: ' novelty: the proposed model is a small extension of a previous work (wang et al., 2017) # 2. clarity and motivation in general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: a) page 2 “unlike the conventional memory transition function, it learns the size of temporal interactions.	2
strengths:  well motivated and relevant topic.	2
strengths: the problem is wellposed and wellmotivated.	2
i had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in highd.	0
my primary concern with the work is that it is not clear to me how the specific two player game is motivated.	0
adding noise to generate responses is somewhat new, but that doesn’t seem to be well motivated or justified.	0
i however find the approach wellmotivated and pertinent.	0
review: i do not recommend this paper for publication in iclr because i believe: 1) the work is too incremental 2) the comparison to baseline and competing methods is incomplete 3) some design decisions of the proposed method are not well motivated.	0
it is also lack of argument/motivation why the proposed model is special and specific to the problem.	0
1. though the idea of extracting uncertainty is interesting, but i think the motivation and explanation is not enough, so i couldn't find a rationale why we should do this.	0
the motivation of the proposed network is based on the observations that the traditional domainadversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the featurelevel alignment, 3) it lacks the interpretable explanation for the learned feature space.	0
that would not only help the reader understand the approach better but also be much more in line with the main motivation of the paper.	0
pros: ' important motivation for why audio has different properties than images (even if it can be represented as a 'image' spectrogram).	2
given that object manipulation is the specific application of interest, the comparison with diayn and the combined objective with diayn is interesting but little motivation or discussion has been provided in the paper.	0
the motivation for this modified version of adamw are unclear, but the empirical results are convincing and rigorous.	0
this paper lacks of research motivation and solid experimental validation.	0
review:###the motivation for this paper is as follows: why sgd: 1. easy 2. applied in online settings however: 1. convergence proof has been done by others, but the assumptions of their proofs cannot be applied to problems involving deep neural networks, which are highly nonsmooth and nonconvex 2. suffer from gradient vanish 3. sensitive to the input the paper is based on new ideas on alternative minimization method.	0
one of the main motivations of the paper is to be able to generate 44khz audio, but the only results available at this resolution are inception scores that are below those of the 16khz generation, which leaves open the question of whether the goal is effectively achieved.	0
nonetheless, the paper lacks clarity and motivation for the exact form of the bandwidth extraction module, and does not fulfill its promises (importance of progressive training, high quality generation at 44khz), so i am leaning towards rejection.	0
when atmc is described (temperature stages), a motivation is provided but not justified theoretically.	0
the motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting.	0
strengths: ' the reconstruction formulation of the problem is interesting and relevant for the task ' proposed new metric to measure grounding performance weaknesses: ' questionable motivations: it is not clear what application is grounding text to the image useful for? '	2
improvement on image captioning metrics by using grounded text (proposed method) compared to not grounding (gvd) ' limited novelty: extension of gvd, where attention is removed and object locations are used instead # 2. clarity and motivation the paper is generally well written, however there are some concerns on motivations.	0
one concern is related to the motivations of the paper.	0
other comments: the motivation for using longterm predictions to “infer more meaningful novelty” is fine on it’s own but seems to conflict with the choice of random network (rnd) state features.	0
for additional motivation, 'why is posterior sampling better than optimism for reinforcement learning' (osband & van roy 2016) offers some justification on the downsides of modeling novelty bonuses for each state/action independently.	0
i understand the motivation in section 3.4 that is should help to improve the convergence rate, but for benchmarks like halfcheetah, walker, the performance seems to have saturated to a significantly lower value.	0
cons & questions: 0. motivation.	2
 motivation to use the proposed loss balancing rather than that of very similar methods (e.g. gem/agem) is lacking ('these approaches cannot capture the dynamics in the lifelong learning process', but gem's balancing is also dynamic)  solving for beta using optimization (eq. 10), while (eq. 9) should have an explicit, closeform solution.	0
• below eq. 16 it is said « notice that c has been omitted », but it is unclear whether it was not included to alleviate notations or because it disappears naturally in the mathematical derivation of eq. 16 • the motivation for introducing zeta below eq. 17 is unclear, especially since it seems to play an important role considering that zeta = 0.005 << 1 is used in the experiments (with no explanation as to how this specific value was selected) • hat{l}(z_i, theta, phi) (between eq. 17 and 18) does not seem to be defined • what is the motivation for using the gaussian u(z) as described in section 5?	0
the paper provides a thorough background on previous work; however the motivation for having an inputdependent dropout method is relatively weak.	0
i may be alone here which is open to discussions, but the original motivation for mil is for the problem to be weaklysupervised where we only know a highlevel label but no lowlevel labels.	0
it lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in eq. 1 is the crossentropy objective.	0
the motivation for this algorithmic procedure is somewhat 'handwaving', but rather intuitive.	0
1. in term of the motivation, those cases listed in the paper are interesting, but they are not representative.	0
overall the paper is flawed by the lack of clarity in the motivation for the proposed methods, and the lack of retrospective analysis and understanding of why the proposed methods should improve results.	0
a lack of any compelling motivation could be overlooked if the empirical results where compelling.	0
however i don't think this paper solved a sufficiently clear problem and the motivation is somewhat confusing to me, especially when it seems like an analysis of dropout, and that isn't even mentioned until the 7th page.	0
while the motivation is interesting, the author argues this work is novel due to it does not fall under supervised learning, but rather reinforcement learning.	0
currently i vote for borderline reject as i am familiar with rl & pde's but do not understand the motivation and intention.	0
it is also poorly written without clearly stating the motivation for this problem.	0
first of all, the paper lacks a good motivation.	0
the paper addresses an important practical challenge in machine learning, but a confusing problemframing and lack of robust baselines make me skeptical that it is suitable for publication at iclr 2020. a primary concern with this work is its framing of the problem as one of measuring aleatoric (irreducible) uncertainty, but the motivation in transfer learning and interdependence in production ml systems requires models that can characterize epistemic (reducible) uncertainty.	0
the main motivation of the paper seems to be to learn what samples to drop, but the authors do not address what can be done about the dropped samples (i.e what happens if we end up having to drop 80% of the samples?)	0
i understand that these are all fairly heuristic choices, but nevertheless there needs to be proper motivation for all of the above.	0
the main reason i tend to reject this paper is that the motivation of their proposed algorithm is very unclear, lack of theoretical justification and the empirical justification is restricted on ppo  one policy gradient method.	0
the motivation of the paper is also questionable: the introduction section describes some of the peculiarities of the amharic language, such as being morphologically rich, but the proposed method seems completely generic and does not address or exploits any of these peculiarities.	0
this paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically.	0
i can see how its might be useful but would be very happy to see more motivation for this.	0
thats a different paper but i missed the motivation for that here too.	0
cons: the paper is not wellwritten and the experiments and discussions are not support the ideas, more specifically i can mention several concerns: 1 motivation: the flaw of the previously proposed calibration metric is not explained clearly.	0
concerns: 1) in general, paper is hard to follow and main motivation of the accomplished work is not clearly stated in the paper.	0
my main concern is there lack some insightful discussion regarding the problem motivation and the proposed algorithm.	0
pros • interesting approach to extend the framework in [1] to cnns, with use of masks and maskspecific loss • clear motivation for the network bandwidth limited use case cons • hardly any technical novelty because the core ideas are already presented as well as applied to the same task in [1] • it is very surprising that the authors do not even cite [1] in their paper, despite their work being extremely closely related to it • most of the discussion in the related work section is unrelated to the specific task they tackled in the paper (i.e., input/feature selection).	2
major comments i have a number of serious concerns about the paper's motivation, logic, and experiments:  first, the paper motivates the proposed regularization as a way to encourage the network to have line attractor dynamics.	0
in overall, i liked its novel motivation and simplicity of the method, but it seems to me the manuscript should be improved to meet the iclr standard.	0
the end of the related work section is not very clear, you say these methods are problematic because 'the adopted shaping reward yields no direct dependence on the current policy' but there's no explanation or motivation for why that would be a problem.	0
the revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with gail, so i'm updating my score to weak accept.	0
4. part of the motivation for your approach as given in the abstract and intro is not to have to save examples, but your method does require this for computing the regularization term.	0
the motivation for the paper is gans, but gans are not convexconcave.	0
11. except the computational concern, i still don't quite exactly see the motivation behind clustering attention.	0
the authors mention alignment of attention, but the exact motivation for the algorithm, i.e. the motivation for its formulation, are never provided.	0
another downside is that, although sim2real is used as a motivation for this paper (since this is the most typical scenario where states are available during training but not during testing), the experiments have not been performed using physical agents.	0
i do understand that physical environments are harder to manage than simulated ones, but tiny physical environments can be obtained for reasonable prices, and this would have made the paper much stronger and more aligned with its core motivation.	0
i am not very confident in my assessment but i have the feeling that the paper could be greatly improved by some restructuring and giving more insights and motivations.	0
in particular, there is an article making essentially the same points: joan bruna, soumith chintala, yann lecun, serkan piantino, arthur szlam, and mark tygert, 'a mathematical motivation for complexvalued convolutional networks,' neural computation, 28 (5): 815825, 2016 http://tygert.com/ccnet.pdf where these authors make the same conclusions and observe that learning reduces to figuring out the windowing, number of scales, etc. but not the type of filters.	0
my major concerns are the motivation and analysis of the proposed random walk and the training algorithm, whose details cannot be found in the paper.	0
my main concern is the motivation of the paper and the experiments.	0
maybe what it lacks a bit is motivation for a machine learning audience, for why these mathematical tools from differential geometry would be useful or needed by the community to tackle presenting machine learning problems.	0
this weakens its necessity and motivation.	0
strengths  the literature review of existing work on gnn was a pleasure to read and provided a good motivation for the proposed gnnfilm architecture.	2
major comments: 1. motivation unclear:  notational issues in (2): mse(h_t | x) involves y, but y is not specified in the definition.	0
pros: the paper is written clearly and the motivation of designed loss functions are explained well.	2
given my major concerns about the theoretical motivation and comparisons to past work, i do not find the experiments comprehensive enough to prove the value of the proposed approach to the community.	0
this does a good job of motivating the method (i.e. avoiding false positives), but also undermines the motivation behind reward filtering, which is perhaps the main technically novel component of the proposed approach.	0
while i think this work has the potential to be a significant contribution, i rate this a weak reject because the theoretical motivation and analysis of the experimental results are lacking the depth of evidence i would expect for an iclr paper.	0
my main difficulty was with understanding the actual architecture and its motivation, but i believe this is fixable but is a serious impediment to being able to evaluate the paper, as it stands.	0
however, in its current form it does not seem accessible and lacks a lot of the motivation,	0
the approach looks interesting but the motivation of using an might not be well justified in the current presentation.	0
this is a very obvious generalization from binary to narray relations, but the work is very incremental and does not provide any good motivations.	0
 shed some light on the problem of catastrophic forgetting and continual learning without supervision  the model architecture is well defined, but has some weaknesses in the methodology  interesting motivation of trying to mimic how animals learn in an environment	0
exploration might not be only (or main) motivation behind maxentrl but it still a good motivation.	0
4.5/5 [strengths] 'motivation' the motivation for investigating machines' ability to generating questions to gain important information is convincing.	2
such modification is also lack of motivation, unexplained in the paper.	0
the proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.	0
concerns with the motivation and paper overall: — paper seems to emphasize the “unified” perspective of certified training and regularization.	0
in light of the experimental concerns and generally weak motivation/description of the proposed regularizer, i vote for rejecting the current version.	0
the paper addresses an important problem, however i do not find the highlevel motivation behind the proposed approach or the experimental results sufficiently convincing.	0
more minor, but i found it hard to follow the writing in the paper (this is related to the motivation being unclear).	0
moreover, the value is described as q(t), where t is the timestep in the trial; however, elsewhere in the text it is mentioned that the state is not simply t, but contains also the motivation value mu.	0
however, while i find the results slightly interesting, but not very significant, as someone interested in the biology of motivation, i question whether the nature of these contributions would be of broad interest at this venue.	0
personally, i am interested in motivated behaviors and think that future ai developments should take note of this field, but again, the present work does not provide actionable insights into implementing an artificial motivation system.	0
review:###summary  (motivation) this paper proposes a new approach to pruning activations from neural networks, but uses it to understand neural nets better rather than trying to make them more efficient.	0
clarity: i could understand most experiments at a high level, but i found it hard to understand the motivation and the rest of the experiments.	0
in my opinion, the motivation of the paper is clear and the writing is easy to follow, but one potential limitation is the lack of comparison with recent related work, e.g., generate to adapt: aligning domains using generative adversarial networks swami sankaranarayanan, yogesh balaji, carlos d. castillo, rama chellappa https://arxiv.org/abs/1704.01705 deep transfer learning with joint adaptation networks mingsheng long, han zhu, jianmin wang, michael i. jordan https://arxiv.org/abs/1605.06636	0
the motivation in the paper states that existing or algorithms are 'very complicated and consist of many handcrafted rules and heavily rely on expert knowledge, thus being difficult to generalize to other combinatorial optimization problems', but this algorithm seems to suffer the same problems.	0
however, i have the following concerns:  the motivation is not clear.	0
currently, i am for borderline reject but i am happy to increase my rating during the rebuttal, when the authors clarify the motivation for the experiment and their contribution to hrl.	0
here are my main concerns of the current paper: 1. the presentation is a little bit confusing in the motivation section.	0
2. decision: weak reject  the classwise idea for curriculum learning is interesting but the motivation and intuition behind the design of the proposed method is weak.	0
cons: the motivation behind the ideas of the paper and the design of the procedure was not so clear.	0
1. the motivation for matching distributions as opposed to learning the model the traditional autoregressive way is lacking.	0
====== strengths and weaknesses ======  the motivation to make random forests respect the input structure similar to convnets is wellgrounded and important since random forests are still being used in certain applications where computational complexity and/or interpretability are crucial factors.	2
i like this aspect of the method as it makes the proposed algorithm quite novel, but the motivation for why this should work didn't come through for me.	0
overall assessment: the paper has some interesting ideas, but it lacks motivation and significant results.	0
i think the paper is borderline, but i am slightly leaning towards rejection due to the insufficient motivation.	0
however, i have several concerns about the experiments, motivation, and algorithmic decisions which make me hesitant to recommend the paper for acceptance.	0
a comprehensive study in the paper shows a better performance gain compared to the existing method, but it would be better if the gains were substantial or the authors presented a good motivation on why this architecture is good in some cases.	0
the authors claim 3 major motivations for the dcgan; 1) dilated convolutions for introducing music related inductive priors, however the model description does not provide any intuition or insight for why the exact filters used were chosen.	0
comments: my major concern for this paper is its unconvincing motivation and experiment results, especially when the approach is designed for deep learning.	0
comments: [1] i am concerned about the motivation behind this paper  which, according to the paper is that nesterov’s accelerated gradient method with stochastic gradients has huge initial fluctuations.	0
the proposed laplacianpooling ideas could be interesting, and the results encouraging  but i found the mathematical motivation to be not very convincing, and has various (mostly correctable issues).	0
the intuitive motivation provided in the paper aims to add a cooperative component to the twoplayer game, but the minmax objective corresponds to a zerosum adversarial game.	0
cons:  the motivation of this paper seems to be weak.	0
there are lacks of motivation why in the paper there needs bes/bem, the interplay of policies, trajectories, and embedding maps, and why nonsurjective of bem is still fine in this case, etc.. in addition, the definition of statebased, actionbased, rewardbased assume only discrete domains?	0
this is sufficient for analysis and motivation, but taking the developed approaches and applying them to a deep neural network and showing improvements would go a long way towards improving this paper.	0
cons & questions: 1, the main motivation of exploiting the graph decomposition is to save memory such that gcn could be applied to large scale graphs without sacrificing the structural information.	2
for instance, the average performance of mgnet over a set of randomly selected hyperparameters versus the model's performance, the computational cost of training a good mgnet mask generator versus the introduced method, etc. one major motivation behind perturbationbased methods is their blackbox nature.	0
during my first pass of the paper, some of the subsequent definitions were lacking a clear motivation.	0
 the language and motivation in section 4 is somewhat informal; this is not an issue for me, but i wanted to mention it as a something that could potentially be rephrased.	0
the use of errp in rl appears novel, however, there are several concerns that prevent me from recommending acceptance: 1. the motivation for the use of errp is very weak.	0
my main criticism of the paper in its present form is the lack of motivation for the proposed method.	0
the main weaknesses of this paper are listed as follows: 1) the motivation is relatively weak, which is to bring in the analogous building blocks in cnn architectures.	0
the motivation and the theoretical arguments are interesting, but the paper lacks in presentation and sufficient empirical evidence is also lacking to get fully convinced by the claims.	0
strengths:  the biological motivation is quite clear  architecture is simpler than that of previous related work (hgru) weaknesses:  not clear what the objectives/contributions are  no advancement of state of the art in computer vision  no novel insights about brain function  motivation of the 'texturized challenge' is unclear  performance on bsds500 is far from state of the art  value of the qualitative analysis on stylized imagenet is unclear overall, i'm not sure what the goal of this paper is.	2
this is lacking almost any motivation, or discussion on what problem they are trying to solve.	0
weaknesses:  i like the motivation of predicting empathy, but the paper vastly oversells this part: i don't see how predicting the listener's hidden state is the same as modeling empathy.	0
for example, one can use training or validation sets for that but the authors choose the training set without motivation.	0
however, there are many concerns with respect to motivation and techniques that should be resolved.	0
the sandwich bound is novel in and of itself; i understand the need to relate to prior work, but i actually think dropping the language around 'decision states' (maybe outside of the algorithm's motivation) and talking purely in information theoretic terms would improve the paper.	0
 there's lack of analysis / interpretation of results for section 4.1, e.g. what is the motivation of the experiments and what is the conclusion.	0
your motivation (helping people with dyslexia in ethiopia) is certainly laudable, but neither the work carried out nor its presentation meets the standards of our research community.	0
pros: this is an interesting paper with a clear motivation, which is to fix the socalled confirmation bias that appears in pseudolabeling methods for semisupervised learning.	2
however, i have a litany of concerns with the paper itself, concerning its high similarity with a paper published in may, its motivation, its presentation, its empirical evaluation, and the analysis presented within.	0
the motivation is that the current compression methods work for other kinds of neural networks (classification, detection), but perform poorly in the gan scenario.	0
such a weak motivation is mainly because of the insufficient 'intuition' in section 2. the author mentions 'the hessians in deep learning problems are not diagonal', but does not provide further explanation on why more importance should be lay on serving both max and min eigenvalues.	0
unfortunately, the impressive results on dmlab are not sufficient for both the lack of deeper empirical study and better theoretical motivation for the architecture modifications.	0
my main concerns are regarding both the motivation of this work and the quality and soundness of the claims.	0
on the theoretical/motivation side, it is not enough to say that demographic parity is achieved when the corresponding wasserstein distance is 0. what is needed is that demographic parity difference is bounded from above by the corresponding wasserstein distance (i don't know if it is true or not, but would like to know).	0
============= to improve paper:  clarify motivation and how this would inform adversarial training highly nonlinear classifiers;  add related work for robustness to perturbation of linear models, or state that they don't exist;  clarify weaknesses in the claims.	0
the rebuttal does not address my major concern (motivation), nor does it discuss its relationship with related work.	0
the motivation for using her makes sense, but maybe a bit more would be useful to describe why we need text here and not just regular her.	0
overall motivation makes sense, do language grounding in an interactive way much more effectively by leveraging hindsight relabeling but to get around the circular problem of hindsight generation leverage a model of successful behaviors seen thus far.	0
 i understand the motivation of vae  one shot, but i am not very convinced by vae  gradientbased.	0
the motivation keeps repeating that no taskspecific prior knowledge being necessary, but i believe this hinges on 'h' being sensible, which might not be feasible without taskspecific prior knowledge.	0
this claim can be made secondarily or as motivation for continued exploration along this direction, but i think listing them as two distinct contributions is necessary.	0
that being said, i have some concerns about the paper: 1) the motivation for explicitly encouraging diverse policies is a bit confusing, and isn’t very convincing.	0
i think that it's interesting to apply the capsule network architecture to tree classification, but unfortunately it doesn't appear that some of the motivation for capsule networks on images didn't seem to transfer neatly to this setting; for example, there is no equivalent of inverse graphics as there is no reconstruction loss (as pointed out by the authors in section 6.4).	0
the other motivation is that this is a step toward studying an expanded threat model, but the authors have not demonstrated that the learned representations are any bit more robust to common corruptions (could the authors show the generalization performance on cifar10c or generalization to unforeseen corruptions?).	0
however, i think that the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the tradeoffs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface.	0
## weaknesses (1) the motivation for the proposed problem does not convince: why do i need to train a q/a system to infer which components of the true state are captured by the learned representation?	0
i cannot recommend acceptance of the submission in its current form, but i encourage the authors to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.	0
the paper is also quite well written, motivation issues aside, so i would not be upset if it was accepted with the hope that it leads to future work addressing the abovementioned concerns.	0
given that you are not learning distributions but expectations in the form of value functions, how pertinent is the motivation of learning p(y|x) and p(z|y) instead of p(z|x) directly described in the introduction?	0
although some of the results are interesting, i have lots of concern on the motivation of this work.	0
the motivation of the algorithmic design is for the most part clear, but the rationale behind the particular choice of the parameter confidence score is unclear, and should be clarified.	0
major concerns: motivation   it is not clear how the proposed clustering mechanism to discover events allows to successfully capture information that an rnn based approach does.	0
strengths  motivation: the authors consider the correlations between ddi labels (multilabel) which could potentially improve prediction of ddi.	2
the paper lacks also proper citations to previous work and i find the background section and motivation rather weak.	0
i also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper?	0
however i do find the motivation of this paper is a bit weak, and i’m having a hard time finding the highlight of the paper.	0
the work is sensible and the approach is clear but i found the evaluation and motivation lacking in key areas that i mention above.	0
my major concern is that the paper, including the motivation and illustrative example, are too similar to previous work [12].	0
major concerns: (a) motivation: the authors do not discuss or motivate the problem and why it is important to the community.	0
 then follows without transition two pages of background that are mostly definitions but without a proper motivation, these are difficult to process.	0
the first part about the mfcc is interesting and relevant, but it could be in the introduction as a motivation.	0
pros: 1. the paper is well written, the motivation and methods are clearly described.	2
the main reason is that the motivation is not strong enough and it makes the entire work somehow incremental.	0
i think the motivation is adequate, but the review of the literature glosses over related work (or the absence thereof) in predicting the direction of arrows in causal graphs.	0
overall, the idea is clearly presented, but appears to have many critical flaws, enumerated below: 1. it is unclear what the motivation for the symmetry is upon first reading of the paper, i.e., section 3.1 starts by saying 'to overcome the shortcomings of apl,' but up to this point _x0010_i cannot find any exposition that explains what these shortcomings arethe beginning of section 3 just presents the formulation of apl and does not discuss its advantages/disadvantages.	0
pros: 1. the motivation of the work is clear and solves an important task 2. the approach is sensible cons: 1. experimental evaluation is very weak.	2
review: i have several concerns with the paper: 1) geodesics are never motivated: the paper provides no motivation for why geodesics are interesting objects in the first place, so it is not clear to me what the authors are even trying to approximate.	0
to this end, i believe since the motivation is not clear nor the results are generic enough, i find the work incremental specifically after noting that the radius can be deduced from the work of li et al. where the main contribution here is the tightness of the radius for a binary classifier.	0
the motivation for the first method (entropy decreasing along a markov chain due the data processing inequality) seems to only be valid when y := f(x), but not necessarily when y := f(x)  e. for example, let f be the identity function and e be independent to x. how did you resolve this argument against the intuition??	0
however, motivation for css is that there is structure in the recovered signal — however no comparison of the recovered structure is made.	0
primarily, the conditional of x given y is gaussian/logconcave (or at least unimodal, more generally ) but the motivation is based on deep neural networks (for why the gradient is bounded).	0
the following should be addressed:  provide more evidence for the conjectures surrounding the motivation and derivation  supply more varied baselines (e.g. rad model) minor comments:  at the top of page 4, you refer to several times, but i think you mean  in the paragraph after equation (2), the superscript seems to be missing from the  in the first sentence of section 3.1, you refer to 'the single used in equation 2', but equation 2 mentions no  in the first line on page 3, you talk about some region of supp being pushed out of supp , shouldn't this be the other way round since the kl is infinite only if the the support of is not contained within the support of ?	0
the motivation is interesting to me, but the authors do not provide enough justification.	0
the motivation is slightly lacking  it is not clear why we are interested in these three task settings i.e. what we will learn from a difference in their performance, and there is a lack of discussion about which setting makes for better singing voice generation.	0
moreover, the paper lacks motivation of the design choices.	0
main comments: ============== 1. motivation: i found the motivation for the problem understudied a bit lacking.	0
 engaging with the literature on style transfer in language generation would be good, as mentioned above for motivation, but also to situate this work w.r.t to related style transfer work.	0
the authors write in the 'related work' section that gmm with regularization was proposed by [verbeek et al. 2005], but it is an older idea  for example [ormoneit&tresp 1998] in section 3.1, the motivation for the maximization in eq. (2) is unclear.	0
(minor) first/2nd paragraph of 3.1 seems a bit redundant making it hard to follow overall i think the idea to consider metric learning and local adaptation for continual learning is interesting, however the current work is currently lacking in both experimental evidence (appropriate comparisons) and clear motivation/difference to existing work for its particular instantiation of this idea.	0
pros: the motivation for having only one model is interesting the results are promising cons: one of main motivation of the paper is to achieve zeroshot as opposed to previous work.	2
there are no such constraint in ae as well  you can vey well have your hidden representation to be over complete  so this is not a good motivation for gibbs energy 2. main section of 4.1 says that they use mnist but the caption of table 1 says they use fashion mnist 3. the naming convention of the model is hard to follow and the authors notation itself is inconsistent.	0
the paper mentions that 'later we will explain the precise motivation for these choices', but i cannot find any such explanation.	0
i would like to see a bit more motivation of the placement of this work in the grander scheme of the ml literature; at the moment it feels like a letter directly in response to zhang 2016 (which is not a bad way to consider scientific research, but without centering the threads of inquiry this is a slippery slope down an unrecoverable rabbit hole).	0
the motivation is that the given documents in conversational rc tasks are usually long, but most previous approaches do not handle this problem properly.	0
the paper is on a very interesting topic, but it lacks proper methodological motivation, discussion, and backing up of the claims.	0
overall, due to a lack of motivation and the belowmentioned concerns, it’s hard for me to envision this paper offering a significant contribution to the community.	0
the writing seemed relatively clear overall, however in section 2.3 the model architecture, and its motivation, are a bit hard to follow.	0
however, there are some concerns about this work: 1. the motivation of using selforganizing map learning algorithm to create the gene similarity network (gsn) is not clear enough.	0
3.2) was quite terse and lacking in motivation/intuition, even though i don't think that the overall procedure is in fact very complex (and thus excessive notation is probably removing versus adding clarity).	0
i have a number of concerns regarding the motivation, experiments and overall writing quality, so i am currently leaning toward rejecting this paper: 1. the justification for paths in section 2.1 is extremely handwavy.	0
this makes the motivation for sec 3 unclear  the section introduces an abstract bregman divergence view of anomaly detection, but then settles on one specific case of this which can be derived rather directly without this (as was done in the original kliep paper, to my knowledge).	0
i am aware that the authors cite russin et al. but it is rather unclear how and why they incorporate these intuitions/motivation into their work.	0
this is a very specific task, which merges two important problems of natural language understanding but with little motivation as to why the combination of the two makes this problem even more interesting.	0
this setting / motivation is a very relevant one, and the method seems like a sensible start, but neither the proposed approach nor the experimental treatment are substantial or novel enough to warrant acceptance; however hopefully something on this topic and approach with a more thorough empirical and/or theoretical treatment will appear later on from these authors!	0
the proposed approach lacks a strong motivation though; i would like to see more explanation on what problem is solved by adding the bert teacher.	0
it is an interesting idea but the paper lacks motivation and clarity on the technical content.	0
cons 1) for the parameter adaptation module, the approach to finetuning the base model guided by class embedding seems interesting but the motivation is lacking.	2
since the motivation behind the paper is networks that are suitable for ondevice inference and multitask adapters, i find the paper lacking in the following respect: 1. describing how exactly they will implement this architecture ondevice.	0
cons:  experiments do not include an analysis of the performance of the model in the hard to differentiate actions (which is the main motivation for the proposal).	0
the main reasons for this are: 1) the main contribution of the paper (page 4) is poorly explained and lacks motivation.	0
the addition of this regularizer is however largely heuristic, lacking a theoretical motivation in the manuscript.	0
paragraph 4 of section 2 provides some previous works that also extend gcn for graph classification but still, the motivation of this work should be clearly stated in the introduction.	0
weaknesses: 1. the second part of the motivation (fig. 2, right) about blurred images being more generic than those with higher resolution sounds farfetched.	0
my main concern is regarding the motivation and the technical contribution of this paper.	0
although the paper contains some interesting aspects, i find there are several critical limitations, especially (a) lack of motivation, (b) gap of theoretical results, and (c) insufficient experiments.	0
however, this paper also has some weaknesses which prevent me for putting it above the acceptance bar, namely (i) lack novelty of proposed method and (ii) somewhat weak motivation.	0
cons: it would be better to explain the motivation behind forcing the output distributions to be similar among samples within the same class.	0
however the proposed btd3 algorithm is an actorcritic that uses trajectory return to guide it, it seems to conflict with the motivation of using actorcritic.	0
2. the proposed method lack of motivation.	0
— the idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [didericksen, et al., collaborationbased user simulation for goaloriented dialog systems, convai neurips ws, 2017] — even if in a different setting and not clustered) in any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to followup without public datasets, etc. finally, i don’t think the scope of potential impact (if wellexecuted) makes iclr the ideal venue.	0
from a purely methodological perspective, these could constitute a fair contribution, albeit leaning on the incremental side (e.g. [814], also see major point 1 below) and lacking a sound theoretical motivation as presented by the manuscript.	0
furthermore, the main motivation of the paper is to avoid overfitting, but here, there are just two tasks.	0
2. i find the motivation of the method to be lacking.	0
putting aside the motivation which i personally find lacking, the final learning objective is interesting, and seeing that the ablation study find that the reconstruction terms are useful is a surprising and interesting result.	0
strength:  novel and simple idea  good results on mnist areas of improvement:  approach lacking motivation  limited to mnist i found the idea presented on the paper interesting and haven't seen this before.	0
the technical section lacks some motivation.	0
there is a nice motivation for using a hierarchical structure in the introduction, but it is unclear if there is real benefit to this structure and how it could be used to help answer scientific questions, as is ostensibly the goal of this manuscript.	0
the motivation is clear but the design is quite straightforward.	0
overall, i believe this could be a good paper if the authors can address some of my concerns here: 1. the motivation of the loop in the first lstm is not quite clear to me.	0
the authors demonstrate the feedback loop is important in the experiment and text, but the motivation is unclear.	0
please find below my major concerns:  the motivation of each proposed component is not very clear.	0
comments: 1. the introduction section lacks motivation.	0
the experiments are well described and extensive and some of them do give some more substantial performance improvements, but overall i think the paper does not explain the motivation for the algorithm sufficiently for iclr publication.	0
my major concern of this paper is that it fails to state the motivation to adopt adaptive bn clearly.	0
the strengths of the paper are: ' the tackled problem is relevant to the computer vision community the rejection decision is supported by the following reasons: ' missing motivations: it is not clear how teb is able to encode long paragraphlike sentences when compared to other standard decoders ' limited novelty: it seems that the only contribution is how the paragraph is encoded (teb) which details are not clear (see point 3 below) ' writing need to be improved: missing motivations, descriptions and details (see point 2 below) ' results are not consistent (see point 4 below) # 2. clarity and motivation teb is presented as novel model, however it is not clear how it overcomes issues from previous works on paragraph captioning (sec.	2
i have given a weak reject because of the concerns mentioned in the weaknesses strengths: the paper has an interesting motivation  2d feature representations present several limitations because they do not obey intuitive physics constraints and are therefore are not a suitable representations.	2
cons: 1. in terms of motivation, the observation here is that autoregressive lms are able to selfcorrect, and authors found similar results for transformers.	0
strengths: 1. the motivation of the paper is very clear.	2
in summary, this paper shows some technical novelty but the motivation behind the proposed method is not strong enough.	0
the motivation up through and including section 3 is clear, the theoretical results are presented clearly, but the model details in section 4 are unclear:  in the dynamic attention mechanism, how does one a priori choose the number of clusters in computing the gap statistics, and what is the impact?	0
strength  observation of these nas algorithms tends to pick wide but shallow cell type is interesting, motivation of this work is well justified.	0
one motivation comes from maximal entropy reinforcement learning (merl), but which has the ad hoc objective of maximizing the policy entropy.	0
this makes the lack of the motivation of visualizing the effectiveness of features.	0
i haven’t checked the proofs, but i focused on readability and motivations.	0
i'd be happy if the authors could kindly elaborate on the following major weaknesses: (i) unclear scientific motivation of using the rightskewness bias; (ii) lack of clarity in the experimental results; (iii) lack of depth and perspective.	0
for example, the possible monsters/elements/weapons, etc. (2) some proposals lack motivation or explanation.	0
main comments: this paper has a clear motivation and decent experimental results (though some concern on baseline models, see below).	0
1) i understand the motivation of the authors and what they tried to communicate but i find that there is no satisfactory explanation of what theorem 1 is actually saying.	0
pros:  with the vulnerabilities associated with neural networks, the motivation behind building defense mechanisms against adversarial attacks has been welljustified.	2
for starters, all the theoretical motivation describes a particular way in which mixout is supposed to aid in downstream tasks for the case of convex functions, but there are no experiments that match their assumptions and which demonstrate this is the actual behavior we see.	0
the main motivation of the paper is to propose an interpretable architecture with similar performance to black box deep learning architectures.	0
i think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me.	0
this is the motivation why i concern this problem.	0
weaknesses and questions: in general, some more description about the motivation of each metric would be helpful, rather than just stating that its from previous work.	0
there are some limitations to the method, but these can be seen as motivation for future work.	0
my major concern comes from results compared to sgd and motivation.	0
it would be better to cover and discuss more existing methods when summarizing their properties; 2) in the methodology part (section 2), the description is clear but lacks some guidance for the readers to understand the motivation behind the scene.	0
this might seem obvious for the authors, but can help readers that are not familiar with the context access the methodology more easily and understand the motivation better.	0
 the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field.	0
the motivation for providing a new memory update that takes into account the approximation error (from the variational approximation) is clear but i was not sure of the efficacy of the approach.	0
overall initial thoughts: this seems like a nice alternative to adversarial methods, but it does not compare to more recent (last 2 years) models in this space or very thoroughly establish the applicability of its motivation.	0
'incremental learning of skill collections based on intrinsic motivation.'	0
 the evaluation is thorough  the problem and solution are well motivated weaknesses:  while the authors demonstrates that genesis is able to model static scenes, it is not clear how straightforward it is to extend genesis to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation).	0
some minor points:  many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved  some explanation about the choice of auc as a metric would be informative and could help connect to the initial motivation of the method  experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a 'neural network' is trained, the architecture should be specifically given alongside the discussion of the experiment	0
 the paper is wellwritten, and relatively easy to understand, despite a few hardtofollow spots  widely applicable posthoc to any trained neural networks, and potentially faster training than ensembles / bayesian approximated ensembles  (theoretical) stability compared to full hessian inversion # cons motivation wrt other papers unclear, and a lack of comparison.	2
for motivation, it would be helpful to give some examples where the prerequisites of this work are actually met; that is, cases where sufficiently large number of labeled cluster instances are available, but the generative mechanism of the clusters is not.	0
> work for the present volume began by asking the question... i like this paragraph for motivation, but perhaps 'volume' is slightly overwrought?	0
2) quality the paper  being a theoretical analysis rather than a new algorithm  seems mathematically rigorous but lacks motivation and also explanation.	0
i am concern about the core motivation of this work, like to identify or solve any new problems, in addition to experimentally verify the observations during network’s training.	0
authors’ main motivation is to study small batch size experimental setup which is important in segmentation and detection, but they don’t include main competitor (brn) in these experiments.	0
strengths: the model is described in a very detailed manner with contrasts drawn to previous models, which provides excellent motivation for the decisions taken by the authors.	2
in experiment qta2 the authors (with a small issue, see weaknesses) show that their proposed modification to the loss function causes their outputs to be better matched to the speech conditioning as measured by the fixed embedding network trained in stage 1. finally in qt3 they measure how well their generated faces can be used with the original face embedding network to perform image retrieval (i am a bit unclear on the details of this experiment, see questions) strengths  ' the authors' proposed pipeline and modified loss function offers a generalized framework for jumpstarting conditional gan training ' their pipeline does not assume humanspecified labels, but instead access to paired data from different modalities, which is can easily be obtained for many conditional image generation tasks (inpainting, superresolution, colorization, etc.) ' their pipeline also doesn't seem particularly finetuned for the speechdriven image synthesis task, so it seems reasonable to believe it could be adapted to other tasks ' their results are qualitatively compelling, and they make a convincing efforts in experiments qta13 to quantitatively show that the conditioning information is affecting the output ' the paper is generally wellwritten and easy to follow weaknesses  ' without completing the loop, and showing that the second stage of this pipeline helps with identity matching for speakers, i'm not clear on what the motivation for this particular form of conditional image generation is (this is unfortunate, because their framework is quite general, and it seems feasible that the authors could have applied it directly to a task where the output of conditional image generation is directly useful) ' i interpret the main purpose of experiment qta 2 as validating the effectiveness of their proposed loss modification, it seems like a natural experiment to make this claim more convincing is to compare a generator trained with eq 4. against real images (hopefully getting a much lower matching probability than the 76.65% reported in the first experiment of this section) ' it seems like an important ablation study is testing the effect of jumpstarting the gan training with the pretrained networks of stage 1, and reporting what happens when one or both of these networks are initialized randomly (assuming that initializing randomly hurts performance significantly, this would be further evidence of their framework's strengths) ' the novelty of the proposed approach is limited so far as i can tell (slight architectural modifications, and adding a negative sampling term to the discriminator loss) initial rating  weak accept questions  ' in experiment qta3 i'm confused by how including multiple faces from the same video clip affects measuring retrieval accuracy.	2
however the paper is held back by the lack of novelty and lack of clear motivation.	0
despite my concerns about the motivation for this particular task, i think the good results produced by author's methodology indicate this work will be valuable in the context of conditional image generation more broadly.	0
concerns about motivation: i disagree with the original motivation of this paper.	0
i think this motivation is fine but it is not directly related to label corruption because we do not add outofdistribution data but rather the label noise.	0
3. lack of any theoretical insights/motivation for the proposed method.	0
lack of motivation about why using the ladder method rather than another one.	0
i suppose one of the two arguments of the kl term should be the attention distribution for the current time step and the other argument for the next time step (if i understood the motivation in the earlier paragraph correctly), but this is not evident from eq. 4. '	0
4/5 [strengths] 'motivation'  the motivation for improving the generalization of program synthesis by augmenting examples is convincing.	2
cons: 1. the motivation towards “progressive shrinking (ps)” is not that clear.	0
the motivation for using layer normalization was discussed in 2.1.1 but i still do not understand why it is beneficial. '	0
## uncertainty discussion is lacking the motivation and discussion sell this method as an uncertainty quantification method, but almost all of the theoretical development revolves around prediction correction.	0
these are nice motivations, but ultimately we're left with a heuristic method.	0
other notes:  whilst i understand the point about independent marginals in 2.4 i'm not sure i see the motivation as clearly since it seems that the model is much more useful when there is dependent information but maybe there's a usecase i'm not thinking of?	0
though this work is well presented and indicates promising results, i think the paper should not yet be accepted due to the following main reasons: (i) the experimental evaluation indicates promising, but not yet convincing results; (ii) the computational complexity of nap seems to be a major limitation of rapp which is not addressed in the text; (iii) the added value/insights from the theoretical section 4 (motivation of rapp) are not clear.	0
[pros]  clearly written  clear motivation  correct derivations  interesting algorithm [cons]  experiments are a little weak (and focus on a single domain)  would have liked to see an explicit algorithm for the optimization procedure  small lack of clarity in the presentation of section 4.1notation q_t is not introduced for example  more discussion about the evaluation metric  linking it more to prior work	0
2) the choice of approach (in particular, the use of the wasserstein distance to match state distributions, and the manner of learning a local prior by training an autoregressive beta vae) are lacking motivation, and it is unclear if or why these choices are the best way to approach the problem.	0
to be fair, blind 'ppoexploration bonus' suffers from the same problem, but in this paper, the whole motivation is about mobile robots (at least that was my impression after reading it).	0
key reasons for my rejection: 1. my biggest concern is the motivation of this paper.	0
4) i can see the motivation for using the l_var loss term in this setting, but forcing heads in mmah attend to similar positions seems counterintuitive: there is evidence that making attention heads less correlated improves quality (see for example li et al, emnlp 2018 https://www.aclweb.org/anthology/d181317/), but l_var may end up doing the opposite.	0
i am giving this paper a weak reject rating mainly because of weak results and lack of motivation for location modeling problem (where their approach performs significantly better than baselines).	0
i find my concerns on motivation and presentation properly addressed in the feedback and the revised paper as well.	0
the paper gives some motivation but i think the authors could elaborate further on the huge number of applications and potential for significant impact from these models.	0
 easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
having each module be a single convolutional or fullyconnected layer makes intuitive sense, but is there some theoretical motivation for this choice?	0
lacking an explicit motivation, the exercise of writing the canonical parameters of a multivariate gaussian in (2)  (4) is not very informative.	0
overall, i think the paper makes an useful improvement over the nalu, but the intuition and motivation behind is not very clear to me.	0
strengths: the paper is extremely wellwritten with a clear motivation (section 1).	2
the weaknesses above asks for additional motivation and some speculation.	0
detailed feedback: 1) the paper lacks a proper motivation as to why using the norm of the gradient is a better metric than the many others already present in the literature.	0
main concerns:  problem of darts as a motivation the claims of local smoothness/sharpness and generalization are related to network generalization is quite intriguing, however, only using largest eigenvalue of hessian matrix as an indicator of this local shape does not seem to be enough.	0
the paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly.	0
however the paper does not motivate the constraining at all.	0
the method is interesting, novel and seemingly efficient; but it is insufficiently defined, the method is not motivated and experiments are quite weak with little comparisons and no experiments with practical value.	0
 while the authors cite lack of work in such data setting (which is actually incorrect as described below), they do not motivate or justify why such an approach of using either unifilar hsmm or bayesian inference techniques are good starting point to model such information.	0
i appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3d molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain.	0
section 4:  the authors motivate the problem with the pls setup but then they use the learned regularization term x = g_   heta(y) as if it is the mapping given by pi(x|y).	0
 critics the paper is globally well written but not well motivated and sometimes difficult to understand.	0
in the end i’m left with inconclusive results, a weakly motivated story, and a paper that despite exceeding the page limit by a page lacks information density.	0
the authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when farkas' layer is not used.	0
i am unable to assess how the convergence results place in the literature, but i believe they motivate the algorithm.	0
i appreciate that the authors made an effort to not just show empirical results but also motivate their findings by theory, although the argumentation stays a bit superficial.	0
the authors further motivate their work by assuming that these phenomena are due to lack of prior structure that can be alleviated by further supervision during training.	0
that is true, but i am not sure how it helps motivate the paper.	0
one improvement i could suggest to better motivate the proposed approach is to experiment it not only on convolutionalbased networks with image classification tasks but also on recurrentbased networks with text datasets.	0
my intuition is that the weights that touch every subproblem are the most motivated to find the largest principal component, the weights that touch all but one find the next largest, and so forth; i found this idea clever and elegant.	0
while the authors motivate the use of the proposed intrinsic reward for learning to solve tasks in sparse reward environments, the experiments do not include moreover, some of the tasks used for evaluation do not have very sparse reward (e.g. acrobot but potentially others too).	0
in the following, a few concerns: 1) the authors motivate their approach with the goal of 'encoding realworld samples' without accepting disadvantages of vaes such as 'imprecise inference' or ' posterior collapse'.	0
i have several concerns about the clarity of the paper, its applicability to the motivating problem, and the completeness of the results that motivate my 'reject' recommendation.	0
to summarize, i believe the paper in its current state is not wellmotivated and appears very incremental given the prior works of snp and anp.	0
however i am leaning towards rejection as the underlying problem does not seem wellmotivated.	0
major concerns: (a) motivation: the authors do not discuss or motivate the problem and why it is important to the community.	0
first, the heuristic value function: this value function h(s) is defined in the appendix but should be motivated and described (in detail) in the text body.	0
i believe there may be more fundamental (theoretical, principled) arguments to motivate the approach, but this is not explored in the paper: the idea is interesting but not supported by much theoretical insight.	0
estimating the parameters of an equation used in a game is not really interesting.. as we have to know the equation, it has to be simple, we have to extract the trajectory from the game... but there might be other related applications that could motivate this work.	0
11) at the beginning of the paper, the authors motivated the maximum entropy but the final algorithm is based on other approaches.	0
1. the introduction lacks insight into the literature on point cloud or measure networks, including in practice, which would motivate the subject and place it more precisely within the literature.	0
the authors motivate this modification through the maximum mean discrepancy but i was unable to follow this argument  adding additional details to the paper for this argument would greatly help.	0
modern research systems generally don’t rely on a strict encoderdecoder model, but have some notion of statetracking, knowledge base lookup, and a contextsensitive generation step — or similar variants that would mitigate some of the decoder issues that motivate this work (e.g., some combination of [wen, et al., eacl17] (policy network to response decoder), [wu, et al. acl19] (for state generator ), and [lei, et al., acl18] (wrt to generation).	0
i don’t think this paper is strong enough for iclr as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written.	0
however i found the high level motivations given in introduction/sec 3 are similar (at times even the wording) to those of belilovsky et al 2017 which introduced/motivate the data driven approach to this problem.	0
weaknesses:  the proposal of the novel machine learning task of 'ensemble distribution distillation' does not seem very well motivated.	0
this main technical section is very dense, technical choices poorly motivated, and the section is impossible to understand in a single read (i had to read it 34 times).	0
general comment motivated by the comment ('note...perfectly') mentioned 5 above: for all the adversarial evaluation in the paper, the authors should also try cw attacks/blackbox attacks to get a more confident estimate of their model’s robustness.	0
 strengths:   careful initial experiments to motivate the study.	2
the methodology and insights appear novel and well motivated, however i am not familiar with many of the prior work.	0
1. in sec 3.2., this paper tries to motivate the readers to notice the difference between the lssp and dne by introducing example 1. however, i notice that there is a gap that hasn't been presented clearly: example 1 is a general game but does not correspond to a gan, which is of the most interest in the paper.	0
the proposed method is relatively simple, but is wellmotivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset  quantitative measure for saliency methods, so i would give this paper an accept.	0
this reviewer moves to accept the paper for its contributions to intrinsically motivated exploration with thorough discussion of how the technique addresses shortcomings of past methods.	0
i think the authors are headed in the right direction, but compared to prior work in adversarial attacks for deep rl agents (i.e. the huang and lin) i have a few concerns that i feel the authors need to better explain/motivate in their paper.	0
strengths:  the idea is simple and motivated by the fisher vector work (jaakkola & hausler, 1999), which the authors cite.	2
the paper is mostly well written (one notable exception: the form of the loss function is not given explicitly anywhere in the paper) and well executed, but the scope of the work is somewhat limited, and the authors fail to properly motivate the application or put it in a wider context.	0
pros:  overall, the paper was clearly written and well motivated.	2
review:###this paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models.	0
i have many concerns for this paper:  the approach is not well motivated.	0
the paper explain how the discrete model is trained with quite some detail, but the choice itself could be motivated more clearly.	0
maybe true in expectation would mean that e_{a_1, a_2}(indicator(l_true > l_m)) > 1 as n grows ' the assumption about gaussian weights in a1 and a2 seems strong but is at least partially motivated at the end of 3.2 (although not all networks are trained with l2 regularizer) what about the iid assumption?	0
weaknesses  while the work is wellmotivated and the experiments provided show a proofofconcept of the approach with thorough analysis, the paper could be significantly strengthened by considering more domains for inferring physical parameters  simply inferring whether balls will pass above, pass below, or bounce off obstacles is a good first step, but does not provide enough evidence to evaluate the generality of the proposed method to intuitive physics tasks.	0
pros the paper proposes and evaluates a model for fewshot video prediction, which is an interesting and wellmotivated task.	0
i have no experience with these kinds of nlu models, so i can't say with confidence whether the architectural additions proposed are wellmotivated, but to me it feels like there is not a strong justification for adding these particular features to the bert architecture, and the results do not clearly demonstrate their utility except in the 'lexical_overlap' case.	0
this problem is not explained or motivated well in the paper, but instead the readers are referred to the airl paper.	0
the paper is well motivated and experiments are correct, but the quality improvements overall are a little underwhelming.	0
despite these shortcomings i still recommend a weak accept, as the problem is difficult and the paper documents a wellmotivated avenue for approaching it.	0
overall i do not think that the paper is qualifies for acceptance, because a) both contributions are only loosely connected and b) some parts are confusingly written or poorly motivated, making the paper hard to follow.	0
the algorithm is motivated by the exploration exploitation tradeoff problem, however there's no policy doing any exploiting here.	0
although i found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below: 1) the idea of using attention score is simple yet seems effective.	0
it is poorly written and the presented model is not clearly motivated.	0
the loss is motivated by the issue of 'artifacts', which i assume are poor generations due to lacking a good model of the world.	0
overall, the paper is wellmotivated and wellwritten however it lacks technically novelty.	0
generally, though several good ideas were presented, i felt that they were not properly motivated and that the paper generally lacks rigor and clarity.	0
strengths  the paper is wellmotivated and relevant to the ml community  lowlevel speed optimizations are needed but overlooked in the community  reasonable choice of experimental conditions (focus on unbatched cpu evaluation, testing on a selection of 4 different tasks)  proposed techniques are sensible weaknesses (roughly in order of decreasing significance)  gains over tensorflow performance is advertised in the intro, but only mentioned anecdotally in the experiments.	2
second 3.1 is easy to follow but the following part seems less motivated.	0
 was discarding all but the strength subclass from the persuasion dataset and empirically motivated decision or just something you did apriori?	0
 critics the paper is globally well written but not well motivated and sometimes difficult to understand.	0
in the end i’m left with inconclusive results, a weakly motivated story, and a paper that despite exceeding the page limit by a page lacks information density.	0
strengths:  the paper appears well formulated, and well motivated  the experimental results appear quite strong.	2
this is also a viable approach to building models, but why emphasize variational so much if the method is hardly motivated by anything variational?	0
i believe the problem is well motivated, but not very much explored yet.	0
the proposed method is well motivated and quite justified by the experiments abliet lacking comparison with previous published results.	0
i think i got it in the end, but i believe the analysis could be better motivated/presented and the consequences of theorem 1 on design choices or implementation could be clarified.	0
i see why the contribution can be viewed as minor, but it is wellmotivated and looks like a nice set of experiments.	0
pros: extending dropout to other tasks and understanding its general method of action is an important problem, thus making the paper well motivated.	2
the problem is wellmotivated, as saliency maps have been extensively studied for image classification models, but rarely for video classification.	0
cons: 1. adding an additional layer of the dirichlet is not well motivated.	0
overall, this is a very technical work, but the modeling choices need to be better justified/motivated compared to existing works on gcn with mc dropout.	0
strengths:  the paper is wellwritten and wellmotivated.	2
### strengths  the paper is well motivated for the need of good representation learning for remote sensing, and is written well making it easy to follow.	2
i would summarize the main findings as follows:  excitatory principal neurons are more selective and more sparsely connected to each other than local inhibitory neurons, consistent with biology  stimulus selectivity and performance depend only on the number of excitatory cells, but neither on the e/i ratio nor the number of i cells  high selectivity appears to be a general property of principal cells, but does not depend on the sign of local connections, while sparse connectivity is mainly linked to the excitatory local connections strengths:  architectural decisions are well motivated from a neuroscientific perspective  provides a hypothesis why certain connectivity and selectivity patterns emerge in the brain by incorporating biological knowledge into neural network models trained on a specific task  well written and clear, logic development of the arguments weaknesses:  unclear whether classification task is necessary to elicit the authors' observations  interneurons seem unnecessary, raising concerns about relevance of results  also trains on imagenet, but only some results are shown; most from cifar overall i like the paper from the perspective of a neuroscientist, as it provides a kind of normative account of why things in the brain might look the way they do.	2
strengths: 1. the problem is new and wellmotivated.	2
personally, i am interested in motivated behaviors and think that future ai developments should take note of this field, but again, the present work does not provide actionable insights into implementing an artificial motivation system.	0
in addition to experiment, other concerns include: (1) the proposed method needs to be better motivated, although it is interpreted under the framework of maximum likelihood estimation.	0
the idea seems wellmotivated and somewhat new though not revolutionary, and the new data sets are nice (though see comments below about how i'm not qualified to evaluate them), but i don't understand why the proposed algorithm wasn't evaluated on existing data sets as well, and i don't understand why it wasn't compared against other algorithms that purport to do the same thing.	0
i think some highlevel aspects could be better motivated:  why is interpreting blackbox models a better avenue than building interpretable models?	0
it is motivated by that, natural data is often in some manifold but not randomly distributed.	0
1. most modeling decisions are clear and wellmotivated, however the choice to make the transition model f_trans always be applied only “slotwise” might be too restrictive.	0
i understand that such a subdivision is required in order to be computationally feasible, but this needs to be motivated.	0
it is obviously good to introduce more than one contribution in a paper, but this second (and minor) contribution should be well motivated in its own, in order to avoid 'distracting' the reader from the main contribution.	0
pros: ' the method is wellmotivated.	2
the use of an unfolding algorithm similar to lista is not clearly motivated and explained, especially concerning the training procedure when working in the sr context.	0
pros  the writing is great and easy to follow  the method is theoretically motivated  experiments prove effective cons  the proposed method may not work well for complicated environments (1) in the discussion after def.4, given an alignment task set d_{x,y}, how do we know whether a common (w.r.t.	2
pros: 1: i think the paper is well organized and motivated, the regularization of parameters in deep neural network is one of the center problem for effective learning.	2
strengths: the proposed model is well motivated and shows strong performance and generalization ability on several datasets.	2
strengths:  this is an interesting paper that is well written and motivated.	2
the transformer is motivated by the role that attention may play in human information processing  which sounds plausible, but the paper does not expand on this theme.	0
if that is the case, well the results are evidence of a negative result in this respect, which is okay but given how adhoc and poorly motivated the method is to that objective it's not much of a contribution.	0
pros: (): the paper is wellwritten and wellmotivated.	2
my intuition is that the weights that touch every subproblem are the most motivated to find the largest principal component, the weights that touch all but one find the next largest, and so forth; i found this idea clever and elegant.	0
i do have several concerns, summarized below: ' on page 3 the fourier transform is motivated by that rbf networks 'do not generalize as well as relu networks'.	0
overall i think the approach is well motivated and interesting, but the resulting implementation takes too many unmotivated modifications to make work, and the results aren't terribly convincing despite this; as such i currently vote for it's rejection.	0
pros  the paper is wellmotivated.	0
 this work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms.	0
pros: (): the paper is wellmotivated.	2
reconstruction could perhaps be motivated from the point of view of compression, but this paper makes no attempt to examine compression: ratedistortion tradeoffs are not considered, nor are any empirical metrics of compression ratio or likelihood such as bits/dim presented.	0
positive aspects:  the work is well documented and motivated, and i found that the reflexion leading to the method is of good quality.	2
i agree this is wellmotivated but it has little novelty and a similar idea is there in vqa (see “dynamic fusion with intra and intermodality attention flow for visual question answering”).	0
the methodology is however not particularly well motivated and the experiments do not convince me that this proposed measure is superior to other reasonable choices.	0
in addition, the contribution is incremental and not wellmotivated.	0
it seems to me that these are mostly heuristics, but not verified or strongly motivated solutions.	0
even if the problem is well motivated, i have several concerns: i. what is the variable 'x'?	0
the issue is motivated by applications where one modality of data becomes unavailable in the target domain (e.g., when deciding which ads to serve to new users, the predictor may have access to behavior across other websites but not on a specific merchant's website).	0
the method is wellmotivated and explained, but the experiment section is not very clearly written and i’m not confident whether the technique represents an advancement in the stateoftheart or not.	0
the proposed algorithm looks robust and wellmotivated, but the text of the article and the experiments can be improved.	0
strengths: the method is well motivated and would be useful.	2
to summarize, i believe the paper in its current state is not wellmotivated and appears very incremental given the prior works of snp and anp.	0
for example, “fuzzy choquet integration of deep convolutional neural networks for remote sensing” by derek t. anderson et al. second, the authors claim they are using/motivated by choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.	0
pros: the paper addresses an interesting problem in a sound and well motivated way.	2
pros: (): this paper is very wellwritten and very wellmotivated.	2
strengths: the work was wellmotivated formulation is pretty elegant and outperforms the baseline weaknesses: while i liked the somewhat elegant formulation of dynamic testtime scaling proposed by the following work, i don't think this work introduced many novel results nor insights multiscale testtime inference is already standard in stateoftheart architectures such as deeplab.	2
cons:  one of the drawbacks of this method is the decoding strategy although authors present a motivated solution for that.	0
i have a few concerns about some technical details of the paper, as explained below: 1) the paper motivated the new model with difficulty in detecting change points in seasonal time series.	0
the approach was motivated by the similar technique that has been applied on group convolutions, but with a few notable differences too, such as intergroup mixing and lowrank approximation (which also appeared in convnets before, but this still strkes me as a difference in the transformer context).	0
pros: (): the paper is wellwritten, addressed the prior work quite well despite missing a few important work from the past (more on this later) (): the paper is well motivated cons that significantly affected my score and resulted in rejecting the paper are as follows: 1 lack of support for “scalability”: authors claim their method is scalable in several parts of the paper (abstract in line 7, section 3 in the 1st paragraph, and section 5 in discussion).	2
the rest of the machinery is well motivated and well executed, but less novel.	0
cons:  some design choices are not wellmotivated or even problematic.	0
strengths: firstly, the paper is very clearly written and motivated.	2
advantage of using it for unlabeled data is poorly motivated as unlabeled graphs can easily take statistics such as degree as the node labels, which was shown well in practice.	0
however i am leaning towards rejection as the underlying problem does not seem wellmotivated.	0
negative points:  the importance of the problem is motivated by future assessments by downstream tasks but do not address this aspect in the experiments.	0
hence, the experimental results could be more convincing if the paper include more 4. lack of justification for the model architecture: some design choices of the model are not wellmotivated/justified.	0
 the authors say: “we use 5 strokes to reproduce handwritten digits images, 20 strokes to reproduce character images, 100 strokes to reproduce face and object images”  which does seem reasonable, but the actual numbers (5, 20, 100) are not well motivated.	0
first, the heuristic value function: this value function h(s) is defined in the appendix but should be motivated and described (in detail) in the text body.	0
pros  the paper is wellmotivated and tackles a significant problem.	0
3. the comparison with feature norm seems poorly motivated.	0
the approach is motivated from the concern that traditional divergence such as fdivergence or kl divergence may not always exist, in which the spread divergence may be a substitute.	0
that said, i have some concerns about this paper which i list below: 1 perhaps my biggest concern is that the approach is not motivated from a theory stand point.	0
the method makes sense, however the choice of space in which the sample selection is being done is not well motivated or validated.	0
review: i have several concerns with the paper: 1) geodesics are never motivated: the paper provides no motivation for why geodesics are interesting objects in the first place, so it is not clear to me what the authors are even trying to approximate.	0
strengths:  this is an interesting paper that is well written and motivated.	2
overall, this proposed method is well motivated, but the technical novelty is limited.	0
feedback: (1) the contrast between learning playable representations vs. planning via representation learning (as proposed) is poorly motivated.	0
11) at the beginning of the paper, the authors motivated the maximum entropy but the final algorithm is based on other approaches.	0
strengths:  the numerical common sense task and its related challenges are presented and motivated clearly  the data/annotations and their analysis are presented clearly.	2
while the idea is well motivated, this paper should be rejected, primarily due to a lack of experimental results.	0
the role of the different loss components should however be better motivated.	0
the current comparisons are a bit poorly motivated and reductive because they seem to be claimed to be generic imitation learning comparisons, but they confound the training setting, the exploration approach, and other factors.	0
i have two main concerns with this paper: first, i think the method is not really am endtoend joint optimization approach as motivated in eq 4. by using a machinelearned predictor to approximate the accuracy, there are no guarantees in this method.	0
architectural choices are interesting and wellmotivated and combine good ideas from many prior works, including prediction in latent space, and coarsetofine synthesis (mainly used before along spatial axes for image generation, but used here temporally for video generation).	0
review summary: the paper is well motivated, and the proposed model is simple but effective.	0
the basic idea of leveraging hierarchical structure in language is of course sensible, but it's not clear to me how wellmotivated or original this particular instantiation of hierarchical representation is, or what we have learned based on the comparisons reported.	0
i also found the paper somewhat unclear and poorly motivated, particularly the later sections.	0
strengths: 1. the interactive approach to do classification is wellmotivated and looks interesting in realworld settings where computers can interact with users.	2
review:###this paper proposes an approach for deciding when to incrementally vs. fully retrain a model, motivated by the overall setting of iterative model development in realworld slot filling (e.g. info extraction) tasks.	0
pros:  wellmotivated and wellwritten paper.	2
major comments: ' i found the paper poorly motivated and hard to follow.	0
the paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task  the two datasets weren't even combined!	0
the goal to capture string similarity (character overlaps) is well motivated for the name disambiguation task, but it is unfounded on why a 2d mapping would provide additional benefits from the existing features.	0
however, in my personal opinion, it is not motivated enough why multidecoder is necessary (except for an empirical evidences in table 2) when many people use a rnn model as a good but approximate function approximator.	0
meanwhile, this paper is poorly written, the approach is badly (and incorrectly) motivated, and there is not enough model analysis to warrant publication.	0
the method is wellmotivated in section 3, seems quite reasonable and the result improvement over baseline is impressive in table 2. but i do have several important concerns.	0
i don’t think this paper is strong enough for iclr as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written.	0
the paper is poorly written (there are many simple grammar mistakes, the methods are not well motivated, and the methods are described in a confusing way), and more seriously the evaluation of the method is not complete.	0
2.1, the feature augmentation & ensembling are introduced (again, both of which have been done before), but then several more terms are introduced in 2.2 and it is unclear what they are motivated by.	0
the idea of run is well motivated but not surprising.	0
in addition, the paper is poorly motivated and ethical issues are ignored (see below).	0
pros  the paper proposes to tackle plan completion with probabilistic, instead of deterministic observations, which is a promising direction wellmotivated by practical settings.	0
however to this reader's knowledge, this is a new variant that is both creative and motivated by an actual realworld study, which is exciting and alone warrants presentation at the conference in my opinion.	0
the authors show superior performance to ewc (a regularizationbased method), p&c (architecturebased method), den (architecturebased method), pgn (architecturebased method), rcl (architecturebased method), etc. pros:  the paper is wellwritten and has motivated the problem of scalability and forgetting  proposing a new hybrid approach that benefits from the best of both worlds (maximum usage of the capacity with parameter regularization followed by logarithmic architecture growth at arrival of new task using layerwise parameter decomposition.	2
the proposed method is simple but effective, moreover wellmotivated.	0
strengths: (1) the metrics of the paper are well motivated from an informationtheoretic standpoint.	2
 paper is well written/motivated weaknesses:  multinli seem to have too much correlation between tasks.	0
the paper is well written, clearly motivated, highly novel and significant not only in a theoretical sense but also in a practical sense.	0
it is motivated by the need to compress neural nets so they work on embedded devices (smart phones, etc.), and it is in contrast to most other techniques that prune at training, or prune aftertraining but prune weights not nodes.	0
this lack of clarity, along with a complicated design that is not very wellmotivated, make me lean towards rejection.	0
this paper is well motivated: clearly sparse rewards and mode collapse are two problems need to be solved in gan based text generation, however, the following concerns prevent me from finding this paper acceptable in iclr: the “selfplay” idea is widely used in rl.	0
weaknesses:  the proposal of the novel machine learning task of 'ensemble distribution distillation' does not seem very well motivated.	0
this paper is well written and clearly motivated, albeit a bit incremental in its approach.	0
 the evaluation is thorough  the problem and solution are well motivated weaknesses:  while the authors demonstrates that genesis is able to model static scenes, it is not clear how straightforward it is to extend genesis to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation).	0
pros 1. formally define important and wellmotivated issues to improve performance on a limited budget.	0
the approach is well motivated but very heuristical.	0
strengths: (1) writing & clarity: the proposed model is very well motivated, the paper is well written, and clearly presented.	2
this main technical section is very dense, technical choices poorly motivated, and the section is impossible to understand in a single read (i had to read it 34 times).	0
general comment motivated by the comment ('note...perfectly') mentioned 5 above: for all the adversarial evaluation in the paper, the authors should also try cw attacks/blackbox attacks to get a more confident estimate of their model’s robustness.	0
the problem is wellmotivated: since in meta learning we want to leverage information from similar tasks to increate data efficiency, there may be privacy concerns for each of the task owners about both other task owners and the aggregator (metalearner).	0
strengths: 1  well motivated by cognitive science theory and modeling of the human visual system.	2
strengths:  the paper is wellmotivated and tackles an important problem.	2
the methodology and insights appear novel and well motivated, however i am not familiar with many of the prior work.	0
strong points taken in consideration:  simple, wellmotivated metric that uses powerful bertstyle models, without being slow to compute either.	2
i’m also a little bit concerned about the fairness of the clustering experiment, in that the elasticitymotivated clustering algorithm utilizes an auxiliary dataset whereas simple baselines such as kmeans and pca kmeans are not able to use that.	0
to be honest, i do not like the way the authors frame their work (e.g. the way the method is motivated in section 2.3 or calling it a 'unified framework'), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.	0
the utility of training sparse neural networks and shortcomings of densetosparse algorithms like pruning, lotteryticket are nicely motivated at introduction.	0
strengths: (1) writing & clarity: the proposed method is well motivated, the paper is carefully written, and clearly presented.	2
2. the approach of the paper is well motivated intuitively, but could more explicitly show that pcmcnet is needed to fix inferential problems with pcmc and that e.g. sgd and some regularization  the linear parameterization suggested by the original pcmc authors isn't scalable in itself.	0
pros: although i was previously unfamiliar with the pcmc model, using a neural network parametrization seems novel and well motivated.	2
continuous reinforcement learning, bellman contractions, feature engineering) while providing grounds for their idea, but in the end return to the familiar domain of discrete qlearning with semihandcrafted (though theoretically motivated) features where the latency of actions can take a set of fixed values and the state is sampled at fixed intervals.	0
the proposed method is relatively simple, but is wellmotivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset  quantitative measure for saliency methods, so i would give this paper an accept.	0
pros:  the proposed task is interesting and well motivated.	2
strengths: 1. the work in this paper is quite well motivated and the modeling formulation is clear and easy to follow.	2
strengths:  the paper presents a small but interesting and wellmotivated addition to the original algorithm stp.	2
i feel the additions to existing pipelines are well motivated but insufficiently explained.	0
this reviewer moves to accept the paper for its contributions to intrinsically motivated exploration with thorough discussion of how the technique addresses shortcomings of past methods.	0
the paper is well written and clearly motivated but comes across as an incremental improvement on top of prior work.	0
the approach (rarely but consistently training on separately prioritized human experience replays) is well motivated by the shortcomings of past agents (either in overfitting the demonstrated solution or only working in environments with nottoohard exploration challenges).	0
pros of the paper: 1) authors make well motivated arguments about how a near perfect generative model is also susceptible to attacks by providing examples that are adversarial, and have high likelihood and yet are incorrectly labeled.	0
fig 1 right: this is motivated in terms of preserving scaling factors that are lost by the binarization, but the functional form for this makes it look a lot like a learned gating operation.	0
strengths:  states a clear hypothesis that is well motivated by figs.	2
strengths: the paper is well written and well motivated.	2
strengths:  the idea is simple and motivated by the fisher vector work (jaakkola & hausler, 1999), which the authors cite.	2
moreover it’s motivated by and helps real problems, namely l1’s wellknown underestimation and lista’s lack of momentum.	0
advantage of using it for unlabeled data is poorly motivated: why we can learn smth useful when maximizing the mutual information between graphlevel and patchlevel representations obtained via gnn?	0
strengths:  this paper is well motivated and well written.	2
below are some of my concerns: 1) it seems that the natural gradient under wasserstein metric is wellmotivated for models which do not admit a density (to compare with the natural gradient under fisher information metric).	0
pros  the proposed method is relevant and wellmotivated.	0
weaknesses: although i think the paper is very wellmotivated, my first criticism is that discretization itself is not motivated: why is it necessary to have a model with discrete intermediate layers?	0
pros:  overall, the paper was clearly written and well motivated.	2
strengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines.	2
reasons to accept:  original and wellmotivated idea  clearly written paper reasons to reject:  problematic empirical evaluation (e.g., lacking recent baselines)  several performance numbers appear to be below random gcn baseline performance  general applicability of the approach (e.g., to nonhomophilous networks) is not clear	0
since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, ondevice ml) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).	0
## concern 2: use of the bbpmap subprocedure poorly motivated the paper prioritizes a clean and easytoimplement algorithm to resolve practical alignment issues between client cnn and rnn models.	0
as it is, i feel the bbpmap subprocedure in the current algorithm 1 is poorly motivated for the task at hand.	0
(1) like other reviewers, i find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in rl).	0
the paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method.	2
while i like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.	2
the main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model.	1
the authors propose two regularization techniques to address these problems: geometric metrics regularizer and mode regularizer overall, i felt that this is a good paper, providing a good analysis of the problems and proposing sensible solutions  if lacking solid fromfirstprinciples motivation for the particular choices made.	0
detailed comments on the geometric metrics regularizer: the motivation for this is to provide a way to measure and penalize distance between two degenerate probability distributions concentrated on nonoverlapping manifolds, those of the generator and of the real data.	1
pros:  good motivation for the effectiveness of resnets and highway networks  convincing analysis and evaluation	2
 cons:  the effect of this finding of the interpretation of batchnormalization is only captured briefly but seems to be significant  explanation of findings in (zeiler & fergus (2014)) using uie viewpoint missing remarks:  missing word in line 223: 'that it 'is' valid'	0
2. the paper lacks any motivation for use of the particular combination (vrnn and revgrad).	0
the weaknesses:  the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.	0
through this motivation, the authors present noising analogies to standard discounting, as well as kneserney smoothing.	2
3. there is no convincing motivation for the work.	0
i do not find the motivation 'it is related to mobile, therefore it is cool' sufficient.	0
this paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, i do not think it belongs in iclr with a broad and diversified audience.	0
this is a good paper with an interesting probabilistic motivation for weighted bag of words models.	2
however it is not very clear what is the main motivation of the paper.	0
is the main motivation better exploration for policy gradient methods?	1
if the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak.	0
authors should make it clear which is the main motivation.	0
paper is clear to follow, the architecture has appropriate motivation and the experiments seem to be very thorough and over an appropriate range of synthetic and real datasets.	2
the decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model.	1
the paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. the paper also performs some ablation studies such as performing only single round of iteration on decoder, etc. strengths: 1. the paper is wellmotivated with two main motivations  coattending to the document and the question, and iteratively producing the answer.	2
it requires multiple training stages, freezing weights, etc.  the motivation behind figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on jpeg, but the paper discusses a neural network architecture, not dct quantization)	0
the paper currently is at 11 pages (which is too long in my opinion), but i find that section 3.2 (the crux of the paper) needs better motivation and intuitive explanation.	0
the paper claims three main contributions: 1. modification to model architecture (used in oh et al.) by using action at time t1 to directly predict hidden state at t 2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames) 3. exploring different training schemes (tradeoff between observation and prediction frames for training lstm) 1. modification to model architecture  the motivation seems good that in past work (oh et al.) the action at t1 influences x_t, but not the state h_t of the lstm.	2
also, a stronger motivation is required to support the current formulation.	0
the motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech   a major assumption.	1
the main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in stochastic computational graphs.	1
the main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect.	1
the idea and motivation of this paper are interesting and sound.	2
little motivation is provided at the beginning of section 3. figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation).	0
interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for hmm based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g., i. mcgraw, i. badr, and j. glass, 'learning lexicons form speech using a pronunciation mixture model,' in ieee transactions on audio, speech, and language processing, 2013 l. lu, a. ghoshal, s. renals, 'acoustic datadriven pronunciation lexicon for large vocabulary speech recognition', in proc.	1
i really like the motivation of the paper, as interpreting lstms is definitely still a workinprogress, and the high performance of the pattern matching was surprising.	2
the motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users.	2
other comments/questions:  what is the motivation behind using both vanillamemn2n and contmemn2n?	1
the motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in realworld applications.	2
my guess is that the reason behind a seemingly 'adhoc' design of inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets).	0
the motivation of pursuing flatness is also well analyzed with a few experiments.	2
the motivation to seek such minima is due to their better generalization ability.	1
is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)?	1
this is ok for a motivation, but in analyzing the results i think it may be possible to have something more concrete.	0
i have two main concerns regarding the methodology and motivation of this paper.	0
the second concern is about the motivation of this method.	0
the motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.	1
in general the paper is well written and the authors explain the motivation behind the algorithms design in detail.	2
this paper gives a theoretical motivation for tieing the word embedding and output projection matrices in rnn lms.	0
 paper is wellwritten with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.	2
what would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example i could think of in the recent use of gated convolutions 'a trous' for 1d speech signal, popularized in google wavenet (https://deepmind.com/blog/wavenetgenerativemodelrawaudio/).	1
experiments are performed to confirm the therotical intuition and motivation.	2
as expressed in the preliminary questions, i think the authors could improve the motivation for their subexpforce loss.	1
what is the motivation for the chosen 'egograph' representation?	1
important details around network architecture aren't provided, and very little in the way of motivation is given for many of the choices made.	0
this breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some lowdimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.	1
overall, this paper has a good motivation and good novelty.	2
the motivation makes sense, however, very similar work has been done in [1] and already an extension over [2].	2
i'd caution the author's against the uncritical motivation that a problem has previously been studied.	0
if the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance.	1
the motivation of the objective (3) is sensible but could be made clearer via the unification argument above.	0
the authors present a novel approach to surprisebased intrinsic motivation in deep reinforcement learning.	2
the authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains.	2
they present two tractable approximations to their framework  one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to schmidhuber's formal theory of creativity, fun and intrinsic motivation).	1
however, i would have appreciated a more thorough comparison against other recent work on intrinsic motivation.	0
the ideas are similar to previous work in intrinsic motivation (including vime and other work in intrinsic motivation).	1
this paper explores the topic of intrinsic motivation in the context of deep rl.	1
novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep rl is novel (see e.g. kompella et al 2012).	2
given the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of boolean functions representable by linear threshold networks.	1
so since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods, the paper lacks a strong motivation and/or concrete application.	0
the experimental evaluation is limited (a single database and a single baseline)  the motivation of the sparse coding scheme is to perform inference in a feed forward manner.	0
this casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.	0
regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity.	1
as it stands, it is difficult to understand the motivation or intuition behind this work.	0
even thought gans already perform density estimation, the motivation of using bregman divergences is to obtain an objective function with stronger gradients.	2
the motivation for the development of the model should be clearer.	0
the specific motivation for this second contribution is not particularly clear.	0
the motivation comes from information retrieval tasks.	1
this paper has good motivationto study multiview learning from a more information retrieval perspective.	2
the motivation is that this is a natural criterion for information retrieval.	1
although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.	0
one key motivation for the tasks proposed in this work are the existence of games like 20q or battleships where an agent needs to ask questions to solve a given task.	0
it would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.	2
i am missing any motivation as to the usefulness of the suggested analysis to architecture design.	0
section 3 should give a better motivation of the work.	0
the paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here.	0
the motivation of this architecture is not very obvious, and is not well motivated in the paper.	0
the method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness.	0
if no previous references suggest this tree specification, then clear motivation for e.g. the extension beyond cfg should be given beyond the one sentence provided.	1
significance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.	2
there is also some lack of current motivation for the model, and no comparison with tractable models that do not need a variational autoencoder.	0
otherwise, i would remove it from the paper and focus on the nn aspects and maybe mention mental models as motivation.	1
so the nonconvexity of equation (2) should not be the motivation of equation (3).	0
while the motivation is clear, the improvement of the model architecture is minor.	2
while the dataset does give a good motivation to the problem setting, the paper falls a bit short for iclr due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of chalupka et al., arxiv preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.	2
e.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept.	0
there is no clear theoretical motivation of analysis. '	0
this is disappointing since this was a main motivation of the work.	0
i have problems understanding the motivation of this paper.	0
but its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?	0
everything up to section 3.2 is essentially motivation, background, or related work.	2
the paper's motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network.	2
in conclusion, i think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at iclr.	0
the paper has a good motivation and well written.	2
while i like the motivation of the approach, the empirical analysis provided in the paper doesn’t look particularly convincing.	2
i have an impression that the empirical evidence doesn’t align well with the motivation of the proposed approach.	0
more motivation is necessary for the proposed smoothing.	0
i think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multitask jointlearning without any(or much) practical advantage.	1
while the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically.	0
i'm not fully convinced by the motivation for the proposed nonlinearity (|c|^2), as described on page 5. the authors argue that (waldspurger, 2016) suggests that higher order nonlinearities might be beneficial for sparsity.	0
section 2: the motivation of the nonlinearity is not clear.	0
the motivation is that a convolution in space is slower that performing a convolution in the fourier domain.	1
citing the layer normalization paper for the motivation is enough.	2
what is the motivation to introduce a new 2nd order method here?	1
the motivation for the probabilistic model is mostly in terms of images.	2
the main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision.	1
while the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior.	1
as it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused.	0
i find the motivation of the paper suspicious because while the prior may be unimodal, the posterior distribution is certainly not.	0
the basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudolabels generated by two streams.	1
but the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details.	2
but the current version lacks the theoretical motivations and convincing experiments.	2
honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.	2
so is there any motivation that brings about this particular approach?	1
i like the motivation for this algorithm.	2
the main motivation of the paper to base itself on kissme is scalability, see introduction.	1
5. i do see the motivation for introducing openbigrams in an unordered way due to the corresponding evidence from cognitive research.	2
a good application, or a motivation section would be beneficial here.	0
other than the empirical comparison, there is little novelty to this paper, and therefore i think the conclusions drawn need to be more general or the motivation more compelling.	0
bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation.	0
babenko & lempitsky, efficient indexing of billionscale datasets of deep descriptors, cvpr'16 detailed comments: section 1: the motivation for producing binary codes is not given.	0
while this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following: 1) motivation based on the incorrect assumption that the paragraph vector wouldn't work on unseen data 2) numerous basic formatting and bibtex citation issues.	0
the motivation behind relaxation of rank() in eq 1 to nuclearnorm in eq 2 is not clear to me in this setting.	0
the proposed trick adopts highway neural networks and residual networks with an intuitive motivation.	2
the motivation is to recover the same effect compared with sequential sgd, by using a proposed sound combiner.	1
also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.	1
while motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.	0
while introducing the concept, some concern is expressed about the motivation: 'and it is not very clear why small gradients on every sample produces good generalization experimentally.'	0
the main motivation of the paper is the reduction of model size for deep neural networks.	2
pros:  the paper is clear and easy to follow  the experimental results seem to show some benefit from the proposed approach cons: (1) the paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation (2) no comparison with an ensemble (3) full experiments on imagenet under the 'partial privileged information' setting would be more impactful this paper is promising and i would be willing to accept an improved version.	2
while some motivation for it is given, the motivation does not tie into go.	0
at a high level, i do like the motivation of this paper  named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical cooccurrences.	0
finally, this work does not provide any qualitative result or motivation.	0
the motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.	0
in fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.	0
comments: the paper jumps in with no motivation at all.	0
cons: ' the motivation for o(log n) access time is to be able to use the model on very long sequences.	0
what is the motivation and goal of the work beyond mnist benchmarking?	1
maybe the motivation needs to be rephrased a little to be more convincing?	1
# pros the authors motivate each modification well they proposed.	2
this is a nice new application of zeroorder optimization meets deep learning for rl, quite wellmotivated using similar arguments as dpg.	2
maybe the authors could add some text to clarify/motivate this.	1
the r3nn idea is a good one and the authors motivate it quite well.	2
the paper doesn’t motivate the details of the method very well.	0
here are some concrete suggestions:  the findings in section 3.0 which motivate the approach, should be clearly presented in the paper.	0
the authors motivate this from two points of view: (1) having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function.	1
they do a reasonably extensive evaluation with similar approaches and motivate their approach well.	2
a comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.	0
research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten.	0
i appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future.	2
review ''' the paper reads well and clearly motivate the work.	2
in particular:  the authors need to do more work to motivate the gam metric.	0
the beginning of the paper should more clearly motivate its purpose.	0
the paper should clearly motivate and show how different modalities contribute to the final task.	2
the motivation of this architecture is not very obvious, and is not well motivated in the paper.	0
if the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting.	0
specifically, the authors should motivate 'shared memory' more in the introduction and how it different from existing methods that using 'unshared memory' for knowledge base completion.	1
we do not need to invoke easton and basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. hmms) for lipreading has modeled context as well.	0
in conclusion, i think the piecewise constant variational family is a good idea, although it is not wellmotivated by the paper.	2
this paper should better motivate the need for different types of multimodality, and demonstrate that those sorts of things are actually being captured by the model.	1
perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.	1
the present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for.	0
also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.	0
on gpu to motivate the paper.	1
neither do the authors ever motivate why do such a training as opposed to the standard backprop.	0
 they motivate evaluating gan images by using a perceptual similarity metric (msssim) on pairs of samples to quantify diversity in the samples (and detect mode collapse)  they show this metric correlates with a discriminability metric (classification accuracy of pretrained imagenet model on generated samples) .	1
several tricks reused from (andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated.	0
in summary: pros:  clearly written and well motivated.	2
this is done in a simple but elegant and wellmotivated way.	2
from an architectural standpoint, the actual comparisons seem well motivated.	2
the use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on.	1
my overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles.	2
the method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.	2
the work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice.	2
the viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like resnets.	2
this is a nice new application of zeroorder optimization meets deep learning for rl, quite wellmotivated using similar arguments as dpg.	2
the architecture is well motivated, especially with the motivating example, and the operation is shown to validate the intuition as shown in visualizations.	2
the architecture is trained endtoend using the objective and adversarial training strategy of mathieu et al. 2) contributions  the architecture seems novel and is well motivated.	2
the idea is clearly motivated and well described.	2
i think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications).	2
pros  the paper is the first of my knowledge to explicitly measure the bits per parameter that rnns can store  the paper experimentally confirms several intuitive ideas about rnns:  rnns of any architecture can store about one number per hidden unit from the input  different rnn architectures should be compared by their parameter count, not their hidden unit count  with very careful hyperparameter tuning, all rnn architectures perform about the same on text8 language modeling  gated architectures are easier to train than nongated rnns cons  experiments do not reveal anything particularly surprising or unexpected  the ugrnn and rnn architectures do not feel wellmotivated  the utility of the ugrnn and rnn architectures is not wellestablished	2
the problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep rl.	0
that message is novel, albeit a minor modification to a wellknown algorithm, it is well motivated and, i think, a welcome addition to literature concerning exploration in rl.	2
reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do.	2
the pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations.	2
the paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. the paper also performs some ablation studies such as performing only single round of iteration on decoder, etc. strengths: 1. the paper is wellmotivated with two main motivations  coattending to the document and the question, and iteratively producing the answer.	2
this choice of upsampling method is not motivated.	0
while the idea is certainly interesting and wellmotivated, in practice, it turns out to achieve effectively identical rates to jpeg2000.	2
the architecture is well motivated and i can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.	2
this should be described and motivated in the text.	2
the proposed method is a wellmotivated combination of duration modeling hmms with state of the art observation models based on rnns.	2
the proposed dvbf is well motivated, and for the most part the presentation is clear.	2
these assumptions are well motivated by the goal of having meaningful latent variables.	2
then paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position.	2
the paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision.	2
while the approach is novel and wellmotivated, i would very much like to see a comparison against byte pair encoding (bpe).	2
clarity the paper is overall very clear and wellmotivated.	2
pros: this paper is generally well written, and the ssnt model is quite interesting and its application here well motivated.	2
in the paper cmd is used only up to kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the cmd objective, i think this is mostly motivated for computation efficiency.	2
this is motivated by some empirical observations that local minima of good generalization performance tend to have flat shape.	1
minor comments: abstract: how the identity mapping motivated batch normalization?	1
the problem of network compression is a wellmotivated problem and of interest to the iclr community.	2
the hessian weighting is well motivated, and it is also explained how to use an efficient approximation 'for free' when using the adam optimizer, which is quite neat.	2
while the overall contribution is modest (extending offpolicy actorcritic to the application of dialogue generation), the approach is wellmotivated, and the paper is written clearly and is easy to understand.	2
the approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the online version (see comments on 'clarification regarding batch vs. online setting').	2
that scheme should however be better motivated, by the limitations of srsc that should be presented more clearly.	0
this paper proposes the graph convolutional networks, motivated from approximating graph convolutions.	2
even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.	1
globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with rl is not motivated in the two examples (roc optimization and uncertainties).	0
pros:  paper is wellmotivated, exceptionally wellcomposed  provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation  the annealing trick with the straightthrough estimator also seems potentially useful for other tasks containing discrete variables, and the tradeoff in the flush operation is innovative.	2
 on related work, the relation with multiplicative rnn and their generic tensor product predecessor (order 2 networks, wrt c. lee giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).	1
the core idea is clear and wellmotivated  pooling techniques that induce invariance can be used to learn relations.	2
 the method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.	2
this is well motivated as a firststep towards a difficult task.	2
i have three main issues with the paper in its current form, if these can be addressed i believe the paper would be significantly strengthened: 1) although the recursive splitting approach for extracting the 'keyframes' seems reasonable and the feature selection is well motivated i am missing two baselines in the experiments:  what happens if the feature selection is disabled and the distance between all features is used ?	2
what motivated the choice of 'egographs'?	1
the paper is wellwritten and the research is wellmotivated.	2
the auxiliary experiment provided is motivated as follows: 'one solution could be to train these models to predict how many blocks have fallen instead of a binary stability label.'	0
this seems interesting but not motivated.	0
the novelty and the key insights are however not always well motivated or presented.	0
2) contributions  well motivated and implemented attention mechanism to handle the different shapes of c3d feature maps (along space, time, and feature dimensions).	2
the approach is novel, and is motivated by being able to learn policies for robotics.	2
my two key reservations with the paper are as follows: 1. the method is motivated by learning policies for robotics.	1
the paper is well motivated and well written.	2
the new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input.	2
although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.	2
the paper is wellmotivated, and is part of a line of recent work investigating the use of orthogonal weight matrices within recurrent neural networks.	0
overall, the paper addresses a clearcut question with a wellmotivated approach, and has interesting findings on some toy datasets.	2
the argument against fgan (which is a generalization of the regular gan) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective.	1
the two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term.	2
for example, the authors motivated the need of (mini)batch label queries, but never mention it again in section 3, when they describe their main methodology.	0
kl divergence is pretty well motivated here anyway).	2
the idea of boosting generative models is intriguing, seems well motivated and has potential for impact.	2
3) the paper is motivated by possible attacks on a data channel which uses a generative network for compressing information.	1
the paper is well motivated for investigating how much useful information (or how good the representations are) for each layer.	2
the paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.	2
this is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.	0
this work is wellmotivated, very wellexecuted, and can inspire many more interesting investigations along these lines.	2
2. the discussion of 'pairwise residual units' is confusing and not wellmotivated.	0
the motivation of this architecture is not very obvious, and is not well motivated in the paper.	0
for the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem.	2
preliminary rating: i think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends.	0
 overall judgment the method presented in this paper is interesting but not very motivated in some points.	0
i don't believe the specific choice of prior proposed in this paper is very well motivated however.	0
the generative model for trees seems reasonable but is not fully motivated.	0
this work is based on a solid technical foundation and is motivated by a plausible rationale.	2
since the domain being evaluated is a simple 2d maze, using deep networks is not well motivated.	0
as such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.	0
apart from the regularization term, the rest of the model construction is well motivated, i agree with the authors that a multiview approach employing gans is an interesting topic to consider.	2
the methods are wellmotivated, the mathematical derivations appear to be correct, and the presentation is clear enough to me.	2
interactive qa is certainly an interesting problem and is wellmotivated by the paper.	2
the method proposed essential trains neural networks without a traditional nonlinearity, using multiplicative gating by the cdf of a gaussian evaluated at the preactivation; this is motivated as a relaxation of a probitbernoulli stochastic gate.	2
review summary: the paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets.	2
2. the problem is not well motivated, are there any application of this.	0
the problem is well motivated and the author provides an interesting modification to the power algorithm, along with variational bounds on the value function (lemmas 3,4) which are interesting in themselves.	2
the paper presents an interesting modification to power algorithm that is well motivated.	2
the particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.	0
however, overall the system is not well motivated.	0
in conclusion, i think the piecewise constant variational family is a good idea, although it is not wellmotivated by the paper.	2
the choice of this method does not seem to be motivated by the authors.	0
i do not think it is wellmotivated or provides any useful insight whatever.	0
the transformations are motivated in terms of lie group operators, though in practice they are a set of fixed linear transformations.	2
this is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).	2
first, the problem is not well motivated: why do we care about uncertainty (although i believe we do)?	0
overall, the idea in this paper is interesting and the paper is wellwritten and wellmotivated.	2
the present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for.	0
furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (fractaloffractal networks).	0
these changes are also motivated by some analysis.	1
also, the regularization term is a well motivated and reasonable addition.	2
they also consider a regularization term motivated from information theory.	2
comments:  the first section of the paper is clear and wellmotivated.	2
the proposed approach (secs 6 and 7) looks more like a 'little hack' to try to make it vaguely similar to lagrangian relaxation methods than something that is theoretically well motivated.	2
this paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.	1
i think the paper is well motivated.	2
the model adds to ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.	0
while this type of experimental study is reasonable and well motivated, it suffers from a huge problem.	2
it is motivated by the method in which (it is assumed) human translators operate.	1
also, one change in the current paper is the addition of l2 normalization, which is well motivated and helps improve softmax feature distances.	2
while the idea may be novel and interesting, its motivation is not clear for me.	0
for example, the parameter update equation is section 4 is somewhat opaque and requires more discussion than the motivation presented in the preceding paragraph.	0
first, iit is unclear why coverage should be >=0.8 and firerate ~ 1, according to the motivation firerate should equal to coverage: that is each pixel f_p is assigned to a single vc centroid.	0
"first of all, the motivation is clearly given in the 1st paragraph of the introduction: ""in fact for such novel input the algorithm will produce erroneous output and classify it as one of the classes that were available to it during training."	0
2. the motivation behind the proposed model for the clevr task has been explained example of one type of questions – “how many objects are red or spheres?”.	1
 the motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.	2
2. the motivation is strong.	2
it would be good to see more motivation, beside the valuable insight of knowing it’s possible.	2
the theoretical result on lastiterate convergence of omd for bilinear games is interesting, but somewhat wanting as it does not provide an explicit convergence rate as in rakhlin and sridharan, 2013. moreover, the result is only at best a motivation for using omd in wgan training since the wgan optimization problem is not a bilinear game.	0
i find the paper interesting but not very clearly written in some sections, for instance i would better explain what is the main contribution and devote some more text to the motivation.	0
the motivation of the proposed method, pixeldcl, is to remove the checkerboard effect of deconvolutoinal layers.	1
it is a wellwritten paper, however, i am not very convinced by its motivation, the proposed model and the experimental results.	0
the introduction covers relevant literature and nicely describes the motivation for later experiments.	2
 pros:  combination of wavelets & cnn.	2
 cons:  lack of motivation i am not sure to understand the motivation of good reconstruction/homeomorphism w.r.t.	0
a dedicated section on motivation is missing.	0
generally, i find a jarring misfit between the motivation (deep learning for driving, presumably involving millions or billions of parameters) and the actual reach of the methods proposed (hundreds of parameters).	0
given that the discrete setting was the motivation for the dpp over lds, it seems strange not to even look at that case.	1
originality/novelty: the paper, based on wgan motivation, proposes wasserstein distance regularization over kl div.	1
 revision: the authors have adequately addressed quite a few instances where i feel motivations / explanations were lacking, so i'm happy to increase my rating from 6 to 7. i think the proposed title change would also be a good idea.	2
i also find the motivations for the proposed model itself a little unclear.	0
the motivation and architecture is presented very clearly and i am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!	2
while effectively changing the geometry of the problem, no motivation (theoretical or intuitive) is given as to why this normalization scheme should be effective.	0
the motivation is not explained clearly and convincingly.	0
however (perhaps because i'm not familiar with arora et al.) i found the mathematical analysis e.g. s2.2 dense, without any clearlystated intuitive motivation or conclusions (as per the introduction section) about what is going on semantically.	0
 overall the paper is good: good motivation, insight, the model makes sense, and the experiments / results are convincing.	2
"detailed comments (in order of appearance in the paper):  the motivation for using something else than the cosine is unclear, and the results do not seeem very conclusive since, as the authors say ""dspherenets with weight multiplier and loss annealing have the best performance against pgd attacks."""	0
 page 6: o it is not clear why mnist is tested over 200 examples, where there is a much larger test set available o in minmtl i do not understand the motivation from creating superclasses composed of 5 random classes each: why do we need such arbitrary and unnatural class definitions?	0
is there any optimizationrelated motivation here (beyond the single argument that networks are overparameterized)?	1
the motivation for l1 regularization is clear as it promotes sparse models, which lead to lower storage overheads during inference.	2
the quality and the significance of this paper it not high due to the following reasons:  the motivation is misleading in two folds.	0
in this paper, the authors treat the descriptive text as answers, is this motivation still true if the question generation is conditioned on answers, not descriptive text?	1
the dataset essentially contains questions about product reviews and does not match authors motivation/observation about realworld conversations.	0
positives: • novel approach towards more explainable and shorter training times/ less data • solid mathematical description in part 3.3 • setup well explained negatives: • use of colloquial language (the first sentence of the abstract alone contains the word 'very' twice) • some paragraphs are strangely structured • incoherent abstract • only brief and shallow motivation given (no evidence to support the claim) •.	0
no clear hypothesis is formed until section 3. the concept of explainable ai which could have been a good motivation does not find any mentioning.	0
''paper strengths''  the motivation of this work is wellwritten.	2
2) section 4.1 does not explain the initialization but just describes motivation and notation.	0
a probably more important issue is that the algorithm is sequential and hard to parallelize, where parallelization is usually the main motivation to use larger batch sizes.	0
while the relu activation is very common in nn architecture, without more motivations i am not sure what are the impacts of these results.	0
this is really great, just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals.	2
in the related literature, in particular concerning the intrinsic motivation, i think the following papers are relevant: j. schmidhuber, powerplay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem.	2
 while the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them.	0
the motivation is clear and proposed methods are very sound.	2
"suggestion: after capturing the motivation of the task, i suspect that the traditional treetotree (also xtotree) ""statistical"" machine translation methods still can also work correctly in this task."	1
quality: the quality of the writing, notation, motivation, and results analysis is low.	0
the motivation of the paper, and the description of its contribution as compared to existing methods can be improved.	0
''strengths'' i like the highlevel motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images.	2
it would be good to provide a bit more motivation for the choices made here.	2
the networks it is demonstrated on are not particularly large (largeness usually being the motivation for pruning) and the need for making them smaller is not well motivated.	0
pros: 1. the motivation is clear and the presented reformulation is reasonable.	2
my first concern is the motivation.	0
secondly, the motivation of adding the rotation operation is not properly justified.	0
the motivation behind all this is to learn the input features to the svm as opposed to handcrafting them, and use the generalization ability of the svm to do well on tasks which have only a handful of training examples.	2
the paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.	2
while the paper is very wellwritten i have certain concerns regarding the motivation, model, and evaluation methodology followed: 1) a stronger motivation for this model is required.	0
however, i like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy.	2
strengths:  multimodal goal prediction is an important problem to solve in robotic planning applications  some of the results seem qualitatively interesting weaknesses:  the problem is very illdefined  key aspects of the method are unclear  it is unclear how predicting images helps to solve the problem  the method is not evaluated against any comparable baselines unfortunately, i really struggled to understand the context and motivation for this work, as well as other key aspects of the submission.	0
however, even the motivation and context for this prediction task are unclear.	0
the main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the wage operations and operands which influences the power consumption and silicon area in hardware implementations.	0
it depends on the correlation between the gradients of the stochastic kl divergence and the stochastic loglikelihood 4) one of the original motivations for dlda was that the augmentation scheme removed the need for most nonconjugate inference.	1
overall i feel, the motivation and attempt is fine.	2
do you have a motivation for using this specific loss eq. (1), or is it a simple heuristic choice?	1
as stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent.	2
the motivation of the work is not very clear.	0
the neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction.	0
it lacks again any formal proof that the proposed approach exploits scalefree structures or even a proper motivation why this regularization should improve results.	0
a little more development of this idea, and some more concrete motivation for the specific choices of which properties to include, would go a long way in strengthening the paper.	2
(1) my biggest concern is about the motivation of the paper: firstly, another popular approach to speed up reading comprehension models is hierarchical (coarsetofine) processing of passages, where the first step processes sentences independently (which could be parallelized), then the second step makes predictions over the whole passage by taking the sentence processing results.	0
therefore, the motivation of the paper may make more sense if the proposed method is applied to a different nlp task.	2
first of all, the penalty the author suggest is the same as the one suggest by gulrajani for wasserstein gan (there the motivation behind this penalty comes from the optimal transport plan).	1
to summarize, i like the work and i can see clearly the motivation.	2
the authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in section 2.1 without clear motivation and justification.	0
the motivation of iterative inference would be to generate a feature that is easier to classify rather than to match the current fixed classifier.	0
i would be glad to reconsider my grade if the questions regarding the motivation of the twopass decomposition and the comparison with astrid and lee 2017 are answered.	0
what is the advantage over other competitive schemes, etc. in summary, while there is a minor novelty in connecting two separate ideas (cpd and uml) into a joint uml setup, the paper lacks sufficient motivations for proposing this setup (in contrast to say kernelized kmeans), the technical details are unconvincing, and the experiments lack sufficient details or analysis.	0
the authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.	2
3. the setup and ultimately the motivation in context of the setup is fairly artificial  the dataset does have images corresponding to each “dialog” so it is unclear why the associative model is needed in this case.	0
2. novelty of this work is also limited, as the authors acknowledge, that much of the motivation is borrowed from hao et al., while only the expansion mechanism is now normalized to avoid rescaling issues and threshold tuning.	0
here are a few motivations that are mentioned.	1
main issues i have two main concerns about motivation that are related.	0
it is not clear why, if gdn was proposed to eliminate statistical dependencies between pixels in the image, the main motivation is that gdn coefficients are not independent.	0
my main concerns are the following: 1) one motivation of dfm is that in many applications data is a discretization of a continuous process and then can be represented by a function.	0
the motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function.	1
however, the motivation above is not justified well.	0
as a result, the motivation does not seem clear.	0
this is quite a static strategy, i was assuming the authors are going to use some ir method over the web to back up their motivation.	1
 page 1: it is a little hard to follow the motivation against existing methods.	0
the motivation of the problem and applications are presented well, especially for news recommender systems.	2
for example, the main method this article is compared to (ewc) had a very strong section on reinforcement learning examples in the atari framework, not only as an illustration but also as a motivation.	2
most importantly, i'm currently missing a better motivation and especially a more thorough evaluation on how minerva improves over nonrl methods.	0
i am curious to know if there is a good motivation for this and what impact this can have on the training dynamics.	1
"5) part of the motivation for the work is said to be the increasing interest in inference networks: ""in these and related settings, gradient descent has started to be replaced by inference networks."	2
though novel, the motivation behind the semisupervised and active learning setup could use some elaboration.	0
cons/questions: 1. the motivation of the model choice of q is not clear.	0
 introduction: the motivation for detecting adversarial examples is not stated clearly enough.	0
maps shown in figures 6 and 7 provide good motivation for this approach.	2
4. authors’ motivation about concepts is interesting however, if the model bases its prediction on mean of w_c vectors over classes, then i am not sure if authors really achieve what they motivate for.	0
so i find that the main motivation for this work is not well supported.	0
authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.	2
  second paragraph in section 1: nice motivation.	2
this is a perfectly good motivation, but in this case it should be stated upfront, without hiding behind the misapplied “stateoftheart” term.	0
the motivation for this control is to evaluate the impact of the adversarial loss, which is presented as the key conceptual contribution of the paper.	1
the motivation of using this regularization is to enhance the interpretability of the learned representation and avoid overfitting of complex models.	2
from the motivation the advantages of the model did not become very clear to me.	0
my main question is that the motivation/intuition of introducing the syntactic distance variable.	0
this work has a wellestablished motivation: traditional attention for targetdependent sentiment classification cannot model the interaction between target term and context words when making predictions.	2
we see that the authors build models that become more and more complex, but their motivation in combining attention and it/ci is not clear: they learn the relation between context and target twice without any factorization.	0
# novelty and significance  the underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (nstep qlearning with reward prediction loss) are same as those of vpn.	1
so i think that the motivation behind introducing this specific difference should be clear.	2
the motivation of the work is not clear at all.	0
hence, the motivation for the algorithm you present is invalid.	0
 one motivation of the approach is to fix the slow convergence of the method in (franceschi et al, 2017).	1
not quite true  rl robots with highdimensional video inputs and intrinsic motivation learned to explore in 2015: (2) kompella, stollenga, luciw, schmidhuber.	0
the motivation for this choice would be helpful.	2
one would expect to see more discussion on this, given the importance of this property as motivation for the method.	1
the title and the motivation about the genetic algorithm are missing leading and improper.	0
the authors need to provide more justification for this motivation.	1
in particular, the motivation and approach are not clear (sec.	0
furthermore, while part of the motivation of this paper is to use dataset complexity measured with topology to help select architectures, experiments demonstrating that this might be useful are very rudimentary.	0
"2. the ""ordermatters"" motivation is not very convincing."	0
while the paper is well written, the motivation for combining selfattention and autoregressive models remains unclear unfortunately, even more though as the reported quantitative improvement in terms of loglikelihood are only marginal.	0
 (1) the motivation the paper argues that it is more suitable to use nonmetric distances instead of metric distances.	1
the authors make a very nice effort in motivation the paper, relating it with the state of the art and funding their proposal on studies regarding human visual perception.	2
impact: the motivation given throughout the introduction  to add an encoder (inference) network to gans  is a bit odd in the light of the existing literature.	0
it lacks both in sound theoretical justification and intuitive motivation of the approach.	0
the paper is mostly clear and wellpresented, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before section 3.3); and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.	0
what is the motivation to just average the inverse covariance matrices to compute s_c?	1
conclusion: the motivation of this work is valid and deserves attention.	2
if my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and modelbased scoring.	1
since the scenes are simulated from different viewer angles, the pose matrix quite fits the motivation.	1
the medical motivation does frame the clinical problem well.	2
on the whole, i like the line of inquiry and the elegant simplicity of the proposed approach, but the paper has some flaws (and there are some gaps in both motivation and the experiments) that led me to assign a lower score.	0
clarity the paper is clearly written for the most part, but there is room for improvement:  the %length pooling requires a more detailed explanation, particularly of its motivation.	0
convolutional architectures are typically much easier and faster to train than rnns, and the main motivation for rnns is their ability to deal with variable length sequences.	1
first, the motivation for oneshot graph construction is not very strong:  i don't understand why the nondifferentiability argued in (a) above is an issue.	0
the motivation of the proposed methods is clearly illustrated.	2
4. the definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.	1
the main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without markov assumptions).	1
the motivation is that one can stop reading before the end of the text and/or skip some words and still arrive to the same answer but faster.	1
even though the results of this paper is interesting, i have the problem with paper writing and motivation for their architecture:  paper pages are well beyond 8page limits for iclr.	0
 one important thing which is missing in this paper, i didn't understand what is the motivation behind using each of these components?	1
the motivation is to ensure that the gradient for the generator does not vanish.	1
in section 1 : in the explanation of the 3rd problem that gans exhibit i.e. the generator not being able to generalize the distribution of the input samples, i was hoping if you could give a bit more motivation as to why this happens.	1
the motivation of the paper is clear and so is the literature review.	2
a simple paper with: idea, motivation, experiments idea: it proposes a block called adaconvblock that replaces a regular convolution with two steps: step 1: regress convolution weights per pixel location conditioned on the input step 2: do the convolution using these regressed weights since local convolutions are generally expensive ops, it provides a few modifications to the size and shape of convolutions to make it efficient (like using depthwise) motivation:  adaconvblock gives more local context per kernel weight, so that it can generate locally flexible objects / pixels in images motivation is handwavy, the claim would need good experiments.	1
the experiments need to support the motivation / claim better.	1
overall, while i like the and think the goal is good, i think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning.	0
introduction: although in introduction the author discussed a lot of works on hierarchical latent variable model and some motivating examples, after reading it the reviewer has absolutely no idea what the paper is about (except hierarchical latent variable model), what is the motivation, what is the general idea, what is the contribution of the paper.	0
however, it does not adequately motivate the skippath connections or applications of the method to supervised tasks.	0
knowing how much delay we can tolerate will motivate us to design different methods of communication between teacher and student models.	1
comments: 1) this paper is well motivated.	2
the paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used 'on top' of the word embeddings.	0
“then, observe that the same reasoning we used to develop the cfkg acquistion function in (3.2) can be used when when we observe gradients to motivate the acquisition function…”  some misprints, e.g. double “when”  the paper lacks theoretical analysis of convergence of the proposed modification of the knowledge gradient criterion.	1
the paper does not motivate readers in this perspective.	0
"the authors could do more to formally motivate these games as ""difficult"" for any deep learning architecture if possible."	0
the prototype loss is too simplistic to capture cobinding tendencies and the combinationlstm is not well motivated.	0
the prototype vectors are not motiflike at all  can the authors motivate this aspect better?	1
the authors motivate the proposed approach in the context of vicinal risk minimization, but the proposed approach is not well supported by theory.	0
how does this relate to the original papers they cite to motivate this direction (alexandrov 2013)?	1
also could you motivate your choice for l1 norm as opposed to l2 in eq 3?	1
 you motivate your approach (section 3) using a maxvariance criterion (as in pca), yet your formulation actually uses the kldivergence.	0
this parameter seems central to the following bounds on dense gain, yet the authors do not motivate it, and there is no discussion of decaying hidden layer widths in previous sections.	0
the results on semantic segmentation are encouraging and may motivate more research along this direction.	2
 the authors motivate the problem of floor level estimation and tackle it with a rnn.	1
to do so, they first motivate a new measure of gradient size, the gradient scale coefficient which averages the singular values of the jacobian and takes a ratio of different layers.	1
i do not see where they motivate this restriction, and it seems to limit the usefulness of the bias terms in the encoder.	0
however the introduction needs to be improved to better motivate the work.	0
3. why do you motivate the learning method using selfplay?	1
4. authors’ motivation about concepts is interesting however, if the model bases its prediction on mean of w_c vectors over classes, then i am not sure if authors really achieve what they motivate for.	0
you motivate the choice of .alpha^' by wanting to maximize the reduction in the local linear approximation to .mathcal{c} induced by the update on w. however, this reduction grows to infinity the larger the update is.	1
below is a detailed review and questions for the authors: 1. please motivate clearly the need for having a bandit framework.	1
 can you motivate/discuss better why not providing the identity of a game as an input is an advantage?	1
the authors have tried to use mathematical formulations to motivate their choice, but they lack rigorous definitions/developments to make their point convincing.	0
detailed comments/questions:  the use of laplace approximation is (in the paper) motivated from a probabilistic/bayes and uncertainty pointofview.	1
the prediction step is motivated by primaldual algorithms in convex optimization where the term having both variables is bilinear.	1
for future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable ai in the first place.	2
then doing pca before training an autoencoder is not motivated.	0
could each of the components be motivated further through more discussion and/or ablative studies?	1
overall: the experimental results look good, however, the proposed model needs to be better motivated.	0
the paper is wellwritten and offers an interesting combination of ideas motivated from statistical analysis.	2
approach is well motivated with proper references to the recent literature.	2
"(2) cons i) lr ""warmup"" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach."	1
clarity: i would say this is one of the weak points of the paper  the paper is not well motivated and the results are not clearly presented.	0
overall, the paper could use revision but the proposed approach is simple and seems to be theoretically wellmotivated with solid analysis and benefits demonstrated in realworld settings.	2
 the work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding.	1
the problem formulation is explained clearly and it is well motivated by theorems.	2
this is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition.	2
the authors are motivated by two main applications: (i) multiclass classification problems with bandit feedback (ii) ad placements problem in the contextual bandit setting.	1
the paper is wellwritten and proposes a simple, elegant, and wellmotivated solution for the memory bottleneck issue in graph neural networks.	2
my main concern with this work is that it reads more like a tech report: it describes the workings and design choices behind one particular system in great detail, but often these choices are simply stated as fact and not really motivated, or compared to alternatives.	0
the problem and its potential applications are well motivated.	2
this method is never motivated.	0
this objective is motivated by a bound on the empirical risk, where this divergence appears.	1
the procedure involves mean subtraction followed by projecting out the first d principle directions and is motivated by improving isotropy of the partition function.	1
while the paper presents rather extensive experiments, the approach is only weakly motivated and the results are mostly unconclusive.	0
o the reward is a) not clear, and b) not well motivated when it is explained, and c) not explicitly stated anywhere: it is said that the actionspecific reward may be up to 10 times larger than the final reward, but the actual tradeoff parameter between them is not stated.	0
the problem has been well motivated and all the relevant issues point out for the reader.	2
 the paper is well motivated and written.	2
the analysis also wellmotivated about why coldfusion outperform deepfusion.	2
the baseline definition is wellmotivated, and the benefits offered by it are quantified intuitively.	2
the use of svrg is motivated by the fact that it can, in some cases, provide faster convergence than sgd.	2
i also found the setup itself to be poorly motivated.	0
i believe that the manuscript needs some rewriting so that the problem(s) are better motivated and is easier to follow.	2
3. overall, the paper is quite well written and is well motivated.	2
the prototype loss is too simplistic to capture cobinding tendencies and the combinationlstm is not well motivated.	0
"summary: the authors aim to overcome one of the central limitations of intrinsically motivated goal exploration algorithms by learning a representation without relying on a ""designer"" to manually specify the space of possible goals."	1
the technical approach seems well motivated, plausible, and potentially a good contribution, but the experimental work has numerous weaknesses which limit the significance of the work in current form.	0
for instance, how much the existence (and training) of a bla network really help — which is a central new part of the paper, and wasn’t in my view well motivated.	0
"the numerical experiments are motivated as a way to ""understand the capacity of the network with regards to modeling the external environment"" (abstract)."	1
with the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5) ' there are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators) ' a crossentropy loss for the autoencoder does not make much sense in my opinion (?)	1
the model design choices make sense in general, though they could be better motivated in places (see below).	2
i do think the idea is wellmotivated, and represents a promising way to incorporate prior knowledge of concepts into our training of vaes.	2
the paper is clearly motivated and authoritative in its conclusions but it's somewhat lacking in detailed model or experiment descriptions.	0
the design choices are well motivated in chapter 2 which makes the main idea easy to grasp.	2
the paper is largely wellwritten and wellmotivated, the overall setup is interesting (i find the authors' practical use cases convincingwhere one only has access to imperfect data in the first place), and the empirical results are convincing.	2
the first half of the paper is written superbly, providing a sober account of the stateoftheart in hypernymy detection via distributional semantics, and a clear, wellmotivated explanation of the main algorithm (dive).	2
strong points  the method is sound and well motivated.	2
pro: the algorithm is clearly explained, wellmotivated, and empirically supported.	2
the use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.	1
the paper is very well motivated and the formulation and experiments are well presented too.	2
on the other hand, if the method is trying to solve one of the aforementioned problems, then that too should be made clear, and the method should be motivated with respect to existing attempts to solve that problem.	1
second, i find the framework proposed by the authors interesting, but not clearly motivated from a neurobiological perspective, as the similarity between stimuli does not appear to play a role in the optimized loss function.	0
the problem was very wellmotivated, and the analysis was sharp and offered interesting insights into the problem of maze solving.	2
pros: rigorous analysis, well motivated problem, generalizable results to deep learning theory	2
i guess the motivation is the mixture of pyramiddrop and shakeshake motivations, but the main surprising part (forward weight can be negative) is not motivated at all.	0
cons: in my opinion, the substance of the contribution is not enough to warrant a full paper and the problem of timelimited learning is not well motivated: 1) it's not clear how frequently rl agents will encounter timelimited domains of interest.	0
this is certainly a wellmotivated problem, and the procedure is simple but makes sense.	2
"overall, i believe this is a very promising and wellmotivated work, but needs to be ""marinated"" further to be publishable."	0
the paper is clear, very well written, and wellmotivated.	2
the paper is very well written and technical details are well described and motivated.	2
the reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that eq (4) has problems.	0
the proposed approach is neither well motivated, nor well presented/justified.	0
pros:  wellwritten and (fairly) wellmotivated.	2
the small changes described in section 4 are not especially motivated and seem rather minor.	0
in more typical hci experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified.	1
the work is presented clearly, the approach is wellmotivated and related to previous studies, and a thorough evaluation is performed.	2
the process of recreating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated.	0
the problem and the proposed solution is well motivated.	2
– the choice of activations is not motivated, and performance on variants is not reported.	0
the introduction of the kernel inception metric is wellmotivated and novel, to my knowledge.	2
unfortunately, since the game theory was not particularly wellmotivated, i did not find the overall story compelling.	0
the paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms.	1
however the thesis of this work should be better motivated and explained.	0
what motivated you to study this problem specifically for gan in the first place?	1
 the algorithm is based on many heuristics that are not well motivated.	0
the problem is clearly presented and motivated.	2
inappropriate evaluation: while the model is motivated by the need to reduce the generation of wavenet sampling, the evaluation is largely based on the quality of the sampling rather than the speed of sampling.	0
pros: ' theoretically well motivated ' promising results on synthetic task ' potential for impact cons: ' paper suffers from lack of clarity (method and experimental section) ' lack of ablative / introspective experiments ' weak empirical results (small or toy datasets only).	2
quality: the proposed approach is well motivated and the experiments show the limits of applicability range of the technique.	2
ib seems interesting but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done.	0
if these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited.	1
a wellwritten manuscript, though the introduction could have motivated the problem a little better (i.e. why would we want to do this).	2
i thought the paper was clear and wellmotivated.	2
the exposition is clear and the method is wellmotivated.	2
in particular rainbow also uses a version of distributional multistep, which as far as i can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « offpolicyness » of the replay data), but still seems to work well on atari.	2
 the authors consider the problem of ultralow precision neural networks motivated by limited computation and bandwidth.	1
unfortunately, since the game theory was not particularly wellmotivated, i did not find the overall story compelling.	0
the idea of incorporating learned representations with a structured bayesian filtering approach is interesting, but it's utility could be better motivated.	2
"while the authors are clear about the fact that this is a mismatch, i did not find it wellmotivated why it was ""the right thing to do"" to match the training prior, given that the training prior is potentially not at all representative or relevant."	0
the technical exposition is at times difficult to follow with some design decisions of the network layout being quite ad hoc and not well motivated.	0
4) this work is motivated from the objective of causal inference, therefore it might be helpful to add empirical results to show how the proposed method can be used for causal inference.	1
overall, this is a wellwritten paper that tackles an important open problem in training gans using a wellmotivated and relatively simple approach.	2
the original model was motivated by human long term memory and neuroscience.	2
"considers a number of possible alternative models intuitive illustration in fig. 1 cons: misleading use of ""covariance"" the several important concepts including prototype mean/variance, distance, and loss are not well motivated or explained evaluation is too limited"	0
this is motivated by a similar network introduced in the context of semisupervised learning by tarvainen and valpola (2017).	1
in my opinion there are several weak points: 1) the approach to obtain the imagelike representation is not well motivated.	0
this abstraction is nicely motivated and contextualized with respect to previous work.	2
"the property of ""implementation invariance"" proposed in prior work seems poorly motivated, since the entire point of interpretations is that they should explain the computation performed by a specific network."	1
the paper is well written and motivated, and the idea seems fairly original, although the regularisation approach itself is not new.	2
 the related work section is pretty extensive, but i wonder if it should also include work on active learning (bayesian active learning, in particular, has been applied to sensing), submodular optimization (for sensor placement, which can be thought of as a spatial version of active sensing), and reinforcement learning.	0
i don’t really understand the motivation for using the validity of the edge labeling as loss function.	0
the motivation behind using dirichlet instead of gem makes sense, but other than that i fail to find any novelty in the paper.	0
" it seems very odd to me that the ""action should be implicitly encoded into the state representation"" could you elaborate of the motivation for this and the effects?"	0
the main weaknesses of the work lies in its motivation and in the empirical results.	0
i am concerned about both the motivation for and the novelty of this work.	0
i found it difficult to follow the theoretical motivation for performing the work.	0
i think that the main paper should include a brief explanation of this motivation.	0
a key motivation of the paper, as suggested by its title, is to incorporate and analyze the complex human motivations.	0
the motivation of the work is not very strong in my opinion, in particular by adding such a prior the space of possible solutions greatly shrinks and i am afraid that interesting solutions will be lost.	0
the motivation is deferred to section 5 but this makes section 3 quite unreadable.	0
[strengths:] the paper has a clear motivation.	2
3. motivation and advantages of the approach:  the approach is motivated by shortcomings of sentence encodings based on language modeling, such as skipthought, which are computationally intensive due to the large output space and the complicated decoding process.	1
 what is the motivation for having multiple layers without nonlinearity instead of a single layer?	1
 there is no motivations for the use of neither practical or theoretical since the results are only proven for whereas the experiments are done with .lambda = 5,20 or 30.	0
one good motivation for example is adversarial training, e.g. kurakin et al 2017 ‘adversarial machine learning at scale’ that would benefit greatly from faster attacks  whitebox attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.	2
i would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant.	1
the motivation is clearly conveyed in the paper.	2
what is the motivation for doing this?	1
i enjoyed reading the paper and found the motivation and results to be convincing.	2
so the motivation for switching to gd should be made clearer.	0
the motivation, background, and contributions of this paper are all mixed in the abstract and introduction, making it hard to understand what is the current state of learned activations, what this paper introduces, and how the work in this paper relates to prior work.	1
the paper is well written and easy to comprehend the motivations and main contributions.	2
this paper has provided an excellent motivation of their work in sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions.	1
i can hardly understand the motivation of this paper.	0
 the goal/motivation of this paper is not very clearly described.	0
however, the motivation of using rl is missing from the technical contribution.	0
the paper has several severe issues with it: motivation, related work, and theoretical novelty.	0
while i think that the paper contains many interesting ideas, it lacks a good motivation and, more importantly, i find the experimental design (as well as some results) to be very weak.	0
[details] 1. what is the motivation of this work?	1
hope the authors can give stronger motivations for this work.	0
also, the motivation for the hierarchical beta process isn't clear, since each group has a single bernoulli process.	0
"[con #2] i had trouble understanding the motivation for the subject within subject case study in section 5.1. the authors describe the problem as follows: ""imagine a subset of users wish to unlock their phone using facial identification, while others opt instead to verify their right to access the phone using other methods; in this setting, we would wish the face identification service to work only on the consenting subset of users, but to respect the privacy of the remaining users."""	0
for instance, the benefits of and motivation for the proposed approach are not simply stated in the introduction and then demonstrated in the rest of the paper, but instead the paper states some benefits and motivations, explains some technical content, mentions some more benefits, repeats some motivations stated before, etc. many researchers working on representation learning hope to discover the underlying learning principles that lead to representations that seem natural to a human being.	1
what is your motivation to use rnn for kmer sequences?	1
the main motivation was to improve scalability of rl and hrl to large state spaces, but the experiments are on the four rooms domain and the first room of montezuma’s revenge, which is not particularly large scale.	0
the authors define the social dilemma as: “for each individual agent, ‘defecting’ i.e. noncooperative behavior has the highest payoff.” with the intrinsic motivation, agents learn to cooperate.	1
this is good, however, if we’re thinking about situations where agents aren’t trained together and have their own rewards (the authors’ example: “autonomous vehicles are likely to be produced by a wide variety of organizations and institutions with mixed motivations”) then won’t these agents be exploited by rational agents?	0
in details: 1. the motivation of pmes from the perspective of mutual exclusivity is quite confusing.	0
i think the motivation to introduce the new objective function should be stated clearly.	0
first, i never understood the motivation for using rl here.	0
in summary, the main points of concern are:  the limited novelty on the modeling side  the lack of proper motivation on the importance of the addressed problem minor remarks:  yao et al. (2018) “use alternate optimization that breaks the flow of gradient through time” , unclear as their method can be optimized using stochastic gradient descent, as the yao et al. point out.	0
overall: pros: 1. clear writing 2. good motivation description.	2
cons: 1. failed to connect presented work with the motivation.	0
 the motivation for 21 seems a bit confusing to me; what do you mean with insignificant dimensions?	1
i am not clear what is the motivation to introduce the cosh loss function.	0
the motivation for this work is twofold.	1
for this reason, i believe the presentation and motivation of this work is not presented clearly.	0
therefore, i think the end results presented in the experiments do not align with the motivation.	0
4. using wasserstein barycenter to measure sentence similarity seems to be novel, but the motivation is not very clear.	0
this might be fine, but there is no motivation provided for this.	0
additionally, i find the motivation for caring about local optimality unconvincing.	0
the paper has, however, three major problems: the motivation of the paper is somewhat lacking.	0
the authors go through an information theoretic motivation but end up with the standard gan objective function plus a latent space (z) reconstruction term.	0
the motivation and intuition fail to be convincing.	0
i enjoyed reading the paper, the motivation is clear and the problem is important.	2
my main criticism is about the lack of motivation for nonnegative vae and lack of comparison with nmf.	0
i have several questions about the motivation and the method in the paper.	1
the motivation for using videos as a demonstration source could be more clearly stated.	1
this motivation should either be moved to the introduction or removed.	1
the motivation of the paper is well explained.	2
pros: the paper is very wellwritten, the motivation and the goals are quite clear.	2
the authors explain the technical details of the model, but fail to describe any motivation of their model as well as the key assumptions in the model.	0
first, the novelty and motivation of this paper is not clear.	0
the motivation for this paper is misleading.	0
first, the motivation of the paper is unclear.	0
the primary motivation of their paper is transfer learning  the embeddings created by their approach are able to generalize to other hospitals and healthcare settings.	1
i really liked the motivation too.	2
more motivation and explanation of introducing these criteria and notions are needed.	2
can the authors provide more motivation for why pm should outperform psm?	1
motivation for the latter is much less clear.	0
this goes against the main motivation of the paper (i.e. unsupervised distribution information and supervision from dictionaries can be combined for best results), as a completely unsupervised method seems to perform better than (or at least at par with) the proposed semisupervised method.	2
the motivation behind autogen is to address a common problem when training vaes with powerful encoders (e.g., autoregressive models) that the model simply ignores the latent representation and does not associate latent representation with the generated data in a meaningful way.	1
for example in this statement, robots aren't a requirement for evaluating intrinsic motivation.	0
i think the idea of the model is interesting, but i have some concerns about the motivation and the soundness of the model.	1
however, the index of capsules are randomly decided and different indices will result in different output vectors, which seems to conflict with the motivation of position invariance.	0
i appreciate the motivation of this work from the pu learning setting.	0
i like the motivation of this paper since existing algorithms have clear limitations, i.e., using outofdistribution samples.	2
there are also concerns about the motivations behind parts of the technique.	0
however, sections 4 and 5 seem lack somewhat in motivation, since here the authors focus on very specific function classes (sinusoidal functions, oscillatory textures, and weierstrass functions).	0
the motivation and contribution are clear.	2
cons: ' the motivation of using the word embedding and contextual word embedding over the tfidf feature wasn't clear.	0
the paper is wellwritten, including a comprehensive review of previous related works, an meaningful metalevel discussion for motivation, and a clear explanation of the proposed method.	2
thus, the motivation for using another phase to train the .alpha is not strong.	0
 the motivation of this paper is clear, since the sentence of same meaning in different languages should have similar representation in the high level latent space.	2
2: the exposition should explain the motivation for knockoff.	0
however, there are two issues that cloud the paper: 1. the problem motivation (bounding the security of model splitting) is a bit odd.	0
1. motivation: the introduction suggests that the main motivation is to reduce the computational cost of treating all observations within planning (“one must establish a tradeoff between optimality and tractability”), though later, different reasons are offered (“power, processing capability, and cost constraints”).	1
the paper presents sufficient motivation and background, and the proposed algorithmic implementation seems reasonable.	2
what is the motivation of using this method?	1
the motivation of this paper is reasonable, but i have some concerns as follow.	1
so, the motivation for this section just seems a bit unclear to me.	1
i think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful sgd works, such as saga and the like.	2
while the writing overall is clear and the motivation wellwritten, there are many issues with the modeling and experimental work.	2
what is the motivation of the author's experiments?	1
2. i might have missed it, but i couldn't find any motivation on why tanh is used as nonlinearity.	0
as their motivation, the authors quote computational and memory benefits at the time of training in addition to being able to extend shallownetwork analytical frameworks to the individual network layers thus allowing for a theoretical interpretation of their optima.	1
this seems like a very bad motivation for choosing the maxmin formulation.	0
the motivation of the proposed approach is unclear, and warrants a separate point.	0
"2) motivation: the authors mention ""we choose the relu function specifically because the submodularity of the scmm function requires nonnegativity ""."	1
pros:  the model is interesting and the motivation is quite clear  analysis is quite nice  writing is quite clear and decent cons:  extremely lacking experimental validation  there are literally no baseline models, no numbers or any kind of quantitative analysis.	2
second, i think the motivation in section 3.1 could be more convincing.	2
the paper is well written and nicely describes the motivation for the problem being solved.	2
the paper is well written and the motivation and methods are clear from the beginning.	2
cons:  lack of clear motivation and compelling use case.	0
overall, i feel the motivation and intuition behind the proposed method is not clear enough and experimental studies are not sufficient for understanding the behavior of the proposed method as an empirical paper.	0
 the paper is clear regarding motivation, related work, and mathematical foundations.	2
unfortunately, i do not appreciate the motivation of the work.	0
however, i don't fully understand the motivation here.	0
i have some concerns about section 3, which is the main motivation of this work.	0
 the paper is very well written and the motivation of the endtoend trainable framework for graph decomposition problem is very clear and strong.	2
clarity  the problem motivation as well as the methodology is clearly explained.	2
the paper describes the motivation, dataset and the evaluation in great detail .	2
 the overall motivation of the paper to emulate how humans learn by looking at other's action is very well taken.	2
however, the original motivation for having biased negative data are not explained very clear.	0
for the motivation example i assume the following assessment holds true.	2
 section 2, bullet point 3: imprecise, and motivation unclear.	0
first, the motivation of the proposed framework is not convincing for me.	0
1. could authors add more experiments on explaining their motivations?	1
currently, the motivation is not well justified.	0
this will make the motivation more convincing.	2
therefore, i think the paper needs some more motivation and experimental evidence of its superiority over related methods like sifpca in order for it to be accepted.	1
comments: i'm not sure about the high level motivation for developing networks of this kind.	1
i still think the motivation and experiments are not very satisfactory.	0
 this is an interesting application paper and would be of interest to computational biologists and potentially some other members of the iclr community  protein conformation information is not required by their method comments:  the authors should include citations and motivation for some of their choices (what sequence identity is used, what cutoffs are used etc)  the authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as  intpred  the authors use a balanced ratio of positive and negative examples.	2
hence, the motivation of the paper is unclear.	0
furthermore the motivation and positioning of the paper is not carried through in the empirical setup, where they investigate approaches that do training over all of the parameters of the model, breaking the assumption that the parties should be independent and should not share information.	1
the motivation for the “coupled autoencoder” model isn’t clear.	0
"the theoretical ""motivation"" equations in sec."	1
the experimental results also show a limited efficiency improvement according to table 1. although this is a debatable drawback compared with the novelty/contribution concern, it worth to reconsider the motivation of the proposed method given the fact that the automl framework is extremely expensive due to the drl design.	1
 the motivation for using the minmax entropy as a loss function (sec 3) is also not clear.	0
the paper is clearly presented and the intuitive arguments can be readily followed, even though the resulting loss formulation is a bit tricky to justify without expanding upon the underlying motivation.	2
the motivation is clear and comparison with existing work is clearly illustrated.	2
other critiques  in the introduction, instead of simply describing what is commonly done to obtain and evaluate sentence embeddings, it would be better to include a sentence or two about the motivation for sentence embeddings at all.	2
() the motivation is not strong.	0
i have some concerns though: ' the key motivation is not convincing.	0
an open question is: does your model capture the issues of mode covering as mentioned in the motivation?	1
 the motivation of the work is not clear but the novelty seems to be present.	0
(2) it is a little difficult to follow the motivation and contributions of this paper.	0
overall, i think the paper would gain a lot by making the motivation part more formal.	2
2. the motivation of the paper is not clear.	0
can you elaborate on the motivation behind this choice?	1
motivations are not provided clearly.	0
cons:  the motivation for this new domain adaptation setting is not clear to me.	0
i don't understand motivation for l_n(a).	0
the motivation in clear and explanation and flow of the paper are good.	2
however, one motivation for the gate mechanism is to better control the gradients flow.	0
therefore, the motivation in the introduction may be some modification.	1
although it is an interesting problem, it is not immediately clear from the introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and i hope the next version would elaborate on the motivation and significance for this new task.	1
while interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.	2
weaknesses: 1. the paper uses sparse rewards in rl as a motivation.	0
the paper should rewrite the motivation, or better explain why the proposed method addresses the motivation.	0
"the motivation to find a balance between the compactness and ""spreadout"" embedding is reasonable."	2
i didn’t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input.	1
a derivation, or at least a clearer motivation for the algorithm would be useful.	2
(1) i feel the authors should formally define what a spectral inference network is, especially what the network is composed of, what are the nodes, what are the edges, and the semantics of the network and what's motivation of this type of network.	1
normally, the major motivation for discretization is applicationdriven, say, in hospital, the doctor regularly triggers the inspection event.	1
 the motivation is clear: in the fewshot learning regime we expect to have little data to infer the taskspecific latent variables, and so we should perform posterior inference to account for uncertainty.	2
significance: the paper has a good motivation and deals with an important problem.	2
as a summary, pros:  good motivation.	2
however, i do not fully understand the motivation.	0
'clarity: the idea and motivation are very clear and so are the experiments.	2
clarity  overall, the motivation could be clearer.	0
is there a theoretical/intuitive motivation for this?	1
however, the theoretical contribution and the poor motivation of capsules in the graph context remain weak points.	0
could there be greater discussion & motivation for this, and in particular, relationship to work where weights are parameterized using orthogonal matrices, or even orthogonal initialization?	1
i find the way the biological motivation is presented to be a bit misleading/overstated.	0
is there a stronger motivation than that is similar to more classical model/architecture selection?	1
the studied problem is important and the motivation is clear, where the inputs are sets of objects such as values or vectors and how we can learn a good aggregation function to maximally preserve the information in the original sequence.	2
however, i do not think f has a natural motivation to minimize the maxedout error.	0
more motivation for experimental section is needed.	0
the motivation is clear, and the proposed strategy is original and to the point.	2
i think a stronger motivation for their method besides an analysis of some phenomena it captures and a slight improvement on some downstream tasks when combined with cbow is needed though for acceptance.	1
 method: while the mathematical motivation is sound, i'm not sure if the proposed training objective can achieve that goal.	1
i find the motivation in the introduction persuasive and the algorithmic approach sound.	2
i have some questions: (1) one of the motivation proposed by gu et.al. 2018 is that spelling based sharing sometimes is difficult/impossible to get (e.g. distinct languages such as french and korean), but monolingual data is relatively easy to obtain.	1
because of this, the primary motivation in this paper wasn't successfully supported by wallclock time; (2) lowrank approximation: lowrank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?).	1
 the motivation of wc for gan is still unclear.	0
 the motivation of cwc is still unclear.	0
even if it has been used before, i would encourage the authors to restate the motivation in the paper.	1
the novelty and motivation behind this approach is limited.	0
the motivation is to find a good way to impose lipschitz constraint to the learning of neural networks.	2
since the motivation of the architecture design is not very clear, i am not sure whether the architectures could generalize to other benchmarks.	0
cons:  the motivation of approach is questionable.	0
the motivation for the work is quite convincing.	2
 in the motivation (figure 1) it was mentioned that compositions can be done (and are often desirable) along longer paths (enfrruit).	1
 i don't understand the motivation behind holding out everything on the righthand side.	0
i am not ready to recommend acceptance of this paper, because i think the due effort to explain the motivation for research and its potential impacts has not been done in this case.	0
i can intuitively guess what they are but they lack motivation and definition (as far as i see).	0
maybe this is the motivation behind using many decoders.	1
"in terms of presentation, the motivation in introduction is fine, but the following section named ""notations and pseudocodes"" is confusing and has many undefined notations which makes the paper very hard to read."	0
this approach can be in particular useful for the tasks that have ' the paper is very wellwritten the goal and motivation of the paper is quite clear.	2
and, this leads to a clear motivation of the work and a welldefined problem setting.	2
the motivation for this paper is a little confusing.	0
major comments 0. the motivation of the localmlp models is not convincing.	0
the motivation is clear, and the contribution is significant.	2
to me, the motivation is not very strong.	0
not clear what the motivation for this setting is.	0
 section 3.3 is a bit confusing and to be honest i do not get the motivation for the usage of multiple kernels.	0
from this motivation, the 'constrained' bayesian optimization (bo) is applied and analyzed.	2
would you please provide motivations of the work?	1
the paper is difficult to read: the motivation is not well explained, the link between anomaly detection and multiplehypothesis methods (both in the title of the paper) is not clear.	0
the motivation is clear and reasonable.	2
first, the motivation and the rationale of the proposed approach is not clear.	0
motivation is not clear.	0
strengths:  the motivation for the work is clear and the implementation straightforward, combining existing tools from style transfer in a novel way.	2
the references to related work and motivation of the problem is good.	2
pros:  theorem 3.1 (although trivial) provides motivation for optimizing wasserstein distance in the latent space in waes.	1
i could not appreciate the main motivation/challenge in this distinction.	0
the paper does a good job at presenting the motivation, reads well in general, and it is well written (except the paragraph entropy uncertainty in sec.	2
finally, i would have liked to see some motivation for choice of stopping condition.	1
strengths: 1. the motivation for this paper is reasonable and very important.	2
perhaps the authors need to motivate their architecture a bit better.	0
4. the authors try to use proposition 1 to motivate the use of vat for generating complementary examples.	1
can you motivate why it is necessary?	1
giving a bit more details about this point here or elsewhere in the paper would help motivate the work.	1
i'd like to see that the study on some production translation systems, e.g., applying algo 1 to google translator and check its outputs, which can better motivate this work.	2
i have the following concerns/questions: 1) the authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to “generate a priority ordering of architectures for evaluation”.	0
however, the authors miss to motivate their specific approach.	0
the authors motivate this work with arguments about the sampleefficiency required by real robot learning, and demonstrate basic results using a real robot.	1
suggestions: 1authors need to motivate the applications of their work.	1
points of improvement and questions: can you please motivate the form of the reward shaping suggested in (2) and (3)?	1
motivate/explain this choice.	1
moreover, given the datasets and scope of questions related to fairness they need to provide better experiments that motivate the use of their method (compared to simpler methods) or consider other problems where their approach could be more useful.	2
overall, this paper uses neuroscience to motivate a behavior module.	1
"how can you motivate doing the ""replacement"" that you do to generate eqn 12?"	1
the paper is well written and the proposed method well motivate and intuitive.	2
regarding “important task”: pro: the authors motivate the task with tantalizing prospective applications automatically editing text and code, e.g. for grammar, clarity, and style.	1
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
can the authors add more details and motivate these two sentences?	1
the paper is relatively wellwritten, and the authors do a good job of explaining recent work on adaptivity, momentum, and optimism in online learning and convex optimization to motivate their algorithm.	2
pros: ' the paper is well written and well motivated ' the theoretical analysis is solid and provide intuition for more complex problems.	2
 cons: ' the primaldual formulation assumes wasserstein gans using linear discriminator.	0
the paper is well motivated, and the main motivation is nicely presented in fig.1, and the main idea clearly shown in fig.2 in an easytounderstand manner.	2
overall, the paper has a novel idea which is well motivated and executed in terms of experiments.	2
in general, the paper is clear and wellmotivated, but i find the notation sometimes confusing and inconsistent.	0
pros: it is well motivated.	2
this paper is clear, wellmotivated, and wellwritten.	2
the 3.2 is also poorly written and insufficiently motivated.	0
the proposed methods is motivated from pu settings.	1
detailed comments  1) introduction the paper is well motivated and the introduction of the paper clearly states the two main contributions of the paper: a betabernoulli dropout prior and a dependent beta bernoulli dropout prior.	2
 the proposed method is well motivated and easy to understand.	2
pros:  the overall framework is theoretically motivated and intuitive.	2
in general, the paper is wellwritten and wellmotivated.	2
it seems logical and well motivated.	2
> quality the paper is very well written, all concepts are wellmotivated and explained.	2
0. my overall evaluation: this paper is wellmotivated and the result is good.	2
pros:  wellmotivated intuition treating language as structured.	2
overall, the paper is well motivated and well written.	2
the approach is motivated by linguistic communication and it is straightforward to implement.	1
comments: this paper is well motivated.	2
the work is clearly structured and clearly articulate a wellmotivated research problem.	2
pros:  it is an interesting result that adding a weak visual imitation loss dramatically improves rl with sparse rewards  the idea of a visual imitation signal is wellmotivated and could be used to solve practical problems  the method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up rl in practice cons:  it seems possible that imitation only helps rl where imitation alone works pretty well already  some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal gail  the presentation borders on handwavy at parts and may benefit from a clean, formal description the submission tackles a real, wellmotivated problem that would appeal to many in the iclr community.	2
overall, this is a nice idea applied to a wellmotivated problem with promising results, although the exact regime in which the method succeeds could be better characterized.	2
however, the proposed approach is motivated by graph matching and a connection to the graph edit distance is implied.	2
pros:  the approach is well motivated and the proof of theorem 1 which gives the formula of the new gradient estimator seems correct.	2
" this paper is motivated in an interesting application, namely ""explainable representations"" of patient physiology, phrased as a more general problem of patient condition monitoring."	2
pros: the method is well motivated and clearly presented.	2
the engineering is carefully done, wellmotivated, and clearly described.	2
in particular, the authors propose a novel way to initialize residual networks, which is motivated by the need to avoid exploding/vanishing gradients.	2
while the idea of this paper is appealing and wellmotivated.	2
similarly, the types of generalization considered should be well motivated.	2
 overall, while the evaluation could have been more thorough and quantitative, this is a wellwritten paper that proposes an interesting, wellmotivated, and novel method with good results.	2
overall, this paper is wellwritten and sheds light on a wellmotivated problem, makes important progress in understanding the full power of quantized neural networks as a compression technique.	2
the experiments are well motivated and well described.	2
in general, the paper is well written and organized, the presented problem is well motivated, and the approach is very strait forward.	2
the proposed idea is well motivated.	2
 summary: the authors propose tensor ring nets for multitask learning cons: this is a poorly organized paper and poorly motivated.	0
 top pros:  well motivated approach with good examples from clinical setting  sound proof on why information theoretical approach is better than mle based approaches  experiments on diversified data sets to show their approach's performance, with good implementation details.	2
the paper is written clearly and well motivated.	2
3. motivation: the authors are encouraged to rewrite their paper with more motivated storyline.	1
given that the paper is motivated by the paths the gradients are taking during the training, it seems that analysis of the gradients (with and without the noise) is in order.	1
the idea is well motivated of why an active learning integration is needed.	2
in fact, it has a strong overlap with (song et al., 2017) (see #3) ' some modeling choices are not well motivated (see #2) ' ablation study showing that each modeling decision are motivated from a practical perspective is missing (see #4) ' the paper fails in comparing with relevant papers (see #4) # 2. clarity and motivation ' page 1 “many new deep learning methods of action recognition would use idt as one part of their networks to optimize their models ”: this statement is not clear, please provide references of methods that do this.	0
 clarity research is well motivated, and paper presentation shows a nice, coherent story.	2
"4) the paper is well motivated and clearly written ""in a high level"" (see below)."	2
" this paper wants to discuss a new objective function, which the authors dub ""conditional entropy bottleneck"" (ceb), motivated by learning better latent representations."	1
the proposed approach is explained and motivated well.	2
the paper is well motivated and mostly clearly written.	2
it proposes a sequence level objective, motivated by optimal transport, which is used as a regularizer to the (more standard) mle.	1
the problem is motivated well.	2
pros:  integration with scms is interesting  counterfactual variants of algorithms are clearly motivated and interesting  paper is generally wellwritten	2
 cons:  assumption that the agent is given a model with no mismatch is very strong  model class (noise variables  deterministic functions) seems potentially restrictive  questions about impact of approximate inference  experiments could have been more varied	0
review: overall, i found the paper wellwritten, the problem wellmotivated, and the proposed methods clear and reasonable.	2
the proposed method is well motivated (as in section 5).	2
strengths: 1. the technique is a wellmotivated solution for a hard problem that builds on the skipgram model for learning word embeddings.	2
the paper reads well and is well motivated.	2
overall i find the problem interesting, wellmotivated.	2
the idea is motivated by observations (shown in fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. bn) are fragile to very small batch size.	2
it is well structured and well motivated.	2
the paper is well motivated and easily accessible.	2
as the authors write, this work has already been impactful and motivated a great deal of further research.	2
it is motivated by the tendency of latent models to fall into local optima.	1
the approach is novel and well motivated.	2
in general, the idea of replacing bn with wc is interesting and well motivated.	2
 summary: this paper introduces a continuoustime flow, which is motivated from continuoustime gradient flow in the mongeampere equation in optimal transport theory.	1
clarity: the paper is well motivated, clearly written and easy to follow.	2
overall, i think this is a nice and wellmotivated model.	2
the method that was experimentally evaluated can be derived more simply without the twostep procedure by directly taking the gradient and add the heuristically motivated per state indicator.	1
the twostage training procedure is both theoretically motivated and appears to enhance the output quality of vaes w.r.t.	1
the work is motivated by applications where multiple time series need to be combined while they may get updated in an asynchronous fashion.	1
major concerns: 1, as said in related work, a soft version of this paper’s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence.	1
2. actually i’m also not sure if i get the motivation here.	0
i think this paper makes an interesting, original connection between unsupervisedsupervised risk estimation and oneclass neural networks which provides a principled motivation for existing methods [8, 4] and also hints to potential flaws in their formulations, namely that ocnn [4] and softboundary deep svdd [8] make no use of positive samples during learning (as illustrated in figure 2).	2
finally, the authors mention the paper has a cognitive science motivation, in that 'humans learn what variables are and how to use then at a young age' or that 'symbolic thought with variables is learned...', taking a strong 'nurture' stance on the origin of variables.	2
some points of motivation/justification are not very clear after reading: (1) why vanilla enas can’t be used for rl?	0
i liked the motivation and presentation of the paper but see some critical shortcomings:  in theorem 1, shouldn't there be a term that accounts for the complexity of the hypothesis class (vcdim, rademacher complexity etc.).	0
but i am not happy how the authors responded to my concern regarding their motivation: 1.)	0
so for me there is a massive disconnect between the main motivation of the paper and the suggested model.	0
i would hope that the authors will make some effort in making the paper more approachable and practical: ' a more detailed motivation section for the architecture will make it much easier for the readers to understand. '	1
the motivation that the source of indirect supervision on processing unlabeled speech comes from prior knowledge about the world and the context of the speech makes sense to me.	1
 what is the motivation for using median relative iteration reduction as a metric?	1
though different, i believe they share some common motivation, the authors may want to discuss and even compare this reference.	2
i’m not sure how significant are the differences of 0.007 between gradcon and ocgan (table 2 and 3), and those less than 1% in table 4. overall the paper does not seem to have a clear and welldefined motivation, contribution is also vague and trivial compared to literature, plus very convincing empirical justification.	0
however, i find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, detailed under: experimental concerns: — evaluating on 200 examples seems very small.	0
moreover, the network was trained 41 times for different values of the motivation parameter theta.	1
i have some suggestions for how to deepen the connection between the model and the experiment, and some concerns about the necessity of the motivation framework to the pavlovian task: 1) the authors make the prediction that neurons in vp should show (functional) connectivity matching that learned by their model.	1
so the fact that the same network can make predictions in both cases seems more like metalearning than motivationbased action selection.	1
some important details are presented factually without any motivation.	0
the paper mentions two cases as motivations: to calculate the entropy and the variance of the marginal.	1
 the paper studies the label noise problem with the motivation of without estimating the flip rate or transition matrix.	1
one could deduce the motivations of this work thanks to this section, however they should have been state clearer at the beginning.	0
overall i believe the experimental section can be largely improved, and given that the motivation of the paper is nice and the paper is clearly written and nicely presented, it would be a shame to leave the experiment section as it is.	2
the motivation and theoretical sections seems sound and the experimental results are encouraging, however maybe not completely supporting the claim of new ‘state of the art’ on uncertainty prediction.	0
main comments: i vote for rejecting this paper because i believe the experimental section has some design flaws, the choice of tasks used for evaluation is questionable, relevant baselines are missing, the intrinsic reward formulation requires more motivation, and overall the empirical results are not convincing (at least not for the scope that the paper sets out for in the introduction).	0
cons: the weakness of the paper is that the intuition or the motivation behind the design of the proposed method is not so clear.	0
the paper is poorly written, with lots of issues in math notation and poor motivation and explication of what the sections are introducing, and what parts of the presentation is novel and what is already known.	0
as an advice, i think it may be better to not squeeze in all the results into one paper, but rather focus on some aspects and analyze them really well, with clear explanation, motivation, and preferably with demonstrating practical gains that result from the analysis  not just hypothetical, but supported by experiments.	0
 i wonder if the paper could provide a motivation on why the autoaug policy is adopted when training robust model.	1
in particular, i'm not sure that i really understand the motivation of this research question.	1
the motivation for the paper is limited, in that they suggest previous works have suggested adversarial training itself 'overfits' to the given l_p norm.	1
i'm not clearly seeing the motivation for an embedded feature selection method for neural network models: for the datasets considered in the paper, it would seem that training a nonlinear model that used all the features would result in performance at least as good as training the nonlinear model with a prepended stg layer.	0
hence, more analysis or a better motivation would have helped.	0
the paper does not clearly state the motivation of the proposed method.	0
 what was the motivation for using gru for update function in eq 3?	1
the motivation of the study is described appropriately, and the performance is quantitatively evaluated, as shown while table 2. however, the generality of the proposed method, i.e., dual attention, is still ambiguous.	0
i appreciate the motivation that the authors try to validate the use of their objective to learn a 'smooth upper envelope' but most of these statements are somewhat trivial and/or wrong section 4.1 does not actually deliver a valid justification.	0
first, the motivation for this work is not fully clear: why would the error in flow prediction be a good driving force for curiosity?	0
i like the idea / motivation of the paper, but the authors could do a better job of explaining the motivation to people less familiar with techniques based on the determinization framework.	0
the authors do not thoroughly explain the motivation of this paper.	0
however, i find this motivation not clear: if multitask learning and its scalability issue are the reasons why we need continual learning, with the scale of the experiments considered in the paper, wouldn’t it always more beneficial to use multitask learning instead of continual learning?	0
there does not seem to be any particular motivation for these patterns, but this is at least better than poor justifications.	0
overall, the paper is written in an understandable manner with a clear motivation.	2
ultimately, the very highlevel motivation for undertaking this work is fairly reasonable, and the narrow proposed algorithm (train a oneshot imitation system using exploration data) comes through, but it is not presented very well, and i think what precisely the authors are aiming for relative to other approaches is actually a bit confused.	0
for me, that is the most important motivation and background of the ra method.	2
in my view the only reason to motivate sparse attention models would be the following: 1) attention over long sequences where full attention does not scale (this is the motivation behind [3] for example) and 2) better generalization than full/soft attention.	1
supporting arguments: equation 2 needs more motivation.	1
moreover, the selection function has not been demonstrated to behave as the core motivation for the work implies it shouldnamely, modeling .	0
to make the contents in section 4 more concentrated, i suggest moving some of the less relevant experiments presented in section 4 to section 3 as the motivation of adversarial metalearning, or to the appendix.	1
the motivation comes from the sub optimality of uniform quantizers.	1
i think that this paper merits acceptance because (a) the motivation of taking a necessity in practice (somehow selecting models / injecting inductive biases) and making it more explicit in the approach is a good one, and because the thorough empirical survey (and simple, but novel, contribution of a new semisupervised representation learning objective) are likely valuable contributions to this community.	2
what is the motivation of using flow in this work?	1
i would have liked more motivation and intuition behind it.	1
i think this is totally no problem because we are focusing on label noise here, but this makes the motivation about outliers less important when we are talking about label noise.	0
the paper has a wellorganized structure to convey the motivation.	2
the motivation is quite clear, and the writing is easy to follow.	2
the main motivation seems to be that inference in traditional mln is computationally inefficient.	0
questions 1. i think the motivation of why hypernetworks are expected to have less forgetting (than addressing forgetting directly in a network) should be discussed early in the paper.	0
minor comments p1: i like the motivation of cooking recipes for work on program conditioned learning.	2
 i like the justification/motivation given for replacing nsp with sop.	2
the results on gaussian case not only provide intuition, but also provide motivation for the design of attacker and defender architectures in gim attacks.	2
i am willing to amend my vote if the authors provide stronger (not empirical) motivations on why using the gradient norm w.r.t.	2
but you miss the opportunity to clearly motivate why we need to parameterize!	0
the study of this application is well motivated: adversarial examples need not be played in the physical domain (since the audio/video is uploaded directly in a digital format) and copyright detection is a bit more complicated than object classification (because false positives are expensive), small margin between classes.	2
you can still motivate the differences with the mixmatch paper.	1
i have a few concerns about some technical details of the paper, as explained below: 1) the paper motivated the new model with difficulty in detecting change points in seasonal time series.	0
to the best of my knowledge, this was the first paper proposed to use moving patterns in two consecutive observations to motivate agent exploration.	1
the author should do more relative surveys to motivate the authors.	1
the authors motivate their work by saying that soft attention is not interpretable, however i do not see any empirical evidence or analysis of their proposed hard attention mechanism as being more interpretable.	0
so that i could motivate, carefully explain, and evaluate the main model in the paper.	1
question:  in partially observable environments that require agents to wait for something, should a ridemotivated agent consider changes in its own internal clocks (part of the recurrent state) impactful moves?	0
this would motivate the defense more.	1
they motivate this class of function based on theoretical arguments and empirical observations in the context of neural network training loss.	1
so this paper is not well motivated.	0
the problem is interesting and wellmotivated, but i have some concerns with the proposed approach and experiments.	2
the method is interesting, novel and seemingly efficient; but it is insufficiently defined, the method is not motivated and experiments are quite weak with little comparisons and no experiments with practical value.	2
1. this paper is well motivated and structured, which may serve as a guidance for other works.	2
it is motivated by the issue that l1 regularisation violates incoherence and irrepresentability conditions (cf sec1 and sec2 below eq3).	2
it is motivated by a flaw in the diagnostic proposed by kuleshov 2018 (abreviated k2018 below), as explained around eq4, and proposes a replacement diagnostic for uncertainty calibration quality (sec 3 before 3.1).	1
however, at the same time i feel the proposed method would be slightly less motivated without the improved results of the 'pure' robust accuracy, as the method itself is anyway a variant of adversarial training.	0
 this paper is motivated to improve the performance of zeroth order stochastic search by incorporating a gaussian mixture as the candidate solution sampler.	2
this paper is wellmotivated.	2
on the one hand, the paper is well written, well motivated, and the experiments quite thorough.	2
however, i would like it better if it does not contain the part about the cooperative point clouds, which are not very motivated and the experiment settings are not very convincing.	0
 it is not clearly motivated why one should use two separate networks for depth and rgb inputs in light of the additional complexity.	0
most notably, the loss functions are motivated by fluid dynamics, which forces the network to remain more consistent with the governing laws.	1
i think this paper should be accepted as it addresses an interesting and widely applicable scenario, and the combined unsupervised objective is a creative and wellmotivated solution.	2
notation is lacking at times, and a number of steps and concepts are not well motivated and not well described.	0
my main concerns with the paper are as follows: a. the individual components of the approach are poorly motivated.	0
i would not recommend this paper because the method in this paper is not well motivated and the empirical results do not demonstrate the usefulness of the proposed method.	0
the paper is clearly motivated and easy to follow.	2
 the paper is very wellmotivated, written, and methods are described nicely and succinctly.	2
the paper provides a series of mathematical results which will be interesting for the community as they are wellmotivated, intuitively explained, and of clear practical relevance.	2
key reason 2: wellmotivated method with a collection of multiple novel contributions that are interesting and practical.	2
the proposed method is simple, but well motivated, sound, and well explained.	2
there are a couple of weaknesses summarised above and detailed below that i would love to see addressed, but none of them is overly critical: 1. unfortunately the paper suffers from the same issue as the original work on fgru, which it's based on: the fgru architecture seems overly complicated and its numerous details and design choices not well motivated.	0
although simple, the proposed method is wellmotivated, and the reported results are generally convincing.	2
but i do understand that this choice was motivated by some remarks of the neurips fellow reviewers regarding the simplicity of the pointgoal task of the challenge.	0
the approach is motivated both from a theoretical point of view and from a practical point of view.	2
in generally, the paper is well motivated and interesting.	2
the combination of the global and local alignment is novel, nicely motivated, and ablation studies demonstrate that both are needed for good performance.	2
overall, the approach is wellmotivated and performs well empirically.	2
importantly, the noise is chosen such that it optimizes an informationtheoretically motivated objective (ratedistortion/info bottleneck) that ensures that decisionrelevant signal is flowing while constraining the overall channelcapacity, such that decisionirrelevant signal is blocked from flowing.	1
