text	label
one could argue that experiments could be conducted on different environments or that the novelty is limited, but i feel that 'correct' (nononsense, experimentally sound on doom, appendix providing details for reproducibility) and 'milestone' (vizdoom winner) papers should get published.	0
pros and cons: although there is little novelty in the paper, i think the work is of great value in shedding light into some interesting questions around generalization of deep networks.	0
somewhat incremental novelty relatively to fitnet	0
overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and i recommend acceptance.	0
2. the proposed method produces visually appealing results on several datasets 3. the authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the svhn>mnist task 4. the paper is wellwritten and easy to read cons: 1. the novelty of the method is relatively minor (i consider fconstancy term as the main contribution) 2. it feels like the proposed approach would break for more dissimilar domains.	0
cons: 1. the novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.	0
however, the novelty is relatively incremental given previous cited work on multistream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.	0
the paper is very interesting, but the novelty is low compared to the referenced work.	0
preliminary rating: i think this is an interesting paper with convincing results but is somewhat lacking in novelty.	0
deepdsl is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework.	0
overall, the deepdsl framework seems to have real value in its use of scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks.	0
some of the ideas it presents are interesting, but overall the paper lacks novelty and potential impact and stays firmly within the realm of deep learning framework whitepapers such as [1,2,3,4], which to my knowledge don't have a precedent of being accepted at venues like iclr.	0
in light of these existing models, the novelty of the qrnn is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.	0
overall the paper is an enjoyable read and the proposed approach is interesting, pros:  address an important problem  nice empirical evaluation showing the benefit of their approach  demonstrate up to 16x speedup relatively to a lstm cons:  somewhat incremental novelty compared to (balduzizi et al., 2016) few specific questions:  is densely layer necessary to obtain good result on the imdb task.	2
i’m not sure about the novelty of the proposed ngram rnn because i recall seeing similar architectures before but i understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better.	0
the only concern i have is the novelty of this work.	0
my main concern is with the paper`s novelty.	0
my primary concern remains about novelty: the extra data introduced here is welcome enough, but probably belongs in a 'acl short paper or a technical report.	0
the general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix a is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1norm (while other techniques would mostly use l2 regularisation).	0
this is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper.	0
it is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semisupervised learning (especially svmrelated literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.	0
i would say the technical novelty is incremental since the extension is straightforward and similar to previous work.	0
the novelty and the key insights are however not always well motivated or presented.	0
overall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately.	0
the quality of the work is not that bad, but the novelty of the paper is not that good either.	0
first this paper has the style very similar to the sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty.	0
4) conclusion although the novelty with existing video captioning approaches is limited, the paper is relevant to iclr, as the proposed simple but efficient implementation and benefits of spatiotemporal  feature abstraction attention are clearly validated in this work.	0
while there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above.	0
weak points:  probably the main negative point is the amount of novelty and contribution.	0
although the proposed method may be potentially useful in practice (if refined further), i find the method lacks novelty, and the experimental results are not significant enough.	0
in short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.	0
after the rebuttal: the paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal.	0
1) the main concern is that there are no comparisons or even mentions of the work done by tamara berg’s group on fashion recognition and fashion attributes, e.g.,  “automatic attribute discovery and characterization from noisy web data” eccv 2010  “where to buy it: matching street clothing photos in online shops” iccv 2015,  “retrieving similar styles to parse clothing, tpami 2014, etc it is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.	0
as with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the subsubfield of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible.	0
this paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating arabic digits and english letters.	0
pro: ' well written ' exhaustive set of experiments ' learning algorithms with decimal representation ' available source code cons: ' no coherent hypothesis/premise advanced ' two or three bold statements without explanation or references ' some unclarity in experimental details ' limited novelty and originality factor typos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).	0
while the speedup obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for iclr.	0
therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), i don't feel the paper is ready in its current form to be accepted to the conference.	0
cons: 1) the novelty of this work is not enough.	0
my first concern of the paper is on the novelty of the model.	0
weaknesses: =============== 1. claims about the contribution/novelty of the model seem not to hold: 1.1. one of the main contributions is the hierarchical attention/memory (ham): 1.1.1. it is not clear to me how the presented model (eq 68), are significantly different from the presented model in xu et al / yao et al. while xu et al. attends over spatial image locations and yao et al. attend over frames, this model attends over encoded video representations h_v^i.	0
to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations.	0
second, while the technical contributions/novelty are not a focus of the paper's presentation, i am concerned by the lack of methodological advance.	0
essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but i can't point to a technical novelty in the paper that the community can not do without.	0
despite the lack of novelty of the method, i do think that the results are valuable.	0
however, the novelty of the paper is incremental, since the blocked gibbs sampling for nade model is already proposed by yao et al., (2014) and the using nade based model for music modeling has also been proposed by boulangerlewandowski et al., (2012).	0
this is an interesting contribution but it seems to me that the results are too similar to the ones in choromanska et al. and thus the novelty is seriously limited.	0
on the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit.	0
unfortunately, even after reading the authors' response to my prereview question, i feel this paper in its current form lacks sufficient novelty to be accepted to iclr.	0
given the lack of novelty and the missing baselines, i believe the paper in its current form is not ready for publication at iclr.	0
in summary, i found this paper to be wellexecuted/wellwritten, but it's novelty and scope too small.	0
to sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it.	0
it rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.	0
in general, i find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multimodal analysis, and the rest of the paper offers no surprise in implementing such approaches.	0
the proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas.	0
weaknesses: 1. the technical novelty of the paper is limited; it is mainly a comparison of different variants to include attributes.	0
this is a wellwritten paper with good analysis in which i especially like figure 5. however i think there is little novelty in this work.	0
my main concern is novelty.	0
this allows for training of visual models from video, etc. a major concern is the amount of novelty between this work and the author's previous publication at nips 2016. the authors claim a more sophisticated architecture and indeed show an improvement in recall.	0
pros: ' interesting topic ' blackbox setup is most relevant ' multiple experiments ' shows that with flipping only 1~5% of pixels, adversarial images can be created cons: ' too long, yet key details are not well addressed ' some of the experiments are of little interest ' main experiments lack key measures or additional baselines ' limited technical novelty quality: the method description and experimental setup leave to be desired.	2
a major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.	0
as such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.	0
in summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task.	0
an important baseline is missing where realvalued paragraph vectors are learned first, and then converted to binary codes using offtheshelf hashing methods (e.g. random projection lsh by charikar, bre by kulis & darrell, itq by gong & lazebnik, mlh by norouzi & fleet, etc.) given the lack of novelty and the missing baseline, i do not recommend this paper in its current for publication in the iclr conference's proceeding.	0
lack of novelty of yet another standard directed ldalike bag of words/bigram model.	0
cons: there is little novelty  the method is arguably too simple to be called a “method.” rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.	0
overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “frustratingly easy domain adaptation”).	0
(in general the novelty of this paper seems low) experiments:  section 4.3.1 is not an accurate measure of the 'effectiveness' of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a gaussian distribution.	0
the idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty  for example the maddison and tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.	0
the one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not.	0
the main merits of the paper is to present extensive experiments on how well the vanilla bloom filter approach can perform, but overall the novelty is fairly limited.	0
paper summary: the authors proposed to use edgeboxes  fastrcnn with batch normalization for pedestrian detection review summary: results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and overall the work lacks novelty.	0
cons:  lack of novelty.	0
there is a lack of novelty and no significant results.	0
the proposed method performs best among 14 biclustering methods, however, my first concern is that from the methodological point of view, the novelty of the proposed method seems small.	0
cons: novelty of the proposed method some description in the paper is unclear.	0
the overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information.	0
cons:  acgan model itself is of limited novelty relative to other gan approaches that condition on class.	0
however, the experimental results are not significant enough to compensate the lack of conceptual novelty.	0
this is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high.	0
i do like that this paper looks at the autoencoder objective as a way to alleviate the missing mode problem of gans, but i think that alone does not have enough originality to carry the paper.	0
some other points are listed below originality: while there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, i like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches significance: the method achievs stateoftheart performance on the two shown examples on mnist, however these networks are far from the deep networks used in stateoftheart models.	0
all in all i think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance.	0
pros (quality, clarity, originality, significance:): this paper presents a novel metacontroller optimization system that learns the best action for a oneshot learning task, but as a framework has the potential for wider application.	0
concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref defferrard 2016 in the paper) to a semisupervised transductive setting.	0
even though the list of cons here is longer than pro's, i recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques.	2
pro: ' well written ' exhaustive set of experiments ' learning algorithms with decimal representation ' available source code cons: ' no coherent hypothesis/premise advanced ' two or three bold statements without explanation or references ' some unclarity in experimental details ' limited novelty and originality factor typos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).	0
originality: original, but at the moment it is not clear such originality is necessary.	0
although the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.	0
please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.	0
so although the work is solid, the lack of originality lets it down.	0
it seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved.	0
i like that idea but i am worried by the fact the task is changing during learning, since the extracted features are being modified.	0
the pros of the paper are: 1. the idea of using rnn to produce the description of the network and using rl to train the rnn is interesting and promising.	0
although i have listed my concerns, i would like to reiterate that i do find the idea of an actiondependent baseline fascinating.	0
pros:  the idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally.	2
while this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has “proved to be a problem for earlier models based on continuous distributions”.	0
pros: 1. the paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.	2
there are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea.	0
some other points are listed below originality: while there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, i like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches significance: the method achievs stateoftheart performance on the two shown examples on mnist, however these networks are far from the deep networks used in stateoftheart models.	0
any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.	0
strengths  the model is an interesting embodiment of the idea of predictive coding implemented using a endtoend backpropable recurrent neural network architecture.	2
it is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on.	0
it is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties.	0
the proposed idea is quite obvious, but the authors are the first ones to propose to test such a model.	0
in summary, the idea is a good one, but the experiments are weak.	0
strengths: 1. the paper shows performance improvements over existing solutions 2. the idea of learning the quantization instead of using predefined humanmade algorithm is nice and very much in the spirit of modern machine learning.	2
pros: ' clearly written and original idea. '	2
the idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. '	0
pros  the paper is the first of my knowledge to explicitly measure the bits per parameter that rnns can store  the paper experimentally confirms several intuitive ideas about rnns:  rnns of any architecture can store about one number per hidden unit from the input  different rnn architectures should be compared by their parameter count, not their hidden unit count  with very careful hyperparameter tuning, all rnn architectures perform about the same on text8 language modeling  gated architectures are easier to train than nongated rnns cons  experiments do not reveal anything particularly surprising or unexpected  the ugrnn and rnn architectures do not feel wellmotivated  the utility of the ugrnn and rnn architectures is not wellestablished	2
i however recommend the acceptance because of its merit of the idea.	0
for the sake of clarity of the presentation i would drop parts of section 3 ('a combinator library for neural networks') which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper.	0
finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that.	0
the idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation.	0
pros  as an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example.	0
the idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step elementwise with a certain probability.	0
it is still worse that variational dropout on penn tree bank language modeling task, but given the simplicity of the idea it is likely to become widely useful.	0
strengths  simple idea that works well.	2
taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?	0
since the main idea is very interesting, i will be happy to update my score if the above concerns are addressed.	0
effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources.	0
pros:  novel and interesting idea for memory access  nicely written cons:  need to manually specify the lie group to use (it would be better if network could learn the best way of accessing memory)  not clear if this really works better than standard ntm (compared only to simplified version)  not clear if this is useful in practice (no comparison on real tasks)	2
the paper claims three main contributions: 1. modification to model architecture (used in oh et al.) by using action at time t1 to directly predict hidden state at t 2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames) 3. exploring different training schemes (tradeoff between observation and prediction frames for training lstm) 1. modification to model architecture  the motivation seems good that in past work (oh et al.) the action at t1 influences x_t, but not the state h_t of the lstm.	0
some might also question whether iclr is the appropriate venue to introduce a new dataset, but personally i think it's a great idea to submit it here, seeing as it will reach the right people.	0
i like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence.	0
overall, i think the ideas presented in this paper show good potential, but i would like to see an extended analysis in the line of figures 3 and 4 for more datasets before i think it is ready for publication.	0
overall, the ideas and developments in this paper are promising, but it needs more work to be a clear accept for me.	0
it looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)  the definition of the boltzmann policy at end of 2.1 is a bit confusing since there is a sum over 'a' of a quantity that does not depend (clearly) on 'a'  i believe 4.3 is for the tabular case but this is not clearly stated  any idea why in fig. 1 the 3 algorithms do not all converge to the same policy?	0
pros: original clever idea.	2
strong points:  well written, interesting idea of combining various sources of information in a bayesian framework for seq2seq models handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here  which is definitely of interest to the community  strong experimental section, with some strong results (though not complete: see weak points) weak points:  authors do not improve on computational complexity (w.r.t tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)  what about the baseline where you only combine direct, lm and bias contributions (no channel)?	2
pros:  the idea is novel  the approach is described clearly cons:  the experimental evaluation is not convincing, e.g. no improvement on svhn  number of parameters should be mentioned for all models for fair comparison  the effect of droppath seems to vanish with data augmentation	2
nice idea but not complete, model size is not reduced by the large factors found in one of your references (song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49x reduction in model size of vgg (without loss of accuracy).	0
the paper presents two versions of the idea: one which is computationally expensive (and high variance) in that it needs two passes through the same example at a given step, and a temporal ensembling method that is stabler, cheaper computationally but more memory hungry and requires an extra hyperparameter.	0
the idea of this paper is simple but nontrivial.	0
pros:  interesting idea: learning structures of sentences adapted for a downstream task.	2
it seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate.	0
the idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive.	0
the general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix a is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1norm (while other techniques would mostly use l2 regularisation).	0
it is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.	0
pros:  i really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum).	2
extending the idea of the adversarial training to the text tasks is simple but nontrivial.	0
the idea is novel to some extent, as previous paper had already tried to combine pointerbased and standard models, but not as a mixture model, as in this paper.	0
in summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.	0
this is however still a preliminary work and one would like to see the ideas pushed further.	0
i personally find the idea exciting, but in order for it to be of interest to the iclr community i think more emphasis should be put on what insights such a technique allows to gain on trained models.	0
overall i think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story i think it is not yet ready for publication.	0
review summary ''' pros:  i like the idea of bringing multiplicative rnns and their predecessors back into the spotlight.	2
the authors build upon an idea recently presented at the repeval workshop, but are able to collect a significantly larger amount of examples by relying on existing ontologies.	0
however i find the theoretical analysis not very informative, and distractive from the main central idea of the paper.	0
the paper establishes a base result, but does not explore the idea to the extent to which i think an iclr paper should.	0
the main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.	0
i have to emphasize that i like the ideas introduced in this paper, but i am not convinced by the way they are presented and evaluated.	0
the idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images.	0
while the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.	0
the key idea, which is very sensible, is to use a classbased hierarchical softmax, but where the clusters/hierarchy are distributed such that the resulting matrix multiplications are optimallysized for gpu computation, based on their empirical tests.	0
overall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws.	0
potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited  i would encourage the authors to also discuss the limitations.	0
i was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why i’m not highly confident about this as a conference submission (and hence my low score) i am open to be convinced either way.	0
while there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above.	0
one, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact.	0
a full exploration might be its own paper, but the idea is worth at least brief discussion in the text.	0
there is certainly nothing wrong with the tailored approach, but it would help to be clear and detailed about where the presented ideas can be applied out of the box, or how one would go about making the relevant design choices for a range of different problems.	0
overview of the review: pros:  the idea is very interesting.	2
i think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work.	0
although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.	0
the space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a nonlinear extension of mensink et al 2013).	0
i like the basic idea of the paper, but the points above make me think it is not ready for publication.	0
secondly, the idea proposed in the paper is incremental and not new to the field.	0
[r1] https://papers.nips.cc/paper/2275selfsupervisedboosting.pdf pros: novel and intriguing idea strong theoretical guarantees cons: resulting boosted model is unnormalized discriminator based boosting is expensive, due to sampling via mcmc weak experimental section	2
pros: ' the general idea behind the paper seems pretty novel and potentially quite cool. '	2
cons: ' the evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. '	0
i think the idea is nice overall because it allows network designers to better understand the representational power of each layer in the network, but at the same time, this works feels a bit rushed.	0
pros: the idea of extracting upward or downward trends from time series  although these should, ideally be learned, not rely on an adhoc technique, given that this is a submission for iclr.	2
this paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task a in learning a new task b, which not only improves performance on the new task b, but also tries to avoid degradation in performance on a. the general idea is based on encouraging a model trained on a, while training on the new task b, to match fake targets produced by the model itself but when it is trained only on the original task a. experiments show that this method can help in improving the result on task b, and is better than other baselines, including standard finetuning.	0
i weakly vote for acceptance since i like the ideas, but if the paper does not make it in, i would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.	0
pros:  good experimental results cons:  ideas are quite trivial  the experiment on ptb was carried out improperly	2
one could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an 'endtoend trained' system.	0
in short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.	0
this paper introduces three tricks for training deep latent variable models on sparse discrete data: 1) tfidf weighting 2) iteratively optimizing variational parameters after initializing them with an inference network 3) a technique for improving the interpretability of the deep model the first idea is sensible but rather trivial as a contribution.	0
the second idea is also sensible, but is conceptually not novel.	0
similarly, the idea of further optimizing the elbo is interesting but not fully explored.	0
overall the ideas in this paper are good but i'd like to see them a little more fleshed out.	0
the high level idea is still using the recurrent neural network (specifically, gru in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of rnns to tackle the missing value problem.	0
cons:  the idea is somewhat incremental.	0
pros: 1. nice idea on heirarchical attention for modulating the context (document) representation by the taskspecific (query) representation.	2
in my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.	0
the idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak.	0
this is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent.	0
essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but i can't point to a technical novelty in the paper that the community can not do without.	0
it is an interesting idea to go after saddle points in the optimization with an sr1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as adam, other hessian free methods (martens 2012), pearlmutter fast exact multiplication by the hessian.	0
pros: ' the idea is easy to implement. '	2
cons: ' the idea is not very original. '	0
pros the paper presents a neat idea for changes of coordinates at the individual layers.	0
the paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a conference paper.	0
pros: ' clear description ' well built experiments ' simple yet effective idea ' no overclaiming ' detailed comparison with related work architectures cons: ' idea somewhat incremental (e.g. can be seen as derivative from bell 2016) ' results are good, but do not improve over state of the art quality: the ideas are sound, experiments well built and analysed.	2
on the positive side, the paper is clear and wellwritten (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch.	2
the idea of initializing the recurrent network with the cnn is reasonable but is at the level of improving one wrong choice in the original work of bell, rather than really proposing something novel.	0
the above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results.	0
pros:  interesting and novel idea cons:  improper experimental protocols  missing baselines  missing diagnostic experiments [r1] heess, n., williams, c. k. i., and hinton, g. e.	2
this paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.	0
in conclusion, i think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at iclr.	0
i am concerned about this specially because ccgan almost matches the sslgan baseline on stl10, and ccgan2, to me, seems like a hacky way to improve upon the core ccgan idea.	0
overall, the idea is clever and appealing, but i think the paper needs more quantitative validation and better discussion of the relationship with prior work.	0
overall, i believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified.	0
pros: the idea is interesting and interpretable models/representations is an important topic.	2
pros: ' learning over graph is an important topic cons: ' many existing approaches have already exploited the same types of ideas, resulting in very close models ' lack of comparison w.r.t existing models	2
the idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication: performance: the method should be compared with other stateoftheart kshot learning methods (e.g., matching networks by vinyals et al., 2016).	0
such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks.	0
the justifications for the idea still lacks analysis.	0
the proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas.	0
strong points of this paper: 1. the idea of using the methods from natural language processing to graph mining is quite interesting.	2
the writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas.	0
this does not take away from the main idea, but this part needs to be better researched.	0
this might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and i guess it would not work well, as otherwise why implement it differently?)	0
concerning the configuration of nce, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in sec.	0
on the positive side,  the proposed ideas are novel and seem useful.	2
the rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the ingame ai; but then oddly, it gets evaluated on how well it does against the ingame ai!	0
the authors also mention as contribution 'a new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the ingame ai', but this seems a bit exaggerated to me: the idea of pitting ais against each other has been at the core of many ai competitions for decades, so it is hardly something new.	0
i think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture.	0
the proposed idea seems reasonable, but i’m struggling to understand various aspects of the paper.	0
the combined ideas (1)  (2) do produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on.	0
the idea is nice and somewhat novel  most pruning methods concentrate on removal of individual weights, however i haven't done a through research on this topic.	0
the pruning idea has potential, however its efficiency must be more soundly demonstrated (please provide network accuracies at various pruning levels, the method removes one neuron at a time, this allows making of nice plots) rather than laconically stating that a degradation on mnist from 97% accuracy to 92% is not significant (figure 5.).	0
the idea might be valuable but it should be pushed further.	0
overall i think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of imagenet evaluation calls into question its realworld application.	0
demonstrating the stitched network idea on imagenet, comparing with the corresponding vggonly or resnetonly finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.	0
overall i think it is an interesting idea and i would love to see it better developed, thus i am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.	0
also this evaluation is identical to that in judd (2016b) weaknesses: the idea of combining bitserial arithmetic with the dadn architecture is a small one.	0
the idea of having a piecewise constant prior for latent variables is interesting, but the paper is not wellwritten (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.	0
the idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.	0
the idea proposed by this paper is reasonable, but not ground breaking.	0
for example, the sentence: 'the main idea is to gradually marginalize over an increasing range of transformations,' is suggestive but not clear.	0
one of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly.	0
i very much like the idea of the paper, but i am simply not convinced by its claims.	0
i am keeping my score the same for the following reason: in my opinion, accepted papers involving new model architectures should either consist of (1) a small adjustment that leads to a large improvement, or (2) a very innovative idea that leads to comparable performance (or better), but provides many new insights about the problem or can be transferred to many different domains.	0
this seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.	0
of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of vae to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.	0
my main concerns are the following: 1. although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so).	0
summary the core idea of the paper (thresholding gradients to induce sparsity and improve efficiency of rnn training) is interesting and practically useful, if a bit incremental.	0
the basic idea of mac to optimise the nested objective function, which is traditionally learned using methods based on the chainrule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (w step) and over the coordinates (z step).	0
from the later parts of the paper i got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)  i.3 'we use x^ell for ell in z={1, dots, s} where z is a random sample from of training samples' this formulation doesn't make sense.	0
“gradients of coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing.	0
however i think this work is still halfdone, and even though working on this project is a great idea, the authors do not yet do it properly.	0
overall, the idea is simple but feels like preliminary: while it is supposed to be a 'soft bn', bn itself gets better performance than feature penalty, and both together give even better results.	0
the good idea that i encourage the authors to pursue further is d_d, this set of rare but dangerous states, that should be kept around in some form.	0
the use of a mixture of vae is an incremental idea if novel.	0
the idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty  for example the maddison and tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.	0
originality: the suggested idea is reasonable but limited to binary data at this point in time.	0
cons:  the paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.	0
reported results are fair, but not improving over state of the art ' overall idea of limited interest when considering works like s. zhang cvpr 2016 (fast rcnn for pedestrian detection) and l. zhang eccv 2017 (faster rcnn for pedestrian detection) ' issues with the text quality ' limited takeaways quality: low clarity: fair, but poor english originality: low significance: low for acceptance at future conferences, this work would need more polish, improving over best known results on inra, eth, and caltech or kitti.	0
pros:  the paper is clear and easy to follow  the experimental results seem to show some benefit from the proposed approach cons: (1) the paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation (2) no comparison with an ensemble (3) full experiments on imagenet under the 'partial privileged information' setting would be more impactful this paper is promising and i would be willing to accept an improved version.	2
pros:  the idea of progressive attention on features is good, but has been done in [spatial transformer networks, deep networks with internal selective attention through feedback connections]  good visualisations.	2
overall, i think this is a good paper presenting a sensible idea, but i am not convinced by the experiments that the specific approach is achieving its goal.	0
this paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.	0
the spherical interpolation idea is interesting, but after a second thought this does not make much sense.	0
it may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.	0
the idea is very simple, but in practice it can be quite useful in industry settings where adding some backup workders is not a big problem in cost.	0
regularizing by enforcing the training steps to keep colabel similarities is interesting idea but not very novel and the results are not significant.	0
this is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high.	0
it's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful.	0
in summary: pros:  interesting idea  seems to improve performances cons:  paper writing  weak evaluation (only one dataset)  compare only with approaches that does not use the lasttimestep error signal	2
while the authors did not originate the general idea of persisting learning across a series of learning problems, i think it is fair to say that they have advanced the state of the art, though i cannot confidently assert its novelty as i am not deeply familiar with recent work on metalearning.	1
my problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.	0
positives: leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).	2
the novelty lies in the type of architecture which is used in the particular setup of nlp tagging tasks.	1
novelty is not really the aim of this paper since it mostly investigates existing architectures.	1
page 6, last paragraph: missing “.”: “… searching this…” summary: while the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate.	2
the main novelty of the work would be the application of such a scheme to rnns, which are typically more tricky to train than feedforward nets.	2
in terms of the paper, the main threat to novelty comes from previous publications of the same group.	1
while the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load).	2
due to the novelty of the idea, i recommend the paper.	2
also, the novelty in dropping intermediate frames for speedup is marginal.	0
however, the technical novelty and contribution of the proposed architecture and training scheme is not clear.	0
in terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes.	0
little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration.	1
based on its novelty, i suggest an acceptance.	2
that being said, the novelty of this paper is still significant.	2
the main drawback of the paper is its novelty.	0
however, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.	1
the novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model.	2
there is not very much technical novelty in this part.	0
overall, this paper has a good motivation and good novelty.	2
however, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.	0
it is hard to assess 'novelty' of this work, as the individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.	0
using a render engine to generate the initial sample appearance if of limited novelty.	1
however, then the novelty of their framework is not clear  as then the proposed model would just be a couple of neural networks to learn the motion and observation models.	0
here again, the technical novelty is not clear.	0
2) limited technical novelty.	0
it was hard for me to determine the novelty of the contribution.	0
novelty/originality: i didn't find the ideas of this paper very original.	0
the novelty in this paper seems to be the explicit modeling of information gain.	1
this is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty.	1
the authors proposed an way to measure the generation of outofdistribution novelty.	1
this paper proposed a quantitative metric for evaluating outofclass novelty of samples from generative models.	1
overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature).	0
thoughts: the main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question.	1
on a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended.	0
originality:  the novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory).	1
i find the novelty of the system somewhat limited (due to comment (2)).	0
using nade based model with convolution operations on music generation tasks and using blocked gibbs sampling contains some kind of novelty.	1
regarding novelty, the idea of combining a recurrent layer with a cnn, something practically very similar was proposed in bell et al (2016).	1
overall there is not a great deal of novelty other than being a useful study on numerical precision tradeoffs at neural network test time.	0
but since rnn/lstm can be simply interpreted as a probabilistic model, i would consider it a small novelty.	0
therefore conceptual novelty of the paper is limited.	0
hence, the novelty is limited.	0
as possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the blogcatalog dataset.	1
so the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.	0
2) technical novelty is limited.	0
however, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t.	0
to novelty and approach is limited.	0
considering the related work [3], these claims bring a marginal novelty and still 'how and why' should be central in this work.	1
therefore, the main novelty is the use of marginal fisher analysis as a new layer.	2
2. the major novelty seems to be the 2dimensional attention from the table and the pointer to the 2d table.	2
as such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on coreferenced entities, 2) applying pointer mechanism to 2d arrays (tables), and 3) training with supervised alignments.	2
to me, the biggest issue for this paper is that i'm not sure if the paper contains significant novelty.	0
however, there seems to be no novelty in the method, over jonschkowki & brock.	0
it appears that the novelty of the presented method is limited.	0
there seems to be little novelty in this work.	0
it demonstrates an interesting application of lstm sequential modeling to hids problem the overall novelty is limited considering the major technical components like lstm rnn and ensemble method are already established.	0
i’m not sure how strong novelty this paper brings in terms of technical contributions.	0
evaluation: it is hard to see a lot of novelty in this paper.	0
the implementation is not stateoftheart, and there is no novelty in this work.	0
though they claim novelty through the attention being progressive, progressive attention has been done before [spatial transformer networks, deep networks with internal selective attention through feedback connections], and the elementwise multiplicative gates are very similar to convolutional lstms and highway nets.	1
originality: the novelty of the work appears limited: the method is mostly based on a nips 2015 paper by the same authors.	0
originality: the novelty comes from applying the rfn model (including the relu nonlinearity and dropout training) to the problem of biclustering.	1
overall, i believe the proposed method is not very well justified and has limited novelty.	0
the originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.	2
novelty/originality: i didn't find the ideas of this paper very original.	0
originality: the work is a compendium of “practitioners wisdom” applied to a specific task.	1
it has thus limited originality.	0
the idea is original and important for research related to transfer learning.	1
while the authors did not originate the general idea of persisting learning across a series of learning problems, i think it is fair to say that they have advanced the state of the art, though i cannot confidently assert its novelty as i am not deeply familiar with recent work on metalearning.	1
this paper is a proof of concept for this idea.	1
the idea of the paper is quite promising and the experimental results on two datasets show that method is solid.	2
my problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.	0
the key idea is that the critic can be learned in an offpolicy manner so that it is more sample efficient.	2
a simple idea of adding perturbation error to the counts, known from differentiallyprivate literature, is nicely reused by the authors and elegantly applied in a much broader (nonconvex setting) and practical context than in a number of differentiallyprivate and other related papers.	1
the idea is novel and the author explore many methods.	2
the underlying idea is to propose a method that will be able build/modify a graphstructure as an internal representation for solving a problem, and particularly for solving questionanswering problems in this paper.	1
i like the idea of transferring attention maps instead of activations.	2
it seems like a parallel idea added to the paper that does not seem to add much value.	0
the key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant.	1
this would prove the efficiency of the idea.	1
this paper proposes a very simple idea (prune lowweight filters from convnets) in order to reduce flops and memory consumption.	1
while the main idea is almost trivial, i am not aware of any other papers that propose exactly the same idea and show a good set of experimental results.	1
the main idea being that choosing convolution filters that sum over correlated features results in lower gaussian complexity and thus the learner has higher ability to generalize.	1
there is little by way of theoretical development of the ideas using mdp theory.	1
the basic idea of repurposing locallylearned representations for largescale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.	1
although i like the idea of presenting the experiments as being directed towards answering a specific set of questions i feel like the posed questions somewhat distract from the main theme of the paper.	1
there's not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches.	1
the basic idea of steerability makes huge sense and seems like a very important idea to develop.	2
it is also a very old idea in image processing and goes back to simoncelli, freeman, adelson, as well as perona/greenspan and others in the early 1990s.	2
but at the end of the day the idea seems pretty simple  the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image.	0
quality: the basic idea of the paper is interesting and the applied deep learning methodology appears reasonable.	2
2) a similar idea is also presented in a concurrent iclr submission 'variational lossy autoencoder'.	1
i encourage the authors to address the comments below and resubmit as the general idea seems promising.	2
overall this is a wellexecuted work with an interesting though not extremely novel idea.	2
this paper provides an interesting idea to use the optimized mmd for generative model evaluation and learning.	2
overall i think this is a great idea and a very nice contribution to the fast growing metalearning literature.	2
the key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise.	1
the idea of using an inference network is much older, cf. helmholtz machine.	0
this paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative perlayer scale.	1
the proposed idea is novel and interesting.	2
my overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.	2
it is a nice idea, and simple to implement.	2
this paper poses an interesting idea: removing chaotic behavior or rnns.	2
due to the novelty of the idea, i recommend the paper.	2
overall, the paper is well communicated and a novel idea.	2
2. exploring the idea of jumpy predictions:  as stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.	1
especially here when the idea is so simple.	1
it would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.	0
originality: the suggested idea is reasonable albeit heuristics are required.	1
the paper was already wellwritten and introduced a novel idea and addressed an important problem.	2
the general idea is to allow the generator to 'peek ahead' at how the discriminator will evolve its decision boundary overtime with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution.	1
the experiments are well carried out and strongly support the presented idea.	2
using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more.	1
to me, this idea is far more appealing, and the results seem to support this, too.	2
the idea and motivation of this paper are interesting and sound.	2
nevertheless, the central idea in both cases is to solve a series of optimization problems of increasing difficulty.	1
the authors show that the idea of smoothing a highly nonconvex loss function can make deep neural networks easier to train.	1
the paper is wellwritten, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance.	2
the idea of modeling deep learning computation is not in itself particularly novel.	0
the core idea is to note that entropyregularized policy gradient leads to a boltzmann policy based on q values, thus linking pi & q together and allowing both policy gradient and qlearning updates to be applied.	1
the paper is well written and the idea well presented.	2
originality: the suggested idea to speed up search based techniques using neural nets is perfectly plausible.	2
but given that the theoretical contribution is a fairly straightforward application of wellknown ideas, i would have liked to see a stronger experimental section.	0
'claim for independent work'' the authors claimed that the manuscript presented an independent work to chalk et al. 2016 which is online since may 2016. it seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time.                 	1
the idea is using auto encoder to provide extra information for discriminator.	1
the idea is novel, and is relatively unexplored in the research community.	2
the idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic.	1
the idea of using a binary classifier for a twosample testing is not new, as made clear in the paper.	0
the idea seems natural and there is a wealth of experiment to support it.	2
the idea of this paper is reasonable  gradually go from original weights to compressed weights by compressing a part of them and finetuning the rest.	1
the idea is to add to the original loss function a new term that exploits both width and depth of the objective function.	1
i am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)?	0
cifar and svhn are fairly small test cases, though adequate for demonstration of the idea.	0
this paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them.	1
it's a very well written paper with an interesting idea.	2
the idea is using auto encoder to provide extra information for discriminator.	1
the idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations).	1
overall, the paper is very rich with ideas so i think it would be a great contribution to the conference.	2
also the novelty of the proposed idea is limited as siamese networks are used for many years and this work only shows that they can be applied to a different task.	1
the idea of leveraging predictions to train feature representations for discrimination is not new.	0
however, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.	2
overall, the idea in this paper is reasonable and the paper is well written.	2
i like the idea of the paper, however, the experimental evaluation is not convincing.	0
while the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi' with interpolation, penalize the variance, reducing the value of rare actions, etc. furthermore, there is no in depth analysis of how much performance each of these heuristics brings.	0
the idea presented in the paper is interesting and original.	2
extensions of this idea are developed for regularized losses and a weak form of policy learning.	1
specific comments  thanks for the comments on l. still it would be a good idea to clarify this point as far as possible in the main part.	2
but their general perspective is well aligned with the recently proposed idea of “dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.	2
the idea itself is very inspiring, and the experiments are very solid.	2
the idea builds on pooling structures (e.g. spatial/temporal average/max pooling) to focus on pairwise relations.	1
the idea is simple and seems quite effective in picking the right reward functions.	2
i don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since semeval 2012. i wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.	1
the idea is provocative and i think will inspire people in the iclr community.	2
the central idea of the proposed architecture (pra) revolves around the fact that the regular (parallel) macc operations waste a considerable amount of area/power to perform multiplications with zero bits.	1
significance: the paper introduces a nice idea, and present nice experimental results.	2
a more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper.	2
the multitrack idea is great.	2
pro:  incorporating general musical knowledge into the learned network is a good idea and nontrivial.	2
the multitrack idea seems useful and a clear step beyond magenta as far as i understand.	1
it seems equally applicable to the outputs of the magenta system, so it might be interesting to compare this version to magenta as well, to get an idea of how much it contributes to the improvement over the magenta system.	1
(please address this in the rebuttal)  if the stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator.	1
the idea is based on several previous works that aim to optimize vocabulary clustering to improve the speedaccuracy tradeoff often experienced in practice with hierarchical methods.	1
the key idea of the paper is approximate the training loss by its linear approximation.	1
idea with weak results.	0
the main idea of the paper is to rewrite convolutions into multiple convolutions while expanding the number of filters.	1
2. i like the idea of using improper learning to reduce the dimensionality.	2
because the idea is interesting and novel, i think it lies above the acceptance threshold.	2
the ideas behind ckms also are not inherently specific to kernel methods.	1
it combines two ideas: kernel methods and sumproduct network (spn).	1
positive: i think the idea in this paper is interesting.	2
negative: although the idea of this paper is interesting, this paper is clearly very preliminary.	0
it was a great idea to use basis functions like sine, cosine to make the approach more explicit.	2
criticisms: i liked the idea and the intuitions coming from the paper.	2
the basic idea is to repel the current parameter vector from a running average of recent parameter values.	1
the authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder).	1
while the idea of moving the processing for machine learning into silicon contained within the (ssd) data storage devices is intriguing and offers the potential for lowpower efficient computation, it is a rather specialized topic, so i don't feel it will be of especially wide interest to the iclr audience.	0
the idea here is that no one generatordiscriminator pair can be too locked together since they are all being swapped.	2
the idea is to treat the network parameter optimization problem as a bayesian inference problem (which is proposed previously by graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the fisher information.	1
i like the idea of preserving neighborhood relationships across views for retrieval tasks.	2
the idea of the paper is interesting, which mimic the human's behavior.	2
novelty/originality: i didn't find the ideas of this paper very original.	0
in the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new coauthor is submitted and the idea of 'latent attack' is proposed which works much better than the 'classificationbased adversaries'.	1
this is a really nice and natural idea.	2
the idea that equivariance means that the manifold (orbit) is linearized, is incorrect.	0
positively, the idea of combining many heterogeneous feature types into rs is ambitious and fairly novel.	2
i think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task.	1
this paper looks at the idea of fusing multiple layers (typically a convolution and a lrn or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy.	1
this idea is fine.	2
the two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an rnn, and (ii) matrix multiplications are replaced by graph convolution operations.	1
however, both approaches considered  using a pos pattern trie tree to filter out word sequences with pos tags matching those of answers in the training set, and bruteforce enumeration of all phrases up to length n  seem somewhat orthogonal to the idea of 'learning endtoend ' an answer chunk extraction model.	1
the dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [kenton et al, 16], which also targets at the same dataset.	2
the third idea is interesting, and seems to give qualitatively reasonable results.	2
the idea of using shared memory for knowledge base completion is new and interesting.	2
[major comments] i actually do like the idea and am also impressed that this model can work well.	2
i suggest to add a comparison to panyotov's results (in addition to mentioning baidu's results on librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea.	1
hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.	0
the idea of working in the space of affine transforms would be much more appealing if the model can be shown to really generated nontrivial motion patterns.	1
several recent papers also use this idea.	1
comments: 1) the main idea of the paper is build upon similar to a previous work by the same group of author (wang et.al kdd), the major difference appears to be change some of the latent factors to be rnn 2) the author describes a bptt technique to train the model 3) the author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model.	1
although i like the basic idea, the experiments are very weak.	0
the proposed idea is not very original.	0
nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2d grid worlds.	2
the idea seems to be to use a latent variable model and condition it on the image.	1
originality: the suggested idea is great.	2
the idea is great, however, the paper needs more work to be published.	0
review summary: the text is clear, the idea well describe, the experiments seem well constructed and do not overclaim.	2
clarity: easy to read, and mostly clear (but some relevant details left out, see comments below) originality: minor, this is a different combination of ideas well known.	2
i agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (worldcup2014).	2
the proposed idea is interesting.	2
the idea is to do gradient descent in the pixel space, from a given hidden unit in any layer.	1
the idea is practical and seems to work well.	2
the main idea of conditional bigan is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (    ilde{x}) and the output (y), respectively, in addition to standard bigan formulation.	1
for one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion.	2
overall, i believe that the idea of using a probabilistic model to capture polysemy is an interesting idea.	2
the paper is interesting, the idea useful with some interesting insights.	2
overall, the proposed idea seems a simple yet an effective way for regularize cnns to improve the classification performance.	2
the core idea of the paper is not very novel.	0
the key idea is to ensure that models are selfconsistent and accurately predict the future.	1
the ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and 'bulky' (very high number of parameters).	2
the connection is interesting, and there is certainly merit in the idea.	2
the idea for the algorithm is appealing, and it looks like it could find a nice place in the literature.	2
nevertheless the paper does not propose by itself any novel idea for text classification.	0
generating embeddings to interpret spns is a novel idea.	2
however, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.	0
i generally like this idea.	2
pro: i believe that the idea is natural and sound (that is, i do not share the doubts of anonreviewer5).	2
in principle one could implement the ideas in the paper with other, simpler architectures.	1
ideally, the experiments should be extended to some of these more complex tasks.	1
the main idea is straightforward: use a transition system to unroll a computation graph.	2
while having another dataset may not be a bad thing in itself, i almost felt that this dataset was created for the sole purpose of making the proposed ideas work.	1
the idea in the paper is interesting and worth reading.	2
however, the model has minor novelty given existing work that has explored similar ideas.	0
the idea of using hierarchical decoders have been explored before, e.g. [1].	1
while the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically.	0
while the manuscript is wellwritten and the idea is novel, it needs to be extended with experiments.	2
some theoretical justification is presented to describe why using multiple hash functions is a good idea.	2
this paper proposes and tests two ideas.	1
the first idea (1) is fairly straightforward, and it is not clear if it has been tried before.	0
the paper is based on a very simple and natural idea which is acutally a good point.	2
i really like the idea, and also the experiment on the maze which is very interesting.	2
given these large additional costs, the core of the idea is not sufficiently validated, to me.	0
the idea of learning a “residual” with the stitched connnections is very similar in spirit to the resnet work.	1
it’s not immediately obvious why this is a good idea at all.	1
also, there exist a large body of work in semisupervised learning with cotraining based on a similar idea.	1
i would like to see the idea in the future with some extensive experiments on large scale datasets and tasks.	1
it remains unclear why such an information bottleneck is a good idea from a theoretical standpoint.	0
the idea of using neural networks (in general) for nids is old [1].	0
the idea of using some sort of nn on top of a sequence of system calls for nids is published [2].	1
i have no idea whether what we see is impressive or not.	1
this in and of itself is not novel, nor is the idea of optimizing this by adagrad.	0
comments: 1. using additive noise in the input does not seem like a reasonable idea.	0
overall, the idea presented in the paper is interesting, and it tries to solve an important problem.	2
the authors explore the idea of deeplearning a static analyzer.	2
while the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an iclr submission.	2
i like the idea the paper is exploring.	2
the paper simply applies this idea, testing it on seven data sets.	2
and ideally, present additional new insights.	2
in summary, the general idea of having an actorcritic network as a metalearner is an interesting idea.	2
however, in sec 4.1 the idea of background feature suppression is introduced.	2
i enjoyed reading the paper because the idea is simple, smart, and seems to be effective.	2
the main idea and model are presented convincingly and seem plausible.	2
it sounds like a good idea.	2
while this paper has some interesting ideas, it also has a number of problems.	2
or is a postediting model that fixes outputs with more complex operations more ideal?)	2
relatedly, the idea of changing individual words based on local (i.e. wordlevel) scores seems counterintuitive.	2
the idea of the paper is novel and wellpresented, and the memory structure seems reasonable to have advantages in practice.	2
improving on these would greatly strengthen the paper, as the core idea is interesting.	2
this paper was easy to read, the main idea was presented very clearly.	2
it sounds like a reasonable and straightforward idea.	2
the paper is well written and the main idea is clearly presented.	2
the idea considered here is cute.	2
the authors failed to bring the novel idea.	2
the general idea of suprisingdriven feedback is interesting for online prediction task.	2
it is a simple enough idea that seems to bring some significant improvements.	2
i think it is necessary to try the idea on different datasets to see if feedback lstm sees some consistent improvements.	2
major weaknesses: i have two main concerns for this work:  one is related to the novelty as the existing work of xie et al. eccv'16 also proposed similar technique with very similar aim.	0
the novelty of the proposed method is introduction of a teacher that learns to generate a curriculum for the agent.the formulation is simple and elegant as the teacher is incentivised to widen the gap between bob but pays a price for the time it takes which balances the adversarial behavior.	0
i don't really feel qualified to comment on the novelty, since this paper is somewhat out of my area of expertise, but i did notice that the authors' own description of baranes and oudeyer (2013) seems very close to the proposal in this paper.	0
on the negative side, there seems to be relatively limited novelty: we can think of msa as one particular communication (i.e, star) configuration one could use is a multiagent system.	0
while the paper is very well written, i have major concerns regarding the experimental evaluation and the amount of novelty.	0
figure 1 shows a good overview of the proposed approach, but also reveils that there is not much technical novelty beyond combining two already existing approaches into one (unet  spatial transformer module).	0
cons there is little novelty in the proposed method/models  the paper is primarily focused on comparing existing models on a new task.	2
there is however a lack of technical novelty or insight in the models themselves.	0
quality: no obvious mistakes in the proposed method, but has very low novelty (as most methods follows existing studies in especially for online kernel learning).	0
from a methodology pointofview, the paper has limited novelty (transport operators, and learning thereof has been studied elsewhere), but there are some technical insights (likelihood model, use in data augmentation).	0
the experiments are weak 1) in the sense that they are not compared against simple baselines like p(x) (from, say, just thresholding a vae, or using a more powerful p(x) model  there are lots out there), 2) other than knns, only compared with classprediction based novelty detection (entropy, thresholds), and 3) in my view perform consistently, but not significantly better, than simply using the entropy of the class predictions.	0
the 3rd issue is that the novelty for the novelty detection part in the proposed gan seems quite incremental.	0
(cons) 1. the method lacks algorithmic novelty and the exposition of the method severely inhibits the reader from understand the proposed idea.	0
the method is poorly written, severely lacks algorithmic novelty, and the proposed approach shows no empirical gains over random mini batch sampling.	0
my primary concern is with novelty.	0
the technical novelty seems incremental (but interesting) with respect to existing methods.	0
because of this i find the paper to be a bit lacking on the novelty quotient.	0
cons: the paper provides little novelty in terms of model or algorithmic design, as using a cnn to parametrize the latent variables is the only model detail unique to this paper.	0
interesting problem, but limited novelty and flawed evaluation the paper considers the problem of following natural language instructions given an firstperson view of an a priori unknown environment.	0
if so, then to me this is a rather minor novelty and to justify it's importance authors should run a control experiment with the exact same architecture as in dnc, but with a masked similarity kernel.	0
the primary weakness of this work its lack of novelty and lack of evidence of generalization of the approach, which limits its significance.	0
concerning the contribution of the model, one novelty is the conditional formulation of the discriminator.	0
the idea of the paper is good but the novelty is limited.	0
1. this is a good application paper, can be quite interesting in a workshop related to deep learning applications to physical sciences and engineering 2. lacks in sufficient machine learning related novelty required to be relevant in the main conference 3. design, solving inverse problem using deep learning are not quite novel, see stoecklein et al. deep learning for flow sculpting: insights into efficient learning using scientific simulation data.	0
weaknesses: 1. while this work is conceptually interesting, the technical novelty and contributions seem fairly minimal.	0
overall, the relative lack of novelty and comparison with previous work make me hesitant to recommend the acceptance of this paper.	0
in general, lack of substantial experiments makes it difficult to appreciate the novelty of the work.	0
overall, i have a couple of concerns about novelty as well as the experimental evaluation for the authors to address.	0
second concern is limited novelty (from what i understood).	0
on the other hand:  the novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a gamechanger.	0
the paper was improved significantly but still lacks novelty.	0
overall, i don’t think this paper meet iclr’s novelty standard, although the authors present some good numbers, but they are not convincing.	0
this technique reminds me of some common methods in learning from demonstrations, such as those using gps or gmms, but the novelty of this technique is the fact that the subgoal mapping function is learned in an irl fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration.	0
my only major concern is the degree of technical novelty with respect to the original densenet paper of huang et al. (2017).	0
another concern is the novelty in relation to related work.	0
my main concern is about the novelty of the work.	0
instead, i would recommend that in future versions of this document a single network, with a specific router and set of decisions, and with a single algorithm, will be explained with clear notation endtoend beyond the clarity issues, i suspect also that the novelty is minor (if the state does not include any information about the current output) and that the empirical baseline is lacking.	0
the main concern i have is about the novelty of the coreset idea applied to svm.	0
my main concern about the paper is novelty/importance and lack of experimental comparison to other techniques attacking similar complexity problems.	0
the method also lacks novelty and looks like a generalized version of fitnets.	0
however, i find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality.	0
in short: this is a neat paper, but it’s novelty is low.	0
however the novelty factor of this paper is fairly low — recursive neural nets have been applied to code/equations before in similar models.	0
cons: (1) the novelty of the paper is limited as most of the proposed regularizers are more or less straightforward modifications over decov.	0
while the following are the cons:  the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;  lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;  lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);  the novelty of the approach is somewhat limited, although the idea of the extensive experimental comparison is good.	0
the proposed approach is simple and has an appealing compositional feature, but the work is not adequately validated and the novelty is somewhat limited.	0
i am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3way outlier detection).	0
i still think the novelty, significance of the claims and protocol are still perhaps borderline for publication (though i'm leaning towards acceptance), but i don't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence.	0
concerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion.	0
weak points: 1. the novelty of this paper is limited.	0
weaknesses: 1. while this work is conceptually quite novel and interesting, the technical novelty and contributions seem fairly minimal.	0
cons: 1. the novelty is limited.	0
it's simple with no technical novelty but shows consistent improvement and has wide relevance.	0
strengths: simple connections between pacbayes bound and entropy sgd objective is the first novelty.	2
unfortunately, this paper is lack of novelty.	0
this model is a relatively simple extension of the neural statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation.	0
however, while the proposed ideas are interesting, there are concerns about the novelty in relation to a long line of previous work.	0
the lack of practical relevance combined with the notgroundbreaking novelty of the result makes this paper less appealing.	0
overall i think the paper is borderline; the lack of real novelty makes it marginally below threshold in my view.	0
my main concern is with the novelty.	0
therefore, also the paper is wellwritten,  it lacks novelty,  its topic does not perfectly fit topics of interest for iclr, so, i do not recommend this paper to be published.	0
cons: ' the novelty of this paper is marginal.	0
pros:  the proposed model (layer architecture) is simple and easy to implement cons:  the novelty is low  no competitive baseline in experiments	2
overall, this is a nice report, but there is not enough novelty.	0
"main concern: this work heavily relies on the multiagent aspect for novelty : ""houthooft et al. (2016) learned exploration policies via information gain using variational methods."	0
pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!)	2
cons:  the proposed approach follows largely the existing work and thus its technical novelty is weak.	0
pros: ' active learning may be used for improving the performance of deep models for ner in practice ' all the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for ner cons: ' the novelty of this paper is marginal.	2
cons: the is not much technical novelty.	0
the motivation and attempt is quite good, but the paper is written quite poorly without any flow, it is very difficult for the reader to understand the novelty or significance of the work, no intuition is given and descriptions and so short and cryptic.	0
quality: low, minor/incremental contribution due to lack of novelty and lack of significantly useful findings.	0
one of the only novel components is the wordwise dropout (table 5), but the improvement is minor and the technique is too thin to be considered substantial novelty.	0
however, my primary concern is that the model seems somewhat lacking in novelty.	0
"this paper proposes a ""learning to select"" problem which essentially is to select the best among a flexible size of candidates, which is in fact learning to rank with number of items to select as 1. to be able to efficiently train the model without wasting time on the items that are not candidates, the authors applied an existing work in literature named dynamic computation graph and added convolutional layer, and showed that this model outperforms baseline methods such as cnn, fullyconnected, ranksvm etc. as this paper looks to me as an simple application of an existing approach in literature to a realworld problem, novelty is the main concern here."	0
in the current form, this work is somewhere between a theoretical paper and an empirical one, however for a theoretical one it lacks strictness, while for empirical one  novelty.	0
my biggest concern is novelty, as the modifications are minor.	0
the combination of the two parts seems a bit incremental and does not bring much novelty.	0
3) in contrast to the thin experiments and (lack of) technical novelty, the introduction & related work writeups are overdrawn and uninteresting.	0
cons  the novelty is limited.	2
however, in its current form, i am concerned about both the novelty and the clarity of the paper.	0
all in all, the methodological novelty of the paper is small but it has high practical relevance in terms of giving improved accuracy on an important task of dialogue conversation.	0
proposed feature embedding is incremental (lack of novelty and technical contribution) obtained results are encouraging but not good enough.	0
the reviewer does not find the paper suitable for publication at iclr due to the following reasons:  the paper is incremental with limited novelty.	0
i think in general this paper lacks novelty and it shouldn't be surprising that activations from all layers should be more representative than one single layer representation.	0
review: the paper reads well, but as a standard application of attention lacks novelty.	0
their results wrt channel corruption on chime3, on the other hand, are reasonable, because the model matches the problem being addressed… overall assessment: in summary, the paper lacks novelty wrt technique, and as an “applicationofattention” paper fails to be even close to competitive with the stateoftheart approaches on the problems being addressed.	0
strengths: simple idea that can work well in the context of sampling examples for sgd weaknesses: the novelty in the paper is limited.	2
cons: 1 the novelty of this paper is limited.	0
what is the advantage over other competitive schemes, etc. in summary, while there is a minor novelty in connecting two separate ideas (cpd and uml) into a joint uml setup, the paper lacks sufficient motivations for proposing this setup (in contrast to say kernelized kmeans), the technical details are unconvincing, and the experiments lack sufficient details or analysis.	0
cons: i find the novelty of the paper limited: the authors extend the work by (onken et al. 2016) to subtract baseline (a rather marginal innovation) of this approach.	0
this paper is an application paper on detecting when a face is disguised, however it is poorly written and do not contribute much in terms of novelty of the approach.	0
the application domain is interesting, however it is simply a classification problem the paper is written clearly (with mistakes in an equation), however, it does not contribute much in terms of novelty or new ideas.	0
overall the paper is a descent one, but with limited novelty.	0
'weakness'' in general, the paper has some major weaknesses in how the dataset has been constructed, details of the models provided and generally the novelty of the proposed model.                  	0
however i still feel like the novelty is somewhat limited, hence the recommendation.	0
efficient embedding of scalefree graphs in the hyperbolic plane i think this paper does have some novelty in applying it to the skipgram model and using deep walk, but it should make more clear that using hyperbolic space embeddings for graphs is a popular and by now, intuitive construct.	0
i have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty: the paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space.	0
cons:  it appears that novelty is very limited as highly similar work (see above) has been out for a while.	0
cons: ' the novelty and technical contribution is low.	0
ves: 1. my main concern with the paper is the novelty of the contribution to the techniques.	0
the analysis of manifold structure of dnn is important direction, but i am afraid novelty and insight of this work is not enough for acceptance.	0
pros:  generating programs with neural networks is an exciting direction  novel task of generating ui code from ui screenshots  three new datasets of ui images and corresponding code  paper is clearly written cons:  limited technical novelty  limited experiments i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.	2
my main concern with this paper is a lack of technical novelty.	0
on the whole i appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.	0
overall, the paper is wellwritten but the novelty and applicability seems a bit limited.	0
another concern is that the novelty.	0
the idea is quite simple and the novelty is incremental by considering the difference from skiprnn.	0
i have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and i would like to know the author's response.	0
while the strength of this paper is clearly the good writing as well as rigorous experimentation, the main concern i have with this paper is novelty.	0
however, the novelty is hurt by the lack of clarity with respect to the model design.	0
the paper lacks a little bit in novelty since it is basically a variante of gangcl, but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments.	0
the novelty is limited but it is a good idea to speed up the rc models.	0
overall, i see this as an interesting piece of work that may be of interest to researchers exploring questions around language creation and language evolution, but i think the results require more careful analysis and the novelty is relatively limited, at least in the way that the results are presented here.	0
points in favor of the paper: ' addresses an important problem points against: ' only 5fold speedup by parallelization with 5 x 25 workers, and worse performance in the same budget than google vizier (even though that treats the problem as a black box) ' limited methodological contribution/novelty the paper's methodological contribution is quite limited: it amounts to a straightforward parallelization of successive halving (sha).	0
lack of novelty the paper has very limited novelty since the proposed method is a straightforward combination of two prior works on the same topic (unpair/unsupervised image translation or crossdomain image generation) where the two prior works are the dtn work [a] and the unit [b] work.	0
not much technical novelty to be found, but the original contributions are adequately identified and they are interesting on their own.	0
in summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.	0
overall: pros:  a nice idea with some novelty, based on a nontrivial observation  the experimental results how the idea holds some promise cons  the method is not presented clearly enough: the main component modeling the network activity is not explained (the hdda module used)  the results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)  experimental results are partial: results are not presented for multiple defenders, no ablation experiments after revision: some of my comments were addressed, and some were not.	2
clarity: the paper is very clear originality: the theoretical contribution extends the seminal work of bendavid et al., the idea of using adversarial learning is not new, the novelty is mediaum significance: the theoretical analysis is interested but for me limited, the idea of the algorithm is not new but as far as i know the first explicitly presented for multisource.	0
my main problem was about the lack of novelty.	0
so my main problem with this paper, lack of novelty, is addressed and my score has changed.	0
our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims.	0
cons: 1. technical novelty is somewhat limited 2. experiments do not evaluate run time, memory use, computational complexity, or stability.	0
cons: 1. the novelty of this submission seems a little limited.	0
the authors try to claim some novelty in that they apply this wellknown observation to pixellevel prediction networks, but the reviewer fails to see what in this application was nontrivial or surprising.	0
my main concerns with this paper are novelty, reproducibility and evaluation. '	0
the authors do a good job of positioning their study with respect to related work on blackbox adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty.	0
lacking technical novelty.	0
the novelty is incremental in most parts, but the overall system can be seen as novel.	0
so overall given limited novelty but the paper presents useful results, i would recommend borderline leaning towards reject.	0
my biggest concern with the paper is novelty.	0
the most severe issue is lacking novelty.	0
for me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task.	0
pros:  results  novelty of idea  crossover visualization, analysis  scalability cons:  missing background  missing ablations  missing details [after rebuttal: revised the score from 7 to 8]	2
my only concern about the novelty of the paper is that the idea of using cyk chartbased mechanism is already explored in le and zuidema (2015).	0
summary: —— in summary, it is an interesting topic, but i think that the paper does not have sufficient novelty.	0
1. lack of novelty.	0
the main concern of this submission is the novelty.	0
in that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results.	0
however, i have the following concerns on novelty.	0
however i was unable to identify any real novelty in the theory: the fokkerplanck equation has been widely used in analysis of stochastic noise in mcmc samplers in recent years, and this paper mostly rephrases those results.	0
thus i think that although this paper is written well, the theory is mostly recycled and the empirical results in section 4 are known; thus it is below acceptance threshold due to lack of novelty.	0
experiments are only presented with synthetic data but given the potential for the method and its novelty, i believe this can be accepted.	0
my strongest criticism for this paper is against the claim that tumblr post represent selfreported emotions and that this method sheds new insight on emotion representation and my secondary criticism is a lack of novelty in the method, which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module, fused in an output layer.	0
"with respect to the rest of the paper, the level of novelty and impact is ""ok, but not good enough."""	0
for me the novelty of this paper was in its application to the realm of emotion theory, but i do not feel there is a contribution here.	0
my primary concern with the paper is the lack of novelty and relatively little in the way of contributions to the iclr community.	0
furthermore, the paper lacks in novelty aspect, as it is uses mostly wellknown techniques.	0
one that could be improved is p(p_x, p_g) where one loses the fact that the second random variable is y. this work contains plenty of novel material, which is clearly compared to previous work:  the main consequence of the use of wasserstein distance is the surprisingly simple and useful theorem 1. i could not verify its novelty, but this seems to be a great contribution.	0
however, i have problems to find other pros in this submission because the clarity is quite low and in the present form there is no novelty in the proposed procedure.	0
the main issue is the lack of novelty.	0
in summary, the idea of the paper is very interesting to learn dataaugmentation but yet i am not convinced the current paper has enough novelty and contribution and see the contribution of paper as on more the application side rather than on model and problem side.	0
the novelty lies not in the methods used (existing methods are used), but in the way these methods are combined to solve two problems (that so far have been treated separately in ir) simultaneously.	0
with sufficient knowledge of related works from these areas, i find that the authors' proposed method lacks proper evaluation and sufficient novelty.	0
my major concern is the significance and originality of the proposed method.	0
papers 's pros :  clarity  technical results cons:  doubts about the interest and originality the authors provided detailed and convincing answers to my questions.	0
the numbers in the tables are good but i have several comments on the motivation, originality and experiments.	0
quality: this paper is of good quality clarity: this paper is very clear but contains a few minor typos/grammatical mistakes (missing s for plurals, etc.) originality: this paper is original significance: this paper is significant pros  using ecc theory for reducing the memory footprint of a neural network seems both intuitive and innovative, while being grounded in wellunderstood theory.	0
importance: somewhat lack of originality and poor experiments lead to low importance.	0
i believe the significance and originality of this work to be lacking.	0
originality  i find the paper to be very incremental in terms of originality of the method.	0
however, i find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality.	0
"pros:  a clear, wellwritten abstract and introduction  while i am not experienced enough in the field to really comment on the originality, it does seem that the approach the authors have taken is original, and applies deep learning techniques to avoid having to customdesign a ""feature space"" for their particular family of problems."	2
in terms of the originality, the methodology of this method is rather incremental from the prior study (raffel et al), but it shows significant gains from it.	0
on the positive side, the paper is very clear and wellwritten, the sru is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace lstms in most cases but yield fast training.	2
quality: globally correct clarity: paper clear originality: limited with respect to the original definition of algorithmic robustness significance: the paper provides a new theoretical analysis for stochastic learning of deep networks but the contribution is limited in its present form.	0
pros: the paper is a nice read, clearly written, and its originality is well stated by the authors, “addressing the lifetime clustering problem without endoflife signals for the first time”.	2
[originality] i mainly concern about the originality.	0
as there are not enough experimental details to judge, it's hard to figure out the problem, but this ppaper is clearly not publishable at any of the quality machine learning venues, for weakness in originality, quality of the writing, and poor experiments.	0
the embeddingenhancing method has low originality but is effective on this particular combination of model architecture, task and datasets.	0
cons: the idea seems to be simple and does not have significant originality.	0
the originality of applying graph neural networks to the problem of fewshot learning and proposing semisupervised and active learning variants of the task are the primary strengths of this paper.	2
the originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.	0
originality  4 contributions significance  important problem, very active area of research  comparison to very recent algorithms  but no ppo in the evaluation	0
thus, despite the strong results, i would not like to see this work in the proceedings, due to the lack of originality and poor technical discussion.	0
in that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results.	0
since this is not my main area of research i cannot judge its originality in a completely fair way, but it is original afaik.	0
in this regard it lacks a bit of originality.	0
pros the novelty of this paper lies in using a neural network structure to solve a traditional statistical problem which was usually done by a bayesian approach or using the idea of the stochastic process.	0
strengths:  the derivation of the dual formulation is novel  the dual formulation simplifies adversarial training  the experiments show the better behavior of the method compared to adversarial training for domain adaptation weaknesses:  it is unclear that this idea would generalize beyond a logistic regression classifier, which might limit its applicability in practice  it would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which gans are often used  it would be interesting to see if the da results with a kernel classifier are better (comparable to the state of the art)  the mathematical derivations have some errors detailed comments:  the upper bound used to derive the formulation applies to a logistic regression classifier.	2
this would have an impact on the relationship to the mmd  the idea of sample reweighting within the mmd was in fact already used for da, e.g., huang et al., nips 2007, gong et al., icml 2013. what is done here is quite different, but i think it would be worth discussing these relationships in the paper.	0
i gave a 7 because i like the idea, but i wouldn't be upset if the ac recommends submitting to an nlp conference instead.	0
summary an interesting paper with novel theoretical ideas, but insufficient experimental validation.	0
on the overall, while the idea may be of interested, the paper lacks in motivations in connecting to relevant previous works and in providing insights on why it works.	0
the oov handler is also related to positional encoding ideas that have been used in nmt but aren't reference.	0
the idea is interesting to me, but i think this paper still needs some improvements.	0
pros:  good new implementation of an existing idea  significant perplexity gains on character level language modeling  good at domain adaptation cons:  memory requirements of the method  wordlevel language modeling experiments need to be run on larger data sets (edit: the authors did respond satisfactorily to the original concern about the size of the wordlevel data set)	2
while the idea is interesting, the paper felt quite verbose on introducing notations and related work, and a bit lacking on actual change that is being proposed and the experiment to back it up.	0
this paper is outside of my area of expertise, so i'll just provide a light review:  the idea of assuming that the opponent will take the worst possible action is reasonable in widely used in classic search, so making value functions follow this intuition seems sensible,  but somehow i wonder if this is really novel?	0
i like this idea, however, the results are mixed, and the explanation given is plausible, but far from a clearly demonstrated answer.	0
in summary, i like the authors idea to explore the restriction of the function class of dual representations to produce usefulinpractice divergences, but the paper feels a bit middle of the road.	0
while the authors put forward several interesting ideas, there are some shortcomings to the present version of the paper, including:  the design objective seems flawed from the networking point of view: while minimizing the maximal load of a link is certainly a good starting point (to avoid instable queues) one typically wants to minimize delay (or maximize flow throughput).	0
note that the proposed method need not necessarily outperform supervised approaches, but the reader should be provided with some idea of the size of the gap between this unsupervised method and the stateoftheart supervised approach.	0
paper strengths:  the idea of the paper is interesting.	2
cons:  the idea of parameterizing an rl algorithm by goals is not particularly novel.	0
comment:  the idea of tree2tree has been around recently but it is difficult to make it work.	0
(cons) 1. the method lacks algorithmic novelty and the exposition of the method severely inhibits the reader from understand the proposed idea.	0
"c an the authors give experimental evidences for their claim: ""as such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a nongreedy...""  pros  the idea of the paper is clearly presented, the algorithm is easy to follow."	0
the dou and knight papers are interesting, but they’re an adaptation of ideas rather than decipherment per se.	0
review: while the idea proposed in the paper is somewhat novel and there is nothing obviously wrong about the proposed approach, i thought the paper is somewhat incremental.	0
the paper is very well written, and based on a simple but interesting idea.	0
the two mdps are mathematically equivalent, but their qvalues are obtained differently (td0 for the upper mdp and qlearning for the lower mdp) and yet the paper tries to minimize the euclidean distance between them.	0
overall, this is an interesting paper with a good idea, but the training technique is not mature enough for publication.	0
the proposed method is similar to memory qnetwork [oh et al.], but the paper proposes a masked euclidean distance as a similarity measure for contentbased memory retrieval.	0
pros:  the idea is interesting, it well explain the success of gated rnns.	2
while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.	0
pros:  the proposed idea is simple and easy to implement  the results show improvement in terms of visual quality cons:  i agree that the proposed prior should better capture the data distribution.	2
pros:  the idea of forcing different parts of the latent representation to be responsible for different attributes appears novel.	2
overall, i found the main idea of the paper relatively straightforward, but the presentation is a bit awkward in places.	0
the proposed idea is not exceptional original, but the paper has several strong points: ' the relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one. '	2
the underlying idea is interesting, as such, each and every degree of freedom do not synthesize itself similar to the autoencoder setting, but rather synthesize a neighbor, or kneighbors.	0
all in all, i do like the idea as a concept but i am wary about its applicability to real data where defining a good neighborhood metric might be a major challenge of its own.	0
the idea is to use policy gradient style algorithms to update a suite of options using relatively on the positive side, i like the core idea of this paper.	2
the idea of the paper is good but the novelty is limited.	0
cons: while the reviewer is essentially fine with the idea of the method, the reviewer is much less convinced of the empirical study.	0
i think the broad idea of using ea as a substep within a monotonically improving free energy algorithm could be interesting, but needs far more experimental justification.	0
this is a great idea and could be a strong paper, but it's really hard to glean useful recommendations from this for several reasons:  the writing of the paper makes it hard to understand exactly what's being compared and evaluated.	0
pros:  the idea of reducing the computational cost of specialized models makes sense.	2
one idea for evaluation: comparison with ground truth makes sense for pnl, but not so much for general nonlinear because of unidentifiability.	0
strong points: ' to my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). '	2
overall, the works investigates an interesting idea, but lacks maturity to be accepted.	0
the application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly offpolicy samples.	0
pros and cons ============  good results  interesting idea of using the algorithm for rlfd  weak experiments for an application paper  not clear what's new	2
there may be some interesting ideas here, but i think in many places the mathematical description is very confusing and/or flawed.	0
as a summary, the paper contains nice ideas and experimental results are promising, but has nonnegligible mistakes in theoretical parts which degrade the contribution of the paper.	0
clarity, significance and correctness  clarity: the main idea is clearly motivated and presented, but the experiment section failed to convince me (see details below).	0
pros  1. nice idea that allows to decouple the hidden size with the number of hiddentohidden parameters.	0
pros 1. the results on bandit structured prediction problems are pretty good 2. the idea of a learnt credit assignment function, and using that to separate credit assignment from the exploration/exploitation tradeoff is good.	0
the idea of using vaes is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples.	0
the idea is clearly stated (but lacks some details) and i enjoyed reading the paper.	0
the paper reports examples of the relative importance of certain concepts with respect to others in figure 5. pros  the paper proposes a simple and novel idea which could have a major impact on how deep networks are explained.	0
final evaluation  i would like to see this idea published, but not in its current form.	0
the highlevel idea is similar to the evolution method of [real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch.	0
almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea.	0
however i have a few concerns: 1. most of the ideas presented in the paper rely on works by arjovsky et al. (2017), gulrajani et al. (2017), and gulrajani et al. (2017).	0
this is an interesting idea but the results presented in the paper are very preliminary.	0
i liked the overall ideas which is clean and simple, but also found several points still confusing and unclear.	0
cons:  some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect.	0
the idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on mnist.	0
strengths: — the idea of building a compositional model for visual reasoning and visual question answering makes a lot of sense, and, i think, is the correct direction to go forward in these fields.	2
this cannot be called concurrent work as it has been around for a year and has been seen and discussed at length in the community, but the authors fail to acknowledge that their basic idea of a joint generative model and inference procedure is subsumed there.	0
in addition to giving the full paper a purge of unnecessary sentences, my recommendation would be to shorten section 1.1 to a couple of paragraphs (moving some material to the results section if necessary), reduce the bayesopt related work paragraph to a sentence or a most two, cut the lstm related work paragraph (i find the link strenuous at best but that might be my misunderstanding), and cut the intro the bayesopt and gps down to a short paragraph (just give the key highlevel idea then reference to existing work).	0
the paper presents an interesting idea that potentially has useful applications, however the experiments are not convincing enough.	0
e.g., one needs to get to page 5 to understand that the paper is just based on the wgan ideas in arjovsky et al., but with a different application (not gans).	0
revision: the authors have adequately addressed quite a few instances where i feel motivations / explanations were lacking, so i'm happy to increase my rating from 6 to 7. i think the proposed title change would also be a good idea.	0
overall, tr seems like an interesting idea, but it has neither been carefully expanded or investigated.	0
applying the same idea, but only for forget gate/ input gate.	0
the reason i am giving a 2 instead of a 1 is because the core idea behind the algorithm given seems to me to have potential, but the execution is sorely lacking.	0
i do like the idea of the paper, but at the current state, it is hard to evaluate the effective of this paper.	0
another idea would be to use the one/fewshot learning to learn embeddings and evaluate their quality on a semantic task (as suggested in section 3.3), but on a larger scale.	0
general assessment: pros: the central idea seems relevant and interesting, although not all novel.	2
the idea of postprocessing word embeddings to improve their performance is not new, but i believe the specific procedure and its connection to the concept of isotropy has not been investigated previously.	0
strengths: the idea of using latent syntactic structure, and computing crosssentence alignment over spans is very interesting.	2
the idea is to project the data onto a hypersphere not only in the input layer, but also other layers, which leads to better resistance against those attacks.	0
there is nothing wrong with the fundamental idea itself, but given the experimental results it just is not clear that it is working.	0
overall i think your intuitions and ideas are good, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.	0
the coreset idea has been applied to svm in existing work, but this paper uses a new theoretical framework.	0
the main concern i have is about the novelty of the coreset idea applied to svm.	0
my main concerns about this work are have two aspects: (a) novelty 1. the idea is a good one and is great incremental research building on the top of previous ideas.	0
ideally we expect a reader designing a new architecture to have an idea for which way to go for model compression after reading this paper—results are good to decide between clrw –stn, but not informative on how they compare to literature.	0
"i also liked the idea of having the section ""a geometric view of embeddings and tensor decomposition"", but that section needs to be improved."	0
overall, i believe the idea is nice, and the initial analysis is good, but i think the evaluation, especially against other methods, needs to be stronger.	0
(side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act.	0
the main problem is that the work is poorly explained: starting from the task at hand, through the intuition behind the idea how to solve it.	0
o snli data should be described: content, size, the task it is used for pro:  a novel idea of producing natural adversary examples with a gan  the generated examples are in some cases useful for interpretation and network understanding  the method enables creation of adversarial examples for block box classifiers cons  the idea implementation is basic.	2
i can appreciate the very intuitive and colloquial style of the paper, however the discussion of the core idea would benefit from some rigor and formal definitions.	0
i think estimating the operator from the input to the output is interesting, instead of constructing (a, b, c, d) matrices, but this idea and all the techniques are from hazan et.	0
so i think the idea is ok, but not a breakthrough.	0
despite of this limitation i think the paper's idea is ok and the result is worth to be published but not in the current form.	0
pros: the idea is interesting.	2
originality: the idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature.	0
while the following are the cons:  the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;  lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;  lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);  the novelty of the approach is somewhat limited, although the idea of the extensive experimental comparison is good.	0
studying aliasing effects in raw audio neural nets is a great idea, but i feel that this work takes some shortcuts that make the analysis less meaningful.	0
there could be an interesting idea here, but the limitations and applicability of the proposed approach are not clear yet.	0
so, in theory this might be a good idea, but i think this idea is not outofthebox method for implementation.	0
to summarize, i like this idea, but more experiments are needed in order to understand this method merits.	0
if the goal is to use summarization as an extrinsic evaluation of sentence embedding models, there needs to be better justification of this is a good idea when there are so many other issues in content selection that are not due to sentence embedding quality, but which affect summarization results.	0
even though the idea presented is a novel contribution and has potential the paper itself is highly unstructured and confusing and lacks a proper grammar check.	0
however, there are several issues both in the approach and the current preliminary evaluation, which unfortunately leads me to a reject score, but the general idea of combining different specifications is quite promising.	0
"please see my detailed comments in the ""official comment"" the extensive revisions addressed most of my concerns quality ====== the idea is interesting, the theory is handwavy at best (addressed but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (addressed)."	0
the general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (addressed) leaving many details up the the reader's best guess (addressed).	0
pros and cons ============  interesting idea  we do everything asynchronously and in parallel and it magically works (addressed)  many open questions / missing details (addressed)	2
the idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels.	0
the idea of learning to defer (as proposed in the paper) as a means to fairness is not only novel but also quite apt.	0
the idea of weaklysupervised disentangling has also been explored in many other papers, e.g. “weaklysupervised disentangling with recurrent transformations for 3d view synthesis”, yang et al. the description of reference ambiguity seems new and potentially valuable, but i did not find it easy to follow.	0
pros: 1. the authors utilize the idea of prototypes and few shot learning to the task of tfbinding and cooperation.	2
2) pros:  introduced, studied, and supported the novel idea that bnns are robust to adversarial attacks.	2
it could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with bnns (or why that's not a good idea).	0
while the idea of mixing not only features but also labels is new and interesting, its advantage over the existing approach of mixing only features is not shown.	0
i understand and appreciate the authors' argument as to why mixup should work, but it is not sufficiently convincing to me why a convex combination in euclidean space should produce good data distribution.	0
the idea is a simple but useful extension of these previous works.	0
strengths: the proposed idea of generating text using summary sentences is new.	2
the paper is easy to read and well organized  the advantage of the proposed regularization against the more standard l2 regularization is clearly visible from the experiments  the idea per se is not new: there is a list of shallow learning methods for transfer learning based on the same l2 regularization choice [crossdomain video concept detection using adaptive svms, acm multimedia 2007] [learning categories from few examples with multi model knowledge transfer, pami 2014] [from n to n 1: multiclass transfer incremental learning, cvpr 2013] i believe this literature should be discussed in the related work section  it is true that the l2spfisher regularization was designed for lifelong learning cases with a fixed task, however, this solution seems to work quite well in the proposed experimental settings.	0
this idea however is difficult to be applied to deep learning with a large amount of data.	0
see e.g. yang et al. “joint unsupervised learning of deep representations and image clusters”, cvpr 2016. to conclude, the paper has some interesting ideas, but the presentation is not convincing, and the experiments are substandard.	0
i find the general idea of the work compelling, but the particular approach is rather poor.	0
the overall idea and approach being pursued here is a good one, but the model needs further development.	0
pros: simple, interesting idea works well on toy problems, and able to prevent divergence in baird's counterexample cons: lacking in theoretical analysis or significant experimental results	2
the idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.	0
cons: while we liked both the challenge posed and the idea to solve it we found several major issues with the work.	0
i liked the idea of the rhythm evaluation, but again, i have some questions about the specific implementation.	0
section 4.1: the idea to measure ioi distributions is clever, but how exactly is it implemented?	0
pros: 1. the main idea is clearly presented.	2
the idea is nice and simple, however the current framework has several weaknesses: 1. the whole pipeline has three (neural network) components: a) input image features are extracted from vgg net pretrained on auxiliary data; 2) autoencoder that is trained on data for oneshot learning; 3) final classifier for oneshot learning is learned on augmented image space with two (if i am not mistaken) fully connected layers.	0
overall, i like the idea, so i am leaning towards accepting the paper, but the empirical evaluations are not convincing.	0
this model is a relatively simple extension of the neural statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation.	0
the idea of structured matrices in this context is not new, but the diagonal block structure appears to be.	0
for example, in linear networks this is the same as rlsvi, bootstrapped dqn is one way to extend this idea to deep nets, but this is another one and it is much better because xyz.	0
i’m not an expert, but i assume there must be some similar idea in cnns.	0
the idea of this combination is not surprising but the attendee of iclr might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.	0
i think this paper contains an interesting idea, but suffers from poor writing and unprincipled experimentation.	0
i find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below.	0
"however i don't see the link with the ""recursive kernel"" idea (maybe it's just the way to do the computation described in cho&saul(2009) ?)"	0
the idea of extracting policies corresponding to individual automaton states and making them into options seems novel, but it would be important to argue that those options are likely to be useful again under some task distribution.	0
that's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged.	0
the idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth.	0
though the effect is most obvious from the speech recognition experiments in section 4.3, mp also achieves slightly higher performance than baseline for all imagenet models but inceptionv1 and for both object detection models; these results add support to the idea of fp16 as a regularizer.	0
the idea of extracting policies corresponding to individual automaton states and making them into options seems novel, but it would be important to argue that those options are likely to be useful again under some task distribution.	0
that's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged.	0
the idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth.	0
though the effect is most obvious from the speech recognition experiments in section 4.3, mp also achieves slightly higher performance than baseline for all imagenet models but inceptionv1 and for both object detection models; these results add support to the idea of fp16 as a regularizer.	0
the ideas are interesting, but i have some concerns regarding this work.	0
overall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper.	0
"on the one hand, i somewhat agree with the authors that ""while the running time is higher... we expect that it can be improved through further engineering efforts"", but on the other hand, the idea of nested algorithms (""matrixfree"" or ""truncated newton"") always has this issue."	0
"pros: interesting idea, relevant theory provided, highquality experiments cons: no evidence that this is a ""breakthrough"" idea minor comments:  theorems seemed reasonable and i have no reason to doubt their accuracy  no typos at all, which i find very unusual."	2
strengths  the paper is clearly written and effectively makes a simple claim that geodesic distance minimization is better aligned to final performance than euclidean distance minimization between source and target.	2
strengths:  it is a sensible idea to improve the euclidean distance by the geodesic logeuclidean distance to better explore the manifold structure of the psd matrices.	2
it does not explain that, in figure 2, the entropy is able to find the best balancing cost .lamba for geodesic alignment but not for the euclidean alignment.	0
pros:  very easy to follow idea and model  simple merge or rl and sl in an endtoend trainable model  improvements over previous solutions cons:  kmeans experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.	2
using hierarchical latent variables for large action spaces is like a good idea, but placing the work into multiagent seems like a red herring.	0
"11431150. the idea of doing something hierarchical of course makes sense, but also here there are a number of related papers: putting ""hierarchical multiagent"" in google scholar finds works by ghavamzadeh et al., saira & mahadevan, etc. victor lesser has pursued coordination for better exploration with a number of students."	0
the main idea of rl in mdps is that agents do not maximize immediate rewards but instead long term rewards.	0
pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!)	2
concerns: a nearly identical idea to the core idea of this paper was proposed in an arxiv paper this spring, as a commenter below pointed out.	0
the idea is interesting but the evaluation leaves doubts.	0
originality: acceptable, but a very similar idea of embedding contexts is presented in tu et al. (2017) which is not cited.	0
additionally, this idea not only fill the relatviely lacking of theoretical results for gan or wgan, but also provide a new perspective to modify the gantype models.	0
a fairly small, but consistent improvement on the base model and other similar ideas is reported in table 1. i would have liked to have seen results on imagenet.	0
i wonder how applicable this in practice  i frankly didn't see insights here that i can apply to other problems that don't fit into this particular narrowly defined framework  i do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.	0
however some of the techniques such as the way the policy is represented and the way the policy gradient formulation is approximated seems to be novel in the context of deep rl though again these ideas have been explored in the literature of control and rl extensively.	0
although the results are interesting, i have a number of concerns about this work, which are listed below: 1. the idea of tying weights in the neural network in order to compress the model is not entirely new.	0
in general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being.	0
pros: i like the idea and the proposed applications.	2
pros:  the idea of learning a highlevel transition model (rather than a lowlevel transition model) is interesting and has the potential to be very useful.	2
i like the idea of the paper but i would love if the author(s) could improve more theoretical results to convince people.	0
pros: promising/interesting idea cons: not fully developed thin experimental section	2
although the idea seems incremental, the experimental results do seem solid.	0
pros: 1. updating a traditional opendomain qa approach with neural models 2. experiments demonstrate solid positive results cons: 1. the idea seems incremental 2. presentation could be improved	2
however, this doesn't really satisfy me  it is clear that more disturbance will cause this behaviour, but that doesn't mean any disturbance is good, e.g. if i always apply the negative weight and make my model weights go in the wrong direction, i'm pretty sure training loss and gradients will be even larger, but it's a bad idea to do.	0
i like the idea in the paper but it has some limitations as described below: pros: 1. it uses relatively simple and less number of parameters by using shallow fullyconnected neural networks.	2
figure 1 is interesting but it could use better labelling (words instead of letters) overall: pros: wellwritten, good empirical results, wellmotivated and intuitively explained cons: not particularly novel, a modification of an existing idea, more sensitivity results would be nice	2
the idea of avoiding mode collapse by providing multiple samples to the discriminator is not new; the paper acknowledges prior work on minibatch discrimination but does not really describe the differences with previous work in any technical detail.	0
by comparing only to onesample discriminators it leaves open the (a priori quite plausible) possibility that minibatch discrimination is generally a good idea but that other architectures might work equally well or better, i.e., the experiments do not demonstrate that the mmd machinery that forms the core of the paper has any real purchase.	0
the second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration.	0
the second idea is that episodes may terminate due to time out, but we should include the discounted value of the timeout termination state in the return.	0
although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims.	0
the idea proposed in the paper is quite interesting, but the presentation is severely lacking.	0
overall, i think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.	0
the idea of learning a model based on the features from a modelfree agent seems novel but lacks significance in that the results are not very compelling (see below).	0
the idea of reusing a pretrained agent has both pros and cons.	0
many ideas are presented with some abstract motivation, but there is no comparative evaluation to demonstrate what happens when any one piece is removed from the system.	0
the results are fine when the idea is applied to the bidaf model, but are not very well on the drqa model.	0
exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search.	0
cons: the idea seems to be simple and does not have significant originality.	0
i do, however, have two major concerns about the paper: 1. the proposed idea to add a mutual information constraint between the data x and latent code z is a very natural fix to the failure of regular vaes.	0
overall, the core idea in this work is interesting but underexplored. '	0
i have seen this before for sgd (the authors do not claim that the basic idea is novel), but i believe that the application to other algorithms (the authors explicitly consider nesterov momentum and adam) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization).	0
i am sure this idea has been tried before in the 90s but i am not familiar enough with all the literature to find it (a quick google search brings this up: reinforcement learning of active recognition behaviors, with a chapter on nearestneighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997045/node3.html).	0
"my main concern about this paper is that the idea of ""placing the targetdomain features far away from the sourcedomain decision boundary"" does not necessarily lead to 'discriminative features' for the target domain."	0
another concern comes from using the proposed idea in training a gan (section 4.3).	0
strengths: a sound approach; a simple and straightforward idea that is shown to work well in evaluations.	2
strengths: simple idea that can work well in the context of sampling examples for sgd weaknesses: the novelty in the paper is limited.	2
pros: 1) the idea of using cpd for unsupervised metric learning is quite interesting 2) the exploration into the convexity of the cpd parameter learning  although straightforward  is also perhaps interesting.	2
what is the advantage over other competitive schemes, etc. in summary, while there is a minor novelty in connecting two separate ideas (cpd and uml) into a joint uml setup, the paper lacks sufficient motivations for proposing this setup (in contrast to say kernelized kmeans), the technical details are unconvincing, and the experiments lack sufficient details or analysis.	0
using characterlevel models a la ling et al. ' using dictionary embeddings a la hill et al. none of these ideas are new before but i haven’t seen them combined in this way before.	0
unfortunately, while the idea has merit, and i'd like to see it pursued, the paper suffers from a fatal lack of validation/evaluation, which is very curious, given the amount of data that was collected, the fact that the authors have both a training and a test set, and that there are several natural ways such an evaluation might be performed.	0
the paper builds on a simple but useful idea and is able to develop it into a basic method for the goal.	0
this paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at section 3.3. more specifically, if you generate the parameters .theta according to eq. 7 and posit a prior over .theta then you will have a problematic variational bound as there will be a kl divergence, kl(q(.theta) || p(.theta)), with distributions of different support (since q(.theta) is defined only along the directions spanned by u), which is infinite.	0
all in all the idea developed in the paper sounds interesting but the paper organisation seems a bit loose and additional aspects should be investigated.	0
it may sound pedantic, but i don’t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn’t vary too much in the different images of the single object.	0
pros: () the idea introduced is simple and flexible to be used for any cnn architecture () experiments on imagenet1k prove demonstrate its effectiveness cons: () experiments are not thorougly explained () novelty is extremely limited () some baselines missing the experimental section of the paper was rather confusing.	2
the idea appears elegant but rather straightforward.	0
this paper has a nice simple idea at its core, but i don't think it's fully developed.	0
the examples in section 3 are quite thorough, but i feel the basic idea of measuring influence by equation (1) is not on solid footing.	0
i found the paper quite interesting, but meanwhile i have the following comments and questions:  as pointed out by the authors, the idea of this formulation and doubly sgd is not new.	0
the main idea [and its variants] looks solid, but with the plethora of gans in the literature now, after reading i'm still left wondering why this gan is significantly better than others [began, wgan, etc.].	0
the idea is interesting but maybe not pushed far enough in the paper: 'at fixed context x, assuming that the error is a function of the average reward u and of the number of displays r of the context could be a constant could be a little bit more supported (this is a variance explanation that could be tested statistically, or the shape of this 2d function f(u,r) could be plot to exhibit its regularity). '	0
an other option would be to use some counterfactual estimates (see leon bottou &all and thorsten joachims &all) ' if the claim is about a better exploration, i'd like to have an idea of the influence of the tuning parameters and possibly a discussion/comparison over alternatives strategies (including an epsilonn greedy algorithm) besides theses core concerns, the papers suffers of some imprecisions on the notations which should be clarified. '	0
the idea of using class label to adjust each weight’s learning rate is interesting and somewhat novel, but unfortunately the efficacy is not well justified in theory.	0
overall: i like the idea this paper proposes, but i have some misgivings about accepting it in its current state.	0
the paper isn't presented in exactly these terms, but the idea is to consider a uniform distribution over programs and a zeroone likelihood for inputoutput examples (so observations of i/o examples just eliminate inconsistent programs).	0
"typos:  the paper needs a cleanup pass for grammar, typos, and remnants like ""figure blah shows our neural network architecture"" on page 5. overall: there's the start of an interesting idea here, but i don't think the quality is high enough to warrant publication at this time."	0
this paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned).	0
the idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worstcase distribution in some ball around the population distribution.	0
the idea is quite simple and the novelty is incremental by considering the difference from skiprnn.	0
considering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.	0
''' revision: based on the author's work, we have switched the score to accept (7) ''' clever ideas but not endtoend navigation.	0
the idea presented seems to have merit , however, i found the presentation lacking.	0
the core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial).	0
this is a good idea, but it is pretrained, not trained using the saddlepoint objective you introduce.	0
strengths: 1. the idea of sequential counting is novel and interesting.	2
the paper presents a number of interesting experiments and discussions about those experiments, but offers more exciting ideas about training neural nets than experimental successes.	0
pros  ' strong related work section that contextualizes this paper among current work ' very interesting idea to more efficiently find and train best architectures ' excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e. last paragraph of section 4.1, and last paragraph of section 4.2) ' publicly available code cons  ' some very strong experimental results contrasted with some mediocre results ' the balance of the paper seems off, using more text on experiments than the contributions to theory. '	2
the idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains.	0
furthermore, the possibility of using weak labels is put forth but not explained and no ideas on how this may be done are proposed.	0
the novelty is limited but it is a good idea to speed up the rc models.	0
overall, the idea of this paper is simple but interesting.	0
the idea of “soft ordering” enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular.	0
"summary: i like the general idea of learning ""output stochastic"" noise models in the paper, but the idea is not fully explored (in terms of reasonable variations and their comparative performance)."	0
overall: the paper contains an interesting idea, but given the deficiencies raised above i judge that it falls below the iclr threshold. '	0
pros: i liked the posterior sharpening idea.	2
"the main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of jia and liang, emnlp 2017. in that work a ""distractor sentence"" is manually added to a passage to superficially, but not logically, support an incorrect answer."	0
this idea seems interesting but very difficult to evaluate.	0
"the basic idea is to use a multistep dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but i think baselines are more familiar to the rl community) to reduce the variance of a policy gradient estimator, while remaining unbiased."	0
overall: pros:  a nice idea with some novelty, based on a nontrivial observation  the experimental results how the idea holds some promise cons  the method is not presented clearly enough: the main component modeling the network activity is not explained (the hdda module used)  the results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)  experimental results are partial: results are not presented for multiple defenders, no ablation experiments after revision: some of my comments were addressed, and some were not.	2
pros and cons interesting idea for learning quickly from small numbers of samples of expert state trajectories.	2
clarity: the paper is very clear originality: the theoretical contribution extends the seminal work of bendavid et al., the idea of using adversarial learning is not new, the novelty is mediaum significance: the theoretical analysis is interested but for me limited, the idea of the algorithm is not new but as far as i know the first explicitly presented for multisource.	0
i like the idea of using dynamical systems theory to attempt to explain what is going on, but i wonder if it is not being used a bit simplistically or naively.	0
the assumptions that large fourier peaks happen close to origin is probably welljustified from the empirical point of view, but it is a hack, not a well established wellgrounded theoretical method (the authors claim that in their experiments they found it easy to find informative peaks, even in hundreds of dimensions, but these experiments are limited to the svm setting, i have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features).	0
the idea is interesting and the result looks promising, but i do not understand the intuition behind the success of analogizing graph with images.	0
but one of the main ideas of this paper (truncating the planning horizon and replacing it with approximation of the optimal value function) is not new and has been studied before, but has not been properly cited and discussed.	0
here, i have many major concerns about the proposed idea.	0
typo, find the “most orthogonal” representation if the inputs > of the inputs overall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental.	0
2) pros:  novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.	2
the authors are also missing a significant amount of relevant literature on the topic of building word embeddings from characters, for example: finding function in form: compositional character models for open vocabulary word representation, ling et al., 2015 enriching word vectors with subword information, bojanowski et al. 2017 compositional morphology for word representations and language modelling, botha and blunsom 2014 pros:  valid idea cons:  too many missing references  some modeling choices lack justification  experiments do not provide meaningful comparisons and are not reproducible	2
interpolations in the latent space is a good experiment, but in fact the interpolations do not look that great on lsun  it is unclear if the model covers the full diversity of images (mode collapse) it would be more convincing to demonstrate some practical results, for instance inpainting, superresolution, unsupervised or semisupervised learning, etc. 2) the general idea of multiscale generation is not new, and has been investigated for instance in lapgan (denton et al., iclr 2015) or stackgan (zhang et al., iccv2017, arxiv 2017).	0
pros: simple idea with impact; the problem being tackled is a difficult one cons: not many; real systems have constraints between physical dimensions and the forces/torques they can exert some additional related work to consider citing.	2
positive aspects:  the idea of using gans for this goal is smart and interesting  the results seem interesting too weaknesses:  some aspects of the paper are not clear and presentation needs improvement.	2
cons does not provide new theory but combines existing ideas in a new manner.	2
the idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly.	0
"i found the idea of figure 1 very good but in its current form i didn't find it particularly insightful (these ""clouds"" are hard to interpret)."	0
summary: all in all, the idea has potential but there are many missing details.	0
i believe this idea is quite similar to the idea of learning using privileged information, in which the information on teacher model is only used during training, but is not utilised during testing.	0
"2. the idea of using the gradient with respect to the quantization points to learn them is interesting but not entirely new (see, e.g., ""matrix recovery from quantized and corrupted measurements"", icassp 2014 and ""ordrec: an ordinal model for predicting personalized item rating distributions"", recsys 2011, although in a different context)."	0
"novelty: previous papers like ""betavae"" (higgins et al. 2017) and ""bayesian representation learning with oracle constraints"" by karaletsos et al (iclr 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does."	0
pros:  the analysis of the minimax objective is novel and the proof technique introduces several interesting ideas.	2
"the idea is closely related to sparse bayesian learning but the variational approximation is achieved via the local reparametrization trick of kingma 2015, with the key idea presented in section 3.3. minor in the introduction, the authors write ""... weights with a large variance can be pruned as they do not contribute much to the overall computation""."	0
overall, i think the paper contains several novel ideas, but its structure requires a 'significant' rework and in the current form it is not ready for being published.	0
1. the idea is interesting, but the study is not comprehensive yet 2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper  stoecklein et al. deep learning for flow sculpting: insights into efficient learning using scientific simulation data.	0
overall, the idea of this paper is simple but interesting.	0
the idea presented in the paper is interesting, but i have some concerns about it.	0
to summarize, i think the idea is interesting but the paper might not be ready to be presented in a scientific conference.	0
the proposed idea (treeqn) and underlying motivation are almost same as those of vpn [oh et al.], but there is no indepth discussion that shows why treeqn is potentially better than vpn.	0
i still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice).	0
they show that their idea is effective in reducing private information leakage, but this idea alone might not be signifcant enough as a contribution.	0
given tucker et al, its contribution is somehow incremental, but i think it is an interesting idea to use neural networks for control variate to handle the case where f is unknown.	0
to me, the main strengths of the paper is the very clear account of existing gradient estimators (among other things it helped me understand obscurities of the qprop paper) and a nice conceptual idea.	2
pros:  the idea of learning from a compressed representation is a very interesting and beneficial idea for largescale image understanding tasks.	2
pros:  quite simple but promising idea to augment exploration in maml and rl^2 by taking initial sampling distribution into account.	2
the main idea is stated given the premise of fig. 1, that there exist logits which are computed and passed through a softmax neuron, but this is never formally stated.	0
the idea as stated in the abstract and introduction may well be worth pursuing, but not on the evidence provided by the rest of the manuscript.	0
the core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.	0
the basic idea is to learn a separate policy for each context, but regularized in a manner that keeps all of them relatively close to each other, and then learn a single centralized policy that merges the multiple policies via supervised learning.	0
i believe there are some interesting ideas presented in this paper, but in its current form i think that the delta over past work (particularly distral) is ultimately too small to warrant publication at iclr.	0
abstract: the paper presents an interesting, but very preliminary idea, to use quantum inspired complex kernels that can be potentially used inside cnns.	0
other than that, i have the following questions and remarks:  i might have misunderstood the motivation but the gan objective for the `g` network is a bit weird; why is it a good idea to push the counterfactual outcomes close to the factual outcomes (which is what the gan objective is aiming for)?	0
pros:  results  novelty of idea  crossover visualization, analysis  scalability cons:  missing background  missing ablations  missing details [after rebuttal: revised the score from 7 to 8]	2
pros: 1. the idea of using pointer networks for reducing search space of generated queries is interesting.	2
my only concern about the novelty of the paper is that the idea of using cyk chartbased mechanism is already explored in le and zuidema (2015).	0
to me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem.	0
i find the idea to use the multihead attention very interesting, but one should consider the increase in number of parameters in the experimental section.	0
the idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains.	0
in the end, i have trouble making a recommendation: con: i'm not convinced that an endtoend approach to this problem is the best one pro: it's actually a nice idea that seems to have worked out well con: i remain concerned that the objective is not the right one my rating would really be something like 6.5 if that were possible.	0
the idea of incorporating learned representations with a structured bayesian filtering approach is interesting, but it's utility could be better motivated.	0
moreover, i am wondering whether sections 2.2 and 2.3 can be simplified or improvedthe underlying idea seems intuitive, but some of the statements seem somewhat confusing.	0
what the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, i do not think that this paper is relevant for the iclr conference.	0
pros:  very promising results with an interesting active learning approach to multitask rl  a number of approaches developed for the basic idea  a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)  paper is overall well written/clear cons:  comparison only to a very basic baseline (i.e. uniform sampling) couldn't comparisons be made, in some way, to other multitask work?	2
pros: interesting idea.	2
the idea is interesting, however the experiments in this paper is seriously lacking.	0
cons: 1. it is a nice idea and it seems to perform well in practice, but are there careful experiments justifying the 3stage training scheme?	0
oneshot learning of manipulation skills: trains a similar proprioceptiononly model and uses it for object manipulation, similar idea that object properties can be induced from proprioception but in general the citations to relevant robotic manipulation work are pretty sparse.	0
negative aspects: although the premise of the paper is interesting, its execution is not ideal.	0
positive aspects:  emphasis in model interpretability and its connection to psychological findings in emotions  the idea of using tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative.	2
in principle, the idea seems to be clear, but then the description and motivation of the model remains very vague.	0
the idea of using a gaussian model and its associated mahalanobis metric is certainly interesting, but also a timehonored concept.	0
overall the idea appears to be useful but needs more empirical validation (affnist, imagenet, etc).	0
the idea is simple and novel, which is good, but the validation is limited and far from any realistic use.	0
to conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments.	0
the idea is that the threshold effects of discretization make it harder to find adversarial examples that only make small alterations of the image, but also that it introduces more nonlinearities, which might increase robustness.	0
pros and positive remarks: i liked the idea behind this paper.	0
pros:  interesting concept of combining algebraic structure with a data driven method  clear idea development and well written  transparent model with enough information for reimplementation  honest pointers to scenarios where the method might not work well cons:  the method is only intrinsically evaluated (tables 2 and 3), but not compared with results from other motion estimation methods	2
pros   interesting idea  reads well  fairly good experimental results cons   kenneni seems like it couldn't be realistically deployed  lack of an intermediate difficulty task	2
"implementation details all over the place (section 3. is called ""implementation"", but at that point no concrete idea has been proposed, so it seems too early for talking about tensorflow and keras)."	0
the idea for disentangling identity from other factors of variation using identitymatched image pairs is quite simple, but the experimental results on faces and shoes are impressive.	0
in summary, the idea of the paper is very interesting to learn dataaugmentation but yet i am not convinced the current paper has enough novelty and contribution and see the contribution of paper as on more the application side rather than on model and problem side.	0
pros:  the idea is intersting and if evaluated thoroughly it could be quite influential.	2
overall, i think that the idea is interesting, but the paper needs more work and does not meet the iclr acceptance bar.	0
i found the idea of multi region sizes interesting, but no description is given on how exactly they are combined.	0
strengths  the idea of concept learning considered here is novel and satisfying.	2
cons: it is not clear if it improves over something like scheduled sampling, which is a stochastic predecessor of the main idea introduced here.	0
pros: (1) the idea is introduced clearly and rather straightforward.	2
i would say the idea is not bad, but the paper is still not wellprepared.	0
while limited novelty is found in the methodology/engineering  novelty being mainly related to the affine transfer mechanism, results are disappointing.	0
"some sources for the references are presented inconsistency, e.g., cohen and welling, 2017 and dieleman, et al. 2017  some references include the first name of the authors, others use the initial  in references to et al. or not, appears inconsistent  eqns 4, 5, 6, and 8 require punctuation  section 4 line 2, period missing before ""since the fft""  ""coulomb matrix"" > ""coulomb matrix""  figure 5, caption: ""the red dot correcpond to"" > ""the red dot corresponds to"" final remarks: based on the novelty of the approach, and the sufficient evaluation, i recommend the paper be accepted."	1
it is unclear to what extends the novelty of the paper (specific architecture choices) are required.	0
although, i see a value in the idea considered in the paper, it is not clear to me how much novelty does this work bring on top of the following two papers: 1) s. liu.	0
however, it seems to me that the paper is limited both in theoretical novelty and practical usefulness of these results.	0
negatives:  the novelty of the paper is limited.	0
the used visual concepts (vcs) were already introduced by other works (wangt'15), and is not a novelty.	0
in general, there is no clear statement of novelty or contribution until the very end of the paper.	0
although this is interesting and it seems nontrivial to get this running in an efficient, endtoend learnable way, i fear this is simply not enough technical novelty for an icl18 paper.	0
i summary, i am not fully convinced because technical novelty seems very limited and experiments do not sufficiently prove the claims.	0
while this paper addresses an important problem, in its current form the novelty and analysis are limited and the paper has some presentation issues.	0
"the ""novelty"" of this paper is a bit hard to assess."	0
the idea does not seem to be novel, technical novelty is low, and the execution in experiments does not seem to be reliable.	0
i am not sure about the novelty of the paper, as it is a relatively standard definition of bayesian math.	1
even though the regularizers seem a bit different, the large similarity with amtl decreases the novelty of this work.	0
the paper proposes a gan for novelty detection (predicting novel versus nominal data), using a mixture generator with feature matching loss.	1
the paper presents a method for novelty detection based on a multiclass gan which is trained to output images generated from a mixture of the nominal and novel distributions.	1
i liked the paper and the idea of using the discriminator of the gan to detect novelty.	2
2. it is not clear to me why the feature matching loss results in a mixture distribution or more specifically why it results in a mixture distribution which is helpful for novelty detection?	0
this paper proposed a gan to unify classification and novelty detection.	1
there is a logical gap between the ability of saying 'i don't know' and the necessity of novelty detection.	1
this assumption is too strong for the application of novelty detection, since novel data can be whatsoever unseen during training.	2
therefore, it is conceptually and theoretically strange to apply gan to novelty detection, which is the major contribution of this paper.	2
novelty detection sounds very data mining rather than machine learning.	1
i am not sure they are really novelty detection tasks because the real novelty detection tasks should be fully exploratory.	0
this is based on the ifelse statements i see in algorithm 1. conclusion:  since the novelty is limited and it requires explicit program supervision, and the performance is only on par with the stateoftheart (film), i am not convinced that this paper brings enough contribution to be accepted.	0
however, the novelty and the technical contribution are not sufficient for securing an acceptance.	0
there is reasonable novelty in the proposed method compared to the existing literature.	1
it appears to me that the novelty of the paper is limited, in that the main approach is built on the existing began framework with certain modifications.	0
conclusion while the paper's conceptual novelty is low, the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable.	2
the paper makes several claims regarding the novelty and expressiveness of the model and the contributions of the paper that are either invalid or not justified by the experimental results.	0
this includes discussion of the attention mechanism, for which the contributions and novelty are justified only by simple visualizations that are not very insightful.	1
the technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods.	0
unfortunately though, at this point i feel that the theoretical results, which constitute the majority of the paper, are of limited novelty and/or significance.	0
i fail to see the significance nor the novelty in this work (esp.	0
summary of evaluation there is not much novelty in this idea (of optimizing carefully only the last layer as a posttraining stage or treating the last layer as kernel machine in a postprocessing step), which dates back at least a decade, so the only real contribution would be in the experiments.	0
there is almost no novelty here.	0
what is less clear is the distinction between the algorithmic approaches which makes it hard to judge the novelty of this work.	0
well written  good evaluation  good performance compared to prior state of art  technical novelty  semisupervised loss does not yield significant improvement  missing citations and comparisons the paper is well written, structured, and easy to read.	2
the novelty of this paper is limited.	0
however, the authors do prove their work, which increases the novelty.	2
however, beyond this conceptual novelty, the work does not demonstrate a lot of technical novelty or depth.	0
in this regard, it does not show much novelty.	0
invoking results from differential privacy for fixing the issue of validity of pacbayes bound is the second novelty.	2
while the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community.	2
"minor comment regarding novelty: the temperature adjustment method sounds somewhat similar to previous method of increasing the slope described ""adjustable bounded rectifiers: towards deep binary representations"""	1
to summarize, i don't think there is enough interesting novelty in this paper.	0
first, the novelty of the paper is not clear.	1
therefore, the novelty of the proposed method is somewhat weak.	0
originality and novelty: i think much of the ideas considered in this paper is already explored in previous work as it is acknowledged in the paper.	0
while there is not much contribution in terms of technical novelty, i think this is an interesting paper that sheds new lights on limitations of existing methods for learning sentence and document representations.	2
i think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information).	2
i cannot identify a clear contribution or novelty in the paper which is the basis for my recommendation of rejection of the paper.	0
1) the model brings no novelty, or to put it bluntly, it is rather simplistic.	0
2. since mode collapse is a wellknown phenomenon, the novelty of this paper is not sufficient.	0
finally, the novelty of the work is also limited to the common approach of finetuning last layers of a pretrained network.	0
authors use the dataset provided for starcraft: brood war by lin et al, 2017. my impression about the paper is that even though it touches a very interesting problem, it neither is written well nor it contains much of a novelty in terms of algorithms, methods or network architectures.	0
despite its limited novelty, this is a very interesting and potentially impactful paper.	2
2) novelty/significance i think the novelty of this paper is perhaps marginal.	0
in the positioning of the work in the literature, the authors point out that hypernetworks have been proposed before, so it is not clear what is the actual novelty in the proposal.	1
in summary, both novelty and impact seem limited.	0
as a result, it is quite difficult to judge the merits and novelty of the paper.	0
it introduces only some small technical novelty inspired by softkmeans clustering that anyway seems to be effective.	2
the novelty of the method is limited.	0
the main novelty in this paper is the choice of models to be used by speaker and listener, which are based on lstms and convolutional neural networks.	1
essentially, the novelty is using a cnn rather than an rnn to induce document embeddings.	1
the only novelty i find is in applying node2vec (an existing technique) on the dual of the hypergraph to get an embedding for hyperedges.	2
summary: the proposed model, hybridnet is a fairly straightforward variation of wavenet and thus the paper offers a relatively low novelty.	1
for novelty, it is unclear if theorem 2 and theorem 3 are both being stated as novel results.	1
for e.g. when , sin(.pi d^5 .vert x .vert^2) can rather trivially be expressed as a sigmoidal neural network of depth 2. overall, i think this paper has a collection of results that are wellknown to experts in the field and add little novelty.	0
"the only novelty is these ""how"" inputs to the extra attention mechanism that takes a richer word representation into account."	1
in particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper.	2
the novelty in the paper is implementing such a regression in a layered network.	2
it is not clear what is the main novelty compared to the aforementioned paper, other than temporal smoothing of features at the decoder stage.	1
the difference from the original aae is rather small and straightforward, making the novelty mainly in the choice of task, focusing on discrete vectors and sequences.	1
originality: i think there is enough novelty to warrant publication.	1
i also found difficult to measure the degree of novelty of the approach considering the recent works and the related work section should have been much more precise in terms of comparison.	0
overall, i think the constitution and the novelty of this paper are above the bar.	2
novelty the model introduced is a variant of a deep latent gaussian model, where the topmost layer is a discrete random variable.	1
the proposed method is indeed technically sound and have some distinctions to other existing methods in literature, however, the novelty of this work does not seem to be significant as i will elaborate more.	1
the main novelty in this paper is that it uses the label as a third view of a multiview model and make use of cross moments.	2
e.g., it is not clear as shown in figure 3, what is key novelty of the proposed dagan, and how does it improve from the existing gan work.	1
overall, the novelty of the paper seems to be low and it is difficult to understand what exactly is being done.	0
yet, as many papers, it fails to be clear in describing what is its real novelty and the introduction does not help in focussing what is this innovation.	0
besides the limited novelty, the submission leaves several parts unclear.	0
therefore the novelty is limited.	0
this is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results. '	0
not much novelty/originality.	0
originality: the results presented are well known and there is no clear contribution algorithmicwise to the field of rl.	0
the idea of taking a max over instances of partition c_i (def 3) already appeared in the proof of results of xu&mannor, and the originality of the contribution is essentially to add an expectation over the result of the algorithm.	1
162166. l. middleton and j. sivaswamy, hexagonal image processing, springer verlag, london, 2005 the originality of the paper lies in the practical and efficient implementation of gconv layers.	2
as one can see by the title, the originality (application of dcnn) and significance (limited to atm domain) is very limited.	0
the main originality of paper is the block style.	1
the basic idea is to use a hierarchical factorization of the linear map, that greatly reduces the number of parameters while still allowing for relatively complex interactions between variables to be modelled.	1
although the idea seems to be fruitful and interesting i find the paper quite unclear.	0
while the idea may be novel and interesting, its motivation is not clear for me.	0
experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above.	1
also, the idea of using a family of wavelets seems quite straightforward.	1
overall, i think the paper presents a novel and unique idea that would be interesting to the wider research community.	2
the sparse update idea feels very much an afterthought and so do the experiments with spanish.	1
although the idea to train an ensemble of learning machines is not new, see e,.g.	0
but there is no evidence that this is actually due to the proposed idea.	0
although, i see a value in the idea considered in the paper, it is not clear to me how much novelty does this work bring on top of the following two papers: 1) s. liu.	0
however, i think the work demonstrates useful ideas furthering the idea of continuoustime transformations that warrants acceptance.	2
the idea does not seem to be novel, technical novelty is low, and the execution in experiments does not seem to be reliable.	0
the paper is not clearly written, so i outline my interpretation on what is the main idea of the manuscript.	0
while the idea is novel and i do agree that i have not seen other works along these lines there are a few things that are missing and hinder this paper significantly.	0
is the idea that we do not want to use domain knowledge about which symbols correspond to operations vs. which correspond to numbers?	1
however, i do find very interesting the analysis provided in section 3.2. the idea of using meaningful intermediate (and stable) targets for the first two layers seems like a very good idea.	2
unsupervised mt in itself is not a new idea (again clearly acknowledged by the authors).	0
comparison to dnc does not show the effect of the proposed idea (masked euclidean distance).	0
second, a set of 20 qualitative images does not (and cannot) validate any research idea.	0
also, at the end of the explanation of fig. 4 the authors mention a gain of 8%, which in my understanding is not really relevant compared with the total speedup, which can be in the order of 500% overall, the idea of model specialization seem interesting.	2
significance: the idea of using factorization for rnns is not particularly novel.	0
however, idea of using ensembles in the context of (modelbased) rl is not novel, and it comes at the cost of time complexity.	0
i am not sure if it is a good idea to examine the convergence purely from an optimization perspective.	1
while the idea of pivot based common representation learning for language pairs with no parallel data is not new, adding the communication aspect as an additional supervision is novel.	2
the paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas.	0
is the idea limited to feedforward networks, or is it also applicable for recurrentlike networks?	1
while the idea is sound, many design choices of the system is questionable.	0
also, i struggled to make out the yellow box in figure 2, and the positioning of figure 3 on the side is not ideal either.	0
despite simplicity of the idea, i think it can potentially be useful practical tool, as it allows for very fast approximation of wasserstein distance.	2
the motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation.	2
both appear to employ the same core idea of regularizing an lstm using a learned variational distributions.	1
i don’t really see any significant contribution here except “we had this idea for this model, and it works”.	0
"my understanding is that in any dynamical system model, the belief state update is always a deterministic function of previous belief state and observation; this suggests to me that the idea of ""state"" here differs from my definition of ""state""."	1
the idea is that the viewpoint changes until the object is recognized.	1
unlike their claim in the paper, the idea of combining supervised and rl is not new.	0
in summary, while the paper contains some good ideas, i certainly think it needs more work to make a clear case for this method.	0
also, even when doing divideandconquer, the solution obtained in the first line of the algorithm should still be approximate.	1
for one, removing the fully connected classification layer is not a novel idea; all convolutional networks (https://arxiv.org/abs/1412.6806) reported excellent results without an additional fully connected affine transform (just a global average pooling after the last convolutional layer).	1
otherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms.	2
overall, the idea looks interesting and the manuscript is wellwritten.	2
there needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics.	1
in conclusion, because the novelty is also not that big (cp decomposition for word embeddings is a very natural idea) i believe the evaluation and analysis must be significantly strengthened for acceptance.	0
this is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.	2
i think the idea of the paper is interesting and i'm willing to increase (and indeed decrease) my score.	2
the idea to detect and reject adversarial examples has seen quite some work in the last time.	1
3. in presenting autoencoders it is crucial to note that they are all built around the idea of compression.	1
the basic idea of the lambda return assumes td targets are better than mc targets due to variance, which place more weight on shorter returns.	1
it appears to me that the main idea and the overall structure of the proposed algorithm is the same as in the one published by yin et al. (2017), and that only few changes were necessary to include momentum.	1
the idea of training differentially private neural network is interesting and very important to the machine learning  differential privacy community.	2
the paper builds on the rather old idea of minimizing the empirical vicinal risk (chapelle et al., 2000) instead of the empirical risk.	1
assuming i have understood the method correctly, the idea of using tensor products to incorporate higher order interactions between the hidden states at different times appears sensible.	2
the idea is quite straightforward, and the paper is relatively easy to follow.	2
nearest neighbor with euclidean metric is not a strong baseline at all, and not being able to tell if the proposed method is better than that is not a good sign.	1
"similar idea has been explored in early 2000 by finley and joachims in their icml paper titled ""supervised clustering with support vector machines""."	1
significance, quality and clarity: the idea is well motivated: faster training is important for rapid experimentation, and altering the rnn cell so it can be paralleled makes sense.	2
the overall idea is interesting and has many potentials. '	2
specifically, the idea is to use the structure of the pgm to perform efficient inference.	1
using matched samples of tumor and normal from the patients is also a nice idea to mimic cfdna data.	2
they also showed how to extend this idea to the linear case: one replaces the laplacian normally used to build pvfs with a matrix formed by sampling differences phi(s')  phi(s), where phi are features.	1
this three networks need to be clearly described; ideally combined into one endtoend training pipeline.	1
"there are a few ideas the paper discusses: (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 blas rather than sparse operations which have significant overhead and are not ""worth it"" until the matrix is extremely sparse."	1
the idea builds upon previous work in scalable gaussnewton methods for optimization in deep networks, notably botev et al., icml 2017. in this respect, i think that the novelty in the current submission is limited, as the approximation is essentially what proposed in botev et al., icml 2017. the laplace approximation requires the hessian of the posterior, so techniques developed for gaussnewton optimization can straightforwardly be applied to construct laplace approximations.	0
the idea to avoid the gaussian process behaviour is to contradict one of the hypothesis of the clt (so that it does not hold anymore), either by controlling the size of intermediate layers, by using networks with infinite variance in the activities, or by choosing nonindependent weights.	1
in a summary, the main idea of adding rotation to lstm cells is not properly justified in the paper, and the results presented are quite weak for publication in iclr 2018.	0
1. the idea of multilevel binarization is not new.	0
overall, the paper is sloppily put together, so it's a little difficult to assess the completeness of the ideas.	0
the main idea is assuming tuckertype lowrank assumption for both a weight and an input.	1
the paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow.	0
each variable the authors start with this idea and introduce a generalized relu which is specified via a subgradient function only whose local quasiconvexity properties are established.	1
i think the idea of representation learning using a somewhat artificial task makes sense in this setting.	2
a very similar type of ideas comes up in constrained or proximal quasinewton methods, and i have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 siam review paper on fwi by metivier et al. at https://doi.org/10.1137/16m1093239).	1
i like the idea of automatically choosing free parameters using the entropy over the target domain.	2
an idea of the eventual applications of these embeddings would make it easier to determine, for instance, whether the supervised ensembling method applied here would be applicable in practice.	2
the idea is very interesting.	2
initializing evalues to 1: i like this idea.	2
"finally, while the idea of ""closest misclassified examples"" seems interesting, i am not convinced that they are the right way to go when it comes to both building and evaluating robustness."	0
the idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting.	2
the paper represents an empirical validation of the wellknown idea (it was published several times before) to increase the batch size over time.	1
using ideas based on the maximum mean discrepancy (mmd) to compare distributions of minibatches, the authors generalize the idea of mini batch discrimination.	1
the ideas are illustrated through several wellworked microworld experiments.	1
in general, this is an interesting direction to explore, the idea is interesting, however, i would like to see more experiments 1. the authors tested out this new activation function on rnns.	2
the idea of using crosslingual or multilingual representations to seamlessly handle documents across languages is not terribly novel as it has been use in multilignual categorization or semantic similarity for some time.	0
in a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works.	0
the idea is straightforward, the analysis seems correct, and the experiments suggest the method has superior accuracy compared to prior rfms for shiftinvariant kernels.	2
this paper proposes an idea to do faster rnn inference via skip rnn state updates.	1
a little more development of this idea, and some more concrete motivation for the specific choices of which properties to include, would go a long way in strengthening the paper.	1
originality: the paper is based on a very smart and interesting idea and a straightforward use of gans.	2
although i agree that is not an ideal validation, i'm not sure if it's equivalent or not the authors' validation setting, as they don't describe what that is. '	0
while the core idea may appear as appealing, the paper suffers from several flaws (as further detailed afterwards): insufficient related work correctness/rigor of theorem 2.1 clarity of the paper (e.g., sec.	0
the idea of imposing a nonprojective dependency tree structure was proposed previously by liu and lapata (2017) and the structured attention model by kim and rush (2017).	1
the idea is to construct a generative model capable of approximating the posterior distribution over the parameters of deep networks.	1
the idea of integrating autoencoder with continuous clustering is novel, and the optimization part is quite different.	2
presenting sequential data in a windowed format is a standard procedure and not a new idea either.	1
the idea being to construct a superposition of taylor approximations of the individual monomials.	1
as far as i can tell these ideas could have been more simply implemented by training a reranker to score the nbest outputs of the decoder.	1
however, i am not sure that the novelty is significant, since i think that the idea of proposing their optimization is trival.	0
(ii) given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages.	1
one advantage of proposed idea claimed against the skiprnn is that the skimrnn can generate the same length of output sequence given input sequence.	1
the idea is interesting and new.	2
the key idea of this paper is to replace the minimization of the energy function min_y e(f(x), y) with a neural network which is trained to predict the resulting output of this minimization.	1
the idea of empirically studying the manifold / topological / group structure in the space of filters is interesting.	2
"it is basically a straightforward generalization of the idea of punishing, which is common in ""folk theorems"" from game theory, to give a particular equilibrium for cooperating in markov games."	1
but it turns out that the above improvements are achieved with at least three different ideas: (1) the cnnselfattention module; (2) the entire model architecture design; and (3) the data augmentation method.	1
however, i don't think the authors explore the idea well enough  they simply appear to propose a nonparametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches.	0
i did not fully understand the 'retrofitting' idea.	1
the framework adopt the kmedoid clustering idea.	1
it is a reasonable idea and authors indeed show that it works.	2
"the paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and i still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me)."	0
"the main idea is essentially learning the manifold of the data distribution and using gaussian mixture models (gmms) and dictionary learning to train a ""reformer"" (without seeing adversarial examples) to detect and correct adversarial examples."	1
i like the idea of trying to formulate the feature learning problem as a twoplayer minmax game and its connection to boosting.	2
the idea is simple and intuitive.	2
the basic idea, introducing long rage dependecy, would be interesting.	2
the idea is clearly explained and well motivated.	2
conclusion: though with a quite novel idea on solving multitask censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the subspace network.	0
the idea of using mcmcp with gans is wellmotivated and wellpresented in the paper, and the approach is new as far as i know.	2
even the idea of researchers knowing their test set variance makes me very uneasy.	0
setting this as a general multi task learning, the idea consists in jointly learning autoecnoders, one for each domain, for the multiples domain data in such a way that parts of the parameters of the domain autoencoder are shared.	1
i like the idea of the paper.	2
7. the proposed idea seems somewhat related to using low rank factorization of weight matrices for compression.	1
the idea is to treat a nonstationary task as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can deal with nonstationarities at test time.	1
though the experimental results seem to indicate that the idea works, i think the paper does not motivate the loss function and the algorithm well.	0
the key idea is to make use of “syntactic distance” to identify phrases, thus building up a tree for input sentence.	1
pros:  since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a wellmotivated problem  it is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision  the proposed penalized approximation of the problem is simple and straightforward to try  experiments indicate that the method works as advertised cons:  there are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines  experimental gains are very modest compared to an independentlytrained crf  novelty is a bit low, since the trick of differentiating through meanfield updates is wellknown at this point  efficiency: the number of variables in the crf scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems differentiating the minimumcost multicut problem is wellmotivated, and i think the multiplehuman pose estimation problem is a nice application that seems to fit this abstraction well.	2
however, it lacks novelty and is limited to tabular data as presented.	0
cons:  the novelty is limited.	0
the motivation behind using dirichlet instead of gem makes sense, but other than that i fail to find any novelty in the paper.	0
cons:  limited novelty over existing methods.	0
pros:  wellwritten  natural extension of vaegans to video prediction setting  establishes a good baseline for future video prediction work cons:  limited novelty  limited analysis of model/architecture design choices	2
in summary, the paper may lack technical novelty in some respect, but the experiments are convincing in terms of proofofconcept, and the approach is smart.	0
i am concerned about both the motivation for and the novelty of this work.	0
it is possible i am missing some key insight, but i do believe the burden is on the authors to highlight where the novelty is.	0
there is some novelty in the proposed approach, but it is mitigated by the relation to the work of chen et al., cvpr 2018. the experiments show good results, but a more thorough evaluation of the influence of the hyperparameters would be useful.	0
several times, the paper states that some novel algorithm is used, but then provides no further explanation in the text as all description of this novelty is deferred to an appendix.	0
pros:  using channelwise quantization (with max values or momentanalysis) yields improvement over layerwise max approaches  limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)  shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channelwise quantization cons:  unclear how much is gained over layerwise and max value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channelwise methods are the clear winner  unclear if the layerwise set up with momentanalysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channelwise set up; a few more experiments are important to determine specifically if improvement is with respect to channelwise or momentanalysis since only layerwise max results are presented  clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability overall: the paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channelwise quantization (and momentanalysis), but the overall novelty is limited.	2
"my concerns come primarily to relation to prior and relevant work, strength of relevant experimentation, and claims of application and novelty / ""first past the post""."	0
cons: 1. lack of novelty in the proposed method.	0
cons: my main concerns are on the technical novelty and experimental comparison.	0
thus considering the lack of novelty and experimental validation, i recommend rejecting this paper.	0
overall, the paper does not have much novelty, but the results are quite promising.	0
overall the novelty of the approach and the proposed problem is incremental.	0
while the novelty and good structure of the paper are reasons to accept it, i have doubts concerning the soundness of the results due to the experimental setup.	0
reasons to accept the paper  novelty  works in an unsupervised setting  well written/structured reasons to reject  doubts concerning the experimental setting  (minor) related work is not complete  (minor) not all common performance measures are reported  [1] wang, peifeng, shuangyin li, and rong pan.	0
weakness: 1) incremental contributions and limited novelty.	0
pros: 1. the paper is well written 2. experiments are very detailed and thorough cons: 1. the proposed approach lacks novelty 2. experimentation lacks a user study which helps understand if and when gams are at least as interpretable as rulebased approaches.	2
the reasons are 1) incremental novelty; 2) insufficient experiments.	0
i have only some minor concerns: 1) my only serious concern is the degree of novelty with respect to (achlioptas et al., 2017).	0
the paper therefore lacks novelty.	0
lack of novelty: the analytic kl term for spike and slab priors has been derived before in discrete vaes (rolfe, 2017) and in work on weight uncertainty (yarin gal's thesis, blundell et al. 2016).	0
however, this paper is not acceptable due to lack of innovation and novelty.	0
unfortunately this work lacks novelty and isn't clearly presented.	0
2. multiple evaluation metrics and baseline models are considered weaknesses 1. the proposed method is simple and lacks novelty.	0
these changes provide significant improvements on the results but still the novelty of the approach is in that sense limited compared to [1].	0
2. the major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding wes was neither introduced in the related work section nor was it clearly explained in the text.	0
however, the paper is poorly written and hard to follow, and the proposed model lacks of novelty.	0
current research such as das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections.	0
pros: clear description and novelty of the method cons: insufficient experiments.	2
one major concern is about the novelty of the work.	0
in terms of novelty i find the method somewhat lacking: essentially it is close to simply a weighted combination of an ae cost function and a sammon's mapping cost function when using the frd constraints.	0
my main concerns are as follows: 1. the degree of novelty is not very high.	0
the main concern about the novelty is that the proposed method seems like a slight variation on label propagation in order to get nicer features and then just using whatever classifier.	0
cons:  limited technical novelty.	0
the novelty seems really incremental as a combination of two existing concepts and thus there is no clear conceptual improvement.	0
my main concern with this work is novelty.	0
some of the problems are listed as follows: 1. lack of technical novelty.	0
my main concern is that the paper is limited in technical novelty and it suffers from a lack of practical significance.	0
strengths:  a new auxiliary learning algorithm  positive results on cifar data set weaknesses:  novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space  there is no attempt to provide a theoretical insight into the performance of the algorithm  the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance  experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario  the paper is poorly written and sentences are generally very hard to parse.	2
compared to such works, it lacks both novelty and insights about what works and why it works.	0
overall i think the current work lacks novelty, significance and solid experiments to be accepted to iclr.	0
the particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.	0
2. the novelty of the method and the strength of the theoretical results are a bit lacking.	0
my concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of guo etal 2018, which this paper says is the motivation behind their work.	0
however, i am still concerned with the novelty.	0
despite the impressive result, my main concern is with the novelty.	0
this paper lacks novelty and has conceptual mistakes.	0
in summary, the main points of concern are:  the limited novelty on the modeling side  the lack of proper motivation on the importance of the addressed problem minor remarks:  yao et al. (2018) “use alternate optimization that breaks the flow of gradient through time” , unclear as their method can be optimized using stochastic gradient descent, as the yao et al. point out.	0
"the new result is better than prior work (compared to the ""factorized"", the finetuned lm is 2.1% better on the full test set, and 0.3% better on the noveltybased test set), but also uses a lot more unlabeled data than the prior work (if i understand the prior work correctly)."	0
cons:  it lacks novelty: the proposed framework just simply combines the two existing techniques as mentioned above.	0
strengths: ' quality of the paper, although some points need to clarified and expanded a bit more (see #2) ' nice diversity of experiments, datasets and tasks that the method is tested on (see #4) weaknesses: ' the paper do not present substantial novelty compared to previous work (see #3) # 2. clarity and motivation the paper is in general clear and well motivated, however there are few points that need to be improved: ' how is the importance mask (eq. 1) is defined?	2
# 3. novelty the main concern of the proposal in this paper is its novelty.	0
in light of the existing literature, this paper's contribution can be seen as incremental, with relatively low novelty.	0
perhaps i am not understanding the model, but the model description was also not clear nor easy to understand with respect to its novelty.	0
the collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.	0
weaknesses: (1) novelty: i think the novelty of this paper is somehow limited.	0
my main concern is that some of the contributions claimed were already shown in previous work (see method 1 below for details), and the novelty feels a bit limited.	0
"the originality of the work is low, because approaches that propose to replace the nondifferentiable threshold function with a differentiable proxy, in this case membrane potentials, have been known for several years, e.g. ""spiking deep networks with lif neurons"", eric hunsberger and chris eliasmith, 2015, and ""training deep spiking neural networks using backpropagation"", junhaeng lee et al. 2016. the main novelty is therefore the application to autoencoders, but overall this reduces originality and hence relevance for iclr."	0
my other concern is that the novelty seems to be marginal: either extending distillation methods in a very natural form, or weighting the network output using a novelty detector.	0
cons: • my main concern is the limited novelty of the work.	0
the work is lack of novelty and the results do not show significant improvement over existing approaches.	0
it lacks novelty.	0
weaknesses:  the approach has some novelty in the method of generating new images and in the framework itself.	0
but it lacks novelty.	0
however, the novelty is rather limited as similar ideas have been undertaken (e.g., mescheder et al 2018), but in different contexts.	0
1. i think the proposed hypothesis lacks the novelty that iclr audience seeks for.	0
cons: 1) insufficient novelty for algorithm design.	0
the novelty of this work is lacking.	0
reasonable method, but not too much novelty [summary] the paper proposed techniques to pretrain twolayer hierarchical bidirectional or singledirectional lstm networks for language processing tasks.	0
my biggest concern is the novelty of the model, since rtf is still a replaybased method that is very similar as dgrdistill.	0
originality novelty: there is a large body of work on disentanglement that the paper does not cite or compare to for instance, infogan, beta vae https://openreview.net/pdf?id=sy2fzu9gl and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdf note that for example that in beta vae it is a similar idea where but it is on z and z|x and the distance used is kl (since it is has closed form with gaussian) , min_phi loss beta kl (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.	0
my main concern is the limited technical novelty and evaluation:  the main idea of the architecture is extending 2d convolutions in image compression networks to 3d convolutions, and use skip connections for multiscale modeling.	0
the main issue of this paper is the lack of novelty: the three evaluated approaches (diag lstm, qrnn and gated convnet) are not novel, the only novelty is the addition of a 1d convolution, which is not enough for a conference like iclr.	0
overall, this paper feels more like a technical report: the findings could be useful, but its novelty is too limited for iclr.	0
it's application of known techniques, and lacks any ml novelty or sufficient ml interest.	0
the equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.	0
conclusion: due to lack of novelty, i recommend rejection.	0
the paper lacks novelty and at the same time, it is not quite compensating that with a detailed analysis of the work.	0
however there are two issues: 1. it is somewhat unclear from the paper what is the main novelty here (compared to existing morpho neurons), is it the learning of the structuring element s?	0
in addition to the limited technical novelty, i have a few other concerns as well, including some on the experimental evaluation:  realvalued node embeddings obtained from shallow/deep graph embedding methods can be used with 'overlapping' versions of kmeans.	0
in summary, i think the paper lacks both in terms of technical novelty as well as experimental evaluation and therefore doesn't seem to be ready.	0
update: after reading the rebuttal, i am increasing my score to 6. the authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains.	0
hence, this paper lacks enough novelty for publication, and it is not clear from the experiments that the specific method proposed in this paper is better than others in the sota.	0
cons: the main concern is the novelty of this work.	0
in addition to the lack of novelty or new insights, the writing needs serious attention.	0
my main concern for this paper is its novelty and whether it can be generalized to other models.	0
pros: face images appear to be successfully deidentified, while face pose and expression is preserved produces smooth videos without flickering, even though applied perframe paper is well written very good at including details needed for reproduction, such as hyperparameter settings contains ablation study cons: some faint blurriness still can be found in the resulting images large number of hyperparamters that must be tuned only compares with a single alternative deidentification method from 4 years ago (no comparison against modern gan based methods) ablation study has no quantitative metrics, so it is difficult to tell how much impact any individual component of the system had on the overall performance comments: while the proposed system achieves its goal of deidentified videos, its novelty is somewhat limited.	2
in conclusion, this is a well written paper, but the novelty is not apparent and the experimental results are weak, and so i am not convinced this is suitable for iclr.	0
in general, the model is contrived and the novelty of the paper is incremental.	0
specific:  page 2: perturbrbations (typo)  page 3: witho (typo)  page 3: constrained optimization problems typically written as max_{...} .mathcal l(x', y) s.t. x' = t(...) (s.t. for subject to instead of for) but that's matter of taste i guess  page 4: first order > firstorder (consistency)  page 4: tyipcally (typo)  page 4: occurs most common(ly) i am not sufficiently knowledgable about the previous literature to ensure that the claimed novelty of the paper is truly as novel.	0
cons: overall, the novelty of this paper is incremental and some points are not clear.	0
unfortunately, i do not think this paper as it stands currently is ready for publication at iclr for the following reasons: (1) the comparison and discussion with prior literature is lacking; (2) the experiments are rather weak; and (3) the significance and novelty of the method itself seems limited.	0
weaknesses:  the novelty is limited related to multitask learning, thus it is an incremental paper.	0
unfortunately the paper falls short in two main areas:  novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on velickovic et al 2017)  impact: the results achieved in the experiments are very small improvements compared to the baseline of rgcn (~ 0.01 in two experiments and ~ 0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyperparams, ...) however, on a positive note, the paper has been written very well and i really liked the frank discussion on page 8 about results on mutag dataset.	0
the novelty of this paper is limited because the model is relatively incremental.	0
even then, it's questionable whether the paper is suitable for iclr due to lack of methodological novelty.	0
however the novelty is limited which means that numerical experiments should be quite extensive to demonstrate a clear impact on the field.	0
strengths: (1) novelty: i think this paper contains some novelty inside.	2
further, the paper lacks novelty as multilevel fusion has been explored significantly and the changes are rather minor.	0
there are other small differences, such as adding noise and the exact way to impose constraint, but i doubt whether the novelty is significant in this case.	0
novelty: adversarial fairness methods are not new, but in my opinion the authors do a good job of summarizing the literature and formalizing the problem.	0
my main concern is about the novelty of technical contribution which is mainly composed by two: 1) a prediction variance based importance sampling strategy for batch selection and 2) an empirical study the show the merits of approach.	0
cons:  the overall contribution is incremental with limited novelty.	0
my main concern is about novelty.	0
"the proposed method has already been published in the literature but never been called ""representation in a krein space"" (see novelty point)."	0
weaknesses: 1. i am worried about the novelty of the proposed approach.	0
thirdly, combining two existing components is ok but not enough novelty from my point of view.	0
the novelty does not consist in the model since the model derives basically from a siamese network, but more in the approach, mainly the bilateral matching: one input is a context for an entity, the other input is a context for the synonym entity, and the output is the consensus information from multiple pieces of contexts via a bilateral matching schema with leaky unit (highest matched score with its counterpart as the relative informativeness score) and the context aggregation.	0
#update: i've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered.	0
in particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern.	0
with respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references).	0
"in short, i like the simplicity of the idea, but i am uncertain about the degree to which it satisfies iclr's novelty criterion (""present substantively new ideas or explore an underexplored or highly novel question""); i do feel confident that some iclr readers would (perhaps unfairly) describe this approach as ""obvious."""	0
overall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.	0
my biggest concern with the paper is novelty, which is rather low.	0
however, my criticisms remain that the paper is a simple combination of cycle gan and prototypical networks, and lacks new insights/novelty.	0
3. the novelty should be further summarized by highlighting the difference with most related works including but not limited the aforementioned ones.	0
since many rl setups are already distributed, the novelty of the paper mainly comes from sharing a replay buffer (i haven't seen this before but it seems like such an obvious thing to try that i wouldn't be surprised if it has been done) and the way in which learners are forced to follow the best individual.	0
[details] 1. the idea is incremental and the novelty is limited.	0
the problem is important, but the method is lack of novelty.	0
cons: the method is lack of novelty.	0
according to my understanding, the other classical streaming algorithms also do not require labeling but the novelty here i guess lie in learning the oracle (hh) which feels like a logical thing to do as such learning using neural networks worked well for many other problems.	0
on the negative side, i think the relevance and novelty of the results should be explained better.	0
however, the novelty of the method is a concern given the previous work of fan et al. (2017), and the manuscript is not upfront about the differences between the two works.	0
() (2) i see that there is a strong correlation between the le on subset a and e. it looks like you are training on e and you generalize on a. novelty/impact ' novelty is a strong concern, given the work of fan et al. (2017) (strong ).	0
detailed comments: 1. i have concerns about the novelty of the paper: it builds heavily upon previous work on modeling sgd as a stochastic differential equation to understand its noise characteristics.	0
on the negative side:  the novelty of the proposed approach is modest.	0
the model introduced is interesting, but its novelty is limited.	0
revision: the other reviewers raised some concerns that i had overlooked (especially regarding novelty of using matrix factorization to generate your potentials).	0
edit: i'm still not convinced about this article novelty, i really like the overall idea but it seems that this kind of contribution is better suited for short paper.	0
pros:  theoretical guarantees, elegant approach  good empirical results compared to other models  desirable properties: rotationequivariance, lower computational complexity, fewer parameters, robustness and guaranteed stability to deformations cons:  somewhat incremental technical novelty: combination of two previously published methods (qiu et al. 2018 & weiler et al. 2017) comments: 1. i believe the related work section can be improved by explaining more clearly the connection between your work and the cited ones and emphasizing the advantages and limitations of rotdcf compared to other methods in particular, a reader should be able to precisely understand what is the novelty of this work is and what were the technical challenges in combining previously published ideas (such as dcf and sfcnn) 2. how do you determine the truncation in practice?	2
my rating is thus based on the lack of novelty and poor quality of evaluation justifying the actual novel aspects of the paper.	0
"given the limited technical novelty(can be described as oneliner ""store forward pass in 4/8 bit fixed point""), limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), i think this is a boarderline paper on the positive side, the empirical result could still be interesting to some readers in the iclr community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum."	2
cons: 1. the novelty of this paper is limited.	0
weaknesses:  the paper lacks the technical novelty as it does not propose any novel technique.	0
however, the paper lacks the technical novelty and presents only limited experiments and analysis.	0
pros:  this paper is clearly written and includes a thorough and welllaid out empirical component  the contribution to the video action classification and captioning space seems like a worthwhile one cons:  the novelty of this paper mainly seems to be with respect to video classification and captioning; other methodological aspects and empirical themes are interesting but fairly standard more generally.	2
cons:  my main concern with this paper is regarding the novelty.	0
also, the techniques described in the paper are all based on existing techniques, and the paper lacks the technical novelty.	0
there are three major problems with the paper; 1. the novelty of the paper is not sufficient since it is an incremental work on batch normalization (bn).	0
weaknesses:  the novelty of the method is overstated.	0
some concerns, especially its novelty, are listed below.	0
however, while it is interesting and useful, i do i have concerns both on the novelty and experimental comparisons in the current version.	0
along these lines i have concerns about the novelty since de bruin 2015 even uses a similar offpolicy metric for forgetting already.	0
# novelty the general ideas of the paper are sound, however seem to be of rather minor novelty.	0
novelty (2) is only because it hasn't been used in the context of cnns, but there is no technical novelty here.	0
cons • my first concern is the limited novelty of the work.	2
finally, regardless of the clarify of the paper, the novelty in extending capsule networks to a siamese architecture is arguably pretty incremental.	0
in summary: pros  new extension of capsule networks, tackling a more challenging problem than previous work cons  novelty is incremental  paper lacks clarity and is hard to read  results are underwhelming for these reasons, i'm afraid i can't recommend this paper be accepted.	2
the paper suffers from different weaknesses as listed below:  the paper lacks novelty.	0
however, there are some concerns with the overall novelty and some technical details in the paper:  it seems the key contribution of the paper is to add the l_comp part to the already available l_pred part in denton and fergus 2018. the trick use to compose the latent variables is not novel and considering that variational ib is also available, the paper lacks overall novelty.	0
the lack of any detailed description of this step raises doubts regarding this step's novelty/methodological significance.	0
however i don’t feel that this alone is enough novelty for iclr, so i lean slightly toward rejection.	0
cons  the problem is not very well motivated and the novelty is limited.	2
from an applications perspective, there might be some novelty here but it's not clear.	0
so this raises a concern about novelty (although the experimental results are new).	0
i am still trying to evaluate the paper, but for now, my rating for this is low given that the main novelty in the paper: the environments, the evaluations, the tasks are so unclear because of the verbose presentation style on trying to tell us what we already know, such as goalconditioned learning, offpolicy learning, impala, etc.	0
regarding the novelty, w.r.t the compositions, it does use existing srl work but in a different context of nlp problems, this makes novelty a bit weak.	0
overall, i find the novelty of this work overly incremental and its impact potential very limited. '	0
cons: the paper does not present technical novelty.	0
cons:  a simple approach with limited novelty.	0
strengths: the idea of leveraging feature groups in a neural network structure; the novelty of the rese model; weaknesses: the main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use gbdt.	2
the paper studied interesting outlier detection algorithm but the novelty is limited.	0
the combination is no longer likely to be this easy but it would likely work more accurately than anything shown in table 3. novelty: this reviewer feels that augmenting a 2hiddenlayer vae with a softmax classification layer does not seem to be a very significant new contribution by itself.	0
the fact that it is being motivated for the specific problem of reducing catastrophic forgetting during lifelong learning is the main novelty here, but the relative amount of novelty might to be somewhat limited when viewed from this perspective.	0
there is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient lfd techniques such as generative adversarial imitation learning (gail).	0
cons: not enough technical novelty compared to current unsupervised feature learning approaches.	0
weaknesses: 1) i think the main weakness of this paper is the lack of any surprise/significant novelty in the presented approach.	0
overall, i think the paper has some observations that may be slightly interesting; however, it lacks novelty and the analysis or presentation are unconvincing.	0
the proposed idea is highly incremental in terms of technical novelty.	0
i don't believe your approach should necessarily model that space (after all, the novelty is on better modelling the sequence itself, not the possible futures, and the model can be easily extended to do so, either through gans or vaes), but it is important to not mislead the reader.	0
my main concern is wrt the novelty of this work: the novelty of the proposed model seems limited compared to code2vec (alon 2018b).	0
references: a. xi et al. pedestriansynthesisgan: generating pedestrian data in real scene and beyond b. yixiao et al. fdgan: poseguided feature distilling gan for robust person reidentification revision: the rebuttal can not address my concerns, especially the image quality assessment and the novelty of the paper parts.	0
the novelty is incremental.	0
cons: 1) neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a toptier conference.	0
in terms of technical novelty, the work is relatively incremental: (a) the use of multitask objectives in sequence models [1] is relatively common nowadays (there is little mathematical details in the paper, so it’s hard to see how the approach of the paper really differs from extensive related work.).	0
while i find this idea interesting and of potential practical use, i have concerns about novelty and the experimental results and overall i recommend rejection.	0
in summary, it is unclear to me if there is any novelty in the approach (missing references, lack of motivation of the algorithm) and if the results show any improvement over previous works (only one previous work has been compared and the qualitative examples do not show anything particularly interesting).	0
the experimental results also show a limited efficiency improvement according to table 1. although this is a debatable drawback compared with the novelty/contribution concern, it worth to reconsider the motivation of the proposed method given the fact that the automl framework is extremely expensive due to the drl design.	0
cons: 1. the novelty of this paper is replacing the euclidean metric with another existing metric, which has already been used in previous ml models.	0
my main concerns are about novelty.	0
cons: 1. the novelty of the proposed architecture is limited.	0
my main concern is the lack of novelty in the methodology.	0
weaknesses: ' the paper do not present substantial novelty compared to previous work.	0
cons:  limited novelty.	0
i would like to clarify regarding the listed cons:  limited novelty: adversarial training has been wellestablished as a viable defense against adversarial examples, as well as training a single model against an ensemble of adversarial examples crafted on different networks (tramer et al. https://arxiv.org/pdf/1705.07204.pdf).	0
"that is impressive, but i don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions:  the authors argue for ""phonemes and ctc"", and no speech person would disagree with them; in fact (miao et al., 2015) and many other papers show that the wers with a good phoneme based dictionary in english are lower than with a character based model."	0
one of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which i think otherwise.	0
weaknesses: novelty:  in essence, this method relies on the work of marten & grosse to approximate the hessian matrix used in the optimal brain surgeon strategy.	0
this is fine, but not of great novelty.	0
pros good technical enhancements that fix some issues of a popular metalearning framework cons little conceptual and technical novelty [originality] the major problem i found in this work is the lack of conceptual and technical novelty.	2
the motivation of the work is not clear but the novelty seems to be present.	0
the novelty is to be honest incremental and thus below the bar of iclr.	0
pros well written good empirical results cons little novelty.	2
i have concerns about the novelty of this method.	0
weaknesses: 1) i think the main weakness of this paper is its lack of any significant novelty, or inadequate coverage of state of the art in deep learning based flow estimation.	0
overall, i think the paper has some interesting results, however the method is unconvincing/unclear why it should work nor its novelty commendable.	0
novelty: the technical contribution is a very simple extension of simonyan et al. 2013. the main novelty lies within the created dependency graph from the node importance weights, but the usefulness of such graph is unclear.	0
() lack of comparison with recent i2i models () lack of experimental results and ablation studies () unclear novelty 2. major comments  the novelty of this paper is not clear.	0
my main concern is that the novelty from a machine learning and reinforcement learning point of view remains limited while the application seems original and promising.	0
cons:  my main concern with this paper is regarding the novelty.	0
my major concern is about the novelty of this paper.	0
2017. the author combines this two ideas together, which is incremental in terms of novelty.	0
however, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.	0
weaknesses: (1) novelty: i would say the novelty of this paper is rather limited.	0
"cons:  novelty is mainly incremental minor comment:  use a bigger picture for fig. 1  in page 1, introduction, paragraph 2, line 10, ""due the longdistance ..."" ==> ""due to the longdistance ..."" '''''''''' i would like to thank authors for their feedback."	0
after reading their feedback i still believe that novelty is incremental and would like to keep my score.	0
novelty/impact  creation of a new dataset on a new and interesting problem  useful comparison of modern networks on the task  gapnet  lacking technical novelty, insight, and performance is unconvincing  demonstrates that endtoend learning outperforms cell centric approach  was this really surprising or even new information?	0
however, the paper still lacks enough content and novelty, in my opinion, to warrant acceptance.	0
a response addressing my concerns above and emphasizing the novelty of these results for neural networks may push me the other way.	0
the proposed method does not provide considerable technical novelty or insight to compensate for the lack in motivation.	0
overall, due to the lack of novelty and unconvincing results, i feel the paper needs more work before it is ready for publication.	0
in addition, while lacking novelty may be fine in and of itself, the purpose of applying these ideas doesn't have a focused purpose.	0
the novelty of either of those techniques is quite limited as individually they have been suggested before, but the combination of both of them is interesting.	0
the paper is overall well written, however there are two aspects that make the contribution lacking in novelty.	0
my main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.	0
the novelty of this paper seems reasonably high but my impression is that other/stronger baselines would make the study more convincing.	0
comments: 1. i think this paper lacks technical novelty.	0
overall, this is a good work with limited novelty but solid results.	0
i upgrade my score to 6. as authors mentioned, the extension to density estimators is an original novelty of this paper, but i still have some concern that oe loss for classification is basically the same as [2].	0
while the research topic of this paper is interesting, i recommend rejections because i have concerns about novelty and the experimental results.	0
"while i think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what i think the authors need to do to improve the work: (1) the authors emphasize novelty, and being ""first"" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses."	0
cons: 1. one of the main issues of this paper is the lack of novelty.	0
the paper is well written but the evaluation and technical novelty is weak.	0
i do not necessarily see something wrong with the paper, but i'm not convinced of the significance (or sufficient novelty) of the approach.	0
however, the major weakness of the paper is the lack of novelty of the core idea.	0
the reasons for my recommendation, after discussion with other reviews, are  (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above).	0
this, along with my earlier point of lack of novelty are the basis for my decision.	0
the technical novelty however is limited.	0
cons: 1) while the authors tend to sell the idea for both zeroshot (zsl) and fewshot learning, the novelty to zsl is actually much small.	0
cons:  lack of technical novelty and justification of the approach.	0
cons ' this paper does not provide a coherent technical novelty.	2
given that, the novelty of the paper is fairly incremental as it uses nervenet to evaluate fitness and es for the main design search.	0
i have no doubt to the novelty, but the writing could definitely be improved.	0
this does not alterate however the novelty of the approach.	0
weaknesses:  the main weakness is the limited novelty of this paper.	0
the total idea is good, but the novelty is not much.	0
overall, this is a well executed paper, however i am not sure about the novelty of contributions made in the paper.	0
it would be great if there are more direct use of the proposed lower bound, but i appreciate the novelty in this paper on bridging the two subfields.	0
rephrase the novelty argument:  the authors argue to present a “novel attention mechanism” but the attention mechanism used is not new (bahdanau 2014 a & b).	0
this is on the whole a great thing, but perhaps not especially surprising from an originality/novelty perspective.	0
at the same time, i have some concerns about the novelty and the presentation.	0
of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in ying et al. 2018 and simonovsky and komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting.	0
i agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples.	0
the results on lowresources supervised domain adaptation indicate that the method works better than the that of motiian et al. 2017. weaknesses:  novelty is limited: the two algorithms are essentially small modification of the semantic consistency term used in hoffman et al. 2018. they involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other.	0
as the argument is that the proposed loss is better than the reconstruction one and that of hoffman et al. 2018 for lowresource supervised adaptation, it would be worth demonstrating this empirically in table 2. summary: the proposed objective functions are well motivated, but i feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at iclr.	0
however, i do have some concerns, as follows: 1) the novelty of the method is questionable.	0
however, in terms of the proposed architecture, it seems to lack some novelty.	0
while this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective.	0
however, i still have many concerns regarding the notation, the novelty of the paper, and the comparison with related literature, especially on previous overlapped span detection ner papers.	0
it has some elements of novelty but not yet there.	0
as i said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified.	0
my main concerns are novelty of the proposed method, and fairness of experiments.	0
however, i still have concerns about novelty and experiments.	0
the authors want to tell a more motivated storyline from nestrovedualaverage, but that does not contribute to the novelty of this paper.	0
1. my main concern is on the novelty of this paper.	0
2. no need to train the encoder 3. good results on link prediction tasks cons: 1. lack of novelty.	0
the overall technical contribution is incremental and may not have enough novelty to be published in iclr.	0
however i think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model.	0
another concern i have is about the novelty.	0
based on the method, this paper could go either way, but given my concerns on its novelty and writing quality/clarity, i lean slightly towards reject.	0
that is an interesting and important question for the community, but maybe iclr paper is not the best format to present such kind of novelty.	0
overall, the paper proposes a new ppl that is an important direction of study, but have several drawbacks and conference paper format is not the best way to present such kind of novelty.	0
cons 1. the authors do not compare their approach to that of https://arxiv.org/pdf/1711.00455.pdf , both in terms of conceptual novelty and in terms of experimental results.	2
novelty: the proposed approach seems incremental and lacks novelty.	0
significance: given the lack of a rigorous evaluation framework and the lack of novelty of the proposed methods, i believe the significance of the contribution is very low.	0
i do not know of another paper using a similar proof, but i have not studied the proofs of the most closely related papers prior to doing this review, so i have limited ability to vouch for this paper's technical novelty.	0
however, there are a couple of issues that need to be discussed: ▪ the reported performances represent only a limited improvement over the comparing baselines, indicating that the proposed model is promising but it is still immature ▪ the model is sharing many characteristics with (referenced) published methods, which the proposed algorithm is a smart combination of  thus, overall, the novelty of the introduced method is somewhat limited.	0
while the presented ideas are well motivated and it is certainly a good idea to combine deep rl and evoluationary search, novelty of the approach is limited as the setup is quite similar to the erl algorithm (which is still on archive and not published, but still...).	0
however, since the novelty is not so much in the graph convolution method, or in the use of graph methods for treating spherical signals, but in the combined application of the particular graph method proposed to the domain of omnidirectional images, i would expect a more thorough experimental study of the merits of the method and architectural choices.	0
main concerns / comments are:  part of the novelty relays on computing the hessian, and the algorithm goes for very large networks (parameter wise), why?	0
scalability is main advantage of the proposed method, but the authors just employed known nearest neighbor approximation methods, and thus here no technical novelty is shown.	0
main claim of the paper would be in section 3, but the novelty would be weak as mentioned above.	0
such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.	0
without stronger comparison to other closely related work, and lack of citation to several closely related models, the claim of novelty isn't defined well enough to be useful.	0
my primary concern is in relation to related work, clarification of the novelty claim, and more comparison to existing methods in the results tables.	0
i appreciate the authors for providing so much detailed experimental results to the community, but this paper lacks novelty in general.	0
although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, i think that the paper is in dire need of a thorough cleanup before being published.	2
however the novelty is limited and not well explained.	0
my main concern is about the novelty of the approach which looks mostly incremental as a rather direct extension of the robust model (wong & kolter 2018) to costsensitive setting.	0
cons:  the novelty of method is mostly incremental given the prior work of (wong & kolter 2018).	0
the novelty of the submission is however limited and highly similar to current methods.	0
"sec 4.3: there is a reference to nonexistant ""appendix x"" pros: ' overall approach is sound and achieves its objectives cons: ' small amount of novelty; primarily an application of established techniques"	2
although this problem seems interesting, here are several concerns i have: 1.novelty: the overall framework is still conditional gan framework.	0
cons:  in my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the vib from [1] for discriminators in adversarial learning, with the difference of using an adaptive bêta.	0
cons: 1. the contribution and novelty of this paper is incremental and somehow limited, since the analysis ttrnn based on the product nonlinearity already exists, which make the contribution of this paper decreased.	0
o this significantly limits the novelty of the method: it is not ‘an intialization’ method, but a combination of initialization and normalization, which differ from previous ones in some details.	0
however, given cogswell et al. (2015) or xiong et al. (2016), it seems novelty is somehow incremental (i could recognize that this work is different in the sense that it considers local/importance based weighting as well as penalizing correlation based on l1 norm).	0
in term of novelty, the proposed method seems quite incremental.	0
however, my only big concern is about the limited novelty of the method.	0
strengths: ' nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers ' each choice in the model definition are motivated, although some clarity is still missing (see below) weaknesses: ' novelty: the proposed model is a small extension of a previous work (wang et al., 2017) # 2. clarity and motivation in general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation: a) page 2 “unlike the conventional memory transition function, it learns the size of temporal interactions.	2
# 3. novelty novelty is the major concern of this paper.	0
from the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.	0
also add citation (wang et al., 2017) since it ie the same model of that paper  # postdiscussion i increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.	0
# novelty i'm not sure about the novelty of the paper, but i suspect it to be rather incremental.	0
weaknesses the field of metalearning variants is crowded, and this paper struggles to carve out its novelty.	0
it strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.	0
(5), (6) in my review i provided specific, objective criteria by which i have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the dice paper.	0
"negative:  unclear novelty: previous work also gives the goal of preserving input meaning in attacks, even if the attacks themselves do not preserve meaning effectively (ie zhao et al)  unclear attack effectiveness: the chrf scores for charswap and knn methods have higher chrf scores than the ""unconstrained"" method, but it is unclear what this means in context."	0
overall, the software package seems to provide nice functionality with integration into a currently popular machine learning framework, but it’s novelty compared to existing software packages is limited.	0
the main concern of the package is on novelty.	0
weaknesses: ' the novelty of the method falls a little short for a fullscale conference paper.	0
however, my main concerns are: (1) the novelty of the main theorems given the literature, and (2) the carefulness of stating what is known in the literature review.	0
but based on all the weakness above and lack of novelty, i think the paper should be rejected for now.	0
while this approach is reasonable indeed, the novelty of the approach is questionable, not only in light of recent papers but older literature: inference of markov decision processes under constraints is referred to and has been known a long time.	0
cons '''''''  the contribution and novelty of the paper is relatively limited other than the use of shapley value.	2
this paper lacks any novelty/contribution as it just applies wellknown and standard architectures for object detection (ssd) and image classification (lenet) trained with standard algorithms and losses.	0
weakness: 1. the paper is lack of novelty and the motivation is weak.	0
my main concern with regards to the paper is novelty.	0
weaknesses:  novelty/originality is rather incremental.	0
my main concern is the novelty of the paper.	0
the surrogate loss to learn the features coupled with a linear policy evaluation algorithm appear to be novel, but does not warrant, in my opinion, the novelty necessary for publication at iclr.	0
however, the novelty is lacking as it appears to be training using a multiheaded approach (which exists) and the convergence results appear to be a straightforward application of borkars twotimescale proof.	0
the proposed approach is new, but the technical novelty is marginal.	0
the paper strikes me as a valuable contribution once the detail of the experiments are addressed, but personally i am not sure that whether the novelty of this paper is enough for the main conference track.	0
the originality seems incremental in both directions.	0
cons: originality.	0
originality  this is one of a small but growing number of papers connecting deep nets to ode solvers.	0
my main concerns were: (1) the originality, and (2) the results are not convincing.	0
my concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of guo etal 2018, which this paper says is the motivation behind their work.	0
== originality == the idea of matching features/representations across the source domain and target domain is an old idea, but it is executed in an interesting new way in this paper.	0
pros:  originality of the approach cons:  experiments could have been more convincing:  should compare against at least one other stateoftheart domain adaptation method  results on dependency parsing (the most challenging task they consider) were mostly negative  evaluation on other more recent multidomain nlp tasks would have been nice (e.g. multinli)  abstract and intro could provide better description of the conceptual contribution, as well as motivation	2
"the originality of the work is low, because approaches that propose to replace the nondifferentiable threshold function with a differentiable proxy, in this case membrane potentials, have been known for several years, e.g. ""spiking deep networks with lif neurons"", eric hunsberger and chris eliasmith, 2015, and ""training deep spiking neural networks using backpropagation"", junhaeng lee et al. 2016. the main novelty is therefore the application to autoencoders, but overall this reduces originality and hence relevance for iclr."	0
cons:  low originality and missing comparison to related work  unconvincing experimental section minor remarks: ' the title is misleading in the sense, that the spatial part of the spatiotemporal representation is not related to actual space, and further, that the backpropagation is not spikebased ' the figures appear blurry, and in the case of fig. 2 is very hard to read. '	0
i think a higher score is justified if the authors address the following points:  relation to previous work, originality in contrast to what the authors claim, what is predicted here is not exactly the reaction mechanism, but an implementation of the principle of minimal chemical distance, which was already described by ugi and coworkers in 1980 [see jochum, gasteiger, ugi, the principle of minimum chemical distance angew.	0
the idea is simple and seems to be promising, however originality seems incremental.	0
originality and significance: the originality of the paper is relatively low (optimization of an existing algorithm) and the contributions are incremental.	0
therefore, the originality is limited and incremental.	0
since the paper proposes just to encode history with a rnn, the proposed solution lacks of originality, and the contribution of the paper in term of model is quite low.	0
major concerns: there are serious issues with the quality, clarity, originality and significance of this work.	0
(more questions below) would like to have heard the listening tests; or at least read more about how samples were selected (again, perhaps in an appendix and additional sample files) quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).	2
while the paper is clear and well written, i have some concerns about it's quality, originality and significance.	0
cons:  this work fairly lacks its originality since the proposing method heavily relies on the two previous works, selfattention and topk restriction.	0
the majority of the work is clear and wellwritten, and appears to be correct, but i find it lacking in originality.	0
originality  the casting of the problem as domain adaptation is original but from the experiments it was not conclusive as to how much benefit we get.	0
however the modeling techniques being used are lacking in originality.	0
rating of the paper in different aspects ( out of 10) quality 6, clarify 5, originality 8, significance of this work 5 pros: 1. the paper proposes a generic discrete distribution as the variational distribution to run inference for a wide range of models.	2
i am not able to judge if there is significant originality in the theoretical results (theorem 1  corr 12) but if i am not mistaken it is more or less an application of a known result to this particular setting?	0
weaknesses of the paper:  the originality of the approach is limited because it is a relatively straightforward combination of existing techniques for weight and activation pruning.	0
the principal issues i see are as follows:  the originality of the contributions is not clear  missing theoretical discussion  the experimental setup is terse and slightly confusing concerning the originality of the paper, the differences to gama et al., 'diffusion scattering transforms on graphs' are not made clear.	0
overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues).	0
pros good technical enhancements that fix some issues of a popular metalearning framework cons little conceptual and technical novelty [originality] the major problem i found in this work is the lack of conceptual and technical novelty.	2
the results reported are also reasonable within the metalearning context, despite lack of originality.	0
[significance] the significance of this work is marginal, given the lack of originality.	0
[cons] [originality] one major weakness of this paper is its lack of originality.	0
originality the originality of the paper is not stellar, but sufficient for acceptance.	0
the major concern about the paper is the originality of the method.	0
i mainly have one concern regarding its originality.	0
cons: would appreciate greater discussion of the originality of the results; in particular, a more upfront discussion (which is currently concisely presented in the supplementary) regarding algorithms that are similar to layca when less crucial steps are dropped, e.g. yu et al 2017 and ginsburg et al 2018.	0
originality: (6.5/10) as far as i know the idea of parameterized thresholds for relus is original, as is the partial replacement strategy, but i am not an expert in this area.	0
this is on the whole a great thing, but perhaps not especially surprising from an originality/novelty perspective.	0
again, i am not an expert, and i will indicate that in the system of course, but while i cannot completely judge all aspects of the technical relevance and the originality of the approach, i am fairly convinced that the paper deserves to be substantially revised before it can be accepted for publication.	0
quality: strong, clarity:strong, originality:strong, significance: strong, pros:  the paper provides strong theoretical results.	2
originality  while the authors achieve soa results on omniglot and do explore a few options, i feel the work still lacks originality in the formulation or does not have original contributions to either the architectures used or the optimization procedures employed.	0
quality: good, clarity: good, originality: good, significance: good pros:  the paper is fairly well written and the idea is clearly presented  to the best of my knowledge (maye i am wrong), this work is the first one that provides theoretical explanation for the tradeoff between accuracy and robustness  the visualization results supports their hypothesis that adversarially trained models percepts more like human.	2
although the empirical results show the proposed model outperforms several existing models, my concern is still on the originality of the paper.	0
however the originality and significance of this work are weak.	0
quality: borderline, clarity:good, originality: borderline, significance: good, pros:  the benchmark is set up in a online platform with leaderboard which can be easily accessible to people.	2
weaknesses:  novelty/originality is rather incremental.	0
pros:  the proposed approach is interesting and novel  i've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored)  the presented results are quantitatively and qualitatively better compared to a direct prediction baseline  the paper is generally well written, and interesting to read cons: while the method is interesting, it is apriori unclear why this works, and why this has been only explored in context of linear inverse problems if it really does work.	2
the reconsideration of floydwarshall rl in the context of deep neural networks is a refreshing idea and seems worth investigating, but i would need to see much more careful analysis before i could recommend this for publication.	0
5. overall, i think this is a good paper, gives a good overview of an important problem; the matching networks idea is nice and simple; but the paper could be more broader in terms of writing than trying to portray the success of discern specifically.	0
comments: pros:  the idea of heating up the temperature in softmax is interesting, and seems novel in the literature of metric learning.	2
although the reviewer likes the idea of heating up softmax, this paper can be judged as a borderline slightly leaning toward reject, due to the above weak points, the details of which are explained as follows.	0
strengths: the idea of using the hierarchical structure of the labels is innovative and wellmotivated.	2
cons:  the idea is a combination of already known techniques put in practice for the vae.	0
pros: ' nice model and model formulation  learning the basis functions both for the low and high frequency is a nice idea.	2
so, the overall the paper has some new ideas, but is not highly novel compared to previous work.	0
the most interesting contribution is the query as kernel approach: however the concurrent submission “pay less attention with lightweight and dynamic convolutions” obtains better empirical results with a similar idea.	0
pros and cons 1) the idea of an approximate whitening layer is conceptually simple and clear.	2
concerns:  the idea of using patches in domain adaptation is not completely new.	0
the idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence.	0
strengths:  modeling the structure via patches is an interesting idea.	2
weaknesses: method:  the idea of relying on patches to model the structure is not new.	0
overall the generic idea of “quantile regression rl” from section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multiobjective statealigned rewards) makes it more a distraction than an asset.	0
the idea of having a separate class for outdistribution is a very interesting idea but unfortunately previously explored.	0
this is an interesting point of view but the manuscript lacks discussion on several important questions: 1) how is this idea related to regularization?	0
in summary, the paper has a novel idea, but has to be better developed in its empirical part.	0
evaluation: pros:  the idea of the proposed approach is interesting: using variational inference for binary weight neural networks.	2
in summary, i think this paper contains some reasonable results based on a reasonable, moderately novel, idea, but unfortunately, it is not yet ready for publication.	0
i have multiple concerns with the papers: (i) the writing is informal and the ideas are not well explained.	0
(ii) the idea of reaching as many states as possible has been explored in count based visitation (bellemare et al, tang et al) — but no comparisons have been made to any previous work.	0
"3. the authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm."	0
pros:  the idea of evaluating the network capability of memorization for estimating the quantization of the parameters make sense to me and is novel in my knowledge.	2
overall evaluation: the proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand.	0
bottomline: the paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated.	0
overall i find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work.	0
i think this is an idea worth exploring but overall i would not recommend acceptance.	0
pros and cons () interesting idea () diverse experimental results on six datasets including benchmark and realworld datasets () lack of related work on recent catastrophic forgetting () limited comparing results () limited analysis of feature regularizers detailed comments  i am curious how we can assure that svm's decision boundary is similar or same to nn's boundary  supportnet is a method to use some of the previous data.	2
pros  idea of using svm to identify the most important samples for classification makes sense.	0
cons  the idea of storing a small subset of a original dataset for each task has been already explored in [nguyen et al. 18], and thus is not novel.	2
the technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes nonnegativity in a vae setting, but i think this is a good idea.	0
the idea is simple but intuitive.	0
the idea about intent embeddings for zeroshot learning is not fully original (chen, et al., 2016), but this paper extends to both intent classification and slot filling.	0
given the lack of theoretical support for this idea, i would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.	0
pros:  interesting idea, nice results, mostly readable presentation.	2
strengths: the main idea, to match up angles between points on the hypersphere and hamming distance is pretty clever.	2
verdict: i think this work contains a great main idea and could become quite a good paper in the future, but the work required to illustrate and demonstrate the idea is not fully there yet.	0
pros  interesting idea to improve the performance of fewshot learning.	0
the highlevel idea is quite interesting, but the paper itself is quite a long way of convincing me that this is actually a good approach.	0
in overall, i think the proposed idea is interesting, but the authors’ motivation from dpp is arguable.	0
"pros:  a simple idea  encouraging experimental results cons:  confusing read  no clear intuition is given  restricted to lowdimensional datasets  strong baselines needed  the plots are too small to see (impossible to see when printed) other comments:  the authors are using the term ""feature vector"" to refer to a data point."	2
if authors studied how to break down the teacher or student model in a principled way, it could have been a strong contribution but the current version does not differ much from fitnets idea.	0
in that regard, the idea is not brand new, but feels wellapplied in this setting.	0
pros:  proposed idea is novel and proposes an interesting change to existing embeddingbased fewshot learning techniques.	2
the paper is well written and presents a simple, but promising idea to simplify reinforcement learning methods.	0
further, approaches based on optimism in this way go back to lai and robbins (1985) and are not necessarily based on hoeffdings inequality, but rather the more general idea of a highprobability bound on the value.	0
strengths:  the authors develop their idea in close connection to commonly used architectures.	2
quality: paper is technically complex, but based on simple ideas.	0
overall, i think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.	0
the idea of extending riemannian langevin dynamics to functional spaces is elegant, however it is extremely hard to follow the proposed method as details are kept to a minimum.	0
however i think this idea has been explored before with more convincing results and with a more principled approach.	0
overall, my view is that the idea is good, but somewhat small, and it hasn't really yet been proven to make a big difference.	0
overall, i think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough.	0
this idea is not novel but the authors report that there is no previous work that jointly trains sentiment aware embeddings with a sentiment classifier specifically, and makes use of an unlabeled corpus to improve both.	0
pros:  improving joint training of nondifferentiable pipelines is a meaningful and relevant problem  using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea cons:  the main result of the paper concerning sufficient conditions for optimality of the method seems dubious  it is not obvious why this method would outperform simple baselines, and baselines for joint training were tried  the notation seems unnecessarily bloated and overly formal  the exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing the submission describes a method for smoothing a nondifferentiable machine learning pipeline (such as the fasterrcnn detector), so that gradientbased methods may be applied to jointly train all the parameters of the pipeline.	2
the idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [a]) as well as the simple baselines i have mentioned.	0
both max pooling and jaccard distance are not something new, but the author did a great job presenting the idea and proved it's effectiveness through extensive experiments.	0
2. the idea of enforcing a content loss is well motivated but the approach considered seems rather heuristic.	0
pros: the idea behind tracing the partwhole assignments back to primary capsule layer is interesting and original.	2
the idea however is not quite novel.	0
the idea is an interesting one, but this work is at too much of a preliminary stage for a toptier conference such as iclr.	0
the general idea and motivation are generally appealing but the experimental validation is a mess.	0
my concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of guo etal 2018, which this paper says is the motivation behind their work.	0
strong points:  this paper is a novel extension of ideas from single agent rl to multi agent rl, there are clear benefits from doing reward shaping in the right way to make deep rl work better.	2
strengths:  the idea of dividing the hidden states into two parts is interesting as it helps the model to control the effect of unsupervised loss on main supervised task.	2
datasets like mnist and cifar can be used to show that the proposed idea is working as a proof of concept, but it might be difficult to draw a generalized conclusion from the results in the paper like the use of unsupervised loss during training helps.	0
however this has been done by a number of authors:  vlachos and clark (2014): http://www.aclweb.org/anthology/q141042  berant and liang (2015): https://nlp.stanford.edu/pubs/berantliangtacl2015.pdf while the idea of using such oracles for structured prediction tasks in nlp was first proposed by daume iii et al. 2009: https://arxiv.org/abs/0907.0786 furthermore, it has been applied for rnn decoding in nlp, see: https://arxiv.org/abs/1511.06732 apart from this, some further comments:  the subset of sql tackled in this paper is less expressive than what has been done in previous work on atis and geoquery datasets.	0
in conclusion, the paper has a novel idea i like, it is explained clearly, but the work has to mature a bit more in terms of empirical work and interpretation of results.	0
overall, i found the writing very clear, the main idea sound, and paper generally well executed, but i have serious concerns about the significance of the contributions that lead me to recommend rejection.	0
my new rating is 5. i like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:  i expected more complexity in vision and language.	0
pros:  the paper is well written  the idea of fully avoiding float32 is intriguing, but it would be much more interesting if it can be done also at training time.	2
overall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).	0
and many others the main idea of the algorithm is clear, but the description of the pieces is missing.	0
== originality == the idea of matching features/representations across the source domain and target domain is an old idea, but it is executed in an interesting new way in this paper.	0
pros: ' interesting/intriguing idea ' applicability discussed through 3 different examples cons: ' gaps in explanation ' exaggerated claims ' problems inherent to the proposed technique are not properly addressed, brushed off as if unimportant the idea of pvars is potentially interesting and worth exploring; that being said, the paper in its current form is not ready for publication.	2
some criticism/suggestions for improvement: while the idea may be appealing and worth studying, the paper does not address several problems inherent to the technique, such as:  overheads (computational cost for inference, not only in prediction/inference time but also all resources necessary to run the rl algorithm; what is the memory footprint of running the rl?)	0
the propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used.	0
this is true for the idealized flow in continuous time, but hmc with finite step size does generally not converge to the correct distribution.	0
pros: the idea seems to be original enough, simple and easy to implement.	2
it adopts a simple but novel idea on sisr and shows clear improvement against existing method (e.g., srgan).	0
another major concern (technical contributions): how is the idea of randomly dropping weights different from deep expander networks (prabhu et al., eccv 2018)?	0
main idea: this paper studies a problem of the importance weighted autoencoder (iwae) pointed out by rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network.	0
pros: this is a neat idea and achieves competitive results.	2
overall, the paper presents some interesting ideas but the proposed approach seems overcomplicated	0
pros  the idea of using adversarial subspaces to characterize a task is a novel idea which seems to work to some degree.	0
strong points: it is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.	2
weak points: 1) although the ensemble idea is new, the idea of selective selftraining is not novel in selftraining or cotraining of ssl as in the following survey.	0
positive aspects   the idea of applying a kalman filter in a latent space is interesting.	2
in conclusion, i think the paper presents a nice idea but it requires more work in order to pass the iclr acceptance threshold.	0
i think that this approach might be motivated in the very early layers of a vision network, but can hardly be a good idea in higher layers, since they require increasingly specific features for a small subset of the overall imagespace to be learned.	0
the idea of using polynomial transformation is not new, as the author(s) note(s), but in this setup it is backed up with neither with intuitive arguments nor with decent experimental results.	0
i don't mean to argue either way, but if the paper presents a viewpoint which is quite different from the commonly accepted viewpoint (within the specific research field), then there needs to be a much deeper explanation, ideally with concrete evidence to support it.	0
pros:  proposed a set of cascading q functions, to learn a recommendation policy  unified minmax optimization to learn the behavior model and the reward function  interesting idea of using generative adversarial networks to simulate user rewards.	2
"pros:  the idea of developing new methods for viewpoint and video synthesis that allow for ""jumpy"" and ""consistent"" predictions is an important problem."	2
strengths: 1. the idea of nestedmeans clustering is interesting, which somehow shows its effectiveness.	2
thus i think the proposed idea is a little bit incremental.	0
the key idea in this paper (using principal shared directions of perturbations, computed on a small subset of data points) has unfortunately already been proposed and tested in classical (nonequivariant) neural networks  see for example fig 9 in moosavidezfooli, 2017, cited in the paper, and published in cvpr 2017. the present paper proposes however a few additional bits of information with a nice theoretical analysis, while the previous works were mostly based on heuristics.	0
in general, the paper has a nonclassical organisation, with a lot of heuristics that are not discussed in depth  that gives a sort of highlevel impression that the proposed idea is potentially nice, but that but superficially addressed.	0
pros: • the over idea appears to novel, despite its connections to various previous attempts to angular separation (hasnat et al., 2017; liu et al., 2017a; wang et al., 2018; zheng et al., 2018).	2
it's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.	0
"the core novel idea here is the introduction of the ""memory"" block into the wakesleep algorithm, but this is only briefly described."	0
the idea of using hard distributions to train the model is reasonable, but not extremely interesting/exciting to me since this is by now a standard idea in tcs.	0
the idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules.	0
overall, i think this is an interesting idea in a very important area, but one that is not quite ready for publication.	0
quality: the ideas are generally of high quality, but i think there might some typos (or at least some notation i did not understand).	0
given stateoftheart in this topic of regressions over networks, embedding the idea into a recurrent neural setting is novel, but only marginally so.	0
"pros:  interesting idea  reasonable approach by combining existing building blocks cons:  too much focus on stft vs. cqt  too little focus on getting wavenet synthesis right  too limited experimental validation (too restricted choice of instruments)  poor resulting audio quality  feels too much of combining black boxes as a result i rate the paper ""not good enough"" in its current form."	2
the paper considers sgd with a scaled norm; in the nonstochastic case (first equation in section 2.1), it is gradient descent in a fixed noneuclidean norm, but it is the stochastic case that is most interesting.	0
"as for why use the laplacian, prop 8 (combined with prop 6) gives some idea: that we lower the variance, without cheating (ie., we could trivially lower the variance by just multiplying by a small number, but because the operator preserves the sum of the components, it is not ""cheating"")."	0
al. cvpr 2018) with gans, but exploit a simpler idea based on the fusion of two images directly into the image domain with weights generated by a network.	0
none of these ideas is fundamentally new, but the descriptions and their combination is very nice.	0
please give a self implemented baseline method with same features but some standard loss in the shared space to give an idea of the strength of the features.	0
however, the idea is very related to yeh et al.’s work which has already published but not mentioned at all.	0
pros:  interesting idea  experiments are interesting cons:  formal results are either trivial or could be improved in their statements  experimental guarantees only, up to what is hidden in the bigoh notations of theorem 2.2, 2.3. details: ' in theorem 2.2, you need to remove the , unless you point to the taylor theorem that guarantees that for the identity you claim before (5).	2
overall the idea is promising, but the work isn't ready for publication.	0
furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold 'without' the wasserstein distance, but with lipschitz continuity are already elaborated in the wgan paper.	0
() decomposition of r and theta in eq.(1) looks interesting, but there is no supporting ground to grasp the implicit meaning of this idea.	0
the paper proposes an interesting idea, but it does not provide any insights on why it works or why the authors did like this.	0
the idea is simple and seems to be promising, however originality seems incremental.	0
the proposed approach is common a sense heuristic, however no theoretical justification of the idea is presented in the paper.	0
the main idea is to not only train text embedding using context from the same sentence but also take the embedding of the surrounding sentences into account, where the sentence embedding is also contextaware.	0
strong points: (1) the idea of decomposing problems into concepts is interesting and also makes sense.	2
concerns or suggestions: 1. the main idea of using contents to represent a problem is quite simple and straightforward.	0
originality novelty: there is a large body of work on disentanglement that the paper does not cite or compare to for instance, infogan, beta vae https://openreview.net/pdf?id=sy2fzu9gl and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdf note that for example that in beta vae it is a similar idea where but it is on z and z|x and the distance used is kl (since it is has closed form with gaussian) , min_phi loss beta kl (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.	0
pros:  causality and interpretability are major directions of research  seems like a valid contribution on an interesting problem cons:  the highlevel picture is relatively clear but i find important things very difficult to grasp  the kolmogorov model definition i find confusing but i am not an expert in causality (the introduction should give some intuition about what that is and why it is a good idea).	2
each piece of the algorithm seems to draw heavily on previous work; biclustering, diffusion maps, but overall the idea is novel enough.	0
pros: (1) the paper is well written and easy to follow; (2) evaluations look reasonable and fair cons: (1) the idea of using gan for active query synthesis isn’t new.	2
my main concern is the limited technical novelty and evaluation:  the main idea of the architecture is extending 2d convolutions in image compression networks to 3d convolutions, and use skip connections for multiscale modeling.	0
the current version is okay but not very exciting for idea selling.	0
the idea of having a sparse prior in latent space is indeed relevant, the approximation and reparameterization of the spike variable is however functionally appealing.	0
this paper presents an incremental extension to the selfimitation paper by oh, junhyuk, et al. the previous paper combined selfimitation learning with actorcritic methods, and this paper directly integrates the idea into the generative adversarial imitation learning framework.	0
i think the idea is interesting, but there remains some issues very unclear to me.	0
the paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in pathak et al. but in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.	0
the idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before i can recommend acceptance.	0
overall the idea is interesting but the way the structuring element is learned should be discussed in more details and exemplified visually.	0
as the authors will probably agree i am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art.	0
i think the idea of the model is interesting, but i have some concerns about the motivation and the soundness of the model.	0
the resultant algorithm, which gives bonus to poorlyestimated rewards is a good idea.	0
a related approach for dimensionality reduction is also proposed, but i think this only applies to euclidean data, so i am a bit confused about that part.	0
quality: the introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing. '	0
strengths:  the proposed idea is simple and intuitively appealing, and shows much better results than the dqn baseline.	2
summary: this is a simple and intuitively appealing idea, but i find the evaluation to be quite lacking because the tasks already use a language specification (such that actrce seems to be vanilla her in application) and there are no comparisons to previous work.	0
weaknesses  the idea of utilizing backpropagated sensitivity map is not novel for weaklysupervised object localization [1,2], as the proposed method just uses a simpler sensitivity function with linear regression.	0
the idea of utilizing classification network for localizing an object is new, but the ideas of weaklysupervised object localization is already explored in [1,2,3,4,5,6].	0
weaknesses:  the bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4  as the authors point out, the idea of using a batchnormalization like strategy to set an adaptive learning rate has already been explored in the wngrad paper.	0
"cons:  details are lacking about the ""anon"" dataset introduced in this paper (where do the photos come from and the labels, visualization of a few examples...)  there are not many technical issues discussed in the paper and that is fine as the main idea is relatively simple and its effectiveness is mainly demonstrated empirically, but i feel the paper is missing a discussion about the importance of the initial classifier trained to estimate the target prior probabilities for the source labels and whether it is crucial that it has a certain level of accuracy etc.  the approach in the paper implies a practitioner should have access to a very large target dataset and the computational and time resources to appropriately pretrain a complex network for each new target task encountered."	0
the idea is intuitively sensible, but i believe this paper falls short of being ready for publication for three major reasons.	0
also similar idea could be found in many other papers, e.g., wang et al. highresolution image synthesis and semantic manipulation with conditional gans, cvpr' 18. the authors should 1) clarify why this paper makes nonincremental contribution?	0
cons: this is a nice idea and some preliminary experiments show positive qualitative results.	0
the basic idea behind knockoff is to generate artificial input feature vectors, (i.e. knockoffs) that are independent of y, when conditioned on the real feature vector x, but after swapping arbitrary elements with x, are distributed as x. sets of associated features and fdr estimates are obtained by contrasting suited feature selction criteria that measure associations of knockoffs and real features with the target y. lasso coefficients and random forests are used in the paper.	0
i might miss something, but i feel quite confused about what is the main idea after reading the paper.	0
using a generative model as the surrogate distribution for kernel twosample test is novel  an important and new application of deep generative models  strong experiments on synthetic and realworld time series data sets  very clear writing and explanation of the idea  reply sample segments from both directions (past and future) while in the practical setting, cpd is usually sequential and in one directional  lack theoretical understanding of the limit of the neuralgenerator in the kernel twosample test	0
recently there has been a spate of work on generalized cnns that are equivariant to various symmetry groups, such a 2d and 3d rotations, the corresponding euclidean groups (comprising not just rotations but also translations) and so on.	0
strengths: the challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.	2
update: i feel the idea of this paper is straightforward, and the contribution is incremental.	0
overall, while i think the paper contains interesting ideas, i find the current evaluation lacking.	0
i really liked the idea behind the pooling operation: it is simple, seems easy to implement efficiently, and generally makes sense (although see concerns below).	0
the idea is incremental to multitask learning and enable, in a single framework, to validate intermediate features for additional related tasks.	0
the main idea may be okay, but the quality of the paper is below standard of the conference.	0
the idea proposed in this paper is to aid in understanding networks by showing why a network chose class a over class b. to do so, the goal is to find an example that is close to the original sample, but belongs to the other class.	0
1. the authors used mig throughout section 4. but i have no idea what it is.	0
some experiments seem to follow this idea such as 'mdnconad{2, 4, 8, 16}' in table 2. but in these experiments the addition of conad offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16).	0
as i mentioned, the paper has novel and interesting ideas, but it would be greatly improved with some important rewriting.	0
pros:  interesting idea.	2
therefore, the idea from the proposed model is similar to li and jurafsky (2015), because the sense selection is not onehot but a distribution.	0
this idea is simple and straightforward, but the evaluation is not convincing.	0
pros:  it is an interesting result that adding a weak visual imitation loss dramatically improves rl with sparse rewards  the idea of a visual imitation signal is wellmotivated and could be used to solve practical problems  the method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up rl in practice cons:  it seems possible that imitation only helps rl where imitation alone works pretty well already  some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal gail  the presentation borders on handwavy at parts and may benefit from a clean, formal description the submission tackles a real, wellmotivated problem that would appeal to many in the iclr community.	2
i don’t think it is necessarily a bad idea to have another discriminator for the goal, but this part seems somewhat out of place.	0
the idea is interesting but i have a number of concerns.	0
this is probably a good idea, but it does not sound all that creative to me, compare with, e.g. an ar model.	0
this paper explores the idea that minibatch sgd rarely 'crosses' barriers during dnn optimization, but rather uses a 'seemingly' or 'alternate' mechanism, as the authors somewhat mysteriously call it on the first page.	0
this happens not just once but all the time... pros:  two nice little ideas; esp.	2
the first one is wellexplained  easy to read cons:  ideas are not very surprising; and just tested on a few data sets; things could be more robust.	0
i like the idea of trying to qualitatively illustrate the behavior of sgd when optimizing parameters of complex models, such as deep and conv nets, but i think that the contribution is not very substantial.	0
con: o the results so far are interesting, and in places promising, but not so clearly good that this idea doesn't need further evaluation of its usefulness.	0
review summary: pros:  nice idea, sensible implementation.	2
major points:  this idea has been around for quite some time but yes it is now certainly more timely with the new dl tools such as pytorch.	0
the idea of the paper is simple and technically sounded but unfortunately, i found it not well presented with many important technical details missing.	0
pros:  interesting idea.	2
the ideas are clearly stated, although the work is not groundbreaking, but more on combining several ideas into a single one.	0
i think it is an interesting idea, but the current draft does not provide sufficient support.	0
this is a reasonable idea, and the win over skipthought is quite convincing, but the paper is short on substance, and parts are confusing or superfluous.	0
the ideas make sense on a conceptual level, albeit suffering from major practical concerns.	0
interesting idea, but why would one have access to labels in the first place?	0
the idea itself is simple but seems to be effective as shown in tables 2 and 3. what i’m missing here is a simple experiment to see the difference in accuracy performance between 1) when using pr for training the model (which is case b in table 2) against attacks versus 2) when not using pr for training the model against other attacks (similar to table 2, testing phase but using other attack models not pr as an attack model).	0
pros  the idea of learning the difference between two distributions is novel to my knowledge.	0
similar ideas have been explored in prior work such as [li et al. 17] but are not doing exactly what the authors try to do.	0
in sum, the idea seems nice and interesting but the model is straightforward and the current results are very weak in analysis in order to make a good paper.	0
first and foremost, the ideas are vague and poorly motivated.	0
concerning the first contribution, the idea of defining boundary samples according to prediction variance looks fairly common, if not superficial, in modern machine learning.	0
learning deep embeddings in krein spaces quality: average originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: limited experiments  see detailed comments the underlying idea is interesting and probably novel.	2
there are e.g. datasets (protein data  see tino) get very bad classification models if the negativ contributions are removed  accordingly they are not just noise but contain valuable information to the problem  it would be good to add a few sentences at the beginning of the paper to (super brief) review the core idea/concept of siamese networks  the english is sometimes a bit bulky and it would be good to check it by a native speaker e.g ' proposal do not require'  proposal does not require ' unlike the our'  follow by a spellchecker 'embeddng', 'interms', 'simalry'  'followed by removing it by flipping'  with flipping the contribution is not 'removed' but mapped to the positive part of the spectrum  how does your approach compare to a classical embedding of the indefinite kernel matrix into a pseudo euclidean space (see e.g. tino or pekalska)?	0
it is an interesting idea for reducing the number of parameters, but i don't think the experimental section is adequate to judge it.	0
both these ideas seem relatively incremental.	0
the idea of iterative pruning was first considered by mallya et al., 2018 and weight regularization was considered for bayesian neural network by nguyen et al., 2018. pros:  combination of two idea seems novel.	2
cons:  in general, there is lack of explanation/justification on the combination of two ideas.	0
"(p.7, 1st par in sec 6) [related] ""however they did not extend the idea to any ''arbitrary'' nn"" (emphasis mine)."	0
although the idea is interesting and the presented results seem promising, there are some key experiments lacking that may prevent this work from making its claims on robustness and detectability.	0
i assume the idea of the figure 1 model was to show a relationship with mutual information, but for me specifically it was quite confusing.	0
"'clarity'' above average ''significance'' below average ''detailed comments'' _paper strengths_  the idea of leveraging generative models' knowledge of ""maps"" to perform visual localization is interesting.                  "	2
generally, i found that pages 57 describe many ideas, and some of them are individually fairly clearly described, but it is not always clear when one idea is beginning, and one idea is ending, and which ideas can be combined or not.	0
pros: 1) the main idea is simple and easy to understand.	2
strengths:  to my best knowledge, the idea of applying the metalearning to the automatic generation of auxiliary tasks is novel.	2
"in short, i like the simplicity of the idea, but i am uncertain about the degree to which it satisfies iclr's novelty criterion (""present substantively new ideas or explore an underexplored or highly novel question""); i do feel confident that some iclr readers would (perhaps unfairly) describe this approach as ""obvious."""	0
strengths  i applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the gp adapter framework (gpgru) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)  it likewise outperforms both commonly used preprocessing (gruf) [2][3] and the much more complicated neural net architecture (gruhd) from [6] (across two datasets and tasks)  the simplicity of this approach probably lends itself to additional customization and innovation  the literature review seems quite thorough and does an especially nice job of covering recent work on rnns for multivariate time series and irregular sampling or missing values  the experiments are thorough and welldesigned overall.	2
"i am sympathetic to the idea of fixing certain architectural choices, e.g., layers and units in the gru and number of inducing points, across all models because it (a) gives the appearance of a ""fair comparison"" and (b) reduces burden of effort, but i do not agree that it yields a truly fair comparison."	0
originality: the idea of never training over the groundtruth sequence, but training on sampled prefix and an optimized suffix is very novel.	0
here are some questions: (1) the idea should also apply on many “incremental rewards”, for instance, bleu scores in machine translation, etc. do you have any comparison?	0
2017. this paper used a very similar idea as the proposed learning method which relies on incremental rewards to find the “optimal” suffix (for instance, editdistance is a special example).	0
major concerns: 1) the idea is incremental: it applies the greedy algorithm to the outputs of a linearrelu layer, whose inputs are the precomputed similarity scores.	0
there is a thirdparty claim that on a full system, the general idea improves performance, but i take it with a grain of salt as no clean experiment was yet done.	0
simple idea but needs some work in its current format.	0
i improved my score because i think that the idea and the results are worth sharing but i'm still not very convinced of their true impact yet.	0
the idea is simple, but the approach improves the zeroshot translation performance of the baseline model, and seems to be better than either pivoting or training on direct but outofdomain parallel data.	0
[details] 1. the idea is incremental and the novelty is limited.	0
pros:  intuitive idea for a common problem  solution elegantly has the form of a modified policy gradient  convincing experimental results  selfcritique of core idea, and extension to address its main weakness  nicely written text, does not leave a lot of questions cons:  while the core idea is nicely motivated and described and good to follow, section 2.3 feels very dense and too short.	2
the idea is to learn a representation that separates background (dimensions that do not vary across data points, but may be subject to change in a data transformation) and foreground (dimensions that vary between data points under the same background) and then apply a 'fixed' linear transformation in the learned representation space.	0
here are my major concerns:  the idea seems to be very general and indeed is applicable to any latent representation learning method, and not just autoencoders.	0
weaknesses:  the idea and the proposed model are not novel.	0
overall the contribution of this paper is marginally incremental: 1. the major proposed idea is just to add one nograd previous segment into the prediction for next segment.	0
this is similar to residual network idea but more simplified.	0
while the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate.	0
the authors do point to the appendix for their experiments section, however this is not a good idea.	0
there is several interesting ideas and a new dataset introduced, but i would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review.	0
concerns  replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.	0
overall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could.	2
"for instance the .phi function defined by reddi et al. takes as argument all the past gradients g1...gt (see paper at the bottom of page 3) but is used inside algorithm 1 with only the current gradient .phi_t(g_t) or an enigmatic ""max"" of two vectors .phi_t(max(g_t,pg_t)) i have no idea what the actual calculation is supposed to be."	0
it is indeed strongly connected to the idea of imitation learning which has been studied previously, but i like the explicit point from which authors see the problem.	0
conclusion considering that the nid idea has been broadened to contextfree explanations, the paper shows promise, but it is a weak accept because the other contributions do not seem fully worked out.	0
# paper weaknesses  the idea of using block motion vectors from compressed videos (x264, xvid) to capture motion with lowcost has been previously proposed and studied by kantorov and laptev [i] in the context of human action recognition.	0
"the authors only mention that mean replacement pruning extends the idea in ye et al. (2018) to the nonconstrained training setting, but it is very unclear what ""constraints"" are talked about."	0
pros:  interesting idea and fascinating problem.	2
pros:  the paper is well written, very easy to read, well explained (and better formalized than [xiao and al.]);  the idea of deforming images is new (if we forget about [xiao and al.]) and simple;  experiments show what such a technique can achieve on mnist and imagenet.	2
cons:  the paper is a bit weak, in that it is not very dense, and in that there is not much more content than the initial idea;  for instance, more discussions about the results obtained could have been appreciated (such as my remark above about mnist);  for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the gaussian smoothing affect the type of adversarial attacks obtained and their performance  is it possible to fool the network with [very] smooth deformations?	0
the core idea seems quite sound, but the fact that the distributional loss can't be propagated through the full network is troubling.	0
deepström networks quality: good originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: a number of open issues in the presentation  see detailed comments the paper is based on some concepts to link nonlinear data representations by means of nonlinear kernel mappings, with the deep learning approach.	2
i think the main idea of the paper is interesting but the presentation lacks a number of details in the description/parametrization and the experimental design.	0
edit: i'm still not convinced about this article novelty, i really like the overall idea but it seems that this kind of contribution is better suited for short paper.	0
i find the proposed idea simple and elegant but the evaluation lacking, and as such i’m a bit hesitant to outright recommend accepting the paper:  evaluation is not very extensive or detailed.	0
this is an interesting idea, but it doesn't seem to represent what you're doing.	0
cons:  the idea of separating policy and default policy seems similar to having high and low level controller (hlc and llc) in hierarchical control  where llc takes proprioceptive observations as input, and hlc handles task specific goals.	0
the idea is novel and the paper is wellwritten, but concerns about gradient masking prevent me from recommending acceptance just yet.	0
at this point, i think the paper provides a nice idea with a theoretical analysis  but it doesn't provide enough experimental evidence that this works.	0
the authors cite the information bottleneck literature, but we shouldn’t need to read all these papers, the main ideas should be summarized here. '	0
pros:  the general idea is interesting, to reformulate any qa task as a ranking task cons:  the methodology and task are not clear.	2
it seems straightforward but it might be helpful to explicitly state the idea.	0
this is a compelling idea but it needs more support than the theoretical or experimental sections give.	0
(ii) the additional adjustments, which as discussed earlier were not convincing, and (iii) the benchmarks comparing the proposed idea with some of the recent methods that i believe do not reflect a conclusive picture because of the two stated concerns.	0
(7) most importantly: i like the idea very much, but the paper clearly needs more work.	0
while the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.	0
i find the idea interesting, but fear that the 60100 small ensemble models could be replaced by a larger model.	0
strengths: [] the idea of defining weights as samples from a mixture of gaussians is interesting.	2
while the idea is interesting, the execution of the paper is lacking.	0
strengths:  the idea is simple.	2
i appreciated that the onetoone transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.	0
the idea of transferring from one domain to another using mmd as a regularizer appeared in https://arxiv.org/pdf/1605.06636.pdf by long et al  indeed equation (3) of this paper matches exactly equation (10) of long et al. note too that long et al also discuss what kernels work well and which work poorly due to vanishing gradients and propose parametrised solutions.	0
overall, this is a nice paper with a small, incremental idea and substantial experiments that show its practical value.	0
these were good data sets a few years ago and still are good data sets to test the code and sanity of the idea, but concluding anything strong based on the results obtained with them is not a good idea.	0
assuming shared moment estimation for blocks of parameters is definitely meaningful from an information perspective, and has indeed been used before, but it seems to have little to do with the 'decorrelation' idea.	0
pros a good idea, enough experiments that indicate the benefit of casting this as a domain adaptation problem.	0
examplar experiments show the possibility of using the key idea to generate adversarial examples weaknesses:  the experiments are very limited and show just 5 examples of generated images on mnist and imagenet.	0
more broadly, but following from the above: the paper does not provide any real world examples, real or hypothetical, to give the reader an idea of whether the above uniformity assumptionor really any of these assumptionsare wellmotivated or empirically justified.	0
overall, the paper provides interesting idea but the empirical results may be biased due to illposed problem	0
this is not to say these connections are not interesting and should not be talked about, but they are true by construction and that is indeed why using gvfs for knowledge is such a good idea.	0
section 4.1 appears to describe an experiment, but in the end is describing an algorithm described in fig 1. i think the alg requires rollouts which is not ideal, but this is mentioned in passing.	0
comments: 1) the idea of multistage communication is great, but the paper doesn't have a strong point to support this contribution.	0
pros  considering correlations between features from different encoder layers is a good idea  the improved encoder / decoder architecture is significantly more efficient than the cascaded approach of [li et al, nips 2017] cons  somewhat incremental  limited experimental evaluation  qualitative results not clearly better than existing methods  missing citation for multiscale losses limited experimental evaluation one of the key claims of the paper is that “our method with interscale (fig.7(f)) or intrascale feature transform (fig.7(g)) are more similar to the target style than those of singlescale style transfer without considering interchannel correlation” (figure 7 caption); this claim is substantiated primarily by qualitative results in figures 4, 7, and 8. personally i don’t find the results with interfeature correlations to be much better than those with only intrafeature correlations or the results from prior work.	2
the interscale loss seems intuitively like a good idea, but i don’t think the paper presents sufficient experimental evidence to justify it.	0
while the latter is focused on single word settings and is thus solving an easier problem, what would happen if the alzantot et al. method was applied to each while the idea is interesting but incremental, the evaluation of the approach is weak.	0
on the positive side, i think it's a good idea to experiment with various approaches to defend dnns against adversarial attacks, like the background check approach considered in this manuscript (which hasn't gotten a lot of traction in the machine learning community so far).	2
the specific idea of learning a better rating system is interesting, but it's not clear how fleshed out this application is in the paper (more comments on this below).	0
the colearning strategy idea is interesting, but the details of the method are not well explained and the experiment results seem not convincing.	0
cons:  the idea is a simple extension of existing work.	0
presumably, the idea is that the learned representations can generalize to some other task where object compositionality is useful; however i did not see such an experiment.	0
main concern: i think the idea proposed here of using rbfi units is very interesting and intuitive.	0
on the positive side, the main idea is very interesting and has a lot of potential.	2
pros: 1. the idea of converting the architecture characteristics, which is discrete in nature, to continuous variables is interesting.	2
overall i really like the idea in this paper, the latent space is well justified, but i cannot recommend acceptance of the current manuscript.	0
in summary, this manuscript proposes an interesting idea, but not sure empirically how useful it will be since for a complex network, this method may result in relatively big performance decrease.	0
although i like the idea and it seems a very interesting direction for generalisation to new goals, i do think the execution, the particular instantiation and (lack of) indepth evaluation with (at least some of the) existing methods in literature  including uvfas [1] and the different ways sfs have been used for generalisation [2,3,4]  is unfortunately letting it down.	0
originality and significance: i’m a bit split here: i like in principle the idea, but i think this instantiation is (fairly) incremental with respect to the current literature.	0
"the method builds on the overall idea behind an earlier approach, dec (icml, 2016) but has one key difference  a group of autoencoders (ae) is trained simultaneously with the dnn and used to ""weigh"" the clustering loss  the other difference from dec is in that their deep network is the ""encoder"" part of a sae whereas in this paper, the deep network seems to be a general dnn."	0
the idea is simple but seems interesting.	0
pros:  the idea makes sense and it seems gpu friendly in the sense that the flops reduction can be easily converted in a real speedup  results show that the joint use of two resolution can provide better accuracy and lower computational cost, which is normally quite difficult to obtain  the paper is well written and experiments are well presented.	2
the loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?	0
pros: the paper deals with an important problem in rl which is learning robust policies that can transfer better  simple idea based on prior information theory literature is proposed.	2
strengths: the idea of leveraging feature groups in a neural network structure; the novelty of the rese model; weaknesses: the main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use gbdt.	2
overall, the idea as such is not that novel (just applying a bayesian neural network with amortized inference) but the results look quite impressive.	0
this idea is basically similar to nested pitmanyor language models (mochihashi et al. 2009) and twostage language models (goldwater et al. 2011), but the authors seem not to notice these previous work.	0
pros and cons: a) pro: the idea of checking whether nearest neighbors are overwhelmingly inlanguage, is very intuitive, and it is surprising that no one did this before.	0
this idea is mentioned in 4.2 but seemingly dismissed due to the impracticality of performing listening studies, which motivates the use of rmsecqt.	0
the paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.	0
the idea is neat and the experiments suggests that it works, but what comes later in the paper is mostly rather straightforward so i doubt whether it is sufficient for iclr.	0
in general, i find the idea original, and of potential practical use, but i believe the contribution of paper to be limited and not well supported by experiments.	0
the idea in this paper is novel but experiments do not seem to be enough.	0
despite these weaknesses i argue for accepting and presenting the paper at the conference for the following reasons:  modelling capacitylimited agents via ideas from ratedistortion theory (which is very closely related to freeenergy optimization, such as elbo maximization, bayesian inference and the mdl principle) is an underrated topic in reinforcement learning.	0
on a conceptual level, the strong idea is that moving away from strict optimization and infinitecapacity systems is not a shortcoming but can actually help building agents that perform better and generalize better.	0
pros: interesting and novel idea cons: unclear transfer learning model, insufficient experiments.	2
pros:  the idea to include the exploration for pets is somewhat interesting.	2
also, there are a couple more papers that felt relevant to this work but are not mentioned:  estimating accuracy from unlabeled data: a bayesian approach, platanios et al., icml 2016. i believe this is related in how noisy labels are modeled (i.e., section 3 in the reviewed paper) and in the idea of correlation/consistency as a means to detect errors.	0
the current version is okay but not very exciting for idea selling.	0
i like the idea of universal successor features, it seems a bit incremental but i think it is worth exploring.	0
cons:  idea is not that novel relative to all the recent work on learning to supplement training data for fewshot learning (hariharan 2017, wang 2018, gao 2018, and schwartz 2018).	0
while these are interesting ideas and domains to study, i have concerns with the positioning and execution of the paper.	0
combining ideas from dagrnns and deep sets is interesting, although incremental.	0
the idea is well thought but the innovation is indeed low.	0
strengths:  constructing individualized feature space tailored to each query is a novel idea.	2
i like the idea but if we have a good model of the dynamics of the world (the pcpn) then (stochastic) dynamic programming is likely to be a very hard to beat baseline.	0
the proposed idea is highly incremental in terms of technical novelty.	0
d2ke: from distance to kernel and embedding quality: average originality: original significance: relevant for iclr pros:  interesting idea  see detailed comments cons: some technical issues, validity of the results not clear  see detailed comments i have seen this paper already at icml as a review and i am happy to see that the authors have improved the paper.	2
to ensure reproducability of your results i ask you to provide the respective codes e.g. on github (can be done anonymous)  repeating myself from the last review: there is a lot of work addressing that making a kernel psd may not be good idea  you provide experiments for a small number of data where your kernel is now psd but what is with the other data (where e.g. in pekalska and followers it was shown that making them psd is bad ... )  is your approach solving this  or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?	0
pros idea is very interesting and novel, with a nice connection to normalizing flows underexplored area of research, promising first steps.	0
pros 1. novel goalbased exploration scheme cons 1. similar idea has been proposed before for example, dayan (1993) estimates the number of steps to reach any position on the map using successor representations.	2
in general i find this paper an interesting idea, reasonably well communicated but some parts are not clear.	0
revision: although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:  missing experimental evidence for the efficiency of the nn search algorithm.	0
pros:  the idea of linking affordances to 3d object generation is interesting, and relevant to the machine learning and computer vision communities.	2
cons: the idea is not clearly explained.	0
pros: ' using rl to choose the simulator parameters is a good idea.	2
parts of the paper could be clearer quality: ' i believe that although the idea is great, but the quality of the experiments could have been higher.	0
while i find this idea interesting and of potential practical use, i have concerns about novelty and the experimental results and overall i recommend rejection.	0
i think this is a good idea, but it's a very small contribution.	0
in summary, the paper makes sense, but it does not present substantively new ideas.	0
the evaluation methodology is a good idea but is quiet trivial.	0
pros:  using data to learn exploration strategy in tis manner is a novel idea for bandits  good experimental results  well written paper cons:  practical impact may be minimal.	2
pros: 1. comparing to the existing methods using representations for shallow models in hyperbolic geometry, this paper extends the idea to deep neural networks.	2
cons: 1. the novelty of this paper is replacing the euclidean metric with another existing metric, which has already been used in previous ml models.	0
although the idea seems to be interesting and novel, but not enough evidence to prove the efficiency, from both theoretical and numerical perspective, even though many numerical experiments are proposed.	0
the idea of the paper is interesting, but the work is not solid enough.	0
other than the concerns mentioned below, i like the basic idea adopted in the paper to extend vedantam et.	0
while the idea for obtaining a variational upper bound on the generative mutual information is novel and clever, the experiments in the paper are lacking.	0
there is no evidence to demonstrate the effect of constraining the mutual information between x and z. in short, the paper offers what appears to be a very clever idea, but does very little to experimentally explore its effects.	0
overall, the paper brings up some interesting ideas, but it doesn't motivate all its design choices, and doesn't make a clear argument about the settings in which the proposed method would provide an actual advantage.	0
pros: () the idea of iterative inference is potentially effective () the paper is well written and clear () the authors show results on compelling benchmarks cons: () reported improvements are very small () important baselines are missing first, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more flops at test time.	2
the idea presented in the paper is however interesting and timely and deserves to be shared with the wider generative models community, which makes me lean towards an accept.	0
quality: the idea that is introduced seems intuitive and reasonable, but the experiments does not have enough details to prove that this method works (i.e. no quantitative results presented).	0
originality: i am not familiar with the literature of generative models to judge this precisely, but according to the related work section it sounds like an original idea that is worth sharing. '	0
on the positive side, i think the idea of building a set of strategies and trying to adapt the strategies picked based on the observation is an interesting idea.	2
sections 3.2 and 3.3 describe the general idea of the algorithm but i could not find a sufficient definition of some key notions.	0
strengths  the idea of mapping the bias term back to the input is interesting as it shows a common behaviour of the network on inputs that choose the same pieces of the piecewise linear functions.	2
the idea is tested in the context of neural mt, using a model similar to that proposed by bahdanau et al. (2015) with results on englishtofrench and englishtogerman wnt 2014. in the context of beam search, improvements are small (<=0.5 bleu) but statistically significant.	0
this is an interesting idea, and one i certainly wouldn’t have thought of on my own, but i think it is currently lacking sufficient experimental support to warrant publication.	0
pros:  tnteresting idea;  comparison against various attacks.	2
the idea presented in the paper is interesting, but (1) the experimental results are not entirely satisfactory for the moment and (2) only one aspect of the idea is exploited in the paper, which can be made more interesting and impactful while studying both attack and defense setups.	0
maybe, the idea is novel but experiments are only in simulation.	0
i think it is the right direction to go, however at the same time, these ideas are not really new.	0
pros: the paper is well written, the ideas are clearly and succinctly presented.	2
the paper proposes what seems to be a good idea, but it is not yet demonstrated by the current experiments.	0
i acknowledge the fact that this field in particular is moving rapidly, but the paper would be much more impactful by focusing on the more recent and more appropriate starting ground rather than recycling the ideas already covered in prior work (e.g., in sogaard et al.'s paper).	0
pros: the idea sounds interesting and seems like this approach could work well in practice.	2
moreover, the authors also show interesting properties of the holographic loss, along with some interesting properties of the minima etc. cons: my major criticism of this work is that, while this seems like an interesting idea, the authors do not really provide extensive results on real world datasets.	0
the authors are right that the older paper is a modelbased approach, but the idea is that they too were solving infinitehorizon mdps with finite trajectories and not introducing a bias.	0
2) another important weakness is the lack of any convincing argument on why it is a good idea to compute/apply an optical flow algorithm on the intermediate feature maps of a network?	0
this paper seems like a nice idea, but i'm not sure if it's ready for publication.	0
it sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation.. my main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation).	0
in conclusion, the authors seem to depart from the idea that adversarial examples should a) fool the network but b) still feel natural to humans.	0
in conclusion, i find the idea interesting, but the experiments do not show that this architecture can do anything new.	0
the idea may have its merits, but in the present form, it is not acceptable.	0
here are some concerns for the paper: 1. the idea of the model is pretty similar with xu et al. [2018] (training generative adversarial networks via primaldual subgradient methods: a lagrangian perspective on gan), especially the primaldual setting.	0
pros: ' the highlevel idea is compelling. '	2
cons:  the proposed idea is not particularly novel, as it is basically the learning to compare model but with pairwise comparison at each location in the final feature map rather than comparing the feature maps as a whole.	0
in summary, even though i liked the idea and the approach to the problem, the presentation is quite lacking, and in its current form, the paper might lose impact just by virtue of being hard to understand and subsequently being hard to reproduce.	0
although the idea of generating images from sounds with the aid of generative adversarial networks is quite novel and interesting, the paper exhibits several problems starting with the lack of clarity explaining the purpose of the proposed method and the contributions of the work itself.	0
overall, the idea is good but not well developed.	0
strengths: 1) the use of meta learning to improve sample efficiency of irl is a good idea.	2
strengths:  interesting idea of combining multiple noise types along with learned noise  clearly written, includes details of architectures and experiments.	2
the idea introduced in this paper is interesting but the paper is poorly written and organized which makes its through evaluation difficult.	0
quality, originality and novelty: while the high level idea studied in this paper is interesting, however it has been suggested before in prior work [tsipras et al., 2018].	0
overall, i feel the idea of this paper is interesting, but the theory and experiments in the paper are not very strong.	0
"'detailed comments'' _paper strengths_  the idea to use a negative video example for unsupervised detection learning seems novel  the proposed method is simple and the needed data can be collected with widely available equipments  the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example  the authors collected realworld data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)  the authors compare to a number of nonlearning approaches from opensource implementations (the reviewer cannot judge whether any relevant technique is missing)  the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes _paper weaknesses_  the authors clearly reduced the horizontal margins of the standard iclr style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard iclr style template upon update of their manuscript  the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. makerbased approaches could be employed for detecting the object  the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multiobject examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is selfoccluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)  other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging nontraining scenes with heavier clutter, nonseen lighting changes and occlusions to support their robustness claim  the proposed method cannot operate inthewild (e.g. youtube data) as it makes very strong assumptions about the required input data  the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene  the authors make no comparison to other unsupervised detection approaches (e.g. to the selfcited jonschkowski et al. (2017)) to prove shortcomings of other methods on the newly generated dataset  as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for egomotion estimation) makes applications to nonvideo data impossible, also none of the experiments exploit the nontemporal property of the approach to show single frame detection on a more varied set of scenes  the experiment showcasing the proposed application to ""learning from demonstration"" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task  the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)  the reviewer cannot follow the references to objectcentric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an objectcentric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion  the ""random search optimization"" discussed in section 3.3 is not a valid method as it ""solves"" this problem of instable training by picking the best of n runs with varying random seeds  figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach _reproducibility_  given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work.                  "	2
overall i think the idea is new and useful, but is quite straightforward and has some theoretical issues (see below).	0
in summary, the proposed idea is new but straightforward.	0
the idea i think is generally good, but there are several problems with this work.	0
pros  novel idea  relatively wellwritten  sufficient experiment evidence cons  there exist several gaps between the theory and the algorithm i have several concerns.	2
1. the idea is clearly delivered, but there are several practical treatments that are questionable.	0
overall, i think this work is an interesting idea to address a rather important concern in the deep learning community.	0
the key idea is that, during training, a neural network can be used to approximate the functionality of the blackbox functions, which makes the whole system endtoend differentiable.	0
strengths: ' the paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly. '	2
strengths: the combination of gans and knockoff filter is a very promising and intriguing idea.	2
pros: i like this paper: it is a intuitive idea, and the experiments explore exactly what one would hope to gain from the prior (i.e. better initialization, improved sample efficiency).	2
however to sum up the idea: you want to use deep learning techniques to learn some prior on the hashestimation problem, in the form of a heavyhitter oracle.	0
the paper is clear and the main idea is rather interesting, but the presented experimental validations are arguably weak.	0
overall, the idea looks very original and promissing, but the methods are quite difficult to understand under the current form, the messages from the results are not always clear, and the lack of ablative studies makes it difficult to determine which of the mechanisms are crucial in the system performance and which are not. '	0
strengths 1. intuitively, the idea of intralife curiosity is reasonable.	2
on the negative side:  the whole idea of drawing samples to compute the distance between two gaussian distributions seems unnecessarily complicated.	0
i find the idea interesting but feel that much more is needed in order to have a better understanding of how the proposed method works, when it works and when it doesn't work.	0
clarity:  the idea is easy to catch but clarity of the technical part can be improved.	0
(minor) i do not think that the size of the search space a very meaningful metric pros:  good exposition  interesting and fairly elegant idea  good experimental results cons  tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture.	2
the idea of using a structure on the latent prior of a vae to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.	0
the field is moving quickly, so references get missed sometimes, however from what i can tell, the graphcoarsening idea presented here isn't that technically distinct from ying el al.'s.	0
because the idea is not new, and formulating hpg from pg is so straightforward (simply tie the dynamical model over goals), the work seems incremental.	0
pros:  the idea of non expansive network is interesting and important  results indicate some advantages in fighting adversarial examples and label noise cons:  the results for fighting adversarial examples are not significant from a practical perspective  the results for copying with label noise are preliminary and require expansion with more experiments.	2
it's getting to be a joke now in the community that your idea works on mnist, but breaks once you try to push it to something harder.	0
using an explicit map is a great idea but the authors need to acknowledge how handengineered all this is, when comparing it to actual endtoend methods.	0
4. missing literature i think one important recent paper is “multiscale quantization for fast similarity search” nip 2017 to summarize, i like the idea of this paper but i feel there are still gap between the current draft and real working system.	0
i realize that the cfgps approach is domainagnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties.	0
on overall, although the proposed method seems a direct application of arora et al.,2016a, i find their extension novel and quite interesting, but the paper needs more experimental results to validate the idea.	0
strengths 1) the idea of multistep & bidirectional interaction between the retriever and the reader is novel enough (as mentioned above).	2
the idea of additive semantics has been explored in nlp, however it's mostly applicable for primitives with intersective semantics (e.g., a white towel is something that is both white and a towel).	0
however, the major weakness of the paper is the lack of novelty of the core idea.	0
"the authors provide a reasonable explanation of their ""periodicity"" ideas, together with evidence that it can be beneficial, but is not always essential to the algorithm."	0
pros 1. i like the idea of this sentence>tree>sentence autoencoder for semisupervised parsing.	0
cons: 1) while the authors tend to sell the idea for both zeroshot (zsl) and fewshot learning, the novelty to zsl is actually much small.	0
in this paper, the authors investigate a very simple but still very interesting idea of decoupling weight decay and gradient step.	0
cons:' a) while the criterion seems new it is also related to criteria used in the segmentation literature (see among many other https://doi.org/10.1080/01621459.2012.737745) and it would have been a good idea to discuss the relation between temporal clustering and segmentation, even briefly.                  	0
pros: 1. this paper has interesting idea related to dropout, and shows some benefit.	2
overall, this paper has interesting idea but needs more efforts to make the idea convincing.	0
i have some concerns about the idea itself (see below); yet, while i disagree with some of the presented view points, i don't think that diminishes the contribution.	0
== concerns / debate == i have some concerns about the key idea of the paper (in essence, i find it overly simplistic), but i nonetheless find that the paper brings an interesting new idea to the table.	0
i agree that it would be nice to have a euclidean latent space, but doesn't make it so.	0
pros 6. the basic idea (“we should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data”, p.	0
the idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide.	0
accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.	0
(5/10) === pros ===  an interesting idea that pragmatically builds on existing work to provide a practical solution  experiments conducted on several datasets with some interesting results === cons ===  empirical results mixed and little clarity provide on why  other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)  little theoretical development  nothing said about training time difference (even if not the point) overall, i like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur).	2
this is an interesting idea that solves the rubik cube, but the paper lacks a more detailed study.	0
the semisupervised classification experiment is definitely better to assess the representation learning capabilities, but knn suffers with the same issues with the euclidean distance as in the kmeans experiments, and the linear classifier may not be flexible enough for noneuclidean and nonlinear manifolds.	0
the proposed algorithm is simple and does not provide completely new idea, but this paper has a clear contribution connecting the previous main idea of feature extraction, bagofwords, and the prevailing blackbox algorithm, cnn.	0
unlike arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.	0
another complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc. therefore it is unclear whether observed differences are meaningful.	0
"it's fine and interesting to say that the idea was inspired by something about biological neurons, but i think it's a stretch to claim to ""mimic brain function""."	0
originality: (6.5/10) as far as i know the idea of parameterized thresholds for relus is original, as is the partial replacement strategy, but i am not an expert in this area.	0
the total idea is good, but the novelty is not much.	0
even if noone has had that idea before  it would be a minor contribution, but given that there is prior work on metalearning in the context of modelbased rl, this idea itself is not novel anymore.	0
pros and cons pros: 1) the idea of regularizing rl via an informationally asymmetric default policy is interesting.	2
pros: a novel method for a rather new and understudied so far, the work is then interesting for this setting good results reported cons: the criterion used for choosing when examples are outliers seems heuristic, more discussion would be welcomed as well as some qualitative analysis for showing the interest of the method existing baseline of saito'18 not used in the 1st experiment some parts require more justification 'comments: the idea of the method is similar to the one of jia'10 (eq.6) for multi view learning, but this is rather new for openset da.	2
pros:  paper proposes a somewhat complicated but easy to understand idea for open set classification.	2
quality: while i think the idea of a consistency constraint is probably reasonable, i consider this a poorly executed exploration of the idea.	0
pros:  a simple, straightforward idea  a good topic  progress in modelbased rl is always welcome cons:  unclear how this is significantly different from other related work (such as imagination agents)  experimental setup is poorly executed.	2
pros: ' available source code ' good experimental results ' easy to read ' interesting idea of encoding how active the various possible operations are with special weights cons ' tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train darts models, which they then show to transfer to two other data sets, respectively ' shared with most nas papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures ' theoretically not very strong, the derivation of the bilevel optimization is interesting, but i believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested	2
the basic idea of jp is to simply take all generated order of sequences from the original sequence input, which however i found is not new since it has been conceptually discussed already in the literature.	0
the regularization idea is interesting, but it isn’t evaluated so we’re left with theory that doesn’t provide guidance and isn’t evaluated.	0
pros:  the paper is wellwritten and all ideas are clearly presented.	2
overall the paper is well organized and easy to read  the proposed idea is smart: when starting from a synthetic domain there may be several hidden extra information that are generally neglected but that can instead support the learning task  the experimental results seem promising still, i have some concerns  if the main advantage of the proposed approach is in the introduction of the priviledged information, i would expect that disactivating the related pi loss we should get back to results analogous of those obtained by other competing methods.	0
overall i think that the proposed idea is valuable but the paper should better clarify the points mentioned above.	0
not yet great about this paper:  the paper feels premature: there is a nice idea, but restricting the drawing environment to be  some of the choices in the paper are a bit surprising, e.g. the lines in the drawing method are restricted to be at most 16 points long.	0
[summary]  this work proposes an interesting idea of defining complex latent space, but it is doubtful that this work just memorized the mapping between the training images and the latent convolutional parameters.	0
there is only one idea proposed by the authors based on the experimental results – fixing the deeper layers during the warmup phase, but the practical implications of this idea are not discussed.	0
pros: 1. the idea makes sense and the experimental results show solid cons: 1. some questions around generalization are not clearly answered.	2
however, each of 13 above is a straightforward application of an idea that appears elsewhere to spectral graph convolutions, making the work seem incremental.	0
csgnet [sharma et al. 2018] also uses programs to describe 2d and 3d shapes, but the dsl used here is richer as it captures more highlevel regularities using loops and also semantic relationships such as top, support etc. the idea of training a neural program executor and using it for selfsupervised training is quite elegant.	0
i still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper.	0
the strengths of the paper are that: 1) the supervision of the localization subnetwork only depends on euclidean proximity between two positions x and y and therefore uses relative positions, not absolute ones.	2
although the idea is simple, but it seems intuitive since purely aggregating gradient updates might have undesired cancelling effects on each other.	0
pros: the idea is pretty straightforward which makes use of the unique property of coqa.	2
i do believe that some components are more general though (in particular the main new ideas in the paper), so this is not necessarily a major issue, but another example of application of these ideas to a different domain could have strengthened the submission.	0
the good results obtained here from exploration alone also beg the question whether this state representation could be useful to train the agent, by plugging it directly as input to the policy network (which by the way may not be trivial due to the cotraining, but you get the idea).	0
pros: interesting idea for bringing some benefits of graphical models into neural networks using a regularizer.	2
"originality: most of the components are pretty standard, however i value the part that seems pretty novel to me  which is the ""partial zerosum"" idea."	0
"it uses the ideas of the semantic consistency loss and training on adapted data from cycada, but ""fills out"" the model by applying these techniques in both directions (whereas cycada only applied them in the sourcetotarget direction)."	0
ultimately, i think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced.	0
i like the idea of learning a latent structure dag for vaes, but this paper introduces a rather weak way to try to achieve this, and the experimental results are not convincing.	0
ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried eq 8 and it worked well empirically would be a lot better than what is there at the moment.	0
"this paper introduces an astbased encoding for programming code and shows the effectivness of the encoding in two different task of code summarization: 1. extreme code summarization  predicting (generating) function name from function body (java) 2. code captioning  generating a natural language sentence for a (short) snippet of code (c#) pros:  simple idea of encoding syntactic structure of the program through random paths in asts  thorough evaluation of the technique on multiple datasets and using multiple baselines  better results than previously published baselines  two new datasets (based on java code present in github) that will be made available  the encoding is used in two different tasks which also involve two different languages cons:  some of the details of the implementation/design are not clear (see some clarifying questions below)  more stats on the collected datasets would have been nice  personally, i'm not convinced ""extreme code summarization"" is a great task for code understanding (see more comments below) overall, i enjoyed reading this paper and i think the authors did a great job explaining the technique, comparing it with other baselines, building new datasets, etc. i have several clarifying questions/points (in no particular order): ' can you provide some intuition on why random paths in the ast encode the ""meaning"" of the code?"	2
quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.	0
pros: reasonable idea to use fast (sublinear) nn techniques in the ksparse projection step.	2
in summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.	0
"> defend against ""studying on cot and adversarial attacks.."" > could be better formulated references: there are some inconsistencies (e.g.: initials versus first name) pros ====  paper is clear and wellwritten  it seems to me that it is a new original idea  wide applicability  extensive convincing experimental results cons ====  no theoretical guarantee that the procedure should converge  the training time may be twice longer (to clarify)  the adversarial section, as it is, does not seem relevant for me"	2
the idea is simple but brilliant.	0
"pros: ' original idea of using separate ""discriminator"" paths for unknown classes ' thorough theoretical explanation ' a variety of experiments ' very wellwritten, and clear paper cons: ' the biggest problem for me was the unconvincing results."	2
i think that we should not throw out a work with interesting and potentially useful ideas just because it does not set a new sota, especially when the current trend with gans seems to suggest that top performance comes at a compute cost that all but a few groups do not have access to.	0
my notes: the key idea of parameterizing matrices as the svd by construction, but using a regularizer to properly constrain u and v (instead of the expensive cayley transform, or trying to pin the matrices to the stifel manifold) is very intriguing, and i think there is a lot of potential here.	0
pros: the idea of using pairwise similarities to enable a binary classifier encapsulate a multiclass classifier is neat.	2
simple idea and relatively easy to implement cons: ' clarity could be improved, especially in the experimental section ' the motivation for the svhn 04 to mnist 59 is not clear.	0
originality: ' it seems that this idea has been explored before, however i'm not personally familiar with that work.	0
the idea sounds great, but i am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical.	0
pros  the use of wfm instead of a convolutional layer is an interesting idea.	0
this is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm.	0
strengths:  the idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong to extra classes is interesting.	2
pros  [originality] the authors propose a novel idea of learning representations that improves the performance of the subsequent fixed discretization method.	0
originality: the proposed architecture seems novel, but there are many closely related works that are based on the same idea of decomposing the system into a perception, a physics simulation, and a rendering module.	0
the idea of using ensembles looks relatively straightforward, but it hasn’t been used much in synthesis approaches.	0
pros:  the paper is written clearly and relatively easy to read  the idea to replace the nondifferentiable renderer with a differentiable approximation makes sense.	2
in summary, the paper presents an interesting idea but the execution needs some improvement (especially, in terms of evaluation) before the paper is ready to be accepted to the conference.	0
some concerns/questions as below: 1) as the paper is based on som, some illustration of this method would be helpful for readers to understand the idea and learn the major contribution; 2) the authors use nmi and purity to evaluate the clustering performance.	0
pros:  the idea of using sr for pseudo count in deep rl is novel.	2
pros'  original idea for modelling distribution of sequence data  theoretical convergence in the jensen shanon divergence sense  promising experiments 'cons'  no major cons to the best of my knowledge 'typos'  it would be very nice to have black and white / color blind friendly graphs  eq 10 too long  introduce j_m & j_g in sentence  coma at the end of eq 5, and maybe align generator and discriminator in some position (e.g. at the semi colon).                  	2
evaluation: the paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing.	0
for the first idea, this is shown very well by the omniglot experiment but is not evaluated in any other setting.	0
strengths:  interesting idea overall.	2
the paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method.	0
while the presented ideas are well motivated and it is certainly a good idea to combine deep rl and evoluationary search, novelty of the approach is limited as the setup is quite similar to the erl algorithm (which is still on archive and not published, but still...).	0
the main downside of the paper is that the proposed idea essentially consists in replacing the standard .ell_p norm penalty/constraints with a group.ell_p one.	0
this paper is concerned with the idea of inducing multilingual word embeddings (i.e., word vector spaces where words from more than two languages are represented) in an unsupervised way using a mappingbased approach.	0
the basic idea is to use a more refined subset selection mechanism as compared to a' sampling, but at the cost of being able to guarantee an exact sample.	0
significance: i think this is interesting work, but i would recommend the authors consider some examples that clarify where these ideas might impact that problems that the iclr audience would be interested in.	0
pros  an interesting idea of leveraging class dependency in metalearning.	0
the two steps can be individually improved but the idea of separation is quite interesting and novel.	0
while the approach looks interesting, i have a few concerns:  using the bayesian model comparison framework seems to be an interesting idea.	0
the paper ‘learning discrete wasserstein embeddings' describes a new embedding method that, contrary to usual embedding approaches, does not try to embed (complex, structured) data into an hilbertian space where euclidean distance is used, but rather to the space of probability measures endowed with the wasserstein distance.	0
this paper uses the idea from 'a personabased neural conversation model' by li et al and incrementally applies it to the 'multiturn dialogue response generation in an adversarial learning framework' workinprogress by olabiyi et al. the paper by olabiyi uses the idea of adversarial training to the hred work by xing et al (hierarchical recurrent attention network for response generation).	0
to the best of my knowledge the idea is new (although from my perspective is quite straightforward, but see some discussions below).	0
experimental results are promising, but i don't understand how the last experiment relates to the main idea, see comments below.	0
the core idea of mixing the latent spaces of two data samples is interesting and the results seem to indicate that it improves generalization, but i have two main criticisms of this work.	0
figure 1 is a nice idea to illustrate the types of mixed feature distributions, but is not convincing as a toy example.	0
one suggestion is applying the idea to semantic segmentation, which also shares a similar problem setting but easier to evaluate its impact than videos.	0
i like the idea of the paper however i am currently unconvinced by the results that this is the correct method to solve the problem.	0
i am overall positive about the paper: the proposed idea is simple, but is wellexplained and backed by rigorous evaluation.	0
the phenomenon is shown in previous works, but this work interestingly proposes a theoretical model that supports the idea.	0
quality: good, clarity: good, originality: good, significance: good pros:  the paper is fairly well written and the idea is clearly presented  to the best of my knowledge (maye i am wrong), this work is the first one that provides theoretical explanation for the tradeoff between accuracy and robustness  the visualization results supports their hypothesis that adversarially trained models percepts more like human.	2
pros: good empirical results on challenging atari tasks (including sota on montezuma’s revenge without extra supervision or information) tackles a longstanding problem in rl: efficient exploration in sparse reward environments novel idea, which opens up new research directions comparison experiments with competitive baselines cons: the choice of extra loss functions is not very well motivated some parts of the paper are not very clear main comments: motivation of extra loss terms: it is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above.	2
specifically, one of the main contributions goes to using the gaussian mixture to construct the prior, but this is not a whole new idea in vae or gan, nor using the gumbel trick.	0
overall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like iclr.	0
this is partly suggested when the authors mention that the representation could be learned from only a partial goalconditioned policy, but this idea definitely needs to be investigated further.	0
the authors refer to pathak et al. (2017), but not to the more recent burda et al. (2018) (largescale study of curiositydriven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with.	0
the contribution is modest, essentially applying the same idea as the one proposed in maml to a variational objective, but well executed.	0
there are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion.	0
of course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate.	0
4. this general idea is somewhat similar to a june 2018 arxiv paper (taskdriven convolutional recurrent models of the visual system) https://arxiv.org/abs/1807.00053 but this is a novel contribution as it is uses the brainscore dataset.	0
comments: i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the numerical evaluation.	0
the only question i have is how does this hold on a real quantum computer such as ibmq/rigetti quantum computing etc.. or even under a noisy simulator although the paper is sounds and it is a good idea, the presentation is a bit lacking.	0
[paper strengths]: the paper combines ideas from gail and selfimitation learning to propose a method that leverages past best trajectories via inverserl.	2
pros:  the reported compression results with a ganbased framework for large images are impressive  comprehensive set of results with kodak, raise1k and cityscapes datasets  the paper is well written with the core results and idea being well articulated cons:  primary concern: the quality metrics are unclear esp.	2
pros: 1. a nice idea combining universal mdp formulation and hindsight experience replay for hrl that can deal with hierarchies with more than two levels of policies in continuous tasks.	2
it extends the framework of neural processes and conditional neural processes by an incremental seeming idea: self attention on the conditioning set and cross attention.	0
my main complaint would be the incremental nature of the work, as the contributions here are not as significant advances as some preceding ideas that have gone into this work, but still steadily improve on the vision of np and appear to be necessary steps to push the model forward giving this work validity on its own.	0
"suggestions for improvements: 1) i know the use of the term ""context"" is standard in parts of ml, but i have never been convinced that there is a truly new concept here that cannot be addressed using existing statistical ideas: coding ""context"" simply as indicator variables (covariates) is an obvious starting point, even better to pursue hierarchical modelling (i.e. fixed, random, and mixed effects)."	0
overall, an interesting and novel idea, but results are a bit lacking.	0
the basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way.	0
pros:  i think the idea of progressively capturing factors of variation one by one is neat, and this appears to be one of the first successful attempts at this problem.	2
although the idea is interesting, the experiments are lacking.	0
2) using vaes to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet?	0
), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below).	0
note that figure 1 helps getting the overall idea, but another figure showing an architecture diagram with the main model variables would help further.	0
section 4 seems to lack a highlevel idea of what it want to prove  the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high svhn likelihood, making it look like a distracting sideexperiment.	0
this paper is reasonably wellwritten in the sense that the main technical ideas are easy to follow, but there are several grammatical errors, some of which i list below.	0
pros:  appealing, wellmotivated idea for training policies via language.	2
to summarize, the idea and theoretical contribution is significant, but the work can be improved.	0
the main contribution of this algorithm is computational complexity, but i am not very persuaded by the idea of using the gan in order to produce a sublinear (faster) time algorithm for active learning, since training the gan may sometimes take more time that the whole active learning process.	0
they show results on voc 2007. pros: () the idea of casting a nondifferentiable pipeline into a stochastic one is very reasonable () this idea is showcased for a hard task, rather than toy examples, thus making it more realistic and exciting cons: () results are rather underwhelming () important properties of the final approach, such as complexity (time, memory, flops) are not mentioned at all while the idea the authors present seems reasonable and is showcased for a hard problem, such as object detection and on a welldesigned system such as faster rcnn, the results are rather underwhelming.	2
strengths: 1. regularization by sparsity is a neat idea.	2
'pros:'  mostly clear and wellwritten paper  technical contribution (learning many diverse skills) is principled and welljustified (although the basic idea builds on a large body of prior work from related fields as also evidenced from the reference list)  extensive emperical evaluation on multiple tasks and different scenarios (mainly toy examples) showing promising results.	2
it is a good empirical paper demonstrating the practical use of an idea that is simple but reasonable, and in a way that is substantiated using proper cutting edge framework and baselines.	0
also add citation (wang et al., 2017) since it ie the same model of that paper  # postdiscussion i increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.	0
pros: ' extensive theoretical and empirical analysis ' simple idea that generalizes to multiple usecases, which implies robustness of the approach as a methodology cons: ' unimodal assumption is likely not realistic, which would result in misleading visualization of data ' visualization analysis focuses on how classrelationships are preserved rather than faithful representation of each data point, which is a wrong target ' synthetic experiment is conducted on a single, too simplistic one; more examples are needed to understand the capabilities of the model in more detail ' the bias of knowledge distillation is not controlled	2
apart from the basic idea the paper's actual algorithm is hard to read because it is full of lacking definitions.	0
my major concerns are as follows: 1. empirical significance of gsgd: while the idea of exploring the structures of relu neural networks for training based on group theory on graphs is interesting, i do not see significant improvement over sgd.	0
"in the proposed method ""mgail"", the idea is that the discriminator is forced to also discriminate a third class of nonexpert demonstrations, but policy optimization is done ignoring the nonexpert demonstrations and classifying only the usual two classes with the discriminator."	0
pros and cons  overall, while i am supportive of a weak accept because of the idea and it's broad applicability i feel authors should maybe chose one of the tasks and show much more value in using the caml framework.	2
"weaknesses: two of the techniques: ""transformed bellman"" and ""temporal consistency"" seem welllinked thematically, but the expert demonstration idea seems orthogonal."	0
2) pros:  neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).	2
i think it is a good idea to formalize a method for carrying out and assessing adversarial attacks, but the framework proposed here seems too narrow, as it excludes adversarial inputs that are sensible but not a close perturbation of an existing source/reference pair, or ones that contain varying amounts of noise.	0
in the end, the main paper alone only gives an overview of what exists, but gives me no idea on how the package is used.	0
"16. algorithm 1 gives a vague idea about the proposed algorithm, but the text should be revised, the current version is very unclear and confusing 17. pp.56 the text of the authors' attempts to reproduce the results of others' work (from ""we also aimed to implement..."" to ""during the course of learning and exploration"") should be formalised 18. p."	0
on the positive side, the proposed method is a natural and sensible combination of solid technical ideas, and its theoretical analysis appears to be correct.	2
i find the idea compelling, but i am not sure the proposed method is a useful solution.	0
pros: 1) the idea seems slightly novel, simple, and elegant, with respectable results.	2
pros: paper was easy to follow using orthogonal encodings to decorrelate gradients is an interesting idea benchmark results appear promising compared to prior works cons: this work claims that their roformulation is fundamentally different from 1ofk, but i'm not completely sure that's true.	2
although i found the idea quite interesting, my main concern is that the jargon used in the paper makes it hard to understand.	0
pros ''''''':  i really enjoy the idea of identifying the contribution of each body parts.	0
presumably the network would still train and generalize but the conclusions might be different (e.g. the idea that growth of weight norms is the cause of learning low frequency components earlier).	0
figures 1 and 2 should illustrate the core concept, but they lack axis labels (and generally sufficient detail to decode properly), and seem to use the opposite color schemes from eachother to convey the same ideas.	0
weaknesses: 1. the idea of dynamic channel pruning is not novel.	0
object segmentation: visually, it looks nice, but i have now idea how good this segmentation is.	0
cons: the idea is only tested on relatively simple dataset.	0
strengths: the idea is interesting and nicely executed.	2
they adopt the idea of noise resilience in the analysis and obtain a result that has improved dependence in terms of the network dimensions, but involves parameters (e.g., preactivation) that may be large potentially.	0
the paper’s title and the openreview submission name should probably match update following author and reviewer discussion: i agree with others regarding the weakness of the empirical comparison to pseudocounts in particular, but still believe that the paper deserves to be accepted due to the fact that (1) some of the results are really good, and (2) this is a simple original idea that has the potential to drive further advances (hopefully addressing the empirical and theoretical limitations of the current work)	0
the proposal is clever, but there are some philosophical hurdles to overcome and the experimental results offer little quantitative evidence to support this idea.	0
the idea of leveraging the initial state for augmenting the reward function is clever, but there are a few shortcomings of the current paper.	0
comparing with entropy maximization method of egan (dai et al, 2017) is a good idea, but i’m wondering if you can compare it on low dimensional settings (e.g. as in fig 2).	0
the presentation of the core idea is clear but imo there are some key missing details and experiments. '	0
the idea is that at test time, we pass in a text sample and an unpaired voice sample and the setup produces voice in the style of but whose content is , in other words it generates synthetic speech saying .	0
pros:  well written  a simple and novel idea tackling a hard problem  good results on hard tasks cons:  an explanation of why the method should work is missing  plot text is too small (what is the unit of xaxis?)	2
overall, i very much like the idea, but i found many little pieces of confusing explanations that could be further clarified, and also some questionable implementation details.	0
you explain how you get around this in practice, but intuitively and from a highlevel, this idea does not make sense.	0
pros:  a simple idea with good empirical results that would be of interest to the community cons:  (extremely) unclear presentation which hinders the message of the paper.	2
pros: i liked the idea of exploring other avenues for approaching deeprl problems, and challenging existing paradigms or trends.	2
strengths:  this is a simple, easytoimplement idea that could easily be incorporated into existing models and frameworks.	2
however, the novelty of the solution is a bit on the low side, and one could argue that other ways of differentiating through optimization problems [ab] are important missing references and baselines for the experiments.	0
> the contribution and novelty of this paper is largely empirical.	1
the main novelty of this paper is the proposed hierarchical generative model and the associated algorithm.	2
i'm not entirely familiar with recent work in this subfield, so it is difficult for me to judge the novelty of the proposed procedure.	1
i am not clear on the novelty of this formulation, as it appears to have been proposed in a similar form in previous works (e.g., a maximumlikelihood interpretation for slow feature analysis by turner and sahani  eq., (2)) and can probably be considered straightforward.	1
in this work, however, the patches were assumed to be in correspondence, which leaves some novelty to this submission, although reduced.	2
major comments: the conceptual novelty seems a little overstated in the abstract.	0
with the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channelwise setups.	0
these results could give better support for the importance of the novelty.	2
the written focus on novelty detracts from the presentation, and a discussion of neural ode methods (whether acting as activations, or solvers) would serve as good background material.	2
it isn't clear to me where the novelty lies in this work.	0
conclusion while i like the overall topic of the paper, i currently find the conceptual contribution to be too thin, raising doubts on novelty and significance.	0
(1) yi zhu, et al, hidden twostream convolutional networks for action recognition (2) laura sevillalara, et al, on the integration of optical flow and action recognition in conclusion, (1) in terms of novelty, the flow layer is adapted from tvnet, and the fof concept need more clarification.	1
the novelty of the paper is rather limited, both in terms of the convergence analysis and exploiting the lowrank structure in tensor trains.	0
the main idea of this work is to encourage intralife novelty.	1
this limits the novelty.	0
technical novelty: the proposed algorithm seems highly similar to the existing multiobjective nas algorithms, especially the ones based on pareto optimality [1,2,3].	0
however, both aspects are of limited technical novelty.	0
unless the authors can convince me of the novelty of their approach or what i have overlooked in their proposal, i do not recommend this paper for acceptance.	0
the novelty of this paper is somewhat limited.	0
the novelty could be improved.	2
finally, the novelty of the proposed contribution is questionable.	0
the novelty of the scale factors is questionable.	0
in terms of novelty, i found the argument about convergence using diagonal fisher being faster compared with full fisher quite interesting, and its application for large batch training to be insightful.	2
<<cons>> ''the technical novelty is not significant'' this paper does not provide significant technical novelty.	0
in my opinion the biggest strength of the paper is the novelty of the proposed method.	2
the novelty of the bounds derived in loizou & richtarik 2017 is that they apply in stochastic settings.	1
notation and math: r1 in (4) is not clear as .mathbf{r} is not defined properly based on sec 2.2., it is easy to motivate the novelty score from subspace projection rather than qr/gs;  a_n and a_s are both functions of r_{1} which is the perp.	0
the novelty is trivial.	0
but, as discussed earlier, i have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that gams are interpretable.	0
while it is understandable that emphasis is put on novelty and its advantages, it would be interesting to see where the authors see room for improvement.	2
yet, it is hard for me to judge the utility and novelty of the proposed method in light of section 8, where the paper shows that a spectral embedding of the undirected variant of the graph leads to essentially the 'same' eigenvectors (up to renormalization and permutations).	0
while this paper is clear and well written, the novelty of the approach is limited.	1
overall, the novelty is not the strong suit of the paper.	1
in addition, i do not find enough novelty.	0
the justification for combining the empowerment maximization objective is also unclear while being integral to the novelty of the proposed method.	1
the novelty here seems to be the application to this specific problem.	2
() without dependency parser when it is not possible () limited novelty () limited convincing the advantage of gcn itself detailed comments the paper incorporates the graph structures in sentences and kb to make richer representations of conversation and achieves a stateoftheart performance on the dstc2 dataset.	2
however, as the paper combines existing mechanisms to design a model for dialog, the novelty seems to be relatively weak.	0
the novelty is limited.	0
this has limited academic novelty.	0
furthermore, the analysis performed is an ab approach from previous works (giusti et al.2016, and smolyanskiy et al, 2017) and, thus, it is hard to find the novelty here, since similar comparisons have been already performed.	1
the novelty of this method is somewhat low and a few parts of the algorithm and their uses are unclear.	0
the novelty of each is limited.	0
1. the novelty is limited.	0
"(since the reviewer was unclear about the openreview process, this review was earlier posted as public comment) most claims of novelty can be clearly refuted such as the first sentence of the abstract ""...this work presents a new approach to active anomaly detection..."" and the paper does not give due credit to existing work."	0
the novelty is marginal.	0
however, the novelty of this contribution is limited and may not meet the publication standard of iclr.	0
the other types of constraints add some more novelty, however.	2
the paper has several severe issues with it: motivation, related work, and theoretical novelty.	0
overall i think there is limited novelty in the approach; the idea to learn the structure of the data relationally instead of absolutely is pretty straightforward, and is a standard practice for example in nonparametric statistical modeling.	0
novelty/significance: it is not very clear what the novelty of this paper is.	0
i think one interesting novelty that needs to be emphasized is that the model has both: parameters that are point estimates (the parameters of the generators) and parameters that are sampled from a posterior distribution (the weight embeddings).	2
novelty: i wouldn't say this paper proposed a groundbreaking innovation, however, compared to many other submissions that are more obscure rather than inspiring to the readers, this paper presented a very natural extension to something practitioners were already very familiar with: taking an average of word vectors for a sentence and measure by cosine similarity.	2
the novelty of this paper is: 1. to use this we from the current task to generate a silence map (by smoothing the we) for the next task.	2
a lot of the novelty of the work rests on being able to pick up different clusterings with the different facets.	2
a weak novelty of using intermediatelevel input/output representation.	0
as far as i can see, the paper contains almost no novelty as it crudely puts together three existing algorithms without presenting enough motivation.	0
novelty: the idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work.	0
3. novelty relative to lfba (guo) should be better differentiated: the idea of using dct is originated from the lfba paper.	2
the novelty and difference between this paper and the lfba paper should be addressed.	1
while the idea being simple, i am not quite confident about the novelty.	0
novelty in technical parts: the idea of treewidth graph was introduced in bienstock and muñoz (2018).	2
could the authors explain more on the technical novelty?	1
is the novelty then that this is calculated for _all_ edges here?	0
the novelty here is very limited.	0
first, from a novelty standpoint, the proposed pipeline can largely be viewed as introducing a couple heuristic modifications to the set procedure from reference (mocanu, et al., 2018), e.g., substituting an approximate threshold instead of sorting for removing weights, changing how new weights are redistributed, etc. the considerable similarity was pointed out by anonymous commenters and i believe somewhat understated by the submission.	0
finally, i'm not sure the novelty is strong enough since the margin definition comes from [elsayed et al. 2018] and the strong linear relationship has been shown in [bartlett et al. 2017, liao et al. 2018] though in different settings.	1
the novelty of the approach is the main strength of this paper.	2
i find the technical novelty of this work very limited.	0
the main novelty, as the authors argued, is to apply transformations at latent space, with the weights of these transformations are also learned.	2
reconstruction is done to reduce the error between z' and z. novelty: the space of adversarially trained latent variable models has grown quite crowded in recent years.	1
while there is nothing wrong with taking an existing architecture (yolo) and showing that it can successfully be applied to another domain, it does limit the machine learning novelty.	1
from this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward.	2
the idea is to quantify the novelty,significance and corpuswise uniqueness of each word.	2
each word is then assigned 3 scores: novelty score, significance score, and uniqueness score.	1
the main novelty of the paper is in the form of this regularizer.	2
the main novelty of this work is that it uses generative models to infill masked out regions by ssr or sdr.	2
## novelty this is the main weakness for me.	0
the key of the claimed modeling novelty seems to be the selfattention mechanism.	0
4. place the work in context of prior work, and evaluate this work's novelty.	1
strength:  important problem weakness:  the novelty of the proposed method is very marginal  the experiments are quite weak details:  the novelty of the proposed method seems to be very marginal, which simply applies the gcn for link prediction.	0
while the scaled gumbel softmax is the claimed novelty, it is more like an extension of the original muse model (lee and chen, 2017), which proposed the sense selection and representation learning modules for learning senselevel embeddings.	1
as for novelty, it is claimed that this work is the first to use skp connection.	1
there is novelty in the particular combination of techniques that the authors have employed and some of the empirical results show the strength of the technique.	2
the main novelty of the paper is essentially using the teacher intermediate representations as input to the student network to stabilize the training, and applying the strategy to recent networks and tasks.	1
although the novelty is not that significant (combining existing techniques), it showed good results on montezuma’ revenge, which is considered as a very challenging problem for primitive action based rl.	2
the novelty of the paper is the formulation of the order stream and the use of gan for generating the stock data.	1
and the major technical novelty, the expression for <h_j h_l>, is really interesting and useful.	2
i rate this paper as an accept since this is one of the few existing works that demonstrate successful audio generation from noise using gans, and owing to its novelty in exploring representation for audio.	2
the main novelty of this work is that it uses generative models to infill masked out regions by ssr or sdr.	1
novelty  to the best of my knowledge, this is the first paper that formalizes a framework for suboptimality of goalconditioned hrl and i think this is the main contribution of the paper, that might have lasting effect in the field.	2
the main novelty of the proposed approach is that it defines an effective algorithm that can easily and quickly adapt to the changing context/environments.	2
while i find the overall idea interesting, i have some doubts about the experimental evaluation.	1
e.g. in figure c7 i have no idea what the curves correspond to.	1
but it needs more experimental studies to support this idea.	0
seems to be a bad idea as the algorithm might just have been lucky.	0
the basic idea is to combine the graph attention networks (veličković et al. 2017) and the relational graph networks (schlichtkrull et al. 2018) to derive a hybrid networks.	1
"for example, the structure and parametrization of d and q. i would like to have seen e.g. empirical results on full matrices compared to the particular ""diagonal"" struture used, to give an idea of how much we loose by that design choise."	1
the main idea is shown in figure 2, to regard the multiclass labels as hidden variables and optimize the likelihood of the input variables and the binary similarity labels.	1
nonetheless, i think this is an interesting idea and strong work with compelling results.	2
it would be interesting to see if the idea can generalize to continuous action space.	2
as far as i know, the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.	2
[] the paper is wellwritten and the idea is clearly presented.	2
in general i find the paper interesting, with nice ideas and i believe that will be appreciated from researchers that are interested on control theory/signal processing and information theory.	1
i feel that the idea is interesting; however, the paper is less well written and the realization of the idea has drawbacks as well.	1
"the idea is to maximize the mutual information between a local representation (of a ""patch"" defined by graph adjacency) and a global representation (of the entire graph), so those different local patches are encouraged to carry some shared global information."	1
besides, i think this work is a good application of the idea of extraction of rnns on reinforcement learning since no works have introduced this idea into this domain as far as i know.	2
the key idea is to construct a local minima whose risk value is the same as the local least squares solution.	1
it is an interesting idea to incorporate the gumbel distribution’s variance into the approximation in eq. (10).	2
another original idea is the use of sampling to avoid the problem of doing kernel overfitting.	1
simply idea.	1
secondly, whether using bleu on the entire testing dataset is a good idea for benchmarking is controversial.	2
the idea is really interesting.	2
the core idea is to use the lanczos algorithm to obtain a lowrank approximation of the graph laplacian.	1
although the idea looks interesting, there is no theoretical background and no clear intuition.	1
to make the idea more convincing, it is required to test it on much larger datasets, at least imagenet scale, and more desirable to show results in other tasks such as object detection and image segmentation.	1
the idea is to allow the model to interpolate, and more importantly, extrapolate from frames and learn the latent state for multiple frames together.	1
the paper's idea is to do a maximum likelihood fit of a simple surrogate policy to the logged data, and then use the conditional distribution over actions of the surrogate policy to compute inverse propensity scores.	1
the key idea is to oursource expensive computation involved with forwarding images through a model to an untrusted gpu in a way that still allows for the tee to verify the integrity of the gpu's output.	1
overall, i found the paper to be very well written and easy to digest, and the basic idea to be simple.	2
based on a grid approach, the idea applies only to one or twodimensional problems.	1
it is a simple idea that seems to work well in practice.	2
"3) the idea of ""purifying"" the neurons has a potential to provide new techniques to analyze deeper neural networks."	1
in general, this is an interesting work by introducing new idea of slicedwasserstein distance.	1
the idea of bypassing the use of public data by taking uniformly random samples seems interesting.	2
the basic idea is to learn a model that takes correction and history as inputs and output what action to take.	1
the main idea is to reparametrize the neural network after each update by rescaling the weights, without changing the encoded function.	1
up to my knowledge, the idea is novel.	2
see comments/questions below ''  the underlying idea is relatively straightforward in that the proposed methods is a nontrivial application of already known techniques from ml and audio signal processing.	1
while the fact that these covariates are strongly associated to face images and audio recordings had already been discussed in [1, 2], the idea of actually using them to drive the learning process is novel in this particular task.	1
i liked the whole idea of developing a faster algorithm for active learning based on the nearest neighborhood method.	2
it would be also interesting to see if this idea can further improve their performances.	2
the idea of initializing an iterative settling process with a forward pass goes back much farther than this.	1
the basic idea is instead of using a single velocity vector, multiple velocity vectors with different damping factors are used in order to improve the stability.	1
"inspired by this observation, "" the overall idea is interesting, the implementation is correct via a transition prediction models more place could be taken for more detailed results, use appendix to swap some text..."	2
after describing a number of variants of this idea, in the context of irl, bc, etc., the authors conduct a systematic empirical evaluation to assess the effectiveness of the proposal, over the baselines, using a number of rl benchmark problems.	1
the main idea of this paper is to get the best world of both in a unified way.	2
the idea is to consider only c_{1:t1}^k (not consider r_t) and have an attentional model that learn what to recall based only on c. also, why does recall need to depend on r_t?	1
i like the idea of the paper and i believe it addressing a very relevant problem.	2
this paper extends the idea of 'adversarial attacks' in supervised learning of nns, to a full repurposing of the solution of a trained net.	1
the basic idea is to identify the basis paths in the path graph, and convert the weight update in sgd to the weight rescaling in gsgd.	1
the idea of the paper is quite interesting, comparing to the existing metric learning based methods and optimization based methods.	2
pro: interesting new idea for gnn, that lead to more powerful method and open exciting direction of research.	2
the idea of metalearning update networks looks a promising direction worth exploring, indeed.	1
originality '' i am not aware of similar work and believe the idea is novel. ''	0
comments: overall, quite a neat idea  the information from non expert policy can help with representation learning, and the authors show that it is indeed the case via a number of experiments at the same time, i wonder if the third class is required.	1
the authors train models on top of both of these representations, with the idea being to match the performance of the explicit (heavy) model with the implicit model.	1
"the ""analogical decoder"" idea is really nice."	2
such as getting an idea about a regularizer, a prior, a mask, etc. and improved the performance.	1
they main idea is to benefit from binary maps between the query image and the support set (for the case of fewshot learning for the sake of discussion here) to guide the similarity measure.	1
the basic idea is using a variational formulation with dirichletlike boundary conditions to learn a neural committor function.	1
originality  the idea is good and general enough to be applicable for many situations.	2
their idea is separating the parameters in to two groups of context and shared parameters.	1
i would have preferred splitting that idea out into a separate paper, given that the paper is already 20 pages.	2
it seems like it could be a very bad idea on certain settings (for example if the reward has a lot of randomness).	2
overall, i think the paper presents a really nice idea of how to improve modeling of agents.	2
the idea of discriminator has been widely used recently for language generation related tasks.	1
overall, the idea is interesting, and the automatic evaluation based on perplexity, bleu, rouge, etc shows that the proposed methods outperform existing methods.	2
the core idea is to partition the latent states into n partitions, and correspondly have n siamese networks that pull the generated images with the same latent partition towards each other, along with a contrastive loss which ensures generated images with different latent partitions to be different.	1
the key idea is to use siamese networks with contrastive loss.	1
the idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3].	1
the idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.	2
main idea: in particular, the idea here is to simultaneously train a deep q network while choosing actions based on samples for the linear parameters of the last layer.	1
the paper proposes new online optimization algorithms by adding the idea of optimistic updates to the already popular components of adaptive preconditioning and momentum (as used in amsgrad and adam).	1
the idea of optimistic updates has been popular in recent years within the onlinelearning literature, and has been used with particularly great success for achieving improved convergence guarantees for learning equilibria in games.	1
"the idea of applying optimism to adam was already presented in daskalakis et al, 2018. the algorithm in that paper is, in fact, called ""optimistic adam""."	1
the key idea is to use a model that directly predicts multiple time steps into the future.	1
the idea is quite intuitive and the example in linear computational graph in section 3 clearly demonstrates the basic idea.	2
the idea paper is nice however, this draft needs more writing work to bring to a conference standard in my opinion.	1
examples of close set a probably you should phrase as there can exist a close s_{2,4} since … after reading this i got general idea that close set correspond to connected component with no connections except to oldest ancestor and youngest child.	1
i am not even sure if those equations are correctly conveying the idea they are meant to convey.	1
while the overall idea of using logic in this way to help with skill composition is interesting and exciting, i believe several things must be addressed with this work.	2
i like the manner the idea is presented.	2
the overall idea is interesting and novel to the best of my knowledge.	2
the idea is to start by constructing successor features based on a random policy, cluster them to discover subgoals, learn options that reach these subgoals, then iterate this process.	1
the main idea is to first learn sr representation, and then based on the leant sr representation to clusters states using kmeans to identify subgoals.	1
at the same time, this paper adopted the idea form both darts and enas with parameter sharing, and introduces a new differentiable neural architecture search framework.	1
if you could give a distribution of the max(mean_i(xi)) over all graphs (including weights in your distribution), it could give an idea of how unlikely it is for the agent to get a high score without actually learning the causal structure.	1
while the idea itself is interesting, i think that the paper is still in a very early stage and needs more work before it can be accepted.	1
the basic idea is that gradients computed on a batch of highly associated samples encode related information in a single update that independent samples might take multiple updates to capture.	1
specifics: 1. novelty: the concept of formulating al as a mdp for optimisation is now a standard idea.	1
is missing, which has the similar idea to automatically learn the neighborhood proximities.                  4	1
it should be cited as this is a key idea to motivate the paper.	1
the authors introduce the idea of a partial permutation invariant set function and use this to learn node embeddings.	1
the authors should talk about [1] and ideally compare against them.	1
i suggest this idea should be follower by more further studies. '	1
the idea is insightful and proposed models are straightforward.	0
### style i like the idea of testing many different factorisation structures.	2
otherwise i have no idea what tradeoff between accuracy and detection rate you are actually targeting and how to compare the results.	1
overall, i appreciate the general idea and find the proposed approach very interesting.	2
the core contributions of this work are described in sections 3.4 and 3.5, and while i get the general flavor of the idea, i find the exposition here both terse and difficult to follow.	1
"the idea is quite useful, as it is a step towards learning models that can ""discover"" the concept of objects in a visual scene without any supervision."	2
the writing is clear and the idea is an original refinement of earlier work, justified by its exceeding stateoftheart approaches.	2
the novelty of the paper (the proof of convergence) is relegated to the appendix, and too much is spent in the introduction, when actually the idea of having the vfunction depending on a slowly changing network is also not novel in rl.	0
the paper is well written, the main ideas are clearly described.	2
nevertheless, showing empirically that the ideas actually work gives the paper a lot of credibility for being a stepping stone in the area of marl.	2
the only challenges are 1) the design of the bayes filter, in particular when the latent state is continuous, in which the idea used in the paper is very simple, discretization, and 2) dealing with potentially high dimensional statebelief pair, which was handled by the encoders.	1
in my opinion, the proposed idea is a solid combination of existing techniques: montecarlo sampling (step 3), bayes belief update, and policy gradient in pomdp (g(po)mdp).	2
the idea consists of several parts: 1. the authors suggested distilling a fixed randomly initialized network into another randomly initialized trained network in order to use prediction errors as pseudorewards.	1
the authors claim that distillation error is a proxy for visitcounts and experimentally demonstrate this idea on mnist dataset.	1
i liked the idea to use two value heads to evaluate intrinsic and extrinsic values with different discounts.	2
extensive experiments on the atari game montezuma’s revenge investigate several variants of this idea (combined with ppo), with the best results significantly outperforming the current stateoftheart.	2
do you foresee potential issues with this, and if yes do you have any idea to solve them?	1
the highlevel idea is to generate 'pseudolabels' via clustering of the given dataset using existing unsupervised learning techniques.	1
while the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase: 1. abbreviations, notations are not defined: gan, wgangp, dnn, fid (the complete name only shows up in section 4), softplus, sigmoid, d_{.theta_{old}}, ... 2. while the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.	0
while the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.	2
did you consider the idea in fader netowrks (lample et al.'17)', which corresponds to adding a simple adversarial loss on the style embedding?	1
one comparison technique to test disentanglement ability is to compare autoencoder reconstructions with the idea that a setup that has learnt to disentangle would produce higher reconstruction error because it has learnt to separate style and content.	1
although it is mentioned in words in section 3.3, i do not get a clear idea of what the encoder/decoder architectures look like.	1
we can see this from the following three perspectives: 1. proof idea: the proof of this paper is noisy version of the convergence analysis of a simple convex problem it treats the contribution of the nonlinearity and nondifferentiability as bounded noise.	2
1.proof idea: the proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in appendix (at the end of this updated review).	1
the high level idea of the proof is quite different from recent papers, and it would be quite interesting to see how powerful this is for deep neural nets, and whether any insights could help practitioners in the future.	2
the proof idea is to show that a certain gram matrix of the data, which depends also on the weights, has a lower bounded minimum eigenvalue throughout the optimization process.	1
also, while eventually this idea makes sense, it only makes sense within a single episode.	2
the main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning.	1
i think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables.	2
"assuming a ""neuron"" consists of a linear combination of the input, followed by a nonlinear activation function, the idea is to multiply the output of the linear combination by a ""modulator"", prior to feeding it into the activation function."	1
i  on the substance: 1. related concepts and biological inspirations the idea is analogous to attention and gating mechanisms, as the authors point out, with the clear distinction that the modulation happens _before_ the activation function.	1
5. some typos:  abstract: a biological neuron change[s]  abstract: accordingly to > according to  introduction > paragraph 2 > line 11: each target node multipl[i]es iii  conclusion: the idea is interesting and some of the experiments show nice results (eg.	1
the idea is interesting.	2
in the latter paper the idea is to simply realize that with y=ax, x being nxm sparse and a a nxn rotation, one has the property that for m large enough, the rows of x will be the sparsest element of the subspace in r^m generated by the rows of y. this leads to a natural nonconvex optimization problem, whose local optimum are hopefully the rows of x. this was proved in sww for 'very' sparse x, and then later improved in sqw to the linear sparsity scenario.	1
ii) the dataset you used only contains 6 games from the same team in one tournament.	1
my major concern about this paper is the novelty and significance of its results: in terms of connection to ntk, it seems that the connection between neural networks trained with squared loss and the result of ntkbased kernel regression has already been wellstudied by arora, sanjeev, simon s. du, wei hu, zhiyuan li, ruslan salakhutdinov, and ruosong wang.	0
i main concerns still lie in the novelty and experimental results.	0
my major concern is that the technical novelty is somehow limited in terms of the two closely related works of transformer and stochastic shared embedding (sse).	0
(2) they are derived in an adhoc way based on less theoretical background and thus lack novelty.	0
the regularization by gini impurity is a wellknown technique to induce sparsity, lacking novelty.	0
i understand that mcs detection from a matching matrix (and node state vectors) is not exact if we just use hungarianlike linear assignment problem (lap) solvers for a submatrix obtained by a simple thresholding, but both postprocessing normalization and gse parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes.	0
what concerns me the most is the novelty of this work.	0
second, the proposed system is not related to the focus of iclr and it lacks the novelty.	0
(1) for sst, it just connects n existing methods, using the output of method 1 as the input of method 2. the quality of results might be improved but there is little novelty.	0
however, due to lack of novelty, i think this submission may not be qualified for acceptance at this moment.	0
however, the proposed optimization method is, in my opinion, rather incremental (with respect to darts and [4]) and therefore of limited novelty and significance.	0
i'm a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable nas to a search space analogous to the one used in [1].	0
it lacks novelty and the improvement is marginal.	0
all implications are basically confirming already known things in different (pathbased) words, and the impact or novelty is rather small, but nevertheless, it would be informative.	0
i have major concerns about the novelty, and experiments in this work.	0
i have concerns about the novelty of the two main contributions of this work, which are theorems 1 and 2: ' theorem 1 is a direct implication of kantorovich duality, well known in optimal transport. '	0
weaknesses: ' in my opinion, the paper novelty is not significant enough.	0
the paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (omniglot) is shown, while the comparison on market1501 is missing; 4) lack of comparison and discussion of other related works.	0
however, i still have concerns about the novelty of the proposed idea (clusteringfewshot learning) and its similarity to previous works.	0
in other words, this paper lacks novelty, which seems incremental to the existing works.	0
thus, the novelty is incremental.	0
while the authors address some of my concerns, i still believe the contribution/novelty is limited.	0
cons: 1. the novelty is limited.	0
below i have several concerns/suggestions for this paper: 1: novelty.	0
review:###update: bumping up my score after the revisions  nice connections but novelty and practical takeaways unclear summary of the paper: this paper views recent iwaebased [1] methods (iwaestl [2], iwaedreg [3], rws [4, 5]) for training generative models p and inference networks q under a common framework, aisle.	0
in my opinion, the novelty of the proposed approach is not high and the improvements are incremental.	0
i have some concerns regarding the novelty, analysis and also the experiments.	0
so, the contribution of this paper is limited [3] the proposed algorithm is simple and effective, but the novelty is a bit low	0
weakness: ' the paper lacks novelty.	0
novelty/significance: my main concern is about the novelty/significance of the paper.	0
i am giving a score of 3. the engineering effort in the paper is appreciated, but the novelty is lacking.	0
thus, the novelty of this paper is incremental.	0
2) the main issue of this paper is the novelty is incremental.	0
questions/concerns: 1. one of the two major concerns i have is the novelty of this paper in terms of its methodology and empirical value to the community.	0
====================================== despite the potential lack of novelty on method, i do think investigating these myths and instabilities of training transformers is a very interesting direction to pursue.	0
overall, i think there are significant concerns with the paper, both in terms of writing and the soundness/novelty of the technical results and experiments.	0
this largely reduces the novelty of this paper and make it incremental, because using virtual supernodes is not this paper’s original idea.	0
improvement on image captioning metrics by using grounded text (proposed method) compared to not grounding (gvd) ' limited novelty: extension of gvd, where attention is removed and object locations are used instead # 2. clarity and motivation the paper is generally well written, however there are some concerns on motivations.	0
from the technical point of view this is limited novelty, but still an interesting improvement of the model; however the experiments and results do not support the claim that using such model improves image captioning result in a significant way.	0
although i found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below: 1) the idea of using attention score is simple yet seems effective.	0
however, i have concerns about its novelty.	0
other comments: the motivation for using longterm predictions to “infer more meaningful novelty” is fine on it’s own but seems to conflict with the choice of random network (rnd) state features.	0
for additional motivation, 'why is posterior sampling better than optimism for reinforcement learning' (osband & van roy 2016) offers some justification on the downsides of modeling novelty bonuses for each state/action independently.	0
however, there lacks the novelty on the objectives defined in this paper.	0
i do, however, have a slight concern about the novelty of this result.	0
the work has incremental novelty as it seems to be largely based on prior art (not on dnns).	0
in summary, though the problem studied in this paper is interesting, the proposed method is incremental, and has limited novelty contributions.	0
i am mainly concerned with the novelty of this method.	0
though the problem that the authors studied is important, the proposed method lacks novelty and has not been well evaluated comparing with other representation techniques.	0
to raise my score, the following concerns should be clarified or addressed: 1) an initial concern is with the claimed novelty of the work.	0
so i concern about the novelty and the theoretical contributions yu bai, yuxiang wang, edo liberty.	0
first, there is the novelty concern.	0
my main concerns are the novelty of the work, the theoretical soundness of the method for small data settings and its robustness in settings with model misspecification.	0
[disadvantage & improvement] while i am not a direct expert in this area, i do have some concerns regarding the novelty of the method and comparison to previous works.	0
weaknesses of the paper:  the related work section could be greatly improved, thereby showing the limited novelty of the proposed method (qgan).	0
however, the proposed method lacks novelty and the results are not convincing enough.	0
i'm leaning toward rejection partly due to the limited novelty in the approach but mainly due to the experimental evaluation section which requires improvements.	0
overall, the paper is wellmotivated and wellwritten however it lacks technically novelty.	0
though somewhat new, the novelty of this paper may be incremental to me.	0
the paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing lowlevel controller) for the multi agent case.	0
weaknesses: (1) the novelty of the method appears limited.	0
decision: overall, the novelty seems somewhat incremental, but i still feel the work is concrete and meaningful.	0
however, my main concern is the novelty in that the proposed approach is just an extension of [1].	0
it will be also interesting to see how does the proposed method perform on largescale datasets such as domainnet and officehome dataset: domainnet: moment matching for multisource domain adaptation, iccv 2019. http://ai.bu.edu/domainnet/ officehome: deep hashing network for unsupervised domain adaptation, cvpr 2017. http://hemanthdv.org/officehomedataset/ 3) the novelty of this paper is incremental as the theoretical results are extended from cortes et al (2019) and zhao et al (2018).	0
however, i have some concerns on the novelty of this work and therefore i’m giving this paper a weak reject.	0
overall it is a good attempt to reduce the number of weights in the deep rl architecture, but i do think the novelty of this work is a bit thin and the three contributions were not tied together with the main theme of the paper.	0
i have 2 issues with this paper: 1. lack of novelty – most of the components of the proposed algorithm have been researched in other works.	0
while i share the excitement of using crosslingual models to improve monolingual performance, i also feel like this paper lacks novelty and further evaluation to be accepted at iclr, and would be more suited in a more nlpfocused venue.	0
this paper should be rejected because (1) this method only combines existing techs, such as stochastic generative hashing (eq.1 and eq. 6), and lacks novelty; (2) lack of introduction to related work and baselines, (3) the experiments results can not support the claim, i.e. the effectiveness of cgh in marketing area, and (4) paper writing is awful and very hard to follow.	0
i'm mainly concerned about the limited technical novelty: the proposed technique is essentially a heuristic.	0
overall, i think the proposed algorithms lack novelty due to the following reasons.	0
i think the paper is of limited novelty but includes interesting experimental results that helps to better understand the potential and limitations of tensor decompositions in deep learning architectures.	0
the authors propose to use an augmented standard vae loss with a capacity hyperparameter and a lagrange multiplier, but such modifications have been used before and it is not clear to me where is the novelty in there?	0
weaknesses  1. limited methodological novelty: the methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications.	0
additional comments  1. lack of novelty: the data representation, which is presented as one of the core contributions of the paper, is not new.	0
my main concern is the lack of novelty in the proposed method.	0
however, on the other hand, i tend to agree with the reviewer #3 that the paper is interesting from the engineering perspective and it lacks novelty.	0
my major concern is the novelty in this paper since all the contributions can be summarized into adding two additional loss terms, which i would like to discuss below.	0
thus contribution is incremental and novelty is limited.	0
however, i have two main concerns:  conceptual novelty seems to be limited.	0
weakness the main drawback of the paper is the lack of novelty in proposed method.	0
(though i am less positive due to the concern of novelty raised by other reviewers.)	0
major comments:  overall, i find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to vibnet.	0
it would be nice to have the simple baseline of a classifier with a sparsity constraint, i.e. ' i.e. ablate the reconstruction loss i’ve given a reject because 1) the explanation of the method is not very precise and could be greatly improved, 2) the quantitative evaluation is not sufficiently convincing, given the lack of technical novelty), and 3) the qualitative evaluation is handwavy.	0
one of my main concerns, however, is that robustifying d in gan training is not a new idea for some readers [1], so they need more clarification on the novelty of the proposed method, e.g. by discussing about it in related work or by comparing the performance.	0
in terms of novelty the new term, the paper does not make a breakthrough contribution, but i consider this to be sufficient.	0
strengths:  the paper is well written and easy to follow  learning graph representation learning is a very important problem  the performance of the proposed approach are strong on the existing data sets weakness  the novelty of the proposed method is marginal  some realcase study on why the model works are not presented.	2
3. the results are shown in some very recent datasets including taskonomy cons: 1. there is not much novelty in this work.	0
overall, this type of utility analysis exists in the dp literature, but their novelty comes from the idea of averaging the curvature.	0
second, i find the approach lacks novelty and is a straightforward application of the well established temperature scaling method and the expected calibration error to regression tasks.	0
detailed comments: i'd like to recommend 'weak reject' due to the following reasons: 1. lack of novelty: the main idea of this paper (i.e. regularizing the outputs from normal and randomized states) is not really new because it has been explored before [aractingi' 19].	0
in light of the relatively low novelty and the lack of compelling empirical performance for the proposed combined mt system, i do not feel that this paper is appropriate for iclr at this time.	0
[weaknesses]  the novelty of this work is relatively limited.	0
the overall novelty however is a bit limited compared to the existing work, as the major contribution is to propose to use lms as teacher rather than asrs, with the rest of the design to be similar to existing works.	0
although the overall results are improved from the previous methods, the proposed method is lack of novelty.	0
the theoretical novelty is small; the experimental evaluation is lacking and missing crucial baselines, such as normalizing flows.	0
pros • interesting approach to extend the framework in [1] to cnns, with use of masks and maskspecific loss • clear motivation for the network bandwidth limited use case cons • hardly any technical novelty because the core ideas are already presented as well as applied to the same task in [1] • it is very surprising that the authors do not even cite [1] in their paper, despite their work being extremely closely related to it • most of the discussion in the related work section is unrelated to the specific task they tackled in the paper (i.e., input/feature selection).	2
1. the proposed method is straightforward and lacks of significant novelty.	0
overall, i vote for rejection and the main reason is the lack of novelty.	0
main shortcomings:  the novelty of the work is limited.	0
however, it is more like an extended experiment report to me which is very valuable but lacks sufficient technical novelty expected at iclr.	0
# originality the main novelty seems to be coming from the idea of parameter sharing between pairwise payoffs, but the overall architecture seems to be the same as [castellini et al.].	0
cons:  the paper has limited novelty.	0
overall, while the paper provides an interesting observation, it has limited contributions due to lack of novelty.	0
the main concern is the technical novelty.	0
however, there are novelty concerns with the proposed methods.	0
weaknesses: my main concern is about the limited novelty of approach.	0
this is not my area of expertise, but i cannot recommend the paper for publication in its current form as (a) it's not clear to me that the paper improves on existing methods, and (b) it's not clear to me what the real novelty of the work is.	0
however, i feel the whole paper lacks novelty, and have some technical flaws.	0
yes humans have high novelty, but high novelty in itself isn't necessarily good.	0
strengths: (1) novelty: this paper contains novelty in terms of two aspects.	2
cons: the novelty seems a little straightforward.	0
the novelty is somewhat limited: they're plugging together two existing approaches (hart and sab) to allow for interaction, but i am recommending acceptance because i found the experiments surprising (as a negative result)  they show that independent hart models are a strong baseline for tracking in settings that involve interactions, but that are nevertheless solvable without knowledge about the state of other objects.	0
strenghts:  application on activity translation and evaluation by human raters is interesting  the proposed work is a nice application of an encoderdecoder architecture in case of multivariate time series weaknesses:  there is no methodological novelty.	0
overall, given the quality of writing and lack of methodological novelty this paper is not ready for publication.	0
pros: 1. nice application of bert to grounded instruction following tasks 2. good empirical results cons: 1. not much technical novelty 2. empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise) other comments: 1. are the bert weights frozen or finetuned along with the rest of the model?	2
weaknesses:  the paper lacks significant technical novelty.	0
while there is some interesting and potentially useful novelty in the approach, i have some concerns about the empirical evidence and modeling to truly determine the mmlp's utility.	0
however, a major concern of mine is the contribution of novelty of the manuscript.	0
however, both the hypothesis and the technical solution are lacking enough contributions of novelty.	0
cons ' technical novelty: while they address a problem that has not yet seen much attention, their solution is combination existing solutions.	2
cons: the paper lacks technical novelty.	0
cons:  while the replication study is well appreciated, the novelty contribution of the paper is marginally incremental as the model structure is largely unchanged from bert.	0
while the domain of taxonomic classification is interesting, i find there is a lack of novelty on the machine learning part.	0
update: i have read the response and am still think there is a lack of novelty here.	0
my main concern is about the technical novelty of the paper.	0
based on the concerns of novelty, lack of considering node topologies, and lack of comparison with strongly related methods, it is hard to recommend acceptance of this paper.	0
score: the weaknesses of the paper are novelty, clarity, and evaluation.	0
the major issue of the paper is the lack of novelty.	0
my major concerns for this submission are its clarity, novelty, and theoretical depth.	0
overall, the paper is not mature enough to be accepted: there is not enough novelty, and the results lack of novelty, enough delta in performance from prior work, and have high variance.	0
the novelty of the proposed method is incremental over dinan et al. (2018).	0
however, i feel that it might lack novelty.	0
decision: overall, this paper handles an interesting collection of data, but does not add much interesting novelty to the field of representation learning and fails to leverage some opportunities in those datasets to do sth.	0
it is commendable to prepare the datasets for publication and the basic results the authors show make for good first baselines here, but ultimately do not meet the bar for novelty or scientific insights.	0
### weaknesses apart from the contributions of compilation of different datasets which are already present, and classification results on them, there is not much novelty in the paper.	0
if the paper is about a new method, then the novelty is lacking.	0
in regards to the novelty claim, there have been several developments of depthbased (as opposed to timebased) adaptive computation time in the literature, for example: [1] mcgill et al 2017 'deciding how to decide: dynamic routing in artificial neural networks' [2] bolukbasi et al 2017 'adaptive neural networks for efficient inference' [3] figurnov et al 2017 'spatially adaptive computation time for residual networks' (this paper is cited by the authors) these papers do not present sequence models, but the ideas in them readily apply to sequence models.	0
cons: 1 the novelty and contribution is not clear.	0
however, the paper lacks both novelty and indepth analysis.	0
my main concern of the paper is about the novelty and the lack of comparison of existing methods.	0
overall, due to the novelty concern and the lack of comparison, i do not support publication of the paper.	0
(2) lack of novelty: the paper references 'learning disentangled representations with semisupervised deep generative models', but fails to mention that their model is is nearly identical.	0
nevertheless the paper is a little lacking of novelty in the sense that it brings together many existing ideas and provides an analysis of the effect of bringing them together but none of the theory significantly improves over the existing theory.	0
the paper reject: the paper lacks novelty and have very weak experiments.	0
table.1 and table.2 do somewhat show the effectiveness of your method, but its significance is limited especially considering the limited novelty of methodology.	0
[weaknesses] 'novelty & contribution' overall, i do not find enough novelty from any aspects while the overall effort of this paper is appreciated.	0
2. lack of novelty.	0
however, the novelty and impact of the model is somewhat lacking.	0
overall, this paper proposes an interesting trick that seems to work in practice but the novelty remains limited.	0
weaknesses: 1. the novelty of the proposed approach is limited.	0
the major drawback of the paper is a lack of novelty  the goexplore algorithm is already well known, and this paper seems to be a direct application of goexplore to textbased games.	0
# decision there are some concerns regarding the novelty of the proposed method.	0
given the moderate novelty (which is good but not good enough as a standalone reason to accept this paper) and the notstrongenough experimental results, i feel more work is needed for the paper to be readily publishable.	0
however, this work seems like an incremental improvement over hazan et al 2018. to this regard, the only novelty that was the framing of oneshot nas as a recovery of boolean functions from their sparse fourier expansions is not new either.	0
while the authors state the application l0 to a novel context to select features is different from prior work, the novelty is rather incremental.	0
cons: 1. the novelty is not sufficient considering the prior works on sparse neural network training.	0
assessment: overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.	0
in summary: (i) the paper is wellpresented and provides hyperparameter sensitivity results; (ii) the paper is very interesting, but (imo) it should leave clearer message why one should use this method; (iii) the proposed method has tighter regret, but only in some (datadependent) cases and combines existing methods, limiting novelty.	0
'main arguments:'' 1. this works lacks novelty since the main idea is just a simple concatenation of a jpegartifactremoval model and a super resolution model.                  	0
there is not a huge algorithm novelty for the methods proposed in this paper, but they can well address the domain issues and improve the performance.	0
pros  i think the novelty in this paper is most appealing for me to vote for a clear acceptance.	0
if the authors can convincingly demonstrate the novelty of the proposal to learn the terminal signal via extrinsic reward supervision, and if the other reviewers feel similarly convinced, then i would feel more comfortable reevaluating my concerns about the significance of this work.	0
this proposed model is lack of novelty, it applied the attention sum of lstm output on top of bert in the document classification task.	0
unfortunately, there are many concerns which are still not convincingly answered: (i) considering the focus of this work (as admitted by the authors in the rebuttal) is empirical and with limited technical novelty, more comprehensive results on many datasets are required.	0
this reduces the novelty but increases clarity, and may still make the work a useful empirical reference for these benchmarks.	0
generating classlevel interpretations is an interesting idea but its novelty is somewhat unclear.	0
while i find all the concerns that are listed above important, my rating is mainly due to the novelty of this work compared to the prior patchbased random forest techniques for image analysis.	0
however, the proposed method has only a very incremental novelty compared to sporf and the previous approaches in computer vision (e.g., shotton et al., 2011).	0
however, i have major concerns regarding the novelty this paper, the various claims it makes, as well as its experiment setting.	0
major issues/questions: 1. the techniques proposed by this paper lack novelty.	0
weaknesses: 1. the biggest weakness of the paper is wrt novelty.	0
i am a bit concerned about the novelty of the approach.	0
strengths  there is some novelty in the approach.	2
originality: there is some novelty, but it is limited as discussed above.	0
this approach is not very compelling (e.g., comments about limited novelty and lack of concrete examples to boost intuition). '	0
overall, i think the paper doesn’t bring much novelty, but results are promising, and this work represents a step towards making taskoriented dialog system less reliant and hand coded linguistic information.	0
the major concern of mine is its weaknesses in clarity and technical novelty.	0
my major concern is on its novelty and how directly it can provide benefit to computation.	0
strengths:  a simple approach that can be added to any seq2seq translation where success metric can be evaluated efficiently  demonstrated results on multiple applications weaknesses:  the method requires being able to evaluate constraints  the approach is simple and does not seem to present significant novelty over prior methods.	2
however, i have some concerns related to the novelty of the algorithm and some details in the experiments.	0
this all makes sense, but the novelty is quite limited.	0
summary the core idea of the paper, clustering nodes based on signal intensity for hierarchical graph representation learning, is interesting and seems to be useful in practice, but the paper has issues with clarity and the rest of the framework is a bit limited in novelty (diffpool  sparsemax  gin).	0
overall, this is an okay paper with incremental technical novelty and clear presentation.	0
as such, while the experimental results are reasonable, i find this lack of any technical novelty a negative factor.	0
1) marginal novelty compared to related work there have been several maskingbased blackbox methods.	0
the major concern is the novelty.	0
i tend to vote for rejection for this paper mostly because, while it seems to me to be a very efficient and practical engineering project, but relatively lack the novelty in terms of the algorithm.	0
cons:  the proposed algorithm lack novelty.	0
while the application is interesting, there is not much novelty in the method and factoring that, the empirical study is lacking as well.	0
cons: 1. there doesn't seem to be much novelty methodwise other than the input representation of the game state.	0
[weaknesses]  my main concern on this work is its novelty seems to be limited.	0
cons:  the novelty of this paper is limited to some extent.	0
my first concern about this work is its novelty.	0
it is a good extension, but the novelty is limited.	0
it has some novelty, but the novelty is incremental.	0
i think the idea looks interesting, although the novelty is a bit incremental, as it basically combined the two wellknown models (vae and glow).	0
overall : though the direction of this work is interesting but lacks sufficient technical novelty.	0
review:###i think this paper is not enough to accept in iclr because  lack of novelty.	0
a possible downside is the relative lack of novelty, since the method seems like a reasonable extension of the existing work.	0
cons 1. the novelty is not good enough and the method does not seem to be solid enough.	2
overall, the method is promising, but i have the following concerns: ' using the reconstruction error as an anomaly score has been explored many years ago (check replicator neural networks), the novelty here is to enforce that on the latent space in the context of a variational autoencoder.	0
major • the paper lack technical novelty.	0
this paper is leaning toward rejection because (1) the proposed method lacks novelty, (2) it contains technically imprecise parts and (3) the effectiveness is not fully validated in the experiments.	0
it, however, simply employs brutetoforce approach toward the graph isomorphism, lacking novelty.	0
review:###interesting results and analysis but lack of novelty and details.	0
my main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels.	0
my concern with the paper is twofold: 1) the major technique of transformdomain coding is borrowed from previous work (e.g., goyal 2001), hence the novelty of the proposed method is in doubt.	0
2. there seems to be a lack of novelty except combining sinha et al’s theoretical result with gan training objective.	0
overall, the proposed method is evaluated under elaborate and detailed experiments and enjoys promising results, but lacks novelty and theoretical contribution.	0
summary: my main concern about this paper is its novelty, as the method essentially uses the method of narang et al., 2017, albeit with a different threshold, with the sparsity patterns of mao et al., 2017. the experiments demonstrate that the method is effective at pruning, but do not provide any timings to evaluate the resulting speedups.	0
weaknesses: 1. contributions of novelty are limited.	0
factor in the bound may be an interesting development but the novelty does appear to be limited.	0
combining the two methods do provide insights into understanding the interactions between graphs and get really good results on ddi prediction, but the novelty is limited.	0
the major concern of this work is the weak novelty.	0
overall i like the work but find the novelty quite limited, more effort could have been put into motivating the soundness of the use of multiple random walks.	0
we are also concerned about the novelty of the results, and believe most of these results have appeared in previous work.	0
however in terms of novelty, adashift was published at iclr last year (https://openreview.net/forum?id=hkgtkhrckq) and seems to include a closely related analysis and update rule of your proposed optimizers.	0
the reviewer has some main concerns regarding the claimed novelty: 1. pdog computes the difference between outputs of two depthconvolution layers, but there is no evidence that the distribution of feature maps is gaussian or gaussianlike.	0
however, my major concern is about the novelty of this work, given the fact that the theoretical contribution is quite limited.	0
moreover, i am concerned by the novelty of the proposed approach, which seems very similar to infobot, the main difference between them being the replacement of the goals with options thus moving towards less supervision / use of priorknowledge.	0
weakness, 1, the paper kind of lacks of novelty.	0
i have the following concerns about the paper: 1. lack of novelty: a. learning a dynamicsinvariant discriminator with the gradientreversallayer was proposed in stadie et.	0
my main concern regarding this paper is twofold: limited novelty and insignificant performance gain.	0
decision the paper proposal is interesting and adequately evaluated, however, the impact of the paper might be limited by its limited technical novelty and lack of comparisons to strong baselines.	0
cons  the novelty of the paper is limited as it is a somewhat straightforward extension of prior work.	2
i have the following concerns about the paper: 1. lack of novelty – although i appreciate the reparameterization applied to fvim to make it potentially more stable for imitation learning in large state and actionspaces, i don’t think that by itself meets the bar for iclr.	0
however i think there are some weaknesses of this paper ' the novelty is a little thin.	0
the work done is not adequately described, so it is not possible to say much about it, but it seems clear that there is no novelty nor sufficient rigor in it.	0
## suggestions to improve the paper (for 1) add an experiment where the distractor has more natural dynamics so that there is mi between the distractor positions in consecutive frames (e.g. ball bouncing in the image frame in the background instead of randomly jumping to new positions) (for 2) add an experiment with a rewardprediction only baseline, i.e. only actionconditioned reward prediction so that taskirrelevant parts are ignored by default (i.e. also no reconstruction objective, but also no mi objective) > show how planet performance compares to the so far reported numbers when using this representation for planning (for 4) add details about the architecture, hyperparameters and training schedule, for both the method and all comparisons to the appendix (for 5) add references to related works that use cpcstyle mi objectives for representation learning in the context of rl/skill learning:  [1] nachum et al., iclr 2019  applies cpcstyle objective to hierarchical rl setting  [2] anand et al., neurips 2019  investigates mi objectives for representation learning on a wide range of atari games (don't apply to rl)  [3] gregor et al., neurips 2019  while the main proposed model is generative they compare to a contrastive version that uses cpc to learn predictive representations (don't use it for rl)  [4] guo et al., arxiv 2018  similar investigation to [3] of cpcstyle objective for representation learning in rl environments (don't use it for rl)  it should also be mentioned that the original cpc paper already showed that adding cpcstyle auxiliary loss to rl improves performance (even though they did not compare to other modelbased methods)  add qualitative rollouts for predictions from the planet predictive network both with and without distractor to the appendix ## minor edit suggestions  'learning latent dynamics from pixels', hafner et al. is cited twice in the reference section  it might help to add the reward prediction module to fig 3 or mention in the caption that it is omitted, it is only described later in the text and was confusing for me on first sight [novelty]: okay [technical novelty]: minor [experimental design]: okay [potential impact]: high ####################### [overall recommendation]: weakreject  i am inclined to accept this paper but am not fully convinced that the random distractors provide a good intuition about how the proposed method would behave with more natural distractors.	0
because of the resemblance with these many different methods, the method itself does not surprise through the novelty of a new idea, but the authors seemed to have found something that was missing from these methods and that leads to very good results.	0
i agree this is wellmotivated but it has little novelty and a similar idea is there in vqa (see “dynamic fusion with intra and intermodality attention flow for visual question answering”).	0
cons:  the novelty of this work is limited.	0
i have some concerns about this paper: (1) the analysis lacks theoretical insights and does not seem to be very useful in practice; (2) the proposed method for graph sparsification lacks novelty and the experiments are not thorough to validate its usefulness; (3) the writing of this paper is messy, missing many details.	0
it is an interesting task, but the method proposed in this paper is not realistic and lacks novelty.	0
i'd rate the novelty as medium (smart combination of existing methods), but the exemplary experimental evaluation elevates it to more than a systems paper.	0
my main issue with the paper is the lack of novelty.	0
in addition to lack of novelty, the experimental methodology is very weak.	0
in summary, the method makes sense, but its novelty is limited and the improvements are incremental.	0
in terms of conceptual or theoretical novelty, the paper is limited (the method essentially boils down to regular adversarial training with a slightly nonstandard loss function) but the connection to watermarking seems novel and is nicely executed.	0
however, as described below, the paper overstates its novelty and falls short of showing the advantages of the method beyond incremental improvements on framewise image quality metrics.	0
one of my main concerns is the novelty of this paper.	0
last but not least, the statement of technical novelty in section 3 is extremely encrypted.	0
overall, i believe that this paper is below the acceptance threshold due to lack of 1) novelty, 2) significance and 3) depth of analysis.	0
some of the main issues i notice are: 1. the novelty of the proposed (combined) method is unclear, given that it is a relatively straightforward combination of relatively simple and battletested techniques; i don't consider this in general to be a problem, but previous work has explored the problem way more significantly both algorithmically and in modeling terms.	0
[weaknesses] 'novelty' overall, i do not find enough novelty from any aspects while the overall effort of this paper is appreciated.	0
however, the paper in the current state cannot be accepted for the following reasons: (1) the novelty is low, this very type of decomposition is already widely studied (2) the paper is not clear as to what the contributions are, and why they are justified, theoretically or empirically, (3) the review and comparison with the stateoftheart is lacking and (4) the experimental setup is simplistic and not convicing.	0
2. my main concern of this paper is the novelty, however, the empirical results are strong.	0
although the experiment shows successes of the proposed method on several datasets, the major weakness of this paper is the lack of technical novelty and detailed analysis of the proposed method.	0
cons: 1. the primary question is novelty .	0
however, i am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement given the lack of proper baselines.	0
cons:  the novelty of this paper is limited.	0
however, given the lack of theoretical novelty, the experimental methodology needs to be improved in order to significantly strengthen the results (assuming they continue to hold).	0
1. lack of novelty.	0
the method is interesting, however the novelty is low.	0
in that sense, i am slightly concerned that its novelty is limited.	0
however, it is incremental and the novelty is a bit low, compared to many recent closely related works, for example, optimizing neural networks with kroneckerfactored approximate curvature.	0
i have some concerns about the novelty of the attack and the appropriateness of defenses that have been tested.	0
the possible novelty of this paper would thus be in the proposal of the hidden manifold model, but i am not convinced with the significance of the latent task.	0
but i'm concerned with the novelty and contributions of this paper.	0
i still have concerns about novelty so would like to keep my rating unchanged.	0
i can see some new elements, but my knowledge of the previous work in this area is not deep enough to evaluate technical novelty very well.	0
i do, however, have significant concerns about the novelty of the paper as well as its structure and clarity, as detailed below.	0
in conclusion, i think that the paper lacks novelty and that it spends far too much space on 'trivial' properties of the model instead of addressing shortcomings, like the prior specification of the number of classes, which the authors even point out in the discussion.	0
regarding novelty: the shortcomings of transe and improvements to the loss have been discussed quite extensively in prior work.	0
there are however 3 major issues with the current approach: 1 novelty: novak et al. 2018 suggests a very similar notation of sensitivity and they show correlation with generalization.	0
i increase my score to 'weak reject' but not higher because of my concern about the novelty of the work in light of novak et al. 2018.	0
some concerns about the paper are as follows: 1) the novelty of the paper is incremental.	0
14. first sentence in section 4: i would not use “classify” but rather “detect” etc. for anomaly/novelty detection since the task differs from classification.	0
novelty: the proposed approach is rather incremental and lacks novelty.	0
in general, the novelty of this part is incremental.	0
postdiscussion: r1 made several relevant comments about the technical novelty and my concerns weren't fully solved.	0
major • the paper lack technical novelty.	0
i still feel that the novelty of the work is very limited and the authors' response to lacking novelty does not convince me.	0
i thought the paper was written well, and its experiments were done quite carefully, but it was lacking on the novelty front.	0
i'm not particularly convinced by the case for novelty, but i didn't realize that unreal's replay buffer was only 2k transitions instead of 1 million transitions.	0
my major concern is that there is not much novelty in the proposed method compared with gduap.	0
the draft appears improved but my concerns about the novelty and interpretability of the work still stand, leading me to keep my assessment unchanged.	0
here are some of my concerns: 1. the work is somewhat incremental and the novelty mostly lies in pulling a few different methods together that seem to work well in unison.	0
the novelty of this paper is incremental and not technically sound.	0
my assessment is that the submission is below the acceptance bar, mainly due to clarity and novelty concerns.	0
however, due to lack of novelty, i lean to vote for rejecting this paper.	0
weaknesses  my major concern of this paper is the lack of novelty.	0
weaknesses: the novelty of this work seems to be incremental.	0
my main concerns are (a) novelty: despite the claims in the paper  the importance of seasonality is well known and appreciated in timeseries literature, and the proposal to look for changes in seasonality is fairly obvious when dealing with practical timeseries.	0
cons: 1. it is a good application of known techniques, but the novelty is limited.	0
i believe a more extended discussion of this would be valuable for readers and would alleviate some of the concerns regarding the novelty of this contribution and how its place in the broader literature.	0
cons: 1: i think the major argument i have is this method is lack of technical novelty, since it is straight forward to adopt the loss of brabandere et.al 2017 to video cases for including pixels in the same group under ground truth tracking, and the selfsupervised loss is exactly the same as previous methods.	0
cons  using energybased formulation for graph neural network is not novel, and thus the paper lacks novelty in methodology point of view.	2
cons: ' the novelty of the paper is limited as it is mostly a combination of 2 existing methods. '	0
the authors propose to sample from the novelty frontier using a scheme similar to a prior method called skewfit, but replace the vae with a kernelbased density model.	0
2 incremental novelty over the prior work (frcl by titsias et al 2019): this baseline is the closest prior work to this work which according to the experiments shown in table 2 are slightly outperformed by the proposed method.	0
there is also a major concern with respect to novelty and related work.	0
main arguments apart from the interesting approach, this paper should be rejected due to several reasons: (1) the specific application field, considered in this paper, have to be generalized to more general cases, in order to be valuable for machine learning community, (2) from rl algorithms perspective, the novelty of proposed method is questionable due to lack of literature review and advanced approach to deep reinforcement learning, (3) requirement of explainable ai is essential for deployment and in spite of the quite sufficient explanations of the algorithm’s works, this paper does not well justify its superiority over the traditional methods either by theory or practice, due to experiments suffering from the lack of variability and missing the main point of improvement, which leads to generally insufficient contribution (4) the paper is unpolished and lacks specific formulations from the existing literature on related subjects.	0
besides, this leads to the doubtful novelty of the proposed algorithm, although used first time in this specific field, but composed totally from the same components and sometimes in the same combination as in other works related to rl methods in planning.	0
my main concern with this submission is its novelty.	0
to reiterate, the reason i am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion.	0
to reiterate, the reason i am asking for the above experiments is that the paper comes with incremental novelty as the main point, in my opinion.	0
i understand the concept of using the full path for updating the policy, but i do not see significant novelty of the proposed method from the current manuscript.	0
however, i have major concerns regarding the novelty of the proposed method and the theoretical rationale for the key design choices.	0
as a result, the novelty of the proposed method was very incremental and limited from a technology perspective.	0
the proposed model lacks novelty – there seems to be only one nontrivial contribution and i’m not entirely sure how useful it is.	0
overall, the paper presents an interesting problem and awareness that metalearning might be general enough to solve it well, but provides no real novelty in the approach.	0
(2) the proposed techniques are an adhoc combination of existing techniques with major concerns (details below) but also with little novelty (if any) for achieving this combination.	0
(b) technical: the technical contributions of this paper lack novelty and has several flows:  figure 1 seems to show that graph only grows in size.	0
while i don’t think novelty is a strict requirement, if it is absent, then it should be compensated with strong empirical results, but the paper lacks that as well.	0
my concerns lie with the novelty of the proposed model and the insufficiency of the experiments.	0
this is nice, but in the light of the work above, the novelty is unclear.	0
i am also concerned by the novelty of this work and the fact that it is missing references and discussion to prior work that proposes very similar ideas.	0
cons: in my opinion, the paper scores low on novelty and original contribution.	0
however, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way, decision: even though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution.	0
after reading their rebuttal, i still have main concerns about the novelty of the problem and the writing quality.	0
cons: 1. the primary concern with this submission is the novelty.	0
therefore, i think this work is lack of novelty, and still need more time to work on.	0
the layer freezing trick however is relatively wellknown, and thus leaving the novelty of the proposed idea to be limited at what layers they choose to freeze.	0
lack of strong empirical performance along with the limited novelty detailed comments and questions:  the approach naturally extends to few shot classification problems once the mse loss in equation 5 is replaced with an appropriate cross entropy loss.	0
lack of algorithmic novelty is not an issue but the authors should at least discuss similarities to lml (amos 2019) in a clear manner in related work.	0
i recommend it be rejected due to lack of novelty and missing connections to much related work.	0
although the proposed algorithm is practically useful, i believe the submission is premature to be accepted at a conference due to (1) the lack of comparison with existing works on multiagent (reinforcement and imitation) learning and (2) the lack of novelty (it seems that the proposed method simply combines existing neural networks and applies it to multiagent behavior prediction.).	0
weaknesses:  methodological novelty is low  this is a straightforward application of gcn  the objective of qa is a bit suspect for the sole reason that the training and testing is performed using experimentally resolved protein structures.	0
cons: the novelty is minimal and the problem is of interest only to specialists in this domain.	0
summary a good paper overall, but the experiments were relatively weak (common for most iclr submissions) and the novelty was somewhat limited.	0
cons: 1. the overall novelty of the proposed methods is limited.	0
technically speaking, the paper is outstanding, but it lacks novelty in terms of new ideas.	0
weaknesses:  the novelty and technical contributions are limited.	0
overall this looks like a solid rl library, but i am not convinced that it brings enough novelty to the rl software landscape for a published iclr paper — it would better fit in a workhop dedicated to ml libraries for instance, thus the weak reject.	0
review update after author feedback: i am on the fence for this paper, but still leaning towards rejection due to the fact i am still not convinced that this library brings that much novelty compared to existing other libraries (although it seems like a nice rl library, i am not sure iclr is the right venue for talking about it).	0
overall, this is a very well presented work, although it lacks some novelty and a few more thorough experiments to fully understand the improvements they show.	0
## novelty the application is interesting, but the novelty of the architecture itself is limited.	0
however, the limited algorithmic novelty is not the main concern.	0
realizing the relationship with the transformers not only decreases the novelty degree for this paper but also requires the authors to include the transformers in the baselines.	0
i have three major concerns, including a lack of novelty, unconvincing experiments, and poor presentation of the work.	0
however, it just combines nas and pruning parts together, which lacks of novelty in the algorithm level.	0
i lean to reject this paper because: (1) it lacks of novelty, (2) the experiment result is not convincing, (3) some related works are missed, (4) the expression of the paper is not clear.	0
resnets ensemble via the feynmankac formalism to improve natural and robust accuracies, arxiv:1811.10745, neurips, 2019 overall, since this work is a straightforward integration of some existing work, i think this paper lack novelty.	0
overall, this proposed method is well motivated, but the technical novelty is limited.	0
in sum, i think though the paper makes contribution on exploring better flow models but the novelty is relatively weak, the discussion and comparison of related work is insufficient and the experiments are not convincing or have mistakes.	0
## minor edit suggestions  fig 2 seems to define the blue square as the target, the text next to it describes the blue square as the agent, please make coherent  for fig 7: the numbers contained in the figure are not explained in the caption, especially the numbers below the images are cryptic, please explain or omit [novelty]: minor [technical novelty]: minor [experimental design]: okay [potential impact]: minor ################ [overall recommendation]: weakreject  the exposition of the problem and treatment of related work are not sufficient, the actual novelty of the proposed paper is low and the lack of comparison to strong baselines push this paper below the bar for acceptance.	0
however, i found that the paper has some weaknesses: 1. the novelty is not enough.	0
the introduction of the meta learning paradigm and its use to learn the rpn and classification networks are incremental in novelty but interesting.	0
i'm not demanding that the authors write out a claim of novelty that's like a patent, but claiming 'first unsupervised model that can identify objects in a 3d scene' is, in my opinion, clearly incorrect and needs to be qualified.	0
as discussed in the related work, this work can be seen as complementary to many related works such as igl 18, but the novelty of the idea is rather limited.	0
overall, this paper develops a new approach, but its novelty and intuition are unclear.	0
however, there are several concerns with the overall work that makes this paper weaker: (1) the main concern is with the novelty and more importantly the justification/analysis of the contributions proposed approach.	0
it seems like there is a common concern about the novelty among reviewers: improvements over hrn are quite incremental.	0
however it is unclear to me what is the stateoftheart of this field from this paper and its novelty.	0
my major concern is that more empirical investigation is necessary since the formulation provides a minor novelty.	0
<weakness> 1. one major weakness of this work is lack of technical novelty.	0
thus, the method may be practically viable but bear little technical novelty.	0
<conclusion> although this work is practically promising, my initial decision is ‘reject’ mainly due to lack of technical novelty and limited experiments.	0
i think the contribution of this paper is incremental and the idea is of less novelty.	0
this paper could be improved in the following aspects: 1. the novelty of the proposed model is somewhat incremental, which combines some existing methods, especially the unsupervised learning for action representation part that just combines methods such as vae, temporal skip connections… 2. some components of the proposed methods are ad hoc, and are not explained why using this design, such as why bilstm for encoder and why lstm for decoder.	0
i find the topic interesting but i'm concerned about the novelty level of the paper.	0
on the technical side, the novelty is incremental.	0
12) despite the fact that i could not find this paper ready enough and wellposed, i also have a concern about the novelty of the approach.	0
i think the paper still has some novelty and the comments address my concerns.	0
for these reasons, i think the paper should be rejected for lacking novelty and writing quality.	0
i find this paper lacking in novelty.	0
my only concern is novelty.	0
this is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using ungar's (2010) weighted barycenter to perform neighborhood features aggregations.	0
i originally suggested to accept the paper, but agree with the other reviewers that the novelty of the work in this paper likely doesn't meet the bar for acceptance given that the most significant contributions of this paper are around combining good ideas from other papers without much additional novelty.	0
nevertheless, i am not convinced that there is enough novelty and substance on this, i have some concerns on the evaluation, and i think that the overall presentation should also be improved:  i am not convinced by the 'knowledge distillation' approach.	0
unfortunately, i believe this submission should be rejected, for the following reasons: 1. limited novelty: it is a fairly incremental variation compared to the metalearner lstm (ravi & larochelle).	0
i think this is a good application of weaklysupervised mil, but i find the specific contributions to be lacking in novelty and impressiveness of results.	0
although the paper presents an interesting application of ml, i vote for rejection since i) the paper is similar to previously published works and lacks methodological novelty, ii) the description about the data and methods is not written clearly enough, and iii) the evaluation needs to be strengthened by additional baseline models and evaluation metrics.	0
(i did not see this split in the appendix) and etc 2. my main concern is novelty.	0
as it stands, the current novelty proposition is: blackbox optimization of lr and number of iterations in maml style meta learning (hardly novel), architecture modifications (no ablation study if these help or not), small improvement on fss1000 5shot test (what about other shots?	0
the submission (with the title read, highlight and summarize) seems to be suggesting that they are incongruous and that there is novelty in this notion itself, however there is no clear mathematical description or rigorous explanation of this notion.	0
one of my main concerns is about the novelty since both the minmax quantization and bucketing the weights before quantization are not new.	0
however, the concerns on the novelty of the quantization technique and the inference efficiency of the proposed method still remain.	0
although combining existing adversarial detection methods are interesting but i think the paper lacks novelty.	0
one of my major concerns of this paper is the novelty.	0
this paper tries to tackle an interesting problem, but there is unfortunately not much novelty in its approach, and the use of pretrained modules is rather unsatisfactory for iclr.	0
my major concerns about this work are limited applicability of the proposed techniques to certain nlp problems and lack of technical novelty, and therefore i give a weakreject rating.	0
cons: 1. limited novelty & prior works are not properly cited.	0
the authors don’t claim novelty on these aspects, however since these are the major ingredients in their method it is important to comment on this.	0
the proposed method does not win on diversity but has some minor advantage on novelty.	0
the idea of the paper is interesting but i feel the relative novelty of the approach is not sufficient to make it eligible as a full conference paper.	0
i am aware that assessing the novelty of a proposed algorithm is subjective in nature, but i do not see here sufficient novelty that will impact the thinking on these problems.	0
another weakness of this work is the lack of novelty in the proposed methods.	0
i find the description of the experiments confusing: you claim to use multitask training, transfer learning, as well as cotraining  but from what i can tell, the two methods are essentially the 'grover' method with extra text markers, and an autoregressive application of a seq2seq model (which is autoregressive itself in the case of the story model)  there is a concern about novelty: 'unlike these techniques, our task here is automating science journalism'  this in itself is probably not a contribution, however the 'ra' metric could be an important and novel contribution  i must admit i am not sure i understand the ra measure from your description.	0
in terms of originality and novelty, the method does not introduce very novel architectures or algorithms, but rather introduces a new combination of known techniques for the task of visual navigation.	0
the lack of significant technical novelty is acceptable if the proposed method is empirically shown to perform very well.	0
my main concern on the paper is its technical novelty and its performance when compared to the stateoftheart method.	0
weaknesses: (1) novelty: my biggest concern regarding this paper lies in its novelty.	0
the main weaknesses are (1) the novelty of the proposed methods is low (2) results are hard to compare with previous work and some experimental details are omitted.	0
i have several significant concerns about this paper on several facets, i.e., on the novelty and contribution part, on the algorithm part, and the initial claim.	0
in the end, i recommend paper rejection for the following reason:  i feel that the paper is lacking sufficient analysis  the rl experiments with n=1 concern me  the novelty is extremely limited over the literature and (urbanek et al 2019).	0
the paper also lacks novelty in the methods proposed, as they apply previous approaches from ganin et al. in their specific setting, which has been used in other bimodal setting such as unsupervised machine translation.	0
in summary, this paper is well written but does not provide enough technical novelty or an interesting enough problem to solve to be accepted at iclr.	0
however i am a bit concerned about the relatively limited novelty.	0
shortcomings: 1. limited novelty: hierarchical decomposition is extremely specific to the task of pointgoal navigation.	0
moreover, the proposed method is lacking in novelty and the baselines seem inappropriate.	0
while this difference in setting is fairly substantial, it doesn't change the fact that [1] previously unified a similar suite of tasks with more or less the same framework, which i believe significantly weakens this work's main stated claim of novelty.	0
the main concern about this paper is the novelty and the technological contributions.	0
weaknesses:  the novelty is somewhat limited as compared to [8], essentially the difference is to also vary the vocab used between student and teacher models, and also to use up and down projections for knowledge distillation in case the intermediate layer dimensions are different (in which simple matching does not work).	0
overall, i think rlst is clearly a strong model, but i am not sure about the main novelty of the paper (rl), for 3 reasons: 1) if rl is a significant cause in the improvements 2) the way that rl is used is unintuitive in several ways to me 3) how novel the use of rl is compared to prior work i think several of the above concerns may be addressed in the rebuttal and/or other reviewer's remarks, so i am looking forward to reading what others have to say.	0
i am not comfortable enough with this field to carefully assess this paper but i don't think this paper is above the acceptance bar due to (i) relatively weak novelty of the proposed approach and no discussion on its shortcomings and limitations; (ii) experiments with architectures well below the state of the art.	0
while i appreciate the engineering efforts in this paper to glue several recent neural components together, i really think there is a serious lacking of novelty in this work, e.g., gluing several known components together.	0
overall, i am voting for a weak reject largely because of the lack of novelty.	0
this might help nudge this paper over the current hurdle of lacking novelty.	0
strengths: the paper proposed an autoencoder based framework, which can handle the information of network structure, node attributes and node labels simultaneously weaknesses: the novelty of this paper is very limited.	2
in summary, i conclude that this paper addresses a highly interesting and relevant problem, but the novelty (in the form of the novel regularizer) is somewhat limited and has a kind of 'ad hoc' flavor.	0
it seems like there are two sources of novelty: (1) the use of a different number of continuation functions and (2) using a discriminator to estimate the importance ratio, but no details were given about these implementations.	0
given that my assessment of the methodology is correct (please correct me if i am wrong) leading to a lack of novelty and the lack of strong empirical results, i vote to reject this paper.	0
in effect i am currently unable to comment in detail on that contribution, its novelty etc. to me, this is the main area where the paper needs improvement, but it is so critical that unfortunately it significantly impacts my scores experimental results seem quite interesting.	0
the weakness of the paper is the lack of novelty, and reasons are given below.	0
overall, this paper has some novelty in its core idea in the context of adversarial defenses, however, the exposition lacks conviction, and the experiments are lackluster.	0
that is why despite a lack of significant novelty in over 9 pages, i have voted for accepting this paper.	0
strengths:  the paper studies an important problem  human study is interesting weaknesses:  the description of the gramnet is rather short  making it hard to asses the novelty of the method  the structure of the paper could be improved comments: although the study seem to be sound, the observations about texture importance are not surprising (e. g. see https://openreview.net/pdf?id=bygh9j09kx)  making the contribution rather incremental.	2
although the reported results show that there exist contexts where hyperbolic representations perform better than euclidean representations, i vote for reject for the following reasons:  there is a lack of novelty in terms of machine learning methodology: the paper only applies existing ml approaches on different computer vision tasks.	0
however, this paper also has some weaknesses which prevent me for putting it above the acceptance bar, namely (i) lack novelty of proposed method and (ii) somewhat weak motivation.	0
1) about the lack of novelty: 1.1) the doubly normalized attention appears to me identical to em routing proposed in the capsule network (iclr '18, https://openreview.net/pdf?id=hjwlfgwrb).	0
i am inclined to reject this paper as i deem that:  there is a lack in novelty: (a) spatial attention mechanism is based on already existing works in the literature, i.e., spatial feature transform (wang et al, 2018) and attention mechanism (hu et al., 2017).	0
https://arxiv.org/pdf/1902.08153.pdf https://arxiv.org/pdf/1903.08066.pdf https://arxiv.org/pdf/1905.11452.pdf https://arxiv.org/pdf/1905.13082.pdf novelty: the novelty of this paper is not very small, but not as much as the authors seem to indicate by ignoring the above recent literature.	0
the proposed method is simple and intuitive (while the method is incremental, i am fine with the novelty).	0
2. i see the joint classifier in sda as the bulk of the novelty: the testtime augmentation experiments and teacher/student models to relieve the costs of these testtime augmentations are good to have but do not seem novel, and their applicability is not restricted to the sda approach. '	0
3. i think this paper makes some modifications to infogan, but i cannot see significant novelty since all modules presented are existing in the machine learning literature.	0
unclear novelty due to lack of comparisons to many very related approaches.	0
cons: 1) lack of novelty: the autoencoder as the main contribution is greatly based on previous works about interpretable neural networks.	0
the implementation details, such as the use of memory bank, and different data augmentation strategies are useful, but has very limited novelty.	0
in my opinion, this paper is a borderline paper because it lacks novelty in terms of the algorithm itself.	0
i think the novelty in methodology meets the minimum requirement for iclr, but as a result, the bar for the experiment part needs to be high.	0
one example occurs at the bottom of page three when discussing kernel warping; this relies heavily on previously published work, but adequate space does not exist to make the connections to the novelty of this work clear.	0
however, i still have the following concerns about this submission:  my first concern is the novelty of the proposed approach: the idea of “iterative refinement based on mask prediction particularly in machine translation” is not new.	0
= strong/weak points  the method seems to work in practice  the writing is reasonably clear, and it feels sufficiently precise to reproduce the results (given enough time)  little novelty on modeling side (straightforward extension of seq2seq) and data extraction (straightforward application of control flow graphs)  the central novelty in the analysis (extracting 'kinds') seems to contribute very little to results (cf. tab.	0
the novelty is my main concern.	0
my only concern here and hence my weak reject score is due to the novelty of the work.	0
this is a very modest novelty, but i agree it has some benefits, especially when integrating this technique into a deep learning framework, where it would be as easy as a flip of a switch.	0
however, the overall novelty is incremental and experimental results are not solid.	0
pros:  improved performance on benchmark dataset originality: the novelty of this work is limited.	2
pros: 1) transferability across different training strategies is a particularly important novelty of this work, i think.	2
concerns:  while the overall idea is interesting and relevant, the novelty is incremental in its originality to solve the identified concern, considering the paper mainly strings together existing ideas in terms of the methodologies (as summarized above).	0
the work lacks in theoretical novelty since the thompson sampling approach is established and wellknown.	0
i don’t think this paper is strong enough for iclr as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written.	0
(0) the proposed method lacks novelty: the paper basically combines many existing ideas and stacks them together for the task of answer retrieval for conversational question answer dialog.	0
the design and theory seem to be existing already and the novelty is the main concern for acceptance.	0
this lack of novelty along with the relatively small scale environments that is tested on lead me to conclude that this work may be of only limited interest.	0
one major concern i have is about the novelty of the method.	0
concerns #2: novelty of the proposed method  i think the novelty seems to be limited because the proposed method just trains classwise importance weights using the validation set.	0
the novelty of adding additional loss functions and adapting the gradient update of (arpit et al., 2019) seems prima facie incremental.	0
1, their method lacks technical novelty to accept.	0
[cons]  technical novelty of the method is incremental.	0
the strengths of the paper are: ' the tackled problem is relevant to the computer vision community the rejection decision is supported by the following reasons: ' missing motivations: it is not clear how teb is able to encode long paragraphlike sentences when compared to other standard decoders ' limited novelty: it seems that the only contribution is how the paragraph is encoded (teb) which details are not clear (see point 3 below) ' writing need to be improved: missing motivations, descriptions and details (see point 2 below) ' results are not consistent (see point 4 below) # 2. clarity and motivation teb is presented as novel model, however it is not clear how it overcomes issues from previous works on paragraph captioning (sec.	2
however, i find the method presented in this paper to be lacking in significant novelty or new insight and recommend rejection primarily for that reason.	0
this paper provides numerous experiments but this should not replace novelty or presenting as novel something already present several times in the literature.	0
however, the paper is poorly written, with many typos and grammar mistakes (e.g., heisen instead of hessian, weighted delay instead of weight decay,...)  some of the mathematical terms are also not clearly defined: ' eq. 4 uses   ilde{y}, but below the equation the authors used hat{y} ' the beginning of section 3 uses both     ilde{w}^{l} and w^{l}, but both seem to indicate the same thing summary: my main concern about this paper is its novelty, as the method essentially seems to apply existing techniques in the context of network pruning.	0
this may be because they spent a lot of time listing all their theorems, but not as much motivating why these theoretical results add novelty and are important.	0
the main shortcomings of the paper are its limited novelty (multiscale pyramids are an old 'trick' in computer vision), and the use of baselines that are behind sota for the discussed problems  e.g. dai el al, 'secondorder attention network for single image superresolution' could be a more current baseline for sisr (instead of memnet) and liu et al, 'nonlocal recurrent network for image restoration' for denoising (which the authors cite and briefly discuss in the introduction).	0
however i am not a 100% sure about the novelty and big promises and bold claims.	0
my main concern exists in novelty/significance.	0
so, due to concerns around novelty and limited experiments only on traffic junction with some critical details missing, i would give initial rating of reject.	0
although the forgetting problem pointed out by this paper is interesting and worth studying, the proposed method (1) lacks novelty and (2) does not perform well empirically.	0
the proposed mixreview strategy is very straightforward and lacks novelty.	0
the major concern about this paper is lacking novelty and impact on this field.	0
2. the novelty is incremental.	0
technical novelty over lars seems to be incremental, where a large portion of the work is essentially applying lars to adam and demonstrate its effectiveness on bert and resnet.	0
my main concern is that i was unable to find a comparison of the techniques to prior work and an explanation of novelty.	0
in summary, this paper shows some technical novelty but the motivation behind the proposed method is not strong enough.	0
there is not a lot of novelty but the results hold well, so it is a clear contribution.	0
but, its novelty is incremental and competitors are not actually the models capturing diversity.	0
i am inclined to recommend acceptance due to novelty of order robustness analyses and competitive properties of the method, but i would like clarifications to my experimental questions.	0
i appreciate the novelty of the idea of decomposing parameters but it is not clear whether this factorization is actually performed given the high capacity needed to learn these tasks.	0
the model lacks enough technical novelty.	0
cons:  limited novelty of methods.	0
the paper lacks technical novelty other than the training and test data generation approach, but having one available to the community with these apparently desirable characteristics as benchmark data for measuring complex, compositional generalization capabilities, and that could be invaluable to the research community.	0
my only concern is about the novelty since the small loss trick in label noise and the mixmatch approach in ssl are already explored by many recent studies, but to the best of my knowledge, this paper is the first to unify them to solve label noise problems.	0
### novelty ### this paper proposes incremental domain adaptation, which is inspired by li & hoiem's work.	0
the main novelty introduced in their approach is to build a graph representation not of high level code but of assembly code.	0
caveat regarding novelty: i could not find the ideas proposed here in the literature, but its hard to be sure due to 1) the recent explosion of nas papers, 2) the simplicity of certain ideas (e.g., 'zoomed convolutions').	0
i do have concerns for this paper around utility and novelty.	0
however, my main concern is with respect the novelty of the rest of the paper.	0
overall, i like the approach but i am uncertain about the level of novelty.	0
strengths of the paper: ' stateoftheart results on 3 visionlanguage tasks the weak reject decision was mainly guided by the following two weaknesses of the paper: ' clarity of the paper needs to be improved to make the readers understanding the details of the model (see point 2 below) ' limited novelty: the paper is an extension of bert to the visual domain (see point 3 below) # 2. clarity the paper reads quite well, although some points need to be improved: ' how were words split in subwords (sec 3.2)? '	2
my main concern about this paper is the novelty given its similarity to the previous methods.	0
(i) general concerns:  the technical novelty is limited to the combination of knowledge distillation and prior networks.	0
<conclusion> although this work shows a new application of nas for object detection, my initial decision is ‘weak reject’ mainly due to lack of technical novelty, limited experiments and poor writing.	0
my concern regarding novelty remains unchanged but i still suggest acceptance since the contributions are of practical interests and the paper is well written.	0
2. opinion and rationales i’m leaning towards “accept” for this paper since it presents two interesting contributions (albeit of incremental novelty) to the approximate inference area, it has clear execution and super clean presentation, and the experiments clearly demonstrate the values of the proposed approaches.	0
to me, the novelty is incremental.	0
overall, it is a sound idea for lowresource setting with generally positive experimental results, but limited in novelty in terms of the proposed architecture (similar to [1', 2']) and disentangling language and knowledge idea (similar to [3']), lacks comparison with baselines for lowresource setting, and lacks discussion/reference of a few closely related works.	0
however, it does not seem that there is much novelty in the bnn architecture per se, but rather in the training procedure.	0
while there is some concern regarding the significance of novelty, the paper seems meaningful enough to be accepted.	0
weaknesses: i think that this paper is fairly complete; the only question is whether or not it contains enough novelty, as it is a natural extension of swa to the parallelized setting.	0
pros: 1. the paper is well written and easy to follow cons: 1. there is very little novelty in this paper  the notion of overlearning is well established in the literature (osia et al., 2018; chi et al., 2018; wang et al., 2018).	2
however, due to the lack of novelty in any of the applied techniques, and the fact that the experiments could be expanded, i recommend a weak accept.	0
i’m a little concerned with the significance and novelty of the proposed algorithm (it seems like a direct application of a generic dpsgd algorithm) and the utility analysis.	0
though the proposed problem is novel and somewhat interesting, there are also several weaknesses of this work:  the novelty of methodology is somewhat limited.	0
======= novelty ====== the method is novel: 1. probabilistic/bayesian understanding of fewshot learning/meta learning has been proposed, but to date variational inference is the main inference engine used in the literature.	0
the metasampler architecture is new but improved upon naf so i consider the architectural novelty to be minor.	0
in other words, the observation is interesting but the novelty is arguable.	0
overall, the proposed method has a lack novelty and thorough comparison against brn.	0
in experiment qta2 the authors (with a small issue, see weaknesses) show that their proposed modification to the loss function causes their outputs to be better matched to the speech conditioning as measured by the fixed embedding network trained in stage 1. finally in qt3 they measure how well their generated faces can be used with the original face embedding network to perform image retrieval (i am a bit unclear on the details of this experiment, see questions) strengths  ' the authors' proposed pipeline and modified loss function offers a generalized framework for jumpstarting conditional gan training ' their pipeline does not assume humanspecified labels, but instead access to paired data from different modalities, which is can easily be obtained for many conditional image generation tasks (inpainting, superresolution, colorization, etc.) ' their pipeline also doesn't seem particularly finetuned for the speechdriven image synthesis task, so it seems reasonable to believe it could be adapted to other tasks ' their results are qualitatively compelling, and they make a convincing efforts in experiments qta13 to quantitatively show that the conditioning information is affecting the output ' the paper is generally wellwritten and easy to follow weaknesses  ' without completing the loop, and showing that the second stage of this pipeline helps with identity matching for speakers, i'm not clear on what the motivation for this particular form of conditional image generation is (this is unfortunate, because their framework is quite general, and it seems feasible that the authors could have applied it directly to a task where the output of conditional image generation is directly useful) ' i interpret the main purpose of experiment qta 2 as validating the effectiveness of their proposed loss modification, it seems like a natural experiment to make this claim more convincing is to compare a generator trained with eq 4. against real images (hopefully getting a much lower matching probability than the 76.65% reported in the first experiment of this section) ' it seems like an important ablation study is testing the effect of jumpstarting the gan training with the pretrained networks of stage 1, and reporting what happens when one or both of these networks are initialized randomly (assuming that initializing randomly hurts performance significantly, this would be further evidence of their framework's strengths) ' the novelty of the proposed approach is limited so far as i can tell (slight architectural modifications, and adding a negative sampling term to the discriminator loss) initial rating  weak accept questions  ' in experiment qta3 i'm confused by how including multiple faces from the same video clip affects measuring retrieval accuracy.	2
however the paper is held back by the lack of novelty and lack of clear motivation.	0
however, i have serious concerns about the novelty of the proposed approach.	0
i'm not intimately familiar with the recent literature but you mention triascyn & faltings (2019) so perhaps you could also use that and expand a bit more on the novelty here	0
cons: although likely of practical use, the work seems to be lacking in novelty in several respects.	0
weaknesses: (1) novelty: the biggest concern that i have is its technical novelty.	0
it does so by combining ideas from two lines of work, bringing incremental novelty.	0
the novelty and significance of the contributions, however is limited, as many recent works have explored using graphstructured representations and attention in multiagent domains (e.g. vain: hoshen (neurips 2017), zambaldi et al. (iclr 2019), tacchetti et al. (iclr 2019)).	0
novelty is incremental, but if the paper would otherwise be very well written, i think it could qualify for acceptance.	0
however, my main concern about this work is its novelty, especially for the proof techniques used in the current paper.	0
review:###i do not necessarily see something wrong with the paper, but i'm not convinced of the significance (or sufficient novelty) of the approach.	0
the main concern is the novelty, since the proposed method is pretty close to the graph attention network (gat), except using the two smoothness metrics and a slightly different way to compute the attention coefficients.	0
8) in the introduction, they suggest that the major novelty of their method ``assigns nonuniform importance to each block by cheapening them to different extents', but in their method they only randomly assembled the mixedblocktype models.	0
recommendation: for lack of necessary experiments and limit novelty, even if i like part of the approach design, this is a weak reject.	0
however, my major concern is the novelty of this paper or the contribution.	0
major concerns: 1. this paper lacks novelty.	0
i vote weakaccept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.	0
pros: simple, intuitive method draws from existing literature relating to dropoutlike methods little computational overhead solid experimental justification some theoretical support for the method cons: method is somewhat heuristic mitigates, rather than solves, the issue of oversmoothing limited novelty (straightforward extension of dropout to graphs edges) unclear why dropping edges is 'valid' augmentation followupquestions/areas for improving score: it would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well).	2
novelty is a little on the lower side, but thorough writing, results, and insightful comparisons make up for this in my opinion.	0
i have concerns about the novelty and significance of the work, but overall the paper feels very close to being a finished piece of work in spite of its (relatively minor) flaws.	0
weak points include novelty and significance.	0
review:###interesting topic but lacks of novelty #summary: the paper proposes that by applying differential privacy, the performance on outlier and novelty detection can be improved.	0
while this paper is more incremental and novelty may be slightly lacking, i think the breadth of experiments and competitive results warrants an acceptance.	0
weaknesses:  while interesting and useful, i am not completely convinced whether the added novelty over (bergou et al, 2019) is significant enough.	0
cons: there is not much of novelty in the algorithm and architecture.	0
although the end goal here (for pruning) is slightly different, the novelty is a little incremental.	0
especially when the set of is large (high dimensional) ' possibly use a pretrained model and only tune the , mlps overall my decision is weak accept, the paper lacks of novelty and the experiments could be more extensive.	0
i do have some concerns that i mention below (chiefly that the theory presented is a bit of a red herring), but it may be that the overall novelty/contribution outweight these concerns: (1) concern 1: the theorems (thm 3.1, 3.2) apply to the intractable version, and so are not relevant to the actual tractable version of the algorithm.	0
i like the fact that the authors of this work have chosen quite challenging scenarios, but i think the novelty of this submission is a bit weak to be accepted to the conference.	0
in addition to the above concerns, it seems to me that most of the features in r2d3 simply combines those in either dqfd or r2d2, and i couldn’t find out its own algorithmic novelty except “demo ratio” parameter.	0
one concern for this paper is the level of novelty, as each major component of the proposed solution has been explored quite extensively in the existing literature (as mentioned in the related work section).	0
the technical novelty seems incremental as crossvalidating and using a set of models under particular performance constraints does not constitutes a novel contribution.	0
due to the above issues, i feel the novelty of the approach is limited and incremental.	0
my main concerns focus on the novelty (compared to existing methods that are similar but not discussed) and the experiments.	0
the reviewer has some concerns regarding the novelty, real speed up, and guarantee of the sparsity.	0
cons:  while the method based on sliced wasserstein distances sounds new, the novelty seems limited since the idea of whitening the activation distribution to unit gaussian was introduced before as mentioned by the authors.	0
however, i have some concerns that might require clarification or additional experiments: 1. the novelty of the proposed method is somewhat limited in my opinion.	0
cons: 1) novelty of the work seems quite limited.	0
strong points of this paper: 1) novelty.	2
1) the proposed solution lacks novelty.	0
in general, the novelty of this paper is ok, but it’s partially based on deep infomax (dim) published recently.	0
my main concerns are about the incremental novelty and experiments are heavily done on one search run, especially the search space is not the same as baseline darts.	0
main concerns  incremental novelty about channel sampling.	0
weak points:  my first concern is about the novelty of the proposed model.	0
easy to implement and quite widely applicable as a regularization loss addon to multiple existing metalearning methods  impactwise, the paper takes metalearning further from memorization, making methods more capable of operating on lesscarefully designed, more natural datasets (rather than permutation of datasets)  experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets  motivation part is easy to read ################################################################################ cons:  i'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.	0
overall i think the work is interesting but it lacks some justifications regarding the claims made (as mentioned above), and although it is generalizable to other tasks and systems, it does not have sufficient novelty in its algorithm and approach.	0
cons: 1. the work has limited novelty: the learning of the world model (recurrent statespace model) closely follows the prior work of planet.	0
altogether, i think even the technical novelty is incremental, the work is not trivial considering the computational cost.	0
these strategies are not surprising for me and the novelty is incremental.	0
cons:  the novelty of the project is relatively limited.	0
my main concern comes from the novelty of this paper.	0
while i really enjoyed reading the paper and believe that this library could be extremely practically useful, i vote to reject this paper because i do not feel that it has sufficient novelty to be a paper on its own in light of lee et al, 2019. edit: post rebuttal, i'm bumping my score to a weak reject but would have minimal qualms if this paper were to be accepted.	0
cons  the paper novelty is somewhat limited as it is mostly a combination of previously existing techniques.	2
cons: 1. it is a good application of known techniques, but the novelty is limited.	0
my concerns are as follows: ' the novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning. '	0
it's somewhat unclear what the lack of convergence without noveltybound oracle implies.	0
the paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (omniglot) is shown, while the comparison on market1501 is missing; 4) lack of comparison and discussion of other related works.	0
decision: this paper should be rejected because it lacks originality, the assumptions are often too strong, and the rigor of some claims is questionable.	0
my main concern is regarding the originality and significance of this work.	0
the proposed method is technically correct but lacks enough originality.	0
however, the idea is simple and the originality seems incremental.	0
this lack of detail makes it hard to assess the originality of the current paper, and how it differs from existing work.	0
the innovativeness seems low given the several previous proposals for sparse attention, the results are not dramatic enough to compensate for the lack of originality, and the comparison to other models is wanting.	0
i feel the extension is reasonable but it is not a big 'jump' from the original work, and the originality is less significant.	0
unfortunately, the writing is still confusing, some of the claims in the introduction and rebuttal are inexact ([5] does not embed the observations and does work with partially observed environments), and the method lacks originality compared to existing work.	0
the major reason is the lack of technical originality.	0
in terms of originality and novelty, the method does not introduce very novel architectures or algorithms, but rather introduces a new combination of known techniques for the task of visual navigation.	0
the work presented here is a logical extension of these papers, and in some respects lacks originality.	0
however i find it a bit hard to advocate acceptance just yet, mainly because of two concerns that a) this paper is largely about stitching multiple existing techniques (e.g. neural processes, influence functions, monte carlo dropout, etc.) together and may hence be a little bit lacking in its own originality, and b) both the presentation quality and the empirical studies still leave quite a bit to be desired (details to follow).	0
concerns:  while the overall idea is interesting and relevant, the novelty is incremental in its originality to solve the identified concern, considering the paper mainly strings together existing ideas in terms of the methodologies (as summarized above).	0
the general idea is reasonable and the proposed model is technically correct, but i have the following concerns mainly about the originality and the experiments.	0
the proposed formulation is designed for dags, but due to my concern about the correctness raised above, it is difficult for me to evaluate the originality of the present work.	0
the paper’s contributions are useful, but do not reach a level of generality, originality, or depth justifying presentation at iclr.	0
however, this paper should not be judged based on its originality but based on its significance.	0
again, i am very concerned with originality in comparison to lee et al, 2019. even checking out the link to their codebase provides a github repo that is quite similar to this one.	0
review:###this paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models.	0
on the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm.	2
regarding the model i have some concerns:  removing the pooling layers does not seem a very good idea.	0
the authors claim that this idea is novel in the space of parametric density estimation, however i do not know enough about the area to verify the claim.	0
cons:  the core idea lacks solid theoretical supports.	0
the overall idea behind the paper could be interesting, but its realisation in the current form is questionable.	0
1. though the idea of extracting uncertainty is interesting, but i think the motivation and explanation is not enough, so i couldn't find a rationale why we should do this.	0
overall, i personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (ii) is missing from the current manuscript.	0
the idea of applying one lstm for intraclips (shorttime) temporal feature extraction and another lstm for interclips (longtime) temporal feature extraction, resulting in twotimescale, looks reasonable choice but not novel.	0
this quantity is presumably in (0,1/2], so it's a small range to start... but it seems hard to get anything from this bound without some idea of what the constant factors (c, and those hidden in o(epsilon)) and lambda are.	0
a more perceptually meaningful distance than euclidean distance would be a mahalanobis distance (which can essentially weight different coordinates differently), but the random projections use standard inner products and so are unable to capture the appropriate weighting of the different coordinates.	0
the ideas are interesting, however, the paper as it stands is lacking in thorough evaluation.	0
pros: 1. the idea of utilizing edge features looks novel.	2
however after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above).	0
cons: the main issue with the paper is the main idea is similar to [1,2, 3, 4, 5].	0
one criticism is that the paper is light: the author show that a simple idea works (this is a compliment), but i would have expected to have used the remaining space for ablation studies or a discussion on where this leads.	0
i believe that the idea is interesting but the experiments are incomplete and more investigation is required to make this paper stronger.	0
2) this paper has good idea but mainly missing ablation studies.	0
first, the idea of evolving a neural turing machine was first proposed in greve et al. 2016, which the authors cite, but only in passing in the conclusion.	0
second, the idea of learned modules to allow the approach to work across different domains is interesting, but i’m wondering how novel it really is?	0
the paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work.	0
pros  the general idea of the regularization that replaces the generation of noisy samples and optimization over it is conceptually appealing and seems practically useful.	0
in sum, while i like the overall idea and find the work novel and potentially practical, it is difficult to properly evaluate the work due to lack of comparison against stateoftheart data augmentation methods for achieving robustness.	0
the idea is straightforward and intuitive, but not that exciting.	0
however, i still have concerns about the novelty of the proposed idea (clusteringfewshot learning) and its similarity to previous works.	0
# strengths i like the core idea of the paper: assuming some desired distribution of i/o examples, construct the training corpus of random programs in such a way that its associated i/o examples approximate the desired distribution.	2
ideally make sure they all use different compositions of the language operators but still cover the space of combinatorial operator combinations well.	0
the latter is interesting, but is effectively a different implementation of the same core idea of the paper, with two changes: (a) hardcoded fitness function instead of a trained discriminator, (b) tournament selection for population evolution instead of roulette.	0
i like the idea and the overall direction, but the current paper looks a bit preliminary both in evaluation as well as presentation.	0
= strong/weak points  the idea of interpreting the 'humanlikeness' of program behaviors is interesting, and could help substantially with augmenting the traditionally small clean datasets in program synthesis.	0
although this is an incremental work, this idea is relatively novel.	0
the idea proposed in the paper, even in the specific problem context considered, are incremental.	0
while i like the idea overall and this line of work in general, there are still some concerns with the current version of the paper: ' overall, although its design seems more principled, the proposed method does not seem significantly better than the previous (very similar) method of mu et al. i would like to see more evidence in the favour of the proposed method, pointing that we should use that instead of mu's method.	0
the reviewer has the following concerns: 1. while the proposed method is capable of handling adaption of the output space between the source domain and the target domain, it makes use of a multitask subnetwork component, which is a sensible modeling choice but it seems that the idea of leveraging a global discriminator and a classwise discriminator has been exploited in previous works, as pointed out by the authors in section 4.2. therefore, the reviewer is concerned about the modeling contribution made in this paper is somewhat incremental given existing literature.	0
while it is understandable that such a decision is due to the lack of ground truth of binary label in the source domain and numeric response in the target domain, ideally, the discriminative process might gain further benefits from borrowing information across the two domains.	0
however, the idea of variance reduction is not novel and the result is not surprising since the improvement comes purely from spider (fang et al., 2018) and thus this work is somewhat incremental.	0
review:###the paper proposes an interesting idea to perform neural architecture search: first, an autoencoder is pretrained to encode/decode an neural architecture to/from a continuous lowdimensional embedding space; then the decoder is fixed but the encoder is copied as an agent controller for reinforcement learning.	0
the idea is interesting, but there are some problems on both the method's and the experimental sides: 1. nao [1] also embeds neural architectures to a continuous space.	0
this largely reduces the novelty of this paper and make it incremental, because using virtual supernodes is not this paper’s original idea.	0
overall i thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is.	0
although i found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below: 1) the idea of using attention score is simple yet seems effective.	0
some ablation studies are needed to verify that the performance gains are mainly due to the key idea of tap but not due to the skip connections that have been widely used in computer vision tasks.	0
the cited work by schafer&zimmermann only considers deterministic systems  but what is the distinguishing idea of your paper that makes your more general analysis possible?	0
when thinking about reasons that speak against the paper, i'm mostly concerned about the fact that the idea is not yet fully worked out: ' the biggest unresolved aspect is the number 'k' of instances needs to be provided.	0
the main idea is to have a fixed size weight matrix, but learn how to share weights, so that the resulting network can be compressed by storing only unique weights values.	0
pros:  chromatic networks provide a neat idea for managing compact networks via parameter sharing.	2
the idea is very intuitive, and the implementation is kind of an incremental combination of (rebuffi et al., 2018) building models for multitasks and (oreshkin et al., 2018) tailoring based on task embeddings.	0
the new ablation study is especially difficult to understand, since the first set of results are mega without using both memory and new data, but it seems to me that mega 'needs' the memory data to work, so i have no idea what is being ablated here, and i also do not understand how this answers the question why megad performs so well.	0
there are two concerns which i need the authors to address: 1. since the authors claim that they propose to speed up the computation of the euclidean distances in kmeans.	0
another idea for experiments is doing crossdataset evaluations where different datasets may have different leaf categories but shared high level ones.	0
though mirror descent is not their original idea, but using it in the context of learning quantized network is novel and interesting.	0
the general idea of the paper is clear, but i found certain parts can be improved, such as the formulation of infocnf.	0
the idea of discrete codes for fewshot classification is interesting and sufficiently novel, i am likely to increase my score if my concerns are addressed and the experimental section is strengthened.	0
the basic idea of using the typical set to check whether given data points are from a given distribution seems sensible, as guaranteed by theorem 2.1. my concern is about the performance of the proposed method compared to alternatives.	0
i liked the idea of proposing a hypothesis testing approach for finding ood samples generated from a model; however, my main concern is that the approach has some major practical limitation that the authors have also rightly mentioned in their discussion.	0
pros: ' some interesting ideas motivating the approach cons: ' an important error in the main result and various other issues (imprecise statements, lack of details) raise doubts on the theoretical contribution of the paper. '	2
overall, i think the idea is sound and sensible, but i don't think that results are convincing enough for a conference paper.	0
the idea of converting the binary lowrank factorization problem into a realvalued nonnegative factorization problem is interesting but could have been better justified (from an intuitive/theoretical and computational perspective).	0
pros: the idea of formulating the l_mim objective appears interesting, which uses the mixture of p_    heta and q_    heta and minimizes an upper bound to h(x)  h(z)  i(x, z), so it is effectively minimizing a lower bound to i(x, z), under the proposed symmetric distribution.	2
the idea itself is simple but the intuition behind it is rather interesting and (to the best of my knowledge) provides novel insights into the workings of artificial neural networks with relu activations.	0
generally, though several good ideas were presented, i felt that they were not properly motivated and that the paper generally lacks rigor and clarity.	0
pros:  interesting analysis that supports the idea.	2
it employed a very similar idea, although it is not used for cf, but for hashing directly.	0
one is the idea of dropping neurons as an adversarial attack, which i think has been studied empirically but not theoretically (to my knowledge).	0
overall, i find that the idea of unifying qa, text classification and regression is interesting by itself, but the experiments cannot justify their claims well mainly due to the mixed results.	0
the idea of making experiments on pseudoindustrial instances is interesting, but the pdp algorithm trained on those instances (i.e. pdpmodular) is rapidly degrading as the ratio increases.	0
the paper indeed does propose a new idea, but without a comprehensive comparison/evaluation, the inferences on its usefulness may not be conclusive without such studies.	0
pros:  the ideas of introducing a metamapping and treating it similarly or the same as a learning a task itself are novel to me.	2
weaknesses the idea seems to be known.	0
pros: the work presents a simple idea that is presented neatly with sufficient experiments.	2
the idea is quite simple however for example i find the pseudo code in algorithm 2 is quite hard to grasp maybe due to some typos questions to authors:  we only see performance comparison in dialogue response generation experiments.	0
i like the idea for this paper quite a bit, it really got me thinking, but i think it isn't quite structured how i would like.	0
the idea is extremely intriguing and very promising, easily leading to supportive enthusiasm; it is my personal belief however that accepting this work in such a premature stage (and with an incorrect claim) could stunt further research in this direction.	0
ideally, including it as an additional baseline would be even better, but not necessary given the limited rebuttal time.	0
the idea, although a bit incremental, is potentially useful to practitioners, as argued in the introduction, and some empirical result tend to suggest that the method can be useful.	0
weaknesses  the main idea of first unlabeled parsing then predicting labels is hardly novel, and it generally won't help, since the predicted labels could be useful features for determining the transitions.	0
the basic idea seems interesting, but unfortunately in the present form he paper is very difficult to appreciate, as it lacks of important details concerning methodology, experimental results, and comparison with respect to the stateoftheart.	0
overall, i think the idea presented is certainly promising, but needs further development to be qualified for acceptance.	0
one of my main concerns, however, is that robustifying d in gan training is not a new idea for some readers [1], so they need more clarification on the novelty of the proposed method, e.g. by discussing about it in related work or by comparing the performance.	0
#pros:  the proposed model adopted the idea from predictive coding and came up with a relatively novel idea for hsc problems.	2
such “ensemble methods” have been a big topic in supervised learning, where ensemble methods are often effective, but less so in reinforcement learning, where the evidence in support of the idea is, perhaps, weak.	0
first, the introduction glosses over the question of whether ensembles are a good idea; it kinda suggests that their effectiveness has already been established, but does not commit itself to that statement; it does not make a clear claim that might be true or false about that prior work and that would validly support interest in this area.	0
the idea is understandable but some issues remain: 1 training gan is by itself an expensive task and optimization is difficult, so how computationally expensive is this online kd compared to the offline one?	0
additionally, it includes some of the ideas presented in this work, but in a modelfree setting, including a distributional treatment and multistep rollouts, so it is easy to imagine that some of the experimental gains presented here would be erased when using it as a baseline.	0
overall, this type of utility analysis exists in the dp literature, but their novelty comes from the idea of averaging the curvature.	0
cons: the paper is not wellwritten and the experiments and discussions are not support the ideas, more specifically i can mention several concerns: 1 motivation: the flaw of the previously proposed calibration metric is not explained clearly.	0
it is of course a bad idea to model a gaussian with a cauchy distribution (the way it is always a bad idea to have a misspecified model), but i would actually argue that the isotonic regression based approach by kuleshov et al., 2018 is more robust against model misspecification due to its higher flexibility and thus a more powerful approach for poshoc calibration.	0
the organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable.	0
pros:  the idea sounds interesting.	2
curiously, this idea is alluded to in the introduction, but not followed up on.	0
overall, there are some nicely presented ideas but the work is unfortunately incomplete, and the results cannot support the hypothesis laid out.	0
the idea of the design is reasonable, but the theoretical and empirical results are not strong.	0
detailed comments: i'd like to recommend 'weak reject' due to the following reasons: 1. lack of novelty: the main idea of this paper (i.e. regularizing the outputs from normal and randomized states) is not really new because it has been explored before [aractingi' 19].	0
pros:  the idea of using additional outputs to the nmt system and outputs from a contextaware system is neat.	2
the idea is itself new, but very similar ideas are well known in the literature, and it is difficult to conclude that the proposed approach is superior.	0
pros • interesting approach to extend the framework in [1] to cnns, with use of masks and maskspecific loss • clear motivation for the network bandwidth limited use case cons • hardly any technical novelty because the core ideas are already presented as well as applied to the same task in [1] • it is very surprising that the authors do not even cite [1] in their paper, despite their work being extremely closely related to it • most of the discussion in the related work section is unrelated to the specific task they tackled in the paper (i.e., input/feature selection).	2
strengths:  the idea of using checkpoints as free samples of models is new and practical.	2
the idea itself is straightforward but worth exploring, although it does make the fairly strong assumption that one has access to or knowledge of the transformation function of the environment that can be applied to the observations.	0
the significance of the paper is moderate as the key idea of learning disentangled latent variables has been studied, and the paper lacks of evidence to show the pure benefits of introducing z_x as well as the comparison with the related work [ref1].	0
# originality the main novelty seems to be coming from the idea of parameter sharing between pairwise payoffs, but the overall architecture seems to be the same as [castellini et al.].	0
# significance  although the paper presents a new idea very well, the overall idea seems a bit incremental.	0
the following comments are not as critical but fall into the category of ‘nice to have': 5) although the reviewer is aware that there were some experiments in the appendix on the value epsilon, it might be a good idea to have more studies on the influence of this regularisation parameter for other datasets rather than just mnist 6) while figure 1 appears in the beginning of the paper, on page two, it is discussed on page seven, in the experimental section.	0
?the idea is overall interesting, but its relationship with many other methods is not adequately examined.	0
(1) the idea of using decoy images is interesting but computationally expensive.	0
running each model with different random seeds and presenting statistical significance results would be ideal, but such runs can be expensive given the size of the datasets.	0
while the overall idea seems simple but the coordinating among agents can be difficult, where the authors proposed credit assignment techniques to address the issue.	0
the paper proposes to combine the implicit function idea with output mode estimation, however the problem definition is vague, competing bayesian methods and density estimation methods are ignored, the experiments are insufficient with little stateoftheart comparisons, few datasets and clear problems in the learning.	0
it seems like a very neat idea, but it is only briefly explored in the ablation study.	0
the idea might have some merit, but the paper in its current state fails to convey it clearly and is too vague on the experimental settings for the results to be considered in any way.	0
so, to conclude, i find that this paper goes in the right direction and introduces important ideas for pretraining and fine tuning unsupervised abstractive summarization models, but that some decisions about how to use the various ideas (theme and denoising but no supervised learning, indomain vs during pretraining) have not been explored enough.	0
i like the simplicity of the idea, however the analysis is lacking in rigour.	0
overall, the proposed idea is simple and easy to implement, which is the main advantage of the paper, but it is evident that the analysis and evaluation lacks rigour, hence the paper will need to undergo significant revision to be in a publishable state.	0
the premise of this work is to not store old transitions (section 1, the paragraph 'the most simplistic idea is to...'), however this model does store old transitions because it uses prioritized experience replay.	0
but the work in the paper is not in an acceptable form due to the following key reasons: (1) the presentation of the ideas and the algorithm, 'despite the lack of theoretical guarantees' is hard to understand, (2) the dqn baseline compared to (definitely in the augmented chain environment, and possibly in the atari games), are based on a fixed epsilon exploration strategy whereas dqn as proposed uses a epsilonannealing strategy for exploration, (3) the baselines compared to are not comprehensive, (4) generally the paper is rather unpolished.	0
so overall the idea makes sense but i have the following concerns.	0
strengths 1. the idea of using metalearning to deal with the forgetting issue is interesting.	2
the main idea is to apply a “residual learning mechanism”, which resembles an autoregressive model, but here conditioning between sequential steps is done in both latent and input spaces.	0
overall i think the idea is novel and interesting, but the improvement is not big enough to replace existing normalization methods that makes this paper slightly below the acceptance threshold in my opinion.	0
basically, what i am saying is that the idea is nice, but the results look a bit magical.	0
review:###the paper is well written, but severely overestimates the core contributions embedded in section 3. firstly, the idea of using drop out for matrix sensing seems to be a somewhat trivial extension of the work on dropout for matrix factorization  http://proceedings.mlr.press/v84/cavazza18a/cavazza18a.pdf.	0
the writing is clear to understand the main idea but can be improved in various places.	0
this idea is nice, however i am concerned with speed of such transformations in practice.	0
i believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding the presentation and the combination of different assumptions used in the theory.	0
pros: the reverse pass idea is similar to autoencoder paradigm that discards redundant information and only save the essential low dimensional one by comparing the original data and the decoding data.	2
further, the idea of concatenating all tuples of a relational db to be passed through a particular feed forward layer is infeasible for all but the smallest datasets.	0
using math is good, but using more than necessary simply slows down the dissemination of ideas.	0
the idea is good, but it seems like gans have been suggested for imbalanced data sequences before.	0
unfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking.	0
in regards to the novelty claim, there have been several developments of depthbased (as opposed to timebased) adaptive computation time in the literature, for example: [1] mcgill et al 2017 'deciding how to decide: dynamic routing in artificial neural networks' [2] bolukbasi et al 2017 'adaptive neural networks for efficient inference' [3] figurnov et al 2017 'spatially adaptive computation time for residual networks' (this paper is cited by the authors) these papers do not present sequence models, but the ideas in them readily apply to sequence models.	0
pros: this paper extends the idea of hypergradient descent in [almeida et al., 1998; maclaurin et al., 2015; baydin et al., 2017] with a preconditioning method.	2
also, it discusses some open opportunities on how the embeddings can be used as a base knowledge source and some challenges in them pros: 1. the examples and analysis of embeddings involved in the paper is detailed 2. the authors have run lots of experiments to position their idea correctly.	2
the ideas from section 4,5,6 seem to be rather obvious extension of what we know 2. especially, in section 4,5 the idea of finding indexes, is nothing but the projection of x on the mean of (a,b)  which is more of a common practice.	0
# quality  the proposed idea is very simple but seems very effective in practice as shown by the empirical results.	0
# significance  this paper proposes a simple but effective idea that can be potentially widely used by the research community.	0
[overall] pros: 1. the idea of avoiding the logsumexp computation in the dv representation of kl is good.	2
this idea may not be too novel, but definitely is useful.	0
pros of this paper:  the idea of using the lipschitz continuity of the value seems interesting  the paper seems to solve an important problem cons of this work  i think this paper is not yet ready for publication.	2
but, to the best of my judgement, this paper has an interesting idea but is not yet ready to be published in iclr.	0
previous work like [12] have already applied the same idea to different but very similar task, malware detection of binary files.	0
on the positive side: ' the ideas underlying hsimple and hype are natural and clear  we basically want to learn better embeddings for hypergraphs, and there are a couple of obvious ways to do that.	2
on the negative side: ' the ideas are obvious; the results are unsurprising; the paper lacks 'deep insight'.	0
i really like the idea of hype, but it seems strange that entity embeddings are modulated based on position 'only', without regard to relation  that is, it seems like a given entity in position #2 might need to be represented in very different ways depending on which relation is being used.	0
while it remains unclear if stam can generalize to realistic images, the idea of upl is very interesting and speaks for accepting the paper, although there are several weaknesses the authors should address.	0
the authors evaluate whether this regularizer hurst reconstruction error i have found this paper interesting, but confusingly presented and still in a development phase (i.e. good idea, but not properly exploited yet).	0
because of this, i feel that overall the paper starts with a nice idea but has not reached yet the state in which it deserves being published.	0
the experimental results look reasonable and thorough, however the methods are sold on the idea of better representations for data missing from the training set, whereas the results are focused on anomaly detection.	0
overall, the paper presents a promising idea but it needs a more clear and rigorous presentation.	0
i believe that this paper contains interesting and novel ideas, but there are also some inconsistencies and some points that are vague.	0
the evaluation of the idea is not complete while it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other blackbox attack methods in the limited query budget regime, i would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.	0
the idea is conceptually intuitive, however i have some questions regarding the details:  it's surprising that this works at all, since the inference representations seem to come from an entirely different distribution than in training (e.g. figure 2)  given that this works, i'd like to see an ablation in which y is removed (e.g. we go from x and yu to gy), so as to evaluate the utility of the 'correlated response encoder'.	0
while the idea is quite interesting, the paper is poorly written and many times hard to read.	0
also in the introduction, you could consider referencing the following papers on robustness to different threat models: ' engstrom et al., 'exploring the landscape of spatial robustness' ' tramer and boneh, 'adversarial training and robustness for multiple perturbations'  in the related work, you reference a number of works for adversarial training but not the original idea in the paper by szegedy et al.  in section 3.1., the observation that the epsilonball around training examples can cross class boundaries is further analyzed in 'excessive invariance causes adversarial vulnerability' and 'exploiting excessive invariance caused by normbounded adversarial robustness' by jacobsen et al. typos ===== p.6 enumerator > numerator	0
however, i find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, detailed under: experimental concerns: — evaluating on 200 examples seems very small.	0
strengths  the idea of modeling multiobjective as a logical language is novel.	2
the idea is quite simple but seems to be effective (up to 1.9 bleu on german to english and 0.6 bleu on thai to english).	0
to sum up, i like the idea of asynchronous regularization/compression, but i'm not quite satisfied with current version of paper.	0
therefore, from the modelwise, this paper combines several existing ideas, but does not provide new insights or techniques, so the contribution is quite limited.	0
the authors try to illustrate their idea in figure 2, but the figure is also quite hard to understand.	0
there are interesting ideas in this paper, however i have some questions and concerns about some of the claims made in the paper that i would like to see addressed.	0
pros: 1. nice idea for tackling the unbounded action space problem in textbased games.	2
overall comment: i think the paper presents an interesting idea but i have questions regarding its practical significance as highlighted in my specific comments above.	0
in this sense, the main idea of this work is incrementally novel.	0
the idea of combining the strengths of both cnns and crfbased graphical models in a detection framework is interesting.	2
i think the idea of analysing embedders in this way is potentially interesting, but it feels like the paper needs more.	0
overall, while the idea of probing similarity between embedders is interesting, the paper has the following weaknesses:  the use of neighbor overlap to compare embedders has precedence in the context of word embeddings [https://www.aclweb.org/anthology/c161262/ | https://hal.archivesouvertes.fr/hal01806468/ | https://www.aclweb.org/anthology/q181008/].	0
pros: the idea itself is interesting, the related works are discussed well, and mnist experiments are very interesting.	2
while i understand the idea behind this setup, the insight that bert performs poorly if the text is truncated to 8 or 16 tokens is not helpful.	0
'main arguments:'' 1. this works lacks novelty since the main idea is just a simple concatenation of a jpegartifactremoval model and a super resolution model.                  	0
the very idea of a flowbased model for graphs using the message passing algorithm may be considered as a contribution, but this is also blurred because of the concurrent work gnf.	0
the idea seems wellmotivated and somewhat new though not revolutionary, and the new data sets are nice (though see comments below about how i'm not qualified to evaluate them), but i don't understand why the proposed algorithm wasn't evaluated on existing data sets as well, and i don't understand why it wasn't compared against other algorithms that purport to do the same thing.	0
i think that technically this idea is new, but there is important related work [1] that (imo)  does essentially the same thing  better experimentally validates the thing they do  was published in last iclr.	0
this paper is well organized and clearly written, but i cannot agree with the main contribution (klcontrol penalize) of this paper (because the kl penalize idea is very similar to proposed approaches in rl).	0
i am not familiar enough with related work to evaluate this statement, but given it’s true, this is an interesting application of these ideas.	0
other than that, i like the main ideas in the paper, but i think they could be presented better.	0
however, in my opinion there are a few weaknesses in the paper in its current state: (1) the clarity of sections 3.13.2 could be significantly improved as currently the proposed probabilistic model framework is confusing and not welldefined; (2) the experimental results are provided only for embedding spaces of dimensionality 23 which significantly hinders performance of prototypical networks compared to having a much higher dimensional embedding space, so it would help to see comparisons of spe and pn using highdimensional embeddings; (3) the central idea and proposed model seem to be very close to those of [2] which is mentioned in the related work, so, please, list differences with this prior work in the updated version.	0
roughly speaking, the main idea of knockoffs is to construct a duplicate (“knockoff”) of each regression variable which matches its distributional properties but is independent of the response variable when conditioning on the true input variable the knockoff is based on.	0
weaknesses:  overall, the idea looks very original and promising, but i find some technical details are not easy to understand under the current form, especially for nonexperts in this domain.	0
overall, i have the impression that this paper contains interesting ideas, but the presentation is very poor.	0
# strengths ' the idea of applying the pretraining/finetuning paradigm to program analysis tasks makes sense, and has been informally attempted by multiple groups in the community in 2019. this is the first highquality submission to a toptier ml conference i've seen on the subject, though. '	2
generating classlevel interpretations is an interesting idea but its novelty is somewhat unclear.	0
2. decision: weak reject  the classwise idea for curriculum learning is interesting but the motivation and intuition behind the design of the proposed method is weak.	0
3. reasons for decision: pros: the classwise idea used in this paper seems to add a new direction to the area of curriculum learning.	2
cons: the motivation behind the ideas of the paper and the design of the procedure was not so clear.	0
cons  the idea of starting from a small, sparse network and expanding it is not novel.	2
this idea was originally proposed by xie et al. (2019) but this work was not discussed.	0
concluding, the idea is nice but based on the current state of the paper it seems incremental.	0
this idea could be a good tradeoff between full rnn (slow) and no rnn (lack of context), but the following is missing: (1) ablations on speed vs results by locality window, (2) experiments on more widely reported and larger datasets and models, at least including some language modeling task (wiki or lm1b) and some translation task (like ende).	0
i enjoyed the presented ideas, however the authors could have done a great job of clearly presenting their work.	0
i liked the idea quite a lot and would not mind seeing it in the conference, but given the number of issues raised by myself and others it seems that the best route forward is rewriting the paper given the inputs by the reviewers and submitting to a future venue.	0
as a summary, the idea could be interesting, but the paper is not mature enough.	0
despite the concerns regarding the clarity in writing and the rigor in the theory of the paper, i think that the algorithmic idea proposed in this paper is interesting, novel, and practical.	0
i tend to reject the paper in its current form because (1) the idea of using mixture of trees to do policy extraction is somewhat incremental; and (2) experiments do not consistently show significant performance gain over the existing approach.	0
pros:  the core idea of adapting bayesian truth serum to ensemble prediction in machine learning seems sensible  there is some evidence that the methods have an advantage over other common ensemble approaches in practice  although there are quite a few small english mistakes, the paper is well structured and generally quite easy to follow cons & questions: 1. the theorem statements and proofs are underwhelming.	2
theorems provided in the paper doesn't show the goodness of the proposed algorithm, but it tells that the idea of having a mixture model is not a bad idea.	0
i'm aware of work on general semisupervised learning and see the much better performance of this approach compared to things like label propagation, but cannot say for sure whether the idea is novel/significant.	0
i think the highlevel idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.	0
overall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete. '''''''''	0
decision: overall, the paper is poorly written and lack of enough substance, even though the idea seems reasonable.	0
on the other hand, the main concerns are: 'only a rough idea is introduced and discussed, then followed by some nice practical results.	0
in the middle, there is lacking of concrete execution of the proposed idea.	0
overall i feel the idea is interesting but a bit straightforward.	0
typo: 'they main takeaway' > 'the main takeaway' strengths:  good related work  somewhat complete evaluation weaknesses:  no analysis with so many hyparparameters (reg lambda, number of concepts), and thus not sure about the validity of the simulation  idea is interesting but straightforward  not very interpretable results	2
pros: the idea is quite simple and easy to implement.	2
the paper would be greatly improved by adding:  an intuitive paragraph in the intro that explains a concrete example of ds high level, but with enough details for the reader to grasp the idea)  adding a new section right after related work (and before the current '3.	0
the general idea is quite interesting, but i have the following concerns.	0
strengths  explicit centroid selection based on signal intensity variation seems like an intuitive idea worth investigating.	2
summary the core idea of the paper, clustering nodes based on signal intensity for hierarchical graph representation learning, is interesting and seems to be useful in practice, but the paper has issues with clarity and the rest of the framework is a bit limited in novelty (diffpool  sparsemax  gin).	0
the proposed laplacianpooling ideas could be interesting, and the results encouraging  but i found the mathematical motivation to be not very convincing, and has various (mostly correctable issues).	0
i am not sure if the idea of coarsening (via hierarchical pooling) can be meaningfully applied to a widevariety of natural molecules  but it does seem to make sense for some organic molecules  e.g. protein chains.	0
while the idea has some merit, it is an interesting premise to train a recurrent generator to create images in multiple domains, i feel as though the experiments are quite lacking.	0
i kind of get the rough idea why fast convergence over t would be bad, but would total divergence over t not also be bad?	0
please do at least comment on realworld applications, and whether (and how) the ideas here would apply  discussion: 'there is no clear principled way...': well, but practitioners need something.	0
this paper suffers from moderate clarity issues, but i lean toward acceptance primarily because i feel that the central idea is a solid contribution.	0
the 2nd idea is interesting but not very well explained to some extent: 1. the goal of the modified objective function is to encourage the model to use the discrete states (instead of pushing all useful information to the continuous states).	0
the idea is that the product abc (for a, b, c all sparse matrices) can accurately approximate a dense matrix d, but a(b(cx))) requires much less work than dx.	0
i think the paper tackles an interesting line of research with some interesting ideas, but i'm concerned that the implications of the major assumptions that are made are never examined.	0
pros: 1, i like the idea of exploiting graph decomposition.	2
cons:  the idea itself is rather straightforward.	0
the basic idea is that the weights of the underlying rnn/autoencoder are not fixed, but are coming from another rnn/feedforward network which captures the underlying dynamics and adjusts the weights accordingly.	0
i would need to be convinced that we are tackling a real problem, not such an idea of something that may happen but in practice is quite rare or not necessarily that harmful.	0
sure, the wording is not the same, but the idea is there nonetheless.	0
== summary of my opinion == i liked the idea, but i think i liked the core idea of dip more than the extensions that this paper provides.	0
i have very little knowledge of this field, but the main idea idea seemed quite novel and insightful.	0
the idea is quite interesting but there are certain problems in clarity regarding the loss terms and the precise need for them.	0
the main idea is quite clear but how this training procedure achieves that is not coming out clearly in this version.	0
this paper seem to present a good idea that should be published, but is currently not clear enough.	0
i think the idea looks interesting, although the novelty is a bit incremental, as it basically combined the two wellknown models (vae and glow).	0
below are some pros and cons for this paper: cons: () the proposed method is simple and it could be quickly adopted by robotics researchers and practitioners () the method seems to outperform previous algorithms pros: () the idea of working with deterministic systems is restrictive.	2
though training for robustness is an interesting idea in itself, but the general opinion about using interval propagation to train networks is negative.	0
pros: the main idea is to check the nonstationarity of the iterates and decrease the learning rate if stationarity is detected.	2
about rating) i think the idea looks novel, but the method is quite straightforward, and the paper does not incorporate any analysis as a backup for the proposed method.	0
the idea is rather simple, but the results and the execution is inspiring.	0
the proposed method is tested across a diverse set of audio generation tasks overall, the idea of generating audio from 2d spectrogram is original but in my point of view the use of stft is not appropriate in this context, especially with its lossy criterion.	0
the basic idea of this paper is solid, but the mathematical definitions don't seem to capture that idea.	0
pros:  i like the paper and the idea behind being able to improve or even train a student network when the original data is not present.	2
i think it was a good idea to exemplify the differences but is not clear enough.	0
note that we cannot have : log [0.6' exp(e(x|c1))  0.4' exp(e(x|c2)) ] will output c1 with probability 0.6 and c2 probability 0.4. fig. 1 seems to illustrate ideas at first sight, but is not so convinced at second thought.	0
the paper is mostly well written (it could use some proofreading, but the main ideas are explained well).	0
the domains considered are fairly simple, but the idea seems sounds and the results seem good.	0
the idea of reducing the problem to a binary classification between similar and dissimilar examples may look too simple but i) is a common approach in deep metric learning, ii) helps to handle the implicit imbalance problem and iii) suggests possible generalisations to other networkbased problems (for example, where similarity is naturally defined by the existence of absence of a link).	0
pros: ' generative modelling of partially observed data is a very important topic that would benefit from fresh ideas and new approaches ' i really like the idea of explicitly modelling the mask/missingness vector.	2
combining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague.	0
in sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.	0
i want to point out that i really like the ideas presented in this paper and i think it has the potential to make a strong contribution but the issues in their current form require substantial revisions and additions.	0
# concern 1: technical correctness the paper claims at multiple places that the geometry of euclidean space is 'trivial' or 'too simplistic' to meaningfully reflect the structure of the data.	0
the paper states that 'ideally, this step preserves the topology of the data [...]', but this is never analysed.	0
i fully agree with the idea that charts are a suitable way to describe complicated manifolds, but the paper needs more precision when terms such as 'topology' and 'geometry' are being used.	0
i get the idea that increasing the number of charts will probably decrease the reconstruction error, but this comes at the obvious expense of even more parameters.	0
the ideas of the sampling or interpolation experiments go in the right direction, but in their present version, they are not entirely convincing.	0
for figure 4, please show the full space, together will all charts  p.7: please give some ideas (see above) for how to use the covering of the points in practice; i like that the object can be reconstructed with a proper set of charts, but the paper could make the necessity of the technique much more obvious by choosing stronger examples.	0
the idea is good, but the the same idea (same reconstruction goal, also relying on autoencoders) was published this year, see here: https://ecmlpkdd2019.org/programme/awards/ algorithmically speaking, the approach is very close to the above paper; as far as i can tell, the main difference lies in the recursive feature elimination heuristics.	0
however, the idea is simple and the originality seems incremental.	0
strengths, 1, the idea of learning kernels in metalearning is interesting.	2
thus, i think the proposed idea is a little bit incremental.	0
3. regarding the approximation to the bound, approximating log det(i) with log trace(i) is not a good idea: adding a very small eigenvalue will lead to noticeable change in the former, but negligible change in the latter.	0
but when we have a more complicated and less regular graph, then compressing all of that curvature information into a scalar doesn't seem like a good idea, and indeed that's the point of the gu et al work that's being built on here: it mixes and matches various component spaces that each have constant curvature, but altogether have varying curvature.	0
for example, (1) in the last paragraph of page 2, they claimed that the language component is used in their model to capture syntactic information, which i do not feel comfortable to accept; (2) in the first paragraph of page 3, it says 'we define d_j as the bow vector summarizing only the preceding sentences', without further information, i have no idea what a bow vector looks like or how it is constructed; (3) in the last paragraph of page 3, it says using dirichlet priors to make 'the latent representation more identifiable and interpretable, but also facilitates inference', which i really don't know what it means.	0
my intuition is that the weights that touch every subproblem are the most motivated to find the largest principal component, the weights that touch all but one find the next largest, and so forth; i found this idea clever and elegant.	0
2. the proposed algorithm avagrad is a simple but interesting idea.	0
however, i think decoupling the learning rate and the damping parameter by normalizing the preconditioner is a simple but interesting idea, and i am willing to increase my score based on the discussion with the authors and other reviewers.	0
### strengths 1. the illustration of the authors' idea is clear and concise.	2
even shorter time constant signal might be valuable as well, but it would not be visible here... it seems that in sepsis, it was a good idea, as it improves the result compared to the situation of not using the gaussian model.	0
basic idea of the paper seems promising, but reported results are only partial.	0
in addition to this, the paper only presents results on mnist in a toyish setting, this makes me feel the paper may be more suited for publication in a workshop (idea is interesting, small scale experiments to illustrate the insights, but not complete enough to be published at a conference).	0
pros   nice, novel method  good experimental results cons   paper is poorly written  very few details of the scaling factor variations  only one dataset considered although the paper is written badly and the narrative is muddled, the underlying idea is a nice one, which is executed well experimentally.	2
overall, the paper has interesting algorithmic ideas, but there are critical issues in the evaluation and resulting claims being made.	0
i do think there is value in the compositional idea, but for different reasons outlined in the suggestions.	0
i think the idea is still interesting and worth pursuing, but given some of the new observations and experiments run the paper needs to make more changes than i would find reasonable for acceptance.	0
weaknesses of this paper:  novelty: the idea is incremental.	0
the basic idea is very interesting, but i would have expect more interesting use cases as teased by the first sentence of the abstract 'find algorithms with strong worstcase guarantees for online combinatorial optimization problems'.	0
i really like the idea, but i have the feeling that this technique should have been developed for more complicated setting.	0
neither mixup nor the idea of fixing some number of labeled elements in a minibatch is new, but that's not the point  we thought one thing, and this paper suggests that we were wrong about that thing  to me this is exactly the sort of paper it's good to have at conferences.	0
while my concerns and suggestions are extensive, this paper is perhaps unusual in that all of the issues i have are quite fixable; the core idea is good, but its realization and presentation in the paper need a substantial amount of revision.	0
while it is interesting to note that in this exact setup (mostly dim_z=512) lia outperforms the baselines wrt the chosen metrics, a more thorough evaluation would, for instance, sweep the choice of dim_z, and ideally present nll results (which i think are possible to evaluate given that lia has a flow model even if it’s not trained using nll, but i’m not 100% sure on this front and am open to counterarguments on this front).	0
i believe this approach is a combination of core ideas from multiple sources, such as boosting, selfpaced learning, continual learning and other curriculum learning approaches, but overall it seems different enough from each one of them individually.	0
because of the resemblance with these many different methods, the method itself does not surprise through the novelty of a new idea, but the authors seemed to have found something that was missing from these methods and that leads to very good results.	0
the idea of using the pretrained discriminator network seems reasonable, but i am missing what the compression method for the generator network actually is (section 4).	0
i agree this is wellmotivated but it has little novelty and a similar idea is there in vqa (see “dynamic fusion with intra and intermodality attention flow for visual question answering”).	0
this sounds incremental and i think the paper could emphasize much more why this idea is a significant advancement over previous works.	0
section 1 lacks detailed description of the main idea and the proposed optimization methods, which actually confuses the reader.	0
the idea seems original and well executed but i think the experimental section could be improved.	0
pros:  the proposed idea of using an inverse mapping is straightward but shown to be effective.	2
leveraging spatial distortions (or other visually meaningful transformations) to generate adversarial attacks is not a new idea, but the authors seem to have been unaware of this very large body of work.	0
the idea is simple but nonetheless developed in a smart and effective fashion.	0
to sum up, a very nice idea that shows promise but experiments on other problems is missing for a complete picture.	0
in conclusion, the idea proposed in the paper is interesting to me but the effectiveness of each module is not very wellsupported by the experiments.	0
lastly, the idea of designing the global embedding and the task aware embedding is interesting but shouldn’t really be restricted to fewshot learning.	0
i like this idea and have one concern about the experiment.	0
there are multiple ideas introduced at the same time  the mi(s;g) and talking about test time and training time exploration  but the idea itself is not convincing for a sufficient exploration objective.	0
while the idea of using the log trace covariance as a regularizer for the manifold is certainly interesting, it seems fairly incremental upon previous work (i.e., dmwgan).	0
since the idea is very simple, one would expect a lot of theory, but the main theoretical result in section 4 can be copypasted from the previous algorithm since they share the same result.	0
furthermore, the paper lacks a discussion of related work on incorporating wavelet ideas into neural networks.	0
i have several concerns with this work: ' the idea of using wavelet transforms to augment cnns has been more thoroughly explored in prior work (e.g., see [1]). '	0
the paper presented an idea for selfsupervised learning from multiple views, which is not exactly the same, but still in the same regime.	0
5. i thought the idea of looking at feature importance just before clinician intervention was a very clever evaluation, but i wanted the qualitative evaluation to go one step further.	0
obviously it is not possible to augment data for that, but it would be good to state this somewhere as the idealized goal of this work, which you approximate by data augmentation.	0
however, i am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement given the lack of proper baselines.	0
the method of choosing the support set also makes sense to me, however there are many hyperparameters in this idea.	0
i think the overall idea behind the paper is valuable, but i don’t think the submission meets the acceptance bar from a clarity point of view.	0
strengths  the idea is novel.	2
main reason is as follows; i believe the idea is interesting but it needs a significant empirical work to be published.	0
the idea of this paper is intuitive but i feel that it is highly related to the one in khrulkov & oseledets (2018).	0
the paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to 'directly, approximately solve nonconvex nphard optimization problems that arise naturally in unsupervised learning problems.'	0
i think this paper is an interesting idea, and my only other main concern is the transfer results to other nli datasets.	0
pros: • the idea of using the previous unused state in ternary neural network is interesting • overall, the paper is well written.	2
cons: • the idea of using mixed bit width for neural network quantization is not new.	0
overall the idea is interesting but i have some concerns mainly about evaluations and baselines which i am including below.	0
strengths: the proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss.	2
pros: overall, i think the idea of this paper is clear and the whole paper is easy to follow.	2
a similar idea is proposed in [2], where the authors also discussed logical constraints and generative models, but they call the augmented penalty as 'posterior regularization'.	0
my take: the idea in this paper is moderately interesting, wellfounded, has plenty of precedent in the literature (while still being reasonably novel), but the results present only a minimal improvement (a 5% relative improvement in fid over the baseline model from rosca et al on cifar, especially when including sn [which is not a contribution of this paper]) and come at a substantial compute cost, requiring up to 30 extra samples per batch in order to attain this minimal increase.	0
i am pretty borderline on this paper ( about a 5/10) but under the 1368 scoring constraint i tend to lean reject because while i like the idea, i do not think the results are significant enough to support its adoption; i think the relative compute and implementation cost limit this method’s potential impact.	0
the idea of using unlabeled data for continual learning is interesting and to the best of my knowledge this is the first work that suggests using delayed feedback for continual learning but unfortunately they do not consider measuring forgetting and this work seems an online learning method using delayed feedback.	0
atair 57 could work but ideally a different setting such as dmlab 30 or continuous control from pixels.	0
update: while i still feel the exposition could be improved to make the underlying idea clearer, i feel the authors did a good job of addressing my major concerns in their reply, hence i have raised my score to a weak accept.	0
i still have doubts about the general benefit of the approach over bootstrapping but since it is not entirely clear to me one way or the other i feel the idea at least warrants further exploration, and it would be reasonable to accept the paper to make the community aware of it.	0
in particular, the paper proposes an idea and theory to back it up, but it doesn't really propose a practical method, and as a result it doesn't test the theory in practice.	0
theorems 2 and 3 provide a solid foundation for the proposed idea, but it's not clear how they can be used in practice.	0
weaknesses of the paper: 1. the idea of utilizing anchors to reduce the size of features (in your case, the total embeddings of discrete objects to be inferred) has been widely studied in related fields in computer science.	0
the main concern i have is the idea of verifying other explanations using a neural network itself.	0
the idea itself is intriguing but the derivation and some design choice are not very wellexplained.	0
detailed comments ================= the idea of increasing robustness by maximizing interclass margins and minimizing intraclass variance is fairly natural, but the author's discussion of their approach (mainly in sections 1 and 2) is very handwavy and relies on a lot of general intuitions and unproven claims about neural networks.	0
in general, i like the paper, but i have the following concerns: although we can easily get the idea that universal representation is more expressive, however, i did feel a small conceptual gap between isomorphism test and universal representation.	0
(see table 3 of https://arxiv.org/pdf/1511.03339.pdf) using entropy as a measure of network uncertainty is a good idea, but also not a novel finding.	0
the main idea is not very complex, but generally makes sense.	0
the main limitations of the current empirical evaluation are: • only 7 atari games are used (vs 49 in the ppo paper the proposed technique is compared to), without justification for how they were chosen, and it seems like only 1 run is performed on each game (while rl algorithms are well known to exhibit high variance) • on obstacle tower there seems to be also only one run of each algorithm (more runs could be done with different training & testing seeds in order to get an idea of the variance) • there is no comparison to ppornn on obstacle tower • i think a natural and important baseline to compare to is using the same architecture as in fig. 2 but where the mapping phi(o_t) is learned through regular backprop (using the same loss as when learning the mapping i(t)).	0
overall, the idea is an incremental one, although interesting largely based on the fact that this works.	0
the idea is interesting and somewhat reasonable but i still have several concerns.	0
however, the idea is easy to understand and interesting but the contribution does not seem strong enough in its actual state.	0
strengths: i thought the idea is nice, and the results do seem to show improvements in a number of interesting tasks.	2
the authors found that on some datasets, using half of the scans is already enough to reach similar accuracy but speeded up the convergence by a factor of 2. detailed feedbacks:  the paper presents a simple idea that directly uses the nature of jpeg compression.	0
although the idea is cute, i feel the paper fails to convince why spiking nets are more robust to random noise; the explanation using backprop rules in section 3 sounds interesting but does not fully convince me; for example, if we train a cnn by other approach instead of backpropagation, can we also improve robustness to input noise?	0
as said before, the idea is attractive but the paper lacks motivations for the choice of dependency trees as additional linguistic knowledge.	0
i find the idea of the paper interesting but the content not convincing enough.	0
‘3% accuracy’ [relating to figure 1] i understand the idea but accuracy seems misleading.	0
i am still in favor of the idea  applying simple, oldschool method into a new problem, and i also agree with r1 and r2 that the paper is currently lack of details and experimental results.	0
i lean to vote for accepting this paper since the idea is simple but novel, and it achieves good performance.	0
strengths  the idea itself is simple and novel.	2
contributions: on the positive side: a datalog interface to specifying structural zeros in parameter matrices is a good idea.	2
the changes in segnn compared to previous works are incremental or not novel, and the overall idea is the same as alphago/zero.	0
the main idea is fundamentally simple, but it is still difficult to get it from the text.	0
the authors propose a simple but original idea to address this issue: parameterize the network's weight matrices as low rank tensors (in the tucker format) and randomize the weights by sketching the core tensor of the tucker decomposition (in effect, the sketching amounts to randomly setting fibers of the core tensor to 0).	0
the idea is interesting and addresses the problem of sparsity artifacts in randomized defense strategies, but it does not appear clearly why using tensor decomposition techniques is a sound approach for designing robust networks (besides overcoming sparsity artifacts).	0
i believe there may be more fundamental (theoretical, principled) arguments to motivate the approach, but this is not explored in the paper: the idea is interesting but not supported by much theoretical insight.	0
the idea is interesting and definitely worth exploring but to me a more thorough discussion and analysis of why tensor decomposition techniques are relevant is missing.	0
pros: 1. the idea of using randomized tensor factorization for dense is novel 2. it seems that this defense is robust to large perturbation (epsilon), and the accuracy on clean data is high when combined with pgd adv.training.	2
## overall assessment overall, i like the idea of this paper and finds it very interesting and promising, but feel it would be on the borderline or slightly below the bar.	0
the layer freezing trick however is relatively wellknown, and thus leaving the novelty of the proposed idea to be limited at what layers they choose to freeze.	0
overall, i found the idea of the paper interesting, but the attempt to generalise the effect from metalearning to general learning setups hard to follow and detracting from the overall value.	0
overall, the idea is clearly presented, but appears to have many critical flaws, enumerated below: 1. it is unclear what the motivation for the symmetry is upon first reading of the paper, i.e., section 3.1 starts by saying 'to overcome the shortcomings of apl,' but up to this point _x0010_i cannot find any exposition that explains what these shortcomings arethe beginning of section 3 just presents the formulation of apl and does not discuss its advantages/disadvantages.	0
strength: [1] this paper tries to solve an interesting question and the idea is simple and intuitive [2] experiments show that the proposed approach outperforms a number of baselines my comments: [1] main concern: lack of ablation study.	0
2. spectral graph translation looks interesting, but the main idea comes from kunegis et al. (2010).	0
technically speaking, the paper is outstanding, but it lacks novelty in terms of new ideas.	0
my point is that the paper presents exactly the same idea as the original paper of cpc, but with new, very interesting results.	0
10. section 5.4 is a good idea, promising, however it does not really conclude into a very strong statement, and takes essentially 1 full page, which could be used to better clarify the architecture and/or the training procedure (or to reduce towards the ideal page length).	0
the idea is close to many other proposed in the literature, but to my knowledge it is the first time this exact procedure is studied in detail.	0
i like the idea / motivation of the paper, but the authors could do a better job of explaining the motivation to people less familiar with techniques based on the determinization framework.	0
review:###1. the idea of explicitly forcing the encoding space to be flat by putting constraint on metric tensor is simple but neat.	0
figure 3 show example geodesics, but only geodesics going between clusters (i have no idea how such geodesics should look).	0
my issues are: the paper combines some existing ideas in a new way but falls short of justifying the choices it made.	0
3) the overall approach maybe novel in context of pose estimation, but this idea of learning a disentangled generative model is not, and there are several papers which do so with varying amount of supervision e.g. see [1] below for similar ideas, and pointers.	0
major comments: 1. i liked the idea of using depth information to inform visual relationships but i am not sure if the proposed approach is the way to go.	0
ideally, this needs to be quantified to convince the reader that the generated depth maps are 'good' but at the very least the authors need to show qualitative examples (both good, typical and bad) to prove that the pretrained network generates meaningful depth maps.	0
the proposed idea is interesting, but its presentation and evaluation could be significantly improved.	0
pros: their idea to utilize a decision tree for domain adaptation sounds novel.	2
while the idea is interesting, the paper lacks precision in key areas and the method is not placed in context among related work.	0
the idea is to combine online vector quantization with bayesian neural networks (bnn), so that only incremental computation is needed for a new data point.	0
the idea is that teacher not only provides student with the correct answer or correct intermediate representation, but also instructs student network on how to get the right answer.	0
review:###tldr: interesting idea that seems promising, but lacks the maturity required to pass the iclr bar: lacks proper citations, comparisons with the latest works, no ablation study of their contributions.	0
strengths:  the paper studies an interesting problem of adaptive mri reconstruction  the review of mri reconstruction techniques is well scoped weaknesses:  the evaluation is rather limited and performed on one, proprietary, relatively small sized dataset  some simple baselines might be missing i like the idea of adaptive sampling in mri.	2
my main concerns with the paper are: 1. the key idea of using uncertainty to guide sampling was also the main concept in zhang et al. 2019. this submitted paper highlights differences in the models but does not provide an experimental comparison.	0
i find the idea potentially powerful, with the advantage of learning a generative model as well, but wonder how this compares in theory and in practice to simpler stochastic variational inference and modern hamiltonian mcmc.	0
as discussed in the related work, this work can be seen as complementary to many related works such as igl 18, but the novelty of the idea is rather limited.	0
i felt that reggreedy is a really nice idea but the paper did not do it justice.	0
i like the general idea of the paper (as stated above) along with its objectives but i have several concerns.	0
though the performance on the unseen data does not change much, it raises some concerns about the generalizability of the learning approach they have used: in an ideal world with infinite training data, the network would perfectly accurately reproduce the ground truth results, and there should be no difference between the two.	0
the main idea is to learn a transformation of the contexualized representations that will make two word representations to be more similar if they appear in the same syntactic context, but less similar if they appear in different syntactic contexts.	0
i don't quite get the terminological differentiation between 'mapping' and 'extracting' in the introduction, but the idea is clear.	0
the entire paper seems written with the idea of using rugosity as a surrogate of da, but at the end it does not work	0
major comments:  authors need to include more related work and describe the main related paper they mention (peterson et al 2018) as well as describe how their work fits in with previous work  while the idea here is novel and impactful, the experiments used to explain the importance of superordinate labels do have not much compelling information and are not well described  4.2 plots for visualization are mentioned to be in the appendix, but are not there minor comments:  fig2 large subordinate group text would help  lots of typos throughout and grammar mistakes o typo ‘use vgg16’ and then ‘vgg16’ in same paragraph bottom of page 4 o typo top of page 2 “convolutional neural network(cnn)” o appendix list – ‘banna’ typo under fruit o page 1 intro ‘for both behavioral and computer vision’ doesn’t really make sense o page 3 top section ‘new one’ should be ‘new ones’ o bottom of page 3 ‘room from improvement’ o last line of conclusion – ‘classificacation’ consensus: this is a very interesting and potentially impactful idea, but the experiments used to defend and explain the importance of superordinate labels are relatively weak.	0
in summary, i ack that the idea is effective but seems straightforward.	0
i think the contribution of this paper is incremental and the idea is of less novelty.	0
the authors write in the 'related work' section that gmm with regularization was proposed by [verbeek et al. 2005], but it is an older idea  for example [ormoneit&tresp 1998] in section 3.1, the motivation for the maximization in eq. (2) is unclear.	0
however, this idea is too incremental and applying the classification function to graph filter is very trivial.	0
for the three pieces of contribution, section 4.1 (selflabeling, which is highlighted within the title) seems to be a trivial borrowing of an existing idea in crowd sourcing from 1979. it is not clear whether section 4.2 (error estimation) is an original contribution or not, but even if it is original lemma 1 seems marginally trivial.	0
pros:  the idea of training on data from varying quartiles, with the goal of preventing overlyconservative models, is quite intriguing and inspiring.	2
pros: 1. the idea is indeed interesting and afaik, there is no prior works trying to derive embedding for each neuron.	2
there are interesting ideas in this work but in its present form i cannot yet recommend acceptance.	0
i find the proposed idea to be promising and quite intriguing, but i think that the paper has some room for improvement and provided empirical evidence might be insufficient (including for understanding the importance of individual model components), which in turn makes the claims of potential practical attractiveness less justified.	0
overall, the idea is somehow interesting, but the experiments are weak.	0
it's not tremendously original (in that it basically combines two ideas that are already in the literature), but in spite of that, i still think this paper warrants being accepted to iclr.	0
(minor) first/2nd paragraph of 3.1 seems a bit redundant making it hard to follow overall i think the idea to consider metric learning and local adaptation for continual learning is interesting, however the current work is currently lacking in both experimental evidence (appropriate comparisons) and clear motivation/difference to existing work for its particular instantiation of this idea.	0
pros: their idea of making an adaption to gan architecture by replacing the generator by a classifier to select p from u and using the discriminator to distinguish whether the selected data is from p or u for pu learning is interesting, and benefits from not relying on the class prior estimation.	2
minor concerns that do not have an impact on the score  although the problem setting is quite different, the idea of this paper is partially similar to the importance weighting technique adopted in some recent domain adaptation methods [r1, r2].	0
this is a nice idea but would benefit from some more polishing and more extensive testing.	0
while the idea of forming such equivalence sets is very interesting, my concern with the paper is both in terms of impact and experimental methodology.	0
since this manuscript would be for sure and have to be largely rewritten in the future, i would not give too many detailed suggestions but some highlevel suggestions if the authors would like to refine this manuscript further and submit it somewhere else or iclr next year: (1) further improve the idea of combining different sources of information.	0
the idea seems quite novel, but the presentation seems to be more complicated than it needs to be for the idea.	0
summary: a good idea, but not well described, with strong assumptions not discussed, and with lowquality experimental results.	0
some of these results are also mostly anectodal 2)the regularization idea seems interesting, but i am concerned it is showing that the final learned networks have a deepsetlike architecture: more specifically, theorem 3.6 shows rnn can implement permutation invariant functions by making identifying the parameters with the ones of deepperm.	0
though the idea is good, i found the contribution to be too incremental for the paper to be accepted: ' the comparison should include the state of the art adversarial training defenses, pgd adversarial training (madry et al; you might want to cite the iclr 18 paper) and trades (zhang et al., interpreting adversarially trained convolutional nn, icml 2019); ' i would consider the szegedy baseline as obsolete; ' fig. 4 is unclear: the percentage of adversarial examples detected by the detector, but quid of the false alarms; there are quite some typos (nosie; iterarion; tabel; unsafety) and missing words.	0
6. i quite like the idea of predictive coding (albeit the original scheme presented by rao and ballard 1999) as an unsupervised representation learning scheme, but am unsure this is critical for your method and the current approach is not really predictive coding in a sense (or at least the ideas i'm familiar with from cognitive computational neuroscience).	0
i am concerned with the predictive coding idea being highlighted here as a key ingredient, but none of the papers containing the originating idea of predictive coding are mentioned.	0
my main concerns are the following:  representing words by distributions is not a novel idea; it was previously done by brazinskas et al. (2017): embedding words as distributions with a bayesian skipgram model (bsg).	0
while the idea is well motivated, this paper should be rejected, primarily due to a lack of experimental results.	0
the additional elements introduced by the authors  the use of sibling codes and the keyword reconstruction loss  are good ideas, and it is worthwhile having a documented test of the benefit they provide, but they don’t seem to have a major influence on the quality of the model.	0
the idea of decomposing a probabilistic model hierarchically is potentially interesting, but this paper has drawbacks in terms of experimental quality, significance, and presentation.	0
i think the idea of finding such vulnerabilities in pretrained models is interesting, but this paper does not offer much insight on the topic.	0
strengths first of all, i like the idea of applying learning for recomputation decisions, the authors propose a reasonable solution to the problem.	2
the use of transfer as illustrated in fig. 2 is a reasonable idea, but in practice how do we know if this predictor is good enough?	0
the core idea is simple and in itself actually elegant, but i find myself unable to suggest a simple set of edits which would bring this paper into publishable state.	0
overall assessment: the paper presents an interesting idea, but it has shortcomings in technical writing and evaluation.	0
cons: 1. the proposed methods (connected components and agglomerative clustering) seem very basic ideas in this context.	0
i am not an expert in nlp so cannot judge the significance of the ideas or the results, however from what i read, the contributions look basic/very incremental and heuristic.	0
the idea of the paper is interesting but i feel the relative novelty of the approach is not sufficient to make it eligible as a full conference paper.	0
it seems that they have a very similar approach, but extending the idea behind deep value networks rather than structural svm.	0
the idea is simple and appealing but not novel.	0
while i like the general idea, i feel that some experiments are currently lacking to fully support the authors' claims.	0
i think this paper's perspective of applying efficient exploration techniques in a multiagent rl setting is interesting, and it has some advantages and promising results of improvements, but a better execution of the idea would be required.	0
the main idea is that peragent sil objective is used, but only when the social welfare is above some threshold.	0
the idea is simple, but the experimental results show that for the tasks (e.g. v2) that require collaborations from multiple agents, the proposed approach outperforms other baselines including nfsp on both firefighting and search & rescue domains.	0
nfspi uses the same ideas, but adds another sil training loop at the end of each episode that learns based only on past states and action that resulted in higher cumulative reward than expected.	0
== clarity == the paper is very difficult to read because the order in which ideas are introduced and the lack of proper definitions for key terms and notation.	0
architectural choices are interesting and wellmotivated and combine good ideas from many prior works, including prediction in latent space, and coarsetofine synthesis (mainly used before along spatial axes for image generation, but used here temporally for video generation).	0
although the paper is clearwritten and the idea is wellreceived, i found the idea proposed is relatively incremental, given the advances brought by the endtoend memory network.	0
e.g. in fig 4, the ideal explanation i'd want to see is 'not bad at all' regardless of the intervening noun (a journey), but this isn't accomplished if you can only measure interaction between consecutive spans.	0
i am not aware of this approach from previous works, but seems like an interesting idea for action selection and learing a policy conditioned on future visitation of states (eq : 7)  from the proposed set of experimental results, it is not clear whether the proposed method is indeed useful.	0
similar approaches have been proposed before, the main idea that sets it apart is random sampling in blackbox modules in order to get an estimate for the gradient and to combine this with analytic gradients wherever they are available.	0
the idea is conceptually simple and neat, but practically it seems fairly complicated to chain all these derivatives together to obtain an accurateenough estimate of the gradient for gradientdescent.	0
it is generally obvious which is which, but still a good idea to change.	0
this is an interesting an novel idea, but unfortunately the paper is let down by being extremely unreadable: due to a combination of many grammatical errors, and what appears to be plain sloppy writing, it's virtually impossible to follow the main ideas.	0
the overall idea of the paper (using not only surface forms of word but also syntactic information for style transfer without parallel data) is very interesting.	0
there was a good bit of jargon about various architectures and why the exact model of the authors is a good idea but it was not made clear conceptually what the advantage of their approach was.	0
i think there are good ideas in this paper but they are not distilled well.	0
the basic idea of leveraging hierarchical structure in language is of course sensible, but it's not clear to me how wellmotivated or original this particular instantiation of hierarchical representation is, or what we have learned based on the comparisons reported.	0
however, i have some following major concerns about the paper: (1) the idea is not novel, because the use of hierarchical structure to model long text is straightforward and has been widely studied in natural language processing tasks, such as dialogue [1,2] and document summarization [3,4].	0
overall, i believe this idea is just a small incremental work based on deep cfr and it's hard to say this work is novel enough to be accepted by iclr.	0
======================================================== decision: although the idea is interesting, it is highly experimental but i found that it is not straightforward to use it for general metrics although the authors said that it is trivial to generalize this for many metrics.	0
the idea is that if the new data is consistent with the previous model (therefore, incremental learning achieves a bounded error), one can resort to incremental learning; otherwise, a full retraining is required.	0
the idea is intuitive, but it is not supported well by the authors, as no theoretical proof was presented, the results don't show a clear advantage of this method, and it is contradictory with previous results (e.g., that of golmant et al., 2017).	0
it is an interesting idea but the paper lacks motivation and clarity on the technical content.	0
the presentation of the proposed methodology seems to be at some rather high level and, as a consequence, i got the idea but the technical side is not totally clear to me.	0
strengths: 1. based on an intuitive idea, this paper shows its method outperforms the previous works on several tasks.	2
is the first step of extracting sentences a good idea in its own right or is it mostly a computational concern?	0
overall the paper proposed an interesting idea to deal with noisy labels in a meta label correction way, but some unclear parts need to be clarified and improved.	0
the idea is straightforward, but would still make for a good paper is it was evaluated thoroughly and was found to work very well.	0
the results are strong/competitive but the overall idea is highly incremental.	0
the idea of exploiting generated data has been used by previous works [1], but the paper proposes a novel generation method, which is new and useful for universal domain adaptation.	0
the mathematical derivation of the idea is interesting, but at the same time suggests that the authors are doing something completely novel.	0
generally, i believe the idea has conceptual merit, but the empirical evaluation is not sufficient to finally judge its practical value.	0
although i think the idea and the area of application are extremely interesting and relevant, there are some shortcomings that should be addressed before the paper is accepted.	0
as i mentioned in the summary, i like the overall idea, but i am not fully convinced that other models (such as 3d convolutions) would fail in these tasks.	0
pros:  simple and interesting idea that increases the representational power of convolutional networks.	2
pros: 1. the idea of quantization within neural networks, while not a new idea, however looks interesting from the perspective of achieving adversarial robustness.	2
cons: 1. the paper lacks any key insights into why quantization might be a better idea than standard schemes that seek robustballs within which the perturbations are ineffective (see for example parseval's networks, or convex outer adversarial polytopes, cisse et al, or wong and kolter resp.).	0
overall, this paper has some novelty in its core idea in the context of adversarial defenses, however, the exposition lacks conviction, and the experiments are lackluster.	0
the initial ideas and approximations are well explained, but applying it all to neural nets is not that clear i did find it hard understanding how it all works together, and an algorithm for computing all the metrics would in a table would be much easier to follow.	0
cons: 1. the idea is very incremental.	0
i don't expect a quantitatively precise answer here, but just something that gives a rough idea of the orders of magnitude.	0
• 1. there lacks for theoretical analysis to support the idea of using vae for solving irl problem.	0
this is a very interesting idea, but it is not ready for publication.	0
it is not clear if there is a particular contribution here, but at least combining the ideas in an appropriate way is already an interesting contribution.	0
while the overall idea is interesting, the authors seem to have completely missed reference [1] from cvpr 2017, which provides a very similar set of ideas but in a more formal and potentially more powerful setting of multigrid networks, incorporating both multiscale and attention mechanisms.	0
i only have a few comments: ' please reference the logitpairing paper [1] which uses a similar idea (encouraging logits from the same class to be similar) but uses a slightly different formulation and only evaluates it in the realm of adversarial robustness.	0
this is a well written paper but, in its current form, it isn't above the acceptance bar: (i) the idea of using consistency terms as regularizer is not novel and related works on consistency losses have not been adequately cited; (ii) the experiments are in general well executed but are not quite comparable with the stateoftheart.	0
the idea brought by this paper has some merits, but given the discussions and evaluations in the current manuscript, it has not been supported and validated that the proposed model does address those a variety of problems/properties in time series comprehensively and systematically.	0
clarity: the paper is easy to follow since the main idea is quite simple but lack of necessary demonstrative explanations to show the intuition of using holt’s linear method, i.e., why holt’s linear method is better than exponential averaging?	0
the paper also mentions the idea of concept kernel but it does not provide a detailed explanation on the concepts and/or any visualization.	0
2. the idea of collective factorization (eq.(3)) is not novel and lacks detailed justification.	0
overall i think the authors propose an interesting idea but not enough convincing evidence that their method is actually improving unsupervised anomaly detection in its general setting.	0
this paper should be rejected in my opinion due to the following four main reasons: (i) the technical quality of the paper is poor and the main idea not well explained; (ii) the experimental evaluation considers rather simple datasets (mnist, fashionmnist) and only includes two baselines (vanilla ae, ocsvm), but not any major competitors ; (iii) the work is not well placed in the literature and major related work is missing; (iv) the overall presentation is poor; (i) i find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper.	0
the general idea is understandable, but the details are very unclear and i found it extremely difficult to follow.	0
this paper should be rejected because (1) the contribution is incremental, similar ideas has been explored in previous papers; (2) the experiments are too toy to demonstrate the practical use of the proposed method.	0
munit is a great example where the idea of adain is not new, but its learned version applied to image translation yields great results and lots of future directions to explore.	0
i think the general idea of this paper and the questions it poses are very interesting and in fact a rather underexplored area however the experimental results are quite underwhelming.	0
the basic idea may have some commercial value, but the basic premise isn’t welldeveloped enough (or sufficiently contextualized) as a research contribution.	0
— the idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [didericksen, et al., collaborationbased user simulation for goaloriented dialog systems, convai neurips ws, 2017] — even if in a different setting and not clustered) in any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to followup without public datasets, etc. finally, i don’t think the scope of potential impact (if wellexecuted) makes iclr the ideal venue.	0
the idea to use dwt is interesting but not new and the results are only tested on one small dataset.	0
join image and perpixel annotation can be a good idea however the paper needs significant improvement on literature review, model presentation and experiment result to be able to publish in iclr.	0
however, i still have the following concerns about this submission:  my first concern is the novelty of the proposed approach: the idea of “iterative refinement based on mask prediction particularly in machine translation” is not new.	0
however, the idea seems to be pretty incremental as it stacks multiple existing techniques together without many innovations.	0
pros:  novel/original idea  endtoend learnable, allowing to utilize sgd, which better fits for the largescale dataset and multimodal dataset  well written and easy to follow  competitive results over gbdt, which is commonly used in relevant tasks.	2
the idea is simple enough, but slightly hacky, in the sense that they use cycleconsistency in style domain (as is done with images in a lot of literature), but improve upon recyclegan by adding a recurrent neural network to consume multiple frames instead of just one.	0
strength:  novel and simple idea  good results on mnist areas of improvement:  approach lacking motivation  limited to mnist i found the idea presented on the paper interesting and haven't seen this before.	0
however, due to the fact that this work ambitiously introduces a wide range of architectural changes, it suffers from a general lack of thoroughness in empirical support for all of the proposed ideas.	0
if the authors are able to address some of my questions and concerns, i am willing to increase this score, as i do think that some of their proposed ideas are valuable contributions.	0
the paper is presented in a fairly clear way and i have no general negative comments about the idea itself, but i am also not able to assess the claims for stateofart performance.	0
the model combines several already existing ideas, but overall propose an original system for the ms domain adaptation task.	0
concerns:  while the overall idea is interesting and relevant, the novelty is incremental in its originality to solve the identified concern, considering the paper mainly strings together existing ideas in terms of the methodologies (as summarized above).	0
the paper builds on an interesting idea, but may benefit from a thorough revision with these concerns in mind.	0
minor points:  i'm not sure the title is a great idea... it might put off some people, but also i don't think it's really warranted from the results... it seems like neuralpsrl is a potentially interesting avenue, but hardly making anything great (again?)!	0
strengths:  conditioning dilation on learnable weights is a sound idea, which intuitively makes sense as to why it could offer improvements  there is a strong variety of experiments performed  pad kernels are tried on several different architectures, which is good  reporting the negative results at the end of section 4.2 is a strong truthful admission that is rarely done, and we commend the authors for doing this.	2
the idea of treating the residual network as a difference equation is not new, but to the best of my knowledge adding the secondorder term seems to be novel.	0
overall, i like the idea presented in this paper, but it also needs some improvement.	0
however, in p.5, the authors claim that they observed the problems with no multiresolution networks and claim that they came up with this idea inspired by the mechanism in biological vision......... also, the paper claims that they compared their method with sota methods on table 2 caption, but they do not actually cite any sota methods.	0
much of the idea is at the technical level of kernel smoothing (e.g., eq 14), which is useful but very simple and not clearly an iclrtype deep learning technical advance.	0
the proposed method is interesting, but the authors need new ideas to meet the high standard of iclr.	0
the idea behind ensemblenet is similar to the multihead architecture, but instead of just simply multiply upper blocks to obtain several heads, the authors also proposed to shrink these heads.	0
4320–4328, 2018 overall, the paper is interesting but the ideas on which it is based are quite simple and not quite new.	0
the ideas in this paper are definitely interesting, but i'm more inclined to reject this paper for the following reasons: the theoretical contributions are hard to follow and, while the empirical results are encouraging, the technical contribution is small when compared to the existing literature (tzenand raginsky, 2019), (jia and benson, 2019), (rackauckas et.	0
the paper does mention the differences with the other works, but the title and the text make it seem as if the main contribution was proposing the idea of neural sdes.	0
overall, the idea may be good, but in current form this manuscript is not ready for publication.	0
realworld datasets are great, but we have little understanding of what effect exactly is helping the classification here and an additional toy setting would increase the clarity of the modeling ideas.	0
the idea is simple but appealing.	0
i understand that this is an extension of [tung et al. (2018)][1] to include a forward model for planning, but i think you're vastly overselling the idea.	0
the idea of run is well motivated but not surprising.	0
but i have two main concerns: 1. the general idea of the paper is however not new.	0
as it is, the paper currently is easy to read, seems like a good idea and has good results but i feel it is important to add more detail in both methodology and in experimental results especially have more ablations and understand what is truly happening to the model.	0
review:### overall  i thought the authors presented a simple, but interesting idea for encoding temporal information as input to a discrete time model; however, i think the paper could use more intuition for why this approach should work and the experimentation does not appear to support use of the method.	0
the simple form of g_  heta seems to be a key element in the proof of the main theorem but the authors do not say if and how their idea generalises to more complicated cases.	0
i think this is a good paper which shows strong empirical results based on simple but effective idea.	0
overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation.	0
another baseline the paper should consider is soft q learning, where it maintain multiple q function and jointly update q different function to maximize the entropy while moving towards an improvement q. overall, i believe the idea of the paper is novel and interesting, but further improvements should be added in order to improve the score the paper.	0
my main concerns are: (a) the main technical idea in this paper is the use of backcasting and forecasting (i.e. doubly residual connections).	0
the proposed idea makes sense at a high level, and the empirical results look somewhat compelling, but the writeup is not particularly clear (see clarityrelated comments).	0
i appreciate the novelty of the idea of decomposing parameters but it is not clear whether this factorization is actually performed given the high capacity needed to learn these tasks.	0
on the one hand, the paper presents a promising idea for significantly speeding up the cfr algorithm with detailed theoretical justifications, but on the other hand, the (potentially huge) requirement for memory makes me unsure about the strength and practical merit of the algorithm compared to other cfr variants.	0
= strong/weak points  the idea of generalising ntms to 'universal' tms is interesting in itself ...  ... however, the presented solution seems to be only halfway there, as the memory used for the 'program' is still separate from the memory the nutm operates on.	0
firstly, the authors point out that this issue also exists in some tasks like object detection, but i have no idea how to apply this method in object detection to fix this issue.	0
review: this paper was interesting, because it has nice experimental results and seems like a good idea, but i feel like the paper needs to be improved.	0
the idea of the proposed method is to essentially keep the 'atom' distribution unchanged between the training and test data, but maximize the divergence between the 'compound' distributions between them.	0
(vii) since the idea here is to connect states that are closer in terms of time, but further apart, how does the proposed approach comparison to successor options.	0
2. also, there are other papers providing different but insightful ideas to label noise problem.	0
i think the general idea of the paper is interesting, but overall, the paper is very poorly written.	0
originality adding a parametric policy to pets is not the most original idea, but clearly a gap in the current literature.	0
the general idea is reasonable and the proposed model is technically correct, but i have the following concerns mainly about the originality and the experiments.	0
the idea is simple but effective, the experiments are thorough and improvements over the bert baselines are significant.	0
overall, while the idea of patching neural network predictions is really interesting, i have several concerns regarding the paper in its current form.	0
as mentioned before, ideally the model should not only 'patch' its mistakes on problematic samples, but also on samples that are substantially similar to these problematic samples.	0
paper strengths 1. the idea of this paper is novel, and the implementation of this method is easily interacted with any gan model.	2
although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing gan techniques.	0
cons:  while the idea to partition into subspaces and learn a different detection for each of them is novel, it involves training multiple detectors one by one that can be computationally expensive.	0
this is not a big surprise, but i like the simple but clever idea of reset that the paper exploits.	0
the main idea is reasonable, but it requires that the models to compare all perform reasonably well.	0
while it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationallydemanding models, is limiting.	0
this paper is well motivated: clearly sparse rewards and mode collapse are two problems need to be solved in gan based text generation, however, the following concerns prevent me from finding this paper acceptable in iclr: the “selfplay” idea is widely used in rl.	0
i am unfamiliar with prior work in this direction, but the idea of dks seems to be conceptually appealing and as the authors pointout, their approach can be seen as an alternative to spatial sampling for modeling deformations.	0
also, while the authors sell the idea of subsampling kernels, but the finding that kernel sizes beyond 4x4 don't seem to offer any benefit make the idea practically questionable.	0
the idea of dks seems relevant, but both conceptually and empirically it seems very close to deformable convolutions.	0
pros: i really like the key idea of this paper  counterfactually manipulating neurons using its values from that of some other image and observing differences and clustering them to find similar neurons which could be manipulated as a bunch.	2
the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. note on related work: it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that betavae is far from optimal for “extrinsic” disentanglement, the text in section 4.1 should take these results into account.	0
i don't know if there are obvious reasons why this wouldn't work, but it looks like a very sensible idea that could solve asymmetry.	0
the idea is similar as bert like nlp tasks, but for videos, the computational cost and memory cost could be very large.	0
of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated cv error but as far as i am aware there is a general agreement that getting “closer” to leaveoneout setup is not a good idea, furthermore, in my anecdotal experience 10fold cv for a dataset with only 600 samples can really be problematic.	0
the idea seems interesting, the writing is wellwritten, and the analysis seems correct (i did not fully check all steps, but the key steps seems ok to me).	0
overall assessment: pros: i like the ideas in the paper and they are presented well.	2
overall, it is a sound idea for lowresource setting with generally positive experimental results, but limited in novelty in terms of the proposed architecture (similar to [1', 2']) and disentangling language and knowledge idea (similar to [3']), lacks comparison with baselines for lowresource setting, and lacks discussion/reference of a few closely related works.	0
strengths, 1, matching density ratios is a novel and interesting idea.	2
the paper presents an interesting idea, but there are some issues that need to be addressed before published on iclr.	0
the key idea is to stabilize not only with respect to the input perturbations (which most existing works do and end up obtaining an exponential dependency), but simultaneous perturbations at every layer.	0
overall, the paper proposes some simple but interesting ideas, e.g. distribution environments.	0
originality: the core idea is related to (balle et al. 2018), but the paper puts its own twist to it with the projections and applies it to model compression.	0
the idea is very simple (indeed i find it slightly hard to believe no one has tried this before, but i don't know of any references myself) and the results, particularly figure 4, are compelling.	0
the idea is seducing but i see some difficulties due to this approximation.	0
the paper is strong in its idea, formulation, and theory, but is too similar to recent related works which this paper is reluctant to compare to (either in theory, efficiency, or performance).	0
overall i like the idea and the theoretical analysis in this paper, but the experimental results could be further improved.	0
the idea of using the gradient is good, but the explanation of some aspects like the normalization is confusing and felt random.	0
the idea to of different subsystems for route planning and path planning are not new, but the way it's done here does seem interesting to me.	0
overall assessment: i think this paper presents interesting and novel ideas, but it has shortcomings in the presentation.	0
i don't follow this area closely, but from what i can tell it's a novel idea.	0
i would not at all say that that invalidates the contribution of the paper, but readers should have a clear idea of how your model ranks against the top performing ones.	0
in summary, i do like the main idea and the paper has merits, but it requires more evaluation and analysis to be accepted.	0
i note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including iou, f1 score and cd and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. decision: weak reject because the idea is quite interesting, but i believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.	0
the idea is an incremental extension of maml and darts.	0
pros: 1, i like the idea of constructing the knowledge graph as the agent roll out.	2
pros:  the idea in this paper is quite original.	2
i like the idea of employing known data sets with a simple manifold structure, but the setup is somewhat preliminary; i would prefer to see an analysis of border cases or limit cases in which the theorem _almost_ applies (or not); plus, a more indepth analysis of stochastic effects during training: do _all_ models end up being robust if their number of connected components is sufficiently large?	0
the notation for the euclidean balls should be briefly introduced before definition 1. i recognise this as a standard notation, but since this is the first appearance of the symbol, it should be mentioned at least briefly.	0
therefore, although the idea is quite incremental, the overall work is well written and experiments with ablation study on different model’s components bring the value into it.	0
on the other hand, i also have the following concerns:  although applying the kernel type information measure is an interesting idea, its computational complexity would be large.	0
the writeup can be improved in a few places (see detailed comment below); but overall, i find the idea refreshing for optimization and the proof is simple and elegant.	0
decision: overall, the idea seems simple but is quite effective.	0
minor comments: a related idea is “learning neural pde solvers with guarantees” which modulates the finite difference iterative solver with deep networks, but the objective is solving pdes with known dynamics instead of prediction with unknown dynamics.	0
implementation of the idea seems nontrivial, but the authors provide open source code.	0
the main idea of the paper is to propose an estimator for offpolicy evaluation, that computes the ratio of the stationary state distributions (similar to recent previous work liu et al.,) but when the behaviour policy may be unknown.	0
the framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work (see [1] and [2]).	0
the key argument seems to hinge on the idea that the low likelihood tail is of “low confidence.” but this claim is problematic.	0
names such as 'views' are used without explanation, and it's not explained how a device is an input to a program (yes, i get what this means, but it makes in unnecessarily hard to follow the paper) = recommendation i would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider iclr audience.	0
pros   good results  well written  neat idea cons   training details are obfuscated.	2
figure 6, the tire marker is a good idea but image still unclear  i recommend slightly less rotation or an intermediate step between generated image and e.g. front view  for quantitative results/fid: try using hausdorff or chamfer distance on the rendered scenes' pixels.	0
it however doesn't make the the case for why it is a good idea.	0
continuous reinforcement learning, bellman contractions, feature engineering) while providing grounds for their idea, but in the end return to the familiar domain of discrete qlearning with semihandcrafted (though theoretically motivated) features where the latency of actions can take a set of fixed values and the state is sampled at fixed intervals.	0
positives: idea is simple and makes sense intuitively, but not something one would think immediately would work better with a such a small fraction of the compute.	0
overall, the reviewer feels that this paper starts with an interesting idea, but the developments on the theoretical side is a bit thin.	0
overall, the contribution of this paper is novel, and results are promising, but it still has some missing components, especially the idea of combining multiple ibp bounds into one, which can be very effective for adversarial patches, as i will elaborate below.	0
the rationale behind the idea is properly introduced and justified ' the formulation is clear and sound ' the results show improvements over competitors in terms of accuracy and computational cost ' the contributions seem very incremental.	0
overall, a trivial baseline for the proposed method is to solve directly optimal transport with those extra constrains (from side information) directly (maybe with the euclidean cost metric), but not the optimal transport without extra constraints as in the current setting.	0
the idea is not to show new theoretical bounds for generalization gaps but stress the results of an empirical study comparing the already existing measures.	0
pros:  the idea of using a nonparametric model for cl is interesting and can lead to followup work.	2
this in itself is a good idea, but overall is an incremental improvement.	0
i found the paper easy to read with a high quality introduction and background, the results are very convincing and the idea is simple but intriguing.	0
this is one of those things where the idea is straightforward but the devil is in the details.	0
concerningly, the paper is closely related to a few other papers using the spline cnn idea or at least the idea of taking a fixed set of functions and moving it around on the homogeneous space by acting on it with select group elements, most notably 'rototranslational convolutional neural networks for medical image analysis' by bekkers et al.. the main difference of the present paper relative to that one is that the idea is fleshed out in a little more detail and is generalized from se(2) to arbitrary lie groups.	0
the paper is clearly written but the intuitive nature of the core ideas could be better conveyed e.g. by fancy diagrams.	0
one of the main contributions, that of allowing locations to regress outside their 8x8 area, sounds like a good idea but i feel that figure 3 does not adequately show the benefit.	0
the idea of training qnetwork using the result of mcts planning is not new (e.g. ucttoregression in guo et al 2014), but this paper takes further steps: the learned qnetwork is again used for mcts planning as q initialization, the crossentropy loss is used instead of l2loss for the amortized value training, and the total loss combines qlearning loss and the amortization loss.	0
detailed comments: the proposed idea (i.e. designing an adversarial attack based on modelbased rl) is interesting but it would be better if the authors can provide evaluations such as adversarial training and ablation studies for the proposed method (see the suggestion & question).	0
the derivation of the proposed method is not complex but i like the idea that models quantization error as additive perturbation in this context and how it eventually connects with gradient penalty that's widely used in gan training and adversarial robustness.	0
= main concerns = 1. it seems not clear to me why it is a good idea to introduce a second compartment with a threshold in each neuron as described in eqn.	0
pros: 1. the margin maximization idea has been wellexplained, both intuitively and theoretically.	2
cons: 1. the idea of 'shortest successful perturbation' appears like a type of weak training attack, looking for minimum perturbations to just cross the classification boundary, like deepfool [1] or confidence 0 cwl2 attack [2].	0
postrebuttal: my only major concern was the lack of sufficient empirical evidence to support the idea.	0
strengths, weakness, recommendation i like what the authors are trying to do here; embeddings and discriminative models on noneuclidean spaces have been developed, offer credible benefits, and generative models are the next step.	2
in summary, i really liked the algorithmic idea, but skeptical about its practical relevance from the results.	0
i give a weak accept of this paper due to the following reasons: pros:  the idea of converting a set of data points to one point and rehearse at a meta level is a smart and novel idea.	2
pros: novel idea: the insight of this paper is that 'larger network building blocks can be represented by an ensemble of atomic blocks'.	2
though the proposed idea is interesting, the depth and breadth of authors' presentation are simply lacking.	0
the entire paper lacks focus and i suggest authors consider focusing on 12 well thoughtout ideas.	0
pros:  the idea is clearly presented.	2
cons:  while the method based on sliced wasserstein distances sounds new, the novelty seems limited since the idea of whitening the activation distribution to unit gaussian was introduced before as mentioned by the authors.	0
cons (and primary concerns): 1) the idea of matching state distributions in the context of learning behaviors is not new.	2
update: my concerns have been addressed and i have updated the score to 8 '''' this paper introduces 3 neat ideas for training deep reinforcement learning (drl) agents with state variables so that they can handle partially observed environments: 1) model the latent state variable as a belief distribution, using a collection of weighted hidden states, like in the particle filter (pf), with an explicit belief update of each particle, calculation of the weight using an observation function, and a differentiable reweighting function to get the new belief distribution, 2) base the policy on the whole set of particles, by quantifying that set using its mean as well as a collection of k learnable moments (specifically, k moment generating functions, each one corresponding to a dot product between the moment variable and the hidden state of the particle), 3) instead of generating the observations, take again the idea from pf which is to measure the agreement between the current observation o_t and the ith particle state variable h_t^i, via a learnable discriminative function.	0
minor comments:  i think an idea which is somewhat related but hasn't been mentioned in the paper, is the idea of adding noise to the input when training gans [1].	0
cons:  the most critical downside of this paper is its insufficient experiments to support the whole idea, where we will detail in the next.	0
i kind of get the general idea, but it is not clear to me why not use something less expensive, e.g. just random projections, and why do we actually need a full network.	0
i suspect that the underlying idea is solid, but in its current form i cannot recommend publication of the paper.	0
i feel the paper hints at this but does not make this idea explicit.	0
strengths:  the idea is simple and motivated by the fisher vector work (jaakkola & hausler, 1999), which the authors cite.	2
sure, the exact architecture of the model is probably unique, but the idea of modulating a state with a goal is not and has been seen in other work such as [2] and [3] among others.	0
the basic idea is that an estimator over a discrete distribution can be raoblackwellized by conditioning on the event that the discrete realization was produced by being the first sample drawn from an unordered set of samples drawn with replacement.	0
[major comments] predicting the middle point between two states for modeling the dynamics via deep neural networks is not new, but i did not know any other works that use this idea for controlling pdes.	0
strengths: interesting problem and novel idea.	2
in principle there is just one novel idea, namely using kwta activations to improve adversarial robustness, but this claim is investigated thoroughly, in theory, and demonstrated convincingly in experiments.	0
this portion seemed a little bit underdeveloped in the paper, to be honest, but overall the idea of parameterizing a normalizing flow with a hamiltonian dynamical system seems like a good one (e.g., allowing for easier largetimestep inference).	0
detailed comments:  the proposed estimator is not using control variate but using dual structure between value function and stationary distribution ratio, which is a novel idea comparing with similar doubly robust estimators.	0
clarifications and improvements: just for clarity, in the last paragraph on page 4. it says two encoderdecoder models are learnt, but isn't the idea that there is effectively only one encoder and one decoder learned that just put together in different ways during training?	0
the main shortcomings i see are that there are no experimental results comparing this method against any existing results; the authors do compare against their own resnet18 implementation, but this is not ideal.	0
each of these ideas is individually close to ideas proposed elsewhere before in other forms, as the authors themselves acknowledge [vaswani et al 2017, parmar et al 2018, parikh et al 2016, menick et al 2019], but this paper does the important engineering work of selecting and combining these ideas in this specific video synthesis problem setting.	0
pros: 1. novel idea, clear explanation of the method and the tensor factorization scheme.	2
overall, i like the basic idea of the paper but i found the presentation lacking.	0
i do think their idea for a fairness constraint is very interesting, but it gets too bogged down in the details of the mathematical theory.	0
they mention dwork et al. at the beginning but don't really compare it to their idea in detail, even though i think there would be a lot of interesting things to say about this.	0
pros  the idea is fresh, well explained and experiments are sufficiently thorough.	0
cons:  novelty is somewhat low, as it is a straightforward application of existing ideas like gasse et al. neurips’19 to the problem of verification.	0
the key idea is to not keep lowering the step sizes, but  at prespecified times  go back to large step sizes.	0
reasons to accept:  original and wellmotivated idea  clearly written paper reasons to reject:  problematic empirical evaluation (e.g., lacking recent baselines)  several performance numbers appear to be below random gcn baseline performance  general applicability of the approach (e.g., to nonhomophilous networks) is not clear	0
active learning is not my main area of expertise so i can’t judge how novel the proposed idea is, but from an outsider’s perspective, this is a great paper.	0
my concerns are as follows: ' the novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning. '	0
strengths  the idea makes a lot of sense.	2
i am not sure about the novelty of the method, seeing that there is a very similar paper from earlier this year (mathieu et al. 2019).	0
however, i am not sure if the paper presents a case of adequate novelty in terms of ideas as many of them are rather obvious and the current stateoftheart models could also improve considerably using similar experimental setups, which authors also acknowledged in footnote 2. other comments:  section 3.1: please clarify how exactly setting beta2=0.98 would improve stability when training with large batch sizes.	1
the idea is clearly explained and seems sensible, the paper is well written, the execution is competent and the authors provide a sufficient amount of details so that reproducibility should be possible.	2
i would recommend the authors to either work on showing practical usefulness of the technique, showing something for the full bugfinding task (not merely the first, artificial part), or to investigate if (or how) the idea to add bugintroducing changes to a code dataset is conceptually flawed for bugfinding (as this idea is widely used by several other works like allamanis et al 2018b or by https://arxiv.org/abs/1904.01720 which also don't get to practical tools ).	1
the idea is straightforward.	2
strength: 1. the idea of utilizing gcn on the feature cooccurrence graph is interesting and innovative.	2
however, the idea of coattention at different grainlevels have been proposed before.	1
cikd seems like a soft implementation of this idea.	2
the proposed idea in this paper can be considered a simplified version of the transformers.	1
the core idea of the work, thus, seem to be in conflict with itself.	0
the idea to leverage the parameters of a metaoptimizer for adaptation instead of using model parameters is novel and interesting.	2
the idea is quite simple and is based on proximal mappings that lead to implicit update.	2
i think the idea is interesting and novel.	2
i do like the extension of applying the idea in physics problems.	2
the idea is pretty simple and the paper is easy to read.	2
using uncertainty to drive data acquisition and exploration is not a new idea; the concept has been applied to reinforcement learning, active learning, bayesian optimisation, as instantiations of a broad class of methods in experimental design.	1
i could not find any motivation for why this would be a good idea, potentially because the paper does not state any explicit goal/contribution.	0
i think that the paper studies an important problem and contains interesting ideas.	2
the paper is mostly clear, and the idea for joint optimization is very reasonable.	2
why is it a sensible idea?	1
i hope that the authors can spend more efforts clarifying their ideas and make their writing coherent so that readers can have a better experience reading it.	1
overall, i think the idea is interesting and clean and the work is certainly valuable.	2
ideas like initializing the forget gate bias in a certain way are backed up with explanations about letting gradients flow through time easier  why should constrained the output of an lstm (by introducing more trainable variables & computation) be a good thing?	1
i think that the main idea is quite straight forward, which i believe a simple extension of ordinary convolutional filter pruning.	2
i would accept the paper because:  it's well written, with clear motivations  the idea of using dynamic programming to compute the global model tractably is thoughtful and innovative.	2
the idea of sidetuning is that they firstly have a fixed base network and when a new task arrived, they learn the task with a smaller network which transferred the adaptive knowledge from large base network while freezing it.	1
unfortunately, it seems to me that this is not nearly enough work to conclusively show the importance/utility of this idea.	0
